<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kubelet 启动流程分析]]></title>
    <url>%2F2018%2F12%2F23%2Fkubelet_init%2F</url>
    <content type="text"><![CDATA[上篇文章（kubelet 架构浅析 ）已经介绍过 kubelet 在整个集群架构中的功能以及自身各模块的用途，本篇文章主要介绍 kubelet 的启动流程。 kubernetes 版本： v1.12 kubelet 启动流程kubelet 代码结构: 12345678910111213141516171819202122232425262728➜ kubernetes git:(release-1.12) ✗ tree cmd/kubeletcmd/kubelet├── BUILD├── OWNERS├── app│ ├── BUILD│ ├── OWNERS│ ├── auth.go│ ├── init_others.go│ ├── init_windows.go│ ├── options│ │ ├── BUILD│ │ ├── container_runtime.go│ │ ├── globalflags.go│ │ ├── globalflags_linux.go│ │ ├── globalflags_other.go│ │ ├── options.go│ │ ├── options_test.go│ │ ├── osflags_others.go│ │ └── osflags_windows.go│ ├── plugins.go│ ├── server.go│ ├── server_linux.go│ ├── server_test.go│ └── server_unsupported.go└── kubelet.go2 directories, 22 files 1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）123456789101112func main() &#123; rand.Seed(time.Now().UTC().UnixNano()) command := app.NewKubeletCommand(server.SetupSignalHandler()) logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil &#123; fmt.Fprintf(os.Stderr, &quot;%v\n&quot;, err) os.Exit(1) &#125;&#125; 2、初始化 kubelet 配置（cmd/kubelet/app/server.go）NewKubeletCommand() 函数主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。 1234567891011121314151617181920212223242526272829303132333435363738// NewKubeletCommand creates a *cobra.Command object with default parametersfunc NewKubeletCommand(stopCh &lt;-chan struct&#123;&#125;) *cobra.Command &#123; cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc) kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() ... cmd := &amp;cobra.Command&#123; ... Run: func(cmd *cobra.Command, args []string) &#123; // 读取 kubelet 配置文件 if configFile := kubeletFlags.KubeletConfigFile; len(configFile) &gt; 0 &#123; kubeletConfig, err = loadConfigFile(configFile) if err != nil &#123; glog.Fatal(err) &#125; ... &#125; // 校验 kubelet 参数 if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil &#123; glog.Fatal(err) &#125; ... // 此处初始化了 kubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer) if err != nil &#123; glog.Fatal(err) &#125; ... // 启动程序 if err := Run(kubeletServer, kubeletDeps, stopCh); err != nil &#123; glog.Fatal(err) &#125; &#125;, &#125; ... return cmd&#125; kubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为。主要包括监控功能（cadvisor），cgroup 管理功能（containerManager）等。 NewKubeletCommand() 会调用 Run() 函数，Run() 中主要调用 run() 函数进行一些准备事项。 3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）run() 函数的主要功能： 1、创建 kubeClient，evnetClient 用来和 apiserver 通信。创建 heartbeatClient 向 apiserver 上报心跳状态。 2、为 kubeDeps 设定一些默认值。 3、启动监听 Healthz 端口的 http server，默认端口是 10248。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123; ... // 判断 kubelet 的启动模式 if standaloneMode &#123; ... &#125; else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil &#123; ... // 创建对象 kubeClient kubeClient, err = clientset.NewForConfig(clientConfig) ... // 创建对象 evnetClient eventClient, err = v1core.NewForConfig(&amp;eventClientConfig) ... // heartbeatClient 上报状态 heartbeatClient, err = clientset.NewForConfig(&amp;heartbeatClientConfig) ... &#125; // 为 kubeDeps 设定一些默认值 if kubeDeps.Auth == nil &#123; auth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration) if err != nil &#123; return err &#125; kubeDeps.Auth = auth &#125; if kubeDeps.CAdvisorInterface == nil &#123; imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint) kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint)) if err != nil &#123; return err &#125; &#125; &#125; // if err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil &#123; return err &#125; ... // 启动监听 Healthz 端口的 http server if s.HealthzPort &gt; 0 &#123; healthz.DefaultHealthz() go wait.Until(func() &#123; err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil) if err != nil &#123; glog.Errorf(&quot;Starting health server failed: %v&quot;, err) &#125; &#125;, 5*time.Second, wait.NeverStop) &#125; ...&#125; kubelet 对 pod 资源的获取方式有三种：第一种是通过文件获得，文件一般放在 /etc/kubernetes/manifests 目录下面；第二种也是通过文件过得，只不过文件是通过 URL 获取的；第三种是通过 watch kube-apiserver 获取。其中前两种模式下，我们称 kubelet 运行在 standalone 模式下，运行在 standalone 模式下的 kubelet 一般用于调试某些功能。 run() 中调用 RunKubelet() 函数进行后续操作。 4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）RunKubelet() 主要功能： 1、初始化 kubelet 组件中的各个模块，创建出 kubelet 对象。 2、启动垃圾回收服务。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123; ... // 初始化 kubelet 内部模块 k, err := CreateAndInitKubelet(&amp;kubeServer.KubeletConfiguration, kubeDeps, &amp;kubeServer.ContainerRuntimeOptions, kubeServer.ContainerRuntime, kubeServer.RuntimeCgroups, kubeServer.HostnameOverride, kubeServer.NodeIP, kubeServer.ProviderID, kubeServer.CloudProvider, kubeServer.CertDirectory, kubeServer.RootDirectory, kubeServer.RegisterNode, kubeServer.RegisterWithTaints, kubeServer.AllowedUnsafeSysctls, kubeServer.RemoteRuntimeEndpoint, kubeServer.RemoteImageEndpoint, kubeServer.ExperimentalMounterPath, kubeServer.ExperimentalKernelMemcgNotification, kubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount, kubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold, kubeServer.MinimumGCAge, kubeServer.MaxPerPodContainerCount, kubeServer.MaxContainerCount, kubeServer.MasterServiceNamespace, kubeServer.RegisterSchedulable, kubeServer.NonMasqueradeCIDR, kubeServer.KeepTerminatedPodVolumes, kubeServer.NodeLabels, kubeServer.SeccompProfileRoot, kubeServer.BootstrapCheckpointPath, kubeServer.NodeStatusMaxImages) if err != nil &#123; return fmt.Errorf(&quot;failed to create kubelet: %v&quot;, err) &#125; ... if runOnce &#123; if _, err := k.RunOnce(podCfg.Updates()); err != nil &#123; return fmt.Errorf(&quot;runonce failed: %v&quot;, err) &#125; glog.Infof(&quot;Started kubelet as runonce&quot;) &#125; else &#123; // startKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer) glog.Infof(&quot;Started kubelet&quot;) &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445func CreateAndInitKubelet(...)&#123; // NewMainKubelet 实例化一个 kubelet 对象，并对 kubelet 内部各个模块进行初始化 k, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions, containerRuntime, runtimeCgroups, hostnameOverride, nodeIP, providerID, cloudProvider, certDirectory, rootDirectory, registerNode, registerWithTaints, allowedUnsafeSysctls, remoteRuntimeEndpoint, remoteImageEndpoint, experimentalMounterPath, experimentalKernelMemcgNotification, experimentalCheckNodeCapabilitiesBeforeMount, experimentalNodeAllocatableIgnoreEvictionThreshold, minimumGCAge, maxPerPodContainerCount, maxContainerCount, masterServiceNamespace, registerSchedulable, nonMasqueradeCIDR, keepTerminatedPodVolumes, nodeLabels, seccompProfileRoot, bootstrapCheckpointPath, nodeStatusMaxImages) if err != nil &#123; return nil, err &#125; // 通知 apiserver kubelet 启动了 k.BirthCry() // 启动垃圾回收服务 k.StartGarbageCollection() return k, nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...)&#123; ... if kubeDeps.PodConfig == nil &#123; var err error // 初始化 makePodSourceConfig，监听 pod 元数据的来源(FILE, URL, api-server)，将不同 source 的 pod configuration 合并到一个结构中 kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath) if err != nil &#123; return nil, err &#125; &#125; // kubelet 服务端口，默认 10250 daemonEndpoints := &amp;v1.NodeDaemonEndpoints&#123; KubeletEndpoint: v1.DaemonEndpoint&#123;Port: kubeCfg.Port&#125;, &#125; // 使用 reflector 把 ListWatch 得到的服务信息实时同步到 serviceStore 对象中 serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;) if kubeDeps.KubeClient != nil &#123; serviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;services&quot;, metav1.NamespaceAll, fields.Everything()) r := cache.NewReflector(serviceLW, &amp;v1.Service&#123;&#125;, serviceIndexer, 0) go r.Run(wait.NeverStop) &#125; serviceLister := corelisters.NewServiceLister(serviceIndexer) // 使用 reflector 把 ListWatch 得到的节点信息实时同步到 nodeStore 对象中 nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;&#125;) if kubeDeps.KubeClient != nil &#123; fieldSelector := fields.Set&#123;api.ObjectNameField: string(nodeName)&#125;.AsSelector() nodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;nodes&quot;, metav1.NamespaceAll, fieldSelector) r := cache.NewReflector(nodeLW, &amp;v1.Node&#123;&#125;, nodeIndexer, 0) go r.Run(wait.NeverStop) &#125; nodeInfo := &amp;predicates.CachedNodeInfo&#123;NodeLister: corelisters.NewNodeLister(nodeIndexer)&#125; ... // node 资源不足时的驱逐策略的设定 thresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim) if err != nil &#123; return nil, err &#125; evictionConfig := eviction.Config&#123; PressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration, MaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod), Thresholds: thresholds, KernelMemcgNotification: experimentalKernelMemcgNotification, PodCgroupRoot: kubeDeps.ContainerManager.GetPodCgroupRoot(), &#125; ... // 容器引用的管理 containerRefManager := kubecontainer.NewRefManager() // oom 监控 oomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder) // 根据配置信息和各种对象创建 Kubelet 实例 klet := &amp;Kubelet&#123; hostname: hostname, hostnameOverridden: len(hostnameOverride) &gt; 0, nodeName: nodeName, ... &#125; // 从 cAdvisor 获取当前机器的信息 machineInfo, err := klet.cadvisor.MachineInfo() // 对 pod 的管理（如: 增删改等） klet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager) // 容器运行时管理 runtime, err := kuberuntime.NewKubeGenericRuntimeManager(...) // pleg klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock&#123;&#125;) // 创建 containerGC 对象，进行周期性的容器清理工作 containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady) // 创建 imageManager 管理镜像 imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage) // statusManager 实时检测节点上 pod 的状态，并更新到 apiserver 对应的 pod klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet) // 探针管理 klet.probeManager = prober.NewManager(...) // token 管理 tokenManager := token.NewManager(kubeDeps.KubeClient) // 磁盘管理 klet.volumeManager = volumemanager.NewVolumeManager() // 将 syncPod() 注入到 podWorkers 中 klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache) // 容器驱逐策略管理 evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock) ...&#125; RunKubelet 最后会调用 startKubelet() 进行后续的操作。 5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）startKubelet() 的主要功能： 1、以 goroutine 方式启动 kubelet 中的各个模块。 2、启动 kubelet http server。 123456789101112131415func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) &#123; go wait.Until(func() &#123; // 以 goroutine 方式启动 kubelet 中的各个模块 k.Run(podCfg.Updates()) &#125;, 0, wait.NeverStop) // 启动 kubelet http server if enableServer &#123; go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling) &#125; if kubeCfg.ReadOnlyPort &gt; 0 &#123; go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// Run starts the kubelet reacting to config updatesfunc (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123; if kl.logServer == nil &#123; kl.logServer = http.StripPrefix(&quot;/logs/&quot;, http.FileServer(http.Dir(&quot;/var/log/&quot;))) &#125; if kl.kubeClient == nil &#123; glog.Warning(&quot;No api server defined - no node status update will be sent.&quot;) &#125; // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil &#123; go kl.cloudResourceSyncManager.Run(wait.NeverStop) &#125; if err := kl.initializeModules(); err != nil &#123; kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) glog.Fatal(err) &#125; // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil &#123; // Start syncing node status immediately, this may set up things the runtime needs to run. go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; go kl.nodeLeaseController.Run(wait.NeverStop) &#125; &#125; go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Start loop to sync iptables util rules if kl.makeIPTablesUtilChains &#123; go wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop) &#125; // Start a goroutine responsible for killing pods (that are not properly // handled by pod workers). go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop) // Start component sync loops. kl.statusManager.Start() kl.probeManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil &#123; go kl.runtimeClassManager.Run(wait.NeverStop) &#125; // Start the pod lifecycle event generator. kl.pleg.Start() kl.syncLoop(updates, kl)&#125; syncLoop 是 kubelet 的主循环方法，它从不同的管道(FILE,URL, API-SERVER)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 Pod 处于期望的状态。 12345678910111213141516171819202122232425262728293031323334func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123; glog.Info(&quot;Starting kubelet main sync loop.&quot;) // syncTicker 每秒检测一次是否有需要同步的 pod workers syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for &#123; if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123; glog.Infof(&quot;skipping pod synchronization - %v&quot;, rs) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue &#125; // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) // if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123; break &#125; kl.syncLoopMonitor.Store(kl.clock.Now()) &#125;&#125; syncLoopIteration() 方法对多个管道进行遍历，如果 pod 发生变化，则会调用相应的 Handler，在 Handler 中通过调用 dispatchWork 分发任务。 总结本篇文章主要讲述了 kubelet 组件从加载配置到初始化内部的各个模块再到启动 kubelet 服务的整个流程，上面的时序图能清楚的看到函数之间的调用关系，但是其中每个组件具体的工作方式以及组件之间的交互方式还不得而知，后面会一探究竟。 参考：kubernetes node components – kubeletKubelet 源码分析(一):启动流程分析kubelet 源码分析：启动流程kubernetes 的 kubelet 的工作过程kubelet 内部实现解析]]></content>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet 架构浅析]]></title>
    <url>%2F2018%2F12%2F16%2Fkubelet-modules%2F</url>
    <content type="text"><![CDATA[一、概要kubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。 kubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。 二、kubelet 的主要功能1、kubelet 默认监听四个端口，分别为 10250 、10255、10248、4194。 1234LISTEN 0 128 *:10250 *:* users:((&quot;kubelet&quot;,pid=48500,fd=28))LISTEN 0 128 *:10255 *:* users:((&quot;kubelet&quot;,pid=48500,fd=26))LISTEN 0 128 *:4194 *:* users:((&quot;kubelet&quot;,pid=48500,fd=13))LISTEN 0 128 127.0.0.1:10248 *:* users:((&quot;kubelet&quot;,pid=48500,fd=23)) 10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。 10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 --healthz-port 和 --healthz-bind-address 来指定监听的地址和端口。 12$ curl http://127.0.0.1:10248/healthzok 4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 http://localhost:4194 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 --cadvisor-port 可以指定启动的端口。 1$ curl http://127.0.0.1:4194/metrics 10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。123456// 获取 pod 的接口，与 apiserver 的 // http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName= 接口类似$ curl http://127.0.0.1:10255/pods// 节点信息接口,提供磁盘、网络、CPU、内存等信息$ curl http://127.0.0.1:10255/spec/ 2、kubelet 主要功能： pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。 三、kubelet 组件中的模块 上图展示了 kubelet 组件中的模块以及模块间的划分。 1、PLEG(Pod Lifecycle Event Generator）PLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。 2、cAdvisorcAdvisor（https://github.com/google/cadvisor）是 google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。 3、OOMWatcher系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。 4、probeManagerprobeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。livenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。readinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 5、statusManagerstatusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。 6、containerRefManager容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。 7、evictionManager当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 --eviction-hard= 来决定 evict 的策略值。 8、imageGCimageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 --image-gc-high-threshold 和 --image-gc-low-threshold 来设置。 9、containerGCcontainerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。 10、imageManager调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。 11、volumeManager负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。 12、containerManager负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 --cgroups-per-qos 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 true，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。 13、runtimeManagercontainerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 --container-runtime 来定义是使用docker 还是 rkt，默认是 docker。 14、podManagerpodManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 在 v1.12 中，kubelet 组件有18个 manager： 123456789101112131415161718certificateManagercgroupManagercontainerManagercpuManagernodeContainerManagerconfigmapManagercontainerReferenceManagerevictionManagernvidiaGpuManagerimageGCManagerkuberuntimeManagerhostportManagerpodManagerproberManagersecretManagerstatusManagervolumeManager tokenManager 其中比较重要的模块后面会进行一一分析。 参考：微软资深工程师详解 K8S 容器运行时kubernetes 简介： kubelet 和 podKubelet 组件解析]]></content>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 架构中的几个核心概念]]></title>
    <url>%2F2018%2F12%2F05%2Fdocker-introduces%2F</url>
    <content type="text"><![CDATA[一、Docker 开源之路2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。 2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。 从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。 二、Docker 架构 三、核心概念docker 1.13 版本中包含以下几个二进制文件。123456$ docker --versionDocker version 1.13.1, build 092cba3$ dockerdocker docker-containerd-ctr dockerd docker-proxydocker-containerd docker-containerd-shim docker-init docker-runc 1、dockerdocker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。 2、dockerddockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面： 接收并处理 docker client 发送的请求 管理所有的 docker 容器 有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。 3、containerdcontainerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有： 容器生命周期管理 日志管理 镜像管理 存储管理 容器网络接口及网络管理 4、containerd-shimcontainerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是： 它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC) 即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的 向 containerd 报告容器的退出状态 有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。 5、runCrunC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。 一个例子，使用 runC 运行 busybox 容器:12345678910111213141516171819# mkdir /container# cd /container/# mkdir rootfs准备容器镜像的文件系统,从 busybox 镜像中提取# docker export $(docker create busybox) | tar -C rootfs -xvf - # ls rootfs/bin dev etc home proc root sys tmp usr var有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成# docker-runc spec# lsconfig.json rootfs# docker-runc run simplebusybox #启动容器/ # lsbin dev etc home proc root sys tmp usr var/ # hostnamerunc 参考：Use of containerd-shim in docker-architecture从 docker 到 runCOCI 和 runc：容器标准化和 dockerOpen Container Initiative]]></content>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 常用 API]]></title>
    <url>%2F2018%2F09%2F02%2Fkubernetes-api%2F</url>
    <content type="text"><![CDATA[kubectl 的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数 -v=9 可以看调用的相关 API，例： $ kubectl get node -v=9 以下为 kubernetes 开发中常用的 API： Markdown 表格显示过大，此仅以图片格式展示。]]></content>
  </entry>
  <entry>
    <title><![CDATA[etcd 启用 https]]></title>
    <url>%2F2017%2F03%2F15%2Fetcd-enable-https%2F</url>
    <content type="text"><![CDATA[1， 生成 TLS 秘钥对 2，拷贝密钥对到所有节点 3，配置 etcd 使用证书 4，测试 etcd 是否正常 5，配置 kube-apiserver 使用 CA 连接 etcd 6，测试 kube-apiserver 7，未解决的问题 SSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。 因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。 Generate self-signed certificates 官方参考文档 官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 easy-rsa。以下步骤遵循官方文档： 1， 生成 TLS 秘钥对生成步骤： 1，下载 cfssl 2，初始化证书颁发机构 3，配置 CA 选项 4，生成服务器端证书 5，生成对等证书 6，生成客户端证书 想深入了解 HTTPS 的看这里： 聊聊HTTPS和SSL/TLS协议 数字证书CA及扫盲 互联网加密及OpenSSL介绍和简单使用 SSL双向认证和单向认证的区别 1，下载 cfsslmkdir ~/bin curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x ~/bin/{cfssl,cfssljson} export PATH=$PATH:~/bin 2，初始化证书颁发机构1234mkdir ~/cfsslcd ~/cfsslcfssl print-defaults config &gt; ca-config.jsoncfssl print-defaults csr &gt; ca-csr.json 证书类型介绍： client certificate 用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。 server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。 peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。 3，配置 CA 选项123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566$ cat &lt;&lt; EOF &gt; ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;server&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot; ] &#125;, &quot;client&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;client auth&quot; ] &#125;, &quot;peer&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;$ cat &lt;&lt; EOF &gt; ca-csr.json&#123; &quot;CN&quot;: &quot;My own CA&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;US&quot;, &quot;L&quot;: &quot;CA&quot;, &quot;O&quot;: &quot;My Company Name&quot;, &quot;ST&quot;: &quot;San Francisco&quot;, &quot;OU&quot;: &quot;Org Unit 1&quot;, &quot;OU&quot;: &quot;Org Unit 2&quot; &#125; ]&#125;生成 CA 证书：$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -将会生成以下几个文件：ca-key.pemca.csrca.pem 请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。 4，生成服务器端证书12345678$ echo &apos;&#123;&quot;CN&quot;:&quot;coreos1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&quot;10.93.81.17,127.0.0.1,server&quot; - | cfssljson -bare serverhosts 字段需要自定义。然后将得到以下几个文件：server-key.pemserver.csrserver.pem 5，生成对等证书1234567891011$ echo &apos;&#123;&quot;CN&quot;:&quot;member1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=&quot;10.93.81.17,127.0.0.1,server,member1&quot; - | cfssljson -bare member1hosts 字段需要自定义。然后将得到以下几个文件：member1-key.pemmember1.csrmember1.pem如果有多个 etcd 成员，重复此步为每个成员生成对等证书。 6，生成客户端证书123456789$ echo &apos;&#123;&quot;CN&quot;:&quot;client&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare clienthosts 字段需要自定义。然后将得到以下几个文件：client-key.pemclient.csrclient.pem 至此，所有证书都已生成完毕。 2，拷贝密钥对到所有节点 1，拷贝密钥对到所有节点 2，更新系统证书库 1，拷贝密钥对到所有节点12345$ mkdir -pv /etc/ssl/etcd/$ cp ~/cfssl/* /etc/ssl/etcd/$ chown -R etcd:etcd /etc/ssl/etcd$ chmod 600 /etc/ssl/etcd/*-key.pem$ cp ~/cfssl/ca.pem /etc/ssl/certs/ 2，更新系统证书库123$ yum install ca-certificates -y $ update-ca-trust 3，配置 etcd 使用证书12345678910111213141516171819202122232425262728293031323334353637$ etcdctl versionetcdctl version: 3.1.3API version: 3.1$ cat /etc/etcd/etcd.confETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;#监听URL，用于与其他节点通讯ETCD_LISTEN_PEER_URLS=&quot;https://10.93.81.17:2380&quot;#告知客户端的URL, 也就是服务的URLETCD_LISTEN_CLIENT_URLS=&quot;https://10.93.81.17:2379,https://10.93.81.17:4001&quot;#表示监听其他节点同步信号的地址ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.93.81.17:2380&quot;#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.93.81.17:2379&quot;#启动参数配置ETCD_NAME=&quot;node1&quot;ETCD_INITIAL_CLUSTER=&quot;node1=https://10.93.81.17:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#[security]ETCD_CERT_FILE=&quot;/etc/ssl/etcd/server.pem&quot;ETCD_KEY_FILE=&quot;/etc/ssl/etcd/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/etc/ssl/etcd/member1.pem&quot;ETCD_PEER_KEY_FILE=&quot;/etc/ssl/etcd/member1-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;#[logging]ETCD_DEBUG=&quot;true&quot;ETCD_LOG_PACKAGE_LEVELS=&quot;etcdserver=WARNING,security=DEBUG&quot; 4，测试 etcd 是否正常123456789101112$ systemctl restart etcd如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health&#123;&quot;health&quot;: &quot;true&quot;&#125;$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list $ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar &quot;hello world&quot; $ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar 5，配置 kube-apiserver 使用 CA 连接 etcd1234567$ cp /etc/ssl/etcd/* /var/run/kubernetes/ $ chown -R kube.kube /var/run/kubernetes/在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：--cert-dir=&apos;/var/run/kubernetes/&apos; --etcd-cafile=&apos;/var/run/kubernetes/ca.pem&apos; --etcd-certfile=&apos;/var/run/kubernetes/client.pem&apos; --etcd-keyfile=&apos;/var/run/kubernetes/client-key.pem&apos; 6，测试 kube-apiserver12345678910111213141516$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy$ kubectl get node$ kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate$ ./version.shetcdctl version: 3.1.3API version: 3.1Kubernetes v1.6.0-beta.1 7，未解决的问题1，使用 kubectl get cs 查看会出现如上面所示的报错：1etcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate 此问题有人提交 pr 但尚未被 merge，etcd component status check should include credentials 2，使用以下命令查看到的 2380 端口是未加密的1234$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list 2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379 参考文档： kubernetes + etcd ssl 支持 Security model Enabling HTTPS in an existing etcd cluster]]></content>
  </entry>
  <entry>
    <title><![CDATA[etcd 备份与恢复]]></title>
    <url>%2F2017%2F03%2F02%2Fetcd-backup%2F</url>
    <content type="text"><![CDATA[etcd 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。 etcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。 # etcdctl --version etcdctl version: 3.0.4 API version: 2 官方指出 etcd v2 和 v3 的数据不能混合存放，support backup of v2 and v3 stores 。 特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复若使用 v2 备份数据时存在 v3 的数据则恢复失败 对于 API 2 备份与恢复方法官方 v2 admin guide etcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中： snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。 wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。 # etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup # etcd -data-dir=/home/etcd_backup/ -force-new-cluster 恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。 对于 API 3 备份与恢复方法官方 v3 admin guide 在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。 在命令行设置： # export ETCDCTL_API=3 备份数据： # etcdctl --endpoints localhost:2379 snapshot save snapshot.db 恢复： # etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data 恢复后的文件需要修改权限为 etcd:etcd–name:重新指定一个数据目录，可以不指定，默认为 default.etcd–data-dir：指定数据目录建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir etcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 官方文档翻译]]></content>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 学习笔记]]></title>
    <url>%2F2017%2F02%2F12%2Fkubernetes-learn%2F</url>
    <content type="text"><![CDATA[1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 kubernetes + docker 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 API 进行某些操作。 1，kubernetes 搭建kubernetes 是 google 的一个开源软件，其社区活跃量远超 Mesos，Coreos 的，若想深入学习建议参考《kubernetes 权威指南》，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。 kubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 gitbook 上面某大神的一本书 一 etcd3学习笔记。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。 注意：内核版本 &gt; 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 etcd 节点： # yum install -y etcd # systemctl start etcd master 节点： # yum install -y kubernetes-master # systemctl start kube-apiserver # systemctl start kube-controller-manager # systemctl start kube-scheduler minion 节点： # yum install -y kubernetes docker # systemctl start kubelet # systemctl start kube-proxy # systemctl start docker 2，kubernetes 版本升级以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。 下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 kubernetes-server-linux-amd64.tar.gz 这个包，下载你所要更新到的版本。 升级步骤： 1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上 2，备份二进制程序（kubectl,kube-proxy） 3，将解压后的二进制程序覆盖以前的版本 4，最后重新启动服务 # systemctl stop docker # which kubectl kube-proxy /usr/bin/kubectl /usr/bin/kube-proxy # cp /usr/bin/{kubectl,kube-proxy} /tmp/ # yes | cp bin/{kubectl,kube-proxy} /usr/bin/ # systemctl status {kubectl,kube-proxy} # systemctl start docker 3，kubeconfig 使用若你使用的 kubelet 版本为 1.4，使用 systemctl status kubelet 会看到这样一句话： --api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml 使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 官方文档 有格式说明， 本人已将其翻译，翻译文档见下文。 kubeconfig 文件示例： apiVersion: v1 clusters: - cluster: server: http://localhost:8080 name: local-server contexts: - context: cluster: local-server namespace: the-right-prefix user: myself name: default-context current-context: default-context kind: Config preferences: {} users: - name: myself user: password: secret username: admin # kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true kubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。 require-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。 关于 kubeconfig 的一个 issue，Kubelet won’t read apiserver from kubeconfig。 升级步骤，当然前提是你的 kubelet 版本已经到了 1.5： 1，关闭 kubelet、kube-proxy 服务； 2，注释掉 /etc/kubernetes/kubelet 文件中下面这一行: KUBELET_API_SERVER=&quot;--api-servers=http://127.0.0.1:8080&quot; 然后在 KUBELET_ARGS 中添加： --kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true 这里的路径是你 yaml 文件放置的路径。 3，重新启动刚关掉的两个服务 4，以下为 kubeconfig 配置官方文档的翻译kubernetes 中的验证对于不同的群体可以使用不同的方法. 运行 kubelet 可能有的一种认证方式（即证书）。 用户可能有不同的认证方式（即 token）。 管理员可以为每个用户提供一个证书列表。 可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。 此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。 如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。 相关讨论http://issue.k8s.io/1755 kubeconfig 文件的组件kubeconfig 文件示例： current-context: federal-context apiVersion: v1 clusters: - cluster: api-version: v1 server: http://cow.org:8080 name: cow-cluster - cluster: certificate-authority: path/to/my/cafile server: https://horse.org:4443 name: horse-cluster - cluster: insecure-skip-tls-verify: true server: https://pig.org:443 name: pig-cluster contexts: - context: cluster: horse-cluster namespace: chisel-ns user: green-user name: federal-context - context: cluster: pig-cluster namespace: saw-ns user: black-user name: queen-anne-context kind: Config preferences: colors: true users: - name: blue-user user: token: blue-token - name: green-user user: client-certificate: path/to/my/client/cert client-key: path/to/my/client/key 组件的解释clusterclusters: - cluster: certificate-authority: path/to/my/cafile server: https://horse.org:4443 name: horse-cluster - cluster: insecure-skip-tls-verify: true server: https://pig.org:443 name: pig-cluster cluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。 userusers: - name: blue-user user: token: blue-token - name: green-user user: client-certificate: path/to/my/client/cert client-key: path/to/my/client/key 用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。 contextcontexts: - context: cluster: horse-cluster namespace: chisel-ns user: green-user name: federal-context context 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。 current-contextcurrent-context: federal-context current-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 –context=CONTEXT, –cluster=CLUSTER, –user=USER, and/or –namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。 杂项apiVersion: v1 kind: Config preferences: colors: true apiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。preferences 指定选项(和当前未使用的) kubectl preferences. 查看 kubeconfig 文件kubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 –minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。 创建你的 kubeconfig 文件注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。 在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。 因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改… 以上文件可能对应于使用–token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样： blue-user,blue-user,1 mister-red,mister-red,2 此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。 由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。 类似地，我们也可以选择改变 current-context 的值为 “blue-user”。 在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过 加载和合并规则加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建： 1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。 否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。 对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。 2，此链中第一个被匹配的 context 将被使用： 1，命令行参数 - 命令行选项中 context 的值 2，合并文件中的 current-context 3，此段允许为空 3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）： 1，命令行参数 - user 是用户名，cluster 是集群名 2，如果存在 context 则使用 3，允许为空 4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）： 1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify 2，如果存在集群信息并且该属性的值存在，则使用它。 3，如果没有 server 位置则出错。 5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法： 1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段 2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。 3，如果两者有冲突则失败 6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。 7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。 kubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。 通过 kubectl config 操作 kubeconfig为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。 例如： $ kubectl config set-credentials myself --username=admin --password=secret $ kubectl config set-cluster local-server --server=http://localhost:8080 $ kubectl config set-context default-context --cluster=local-server --user=myself $ kubectl config use-context default-context $ kubectl config set contexts.default-context.namespace the-right-prefix $ kubectl config view 输出： apiVersion: v1 clusters: - cluster: server: http://localhost:8080 name: local-server contexts: - context: cluster: local-server namespace: the-right-prefix user: myself name: default-context current-context: default-context kind: Config preferences: {} users: - name: myself user: password: secret username: admin 一个 kubeconfig 文件类似这样： apiVersion: v1 clusters: - cluster: server: http://localhost:8080 name: local-server contexts: - context: cluster: local-server namespace: the-right-prefix user: myself name: default-context current-context: default-context kind: Config preferences: {} users: - name: myself user: password: secret username: admin 示例文件的命令操作： $ kubectl config set preferences.colors true $ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1 $ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile $ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true $ kubectl config set-credentials blue-user --token=blue-token $ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key $ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns $ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns $ kubectl config use-context federal-context 最后的总结： 所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了： 仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证 将上面的代码段替换为你集群的 api-server endpoint 的信息。 确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。]]></content>
  </entry>
</search>
