{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"c108d6e1bc604e335a042c7ceeada6640b312277","modified":1550326312000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1544263526000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1544263526000},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1544263526000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1544263526000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1544263526000},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1544263526000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1544263526000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1544263526000},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1544263526000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1544263526000},{"_id":"themes/next/README.cn.md","hash":"58ffe752bc4b7f0069fcd6304bbc2d5ff7b80f89","modified":1544263526000},{"_id":"themes/next/README.md","hash":"898213e66d34a46c3cf8446bf693bd50db0d3269","modified":1544263526000},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1544263526000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1544263526000},{"_id":"themes/next/_config.yml","hash":"a265f8f71126e35529fc5f9d58abcd3ec9decfe0","modified":1545011147000},{"_id":"themes/next/package.json","hash":"036d3a1346203d2f1a3958024df7f74e7ac07bfe","modified":1544263526000},{"_id":"source/_posts/etcd-backup.md","hash":"07bb8e8ddcfaac251a5b7fe3613044445de08dff","modified":1544265125000},{"_id":"source/_posts/docker-introduces.md","hash":"1499579f0d3f70c3ec41c4828fcc4e51cab6396c","modified":1544265473000},{"_id":"source/_posts/etcd-enable-https.md","hash":"b6d41d23d296a277615372f297f498e808fbf53f","modified":1544265218000},{"_id":"source/_posts/k8s-audit-webhook.md","hash":"a5ea8f5a7299c010c2e46cc07d3557bf8f79b48e","modified":1548837015000},{"_id":"source/_posts/k8s-crontab.md","hash":"0d883567cac92795a73d88e54ba87608f5bae5be","modified":1550326610000},{"_id":"source/_posts/kubeadm.md","hash":"31a7c68204a348f17927bc649a6b2bbefbf0b647","modified":1547693212000},{"_id":"source/_posts/kubeconfig.md","hash":"8dff92301fa78234d0a28bacb4a5fdac7ae746e6","modified":1547033504000},{"_id":"source/_posts/kubelet-modules.md","hash":"29d34b26a70044302f50d8251405ff92a43bd3eb","modified":1544953670000},{"_id":"source/_posts/kubelet_create_pod.md","hash":"f913ad3c691bd70ecaae8582684e15c8cfb7553d","modified":1546478106000},{"_id":"source/_posts/kubelet_init.md","hash":"9860aeea547d740d78962e99e4eb33f647a02f39","modified":1546325723000},{"_id":"source/_posts/kubernetes-api.md","hash":"c491c2945893e1dd5865eda08bc7e5a91de85a70","modified":1544265321000},{"_id":"source/_posts/kubernetes-learn.md","hash":"c834d1dea6ccbd81fbf9b23048b8fce2ad471d32","modified":1544264940000},{"_id":"source/about/index.md","hash":"a9de5c75bce4ac5c7a02b07091efc86d70fbb60c","modified":1550411780000},{"_id":"source/categories/index.md","hash":"812daa9e1c97a2c72ee357ad92f4de335db3c177","modified":1544263526000},{"_id":"source/tags/index.md","hash":"5118f5301c54915b86d4e92b56a2fa9a9395abfc","modified":1544263526000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1544263526000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1544263526000},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1544263526000},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1544263526000},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1544263526000},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1544263526000},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1544263526000},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1544263526000},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1544263526000},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1544263526000},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1544263526000},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1544263526000},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1544263526000},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1544263526000},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1544263526000},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1544263526000},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1544263526000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1544263526000},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1544263526000},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1544263526000},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1544263526000},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1544263526000},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1544263526000},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1544263526000},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1544263526000},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1544263526000},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1544263526000},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1544263526000},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1544263526000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1544263526000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1544263526000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1544263526000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1544263526000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544263526000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1544263526000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1544263526000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1544263526000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1544263526000},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1544263526000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1544263526000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1544263526000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"6a54c3c85ff6b19d275827a327abbf4bd99b2ebf","modified":1544263526000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1544263526000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1544263526000},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1544263526000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1544263526000},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1544263526000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1544263526000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1544263526000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1544263526000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1544263526000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1544263526000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1544263526000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1544263526000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1544263526000},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1544263526000},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1544263526000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1544263526000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1544263526000},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1544263526000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1544263526000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1544263526000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1544263526000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1544263526000},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1544263526000},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1544263526000},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1544263526000},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1544263526000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1544263526000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1544263526000},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1544263526000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1544263526000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1544263526000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1544263526000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1544263526000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1544263526000},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1544263526000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1544263526000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1544263526000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1544263526000},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1544263526000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1544263526000},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1544263526000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1544263526000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1544263526000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1544263526000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1544263526000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544263526000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544263526000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544263526000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544263526000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544263526000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544263526000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1544263526000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1544263526000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1544263526000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1544263526000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1544263526000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1544263526000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1544263526000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1544263526000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1544263526000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1544263526000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1544263526000},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1544263526000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1544263526000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1544263526000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1544263526000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1544263526000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1544263526000},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1544263526000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1544263526000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1544263526000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1544263526000},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"fcabbb241f894c9a6309c44e126cf3e8fea81fd4","modified":1544263526000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1544263526000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1544263526000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1544263526000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1544263526000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1544263526000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1544263526000},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1544263526000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1544263526000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1544263526000},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1544263526000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1544263526000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1544263526000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1544263526000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1544263526000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1544263526000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1544263526000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1544263526000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1544263526000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1544263526000},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1544263526000},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1544263526000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1544263526000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1544263526000},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1544263526000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1544263526000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1544263526000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1544263526000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1544263526000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1544263526000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1544263526000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1544263526000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1544263526000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1544263526000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1544263526000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1544263526000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1544263526000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1544263526000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1544263526000},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1544263526000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1544263526000},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1544263526000},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1544263526000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1544263526000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1544263526000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1544263526000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1544263526000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1544263526000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1544263526000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1544263526000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1544263526000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1544263526000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1544263526000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1544263526000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1544263526000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1544263526000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1544263526000},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1544263526000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1544263526000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1544263526000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"02fb8fa6b6c252b6bed469539cd057716606a787","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1544263526000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1544263526000},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1544263526000},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1544263526000},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1544263526000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1544263526000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1544263526000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1544263526000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1544263526000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1544263526000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1544263526000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1544263526000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1544263526000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1544263526000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1544263526000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1544263526000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1544263526000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1544263526000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1544263526000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1544263526000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1544263526000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1544263526000},{"_id":"public/search.xml","hash":"6f4cae4e7a9d1948430e56efd39f6f4d78dafe6e","modified":1550411797085},{"_id":"public/about/index.html","hash":"1f43c7e7b0e21b5d9d7407274c7c56c820eff452","modified":1550411797172},{"_id":"public/categories/index.html","hash":"26e0ca8b1fe8a16dc62dd8f8cd916c3b97c5f600","modified":1550411797172},{"_id":"public/tags/index.html","hash":"66f1d5376306b35ddff64fbbcdf39019f4bdff60","modified":1550411797172},{"_id":"public/2019/01/30/k8s-audit-webhook/index.html","hash":"307ad950689e527662188a52946fbf8b69536279","modified":1550411797172},{"_id":"public/2019/01/17/kubeadm/index.html","hash":"f9692dedfe02c12f056d250ef679b1e959bf2f00","modified":1550411797173},{"_id":"public/2019/01/09/kubeconfig/index.html","hash":"d2e0c7694d33fdf7891a4457ccca6682c77ab602","modified":1550411797173},{"_id":"public/2019/02/16/k8s-crontab/index.html","hash":"6013a870bcb355199fa0a07cc842c65e42a38587","modified":1550411797173},{"_id":"public/2019/01/03/kubelet_create_pod/index.html","hash":"86a15a3d805f0b1182d363fd7b0f1763efae6c32","modified":1550411797173},{"_id":"public/2018/12/23/kubelet_init/index.html","hash":"95063f60ac2057be1e850a692cf732f98c686d0b","modified":1550411797173},{"_id":"public/2018/12/16/kubelet-modules/index.html","hash":"c355dd58e71707346ca32986cdd8df50636d7310","modified":1550411797173},{"_id":"public/2018/12/05/docker-introduces/index.html","hash":"9976fd739a20abbdeb298dbbc48c3ba3a34ef4d0","modified":1550411797173},{"_id":"public/2018/09/02/kubernetes-api/index.html","hash":"197bcaf45658b551b48eb788f6867cb94a26c57a","modified":1550411797173},{"_id":"public/2017/03/15/etcd-enable-https/index.html","hash":"4bdbc8423f4babc9176a31ef74a5732fe3a37f02","modified":1550411797174},{"_id":"public/2017/03/02/etcd-backup/index.html","hash":"8e121e20c40391700bc5c6d5e322bd650ad9d66f","modified":1550411797174},{"_id":"public/2017/02/12/kubernetes-learn/index.html","hash":"15fc914f73f7385b2014e82568191517179bc467","modified":1550411797174},{"_id":"public/index.html","hash":"40c47e0b9bfdd1f9cf9797261a0bf5b4e088f1d0","modified":1550411797174},{"_id":"public/page/2/index.html","hash":"51a74e38b44407406e9ba35ef6c17bb22e5c7d4f","modified":1550411797177},{"_id":"public/tags/crontab/index.html","hash":"a664cf9ec47cbcc762cadeb65ef245db8c6b730e","modified":1550411797177},{"_id":"public/tags/kubeconfig/index.html","hash":"c47d626dfd91d300d23864d66fa845d4c4c1c26f","modified":1550411797177},{"_id":"public/tags/audit/index.html","hash":"56518e984093347d401211e64c3ef4b4f9d65bcd","modified":1550411797177},{"_id":"public/tags/log/index.html","hash":"e1f7374c7ccf3de8a9cc703ad8d03859800d908e","modified":1550411797178},{"_id":"public/tags/wait/index.html","hash":"cdf0a0bdb24f99c03f18f6c2d06ef691629541ab","modified":1550411797178},{"_id":"public/tags/kubeadm/index.html","hash":"6ef9b8bc977ec4dd42294273c201f43d2ecae2cd","modified":1550411797178},{"_id":"public/tags/kubelet/index.html","hash":"30fbae595ea8efae0b0273efdb851b950c4f54c3","modified":1550411797178},{"_id":"public/tags/k8s/index.html","hash":"24e4b584ffe1e70794a2f1548d5cc8eaadfd58ed","modified":1550411797178},{"_id":"public/archives/index.html","hash":"a26af49839fd0d6793a068d7292caf821108a0ff","modified":1550411797178},{"_id":"public/archives/page/2/index.html","hash":"1ff154bd8018c37be4f736ecfa51cf74c4d91048","modified":1550411797178},{"_id":"public/archives/2017/index.html","hash":"3d9c7fba2771c2eec7f5e26dd8bb996091611bb5","modified":1550411797178},{"_id":"public/archives/2017/02/index.html","hash":"aa21fbbdf99a031f2072de3c83cf80ba3b0f8fe1","modified":1550411797178},{"_id":"public/archives/2017/03/index.html","hash":"cef8066bd09a67005bdd9aacd71d93da0bee6ff6","modified":1550411797178},{"_id":"public/archives/2018/index.html","hash":"a531771e81f9be995d1c79571c768ecebb508473","modified":1550411797178},{"_id":"public/archives/2018/09/index.html","hash":"c3fe2f95369dddc869c4280542b6821f042f9b68","modified":1550411797178},{"_id":"public/archives/2018/12/index.html","hash":"3eace190675688cc198df2abfdf6f4113a4e222a","modified":1550411797179},{"_id":"public/archives/2019/index.html","hash":"776728d3bc3592742d4b185df53be144e91b8d58","modified":1550411797179},{"_id":"public/archives/2019/01/index.html","hash":"ae697d920b43c7345d8aa860aba1b7212b94aca9","modified":1550411797179},{"_id":"public/archives/2019/02/index.html","hash":"a7facd7797dcb523249071e8ef919869868ef78c","modified":1550411797188},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1550411797196},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1550411797196},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1550411797196},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1550411797196},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1550411797196},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1550411797196},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1550411797196},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1550411797196},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1550411797196},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1550411797196},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1550411797196},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1550411797197},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1550411797197},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1550411797197},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1550411797197},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1550411797197},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1550411797197},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1550411797197},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1550411797197},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1550411797197},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1550411797197},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1550411797198},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1550411797198},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1550411797198},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1550411797198},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1550411797198},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1550411797198},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1550411797198},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1550411797198},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1550411797198},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1550411797198},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1550411797198},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1550411797198},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1550411797883},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1550411797887},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1550411797898},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1550411797898},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1550411797900},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1550411797900},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1550411797900},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1550411797900},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1550411797900},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1550411797900},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1550411797900},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1550411797900},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1550411797900},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1550411797900},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1550411797900},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1550411797900},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1550411797900},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1550411797900},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1550411797900},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1550411797900},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1550411797900},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1550411797900},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1550411797901},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1550411797901},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1550411797901},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1550411797901},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1550411797901},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1550411797901},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1550411797901},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1550411797901},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1550411797901},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1550411797901},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1550411797901},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1550411797901},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1550411797901},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1550411797901},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1550411797901},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1550411797901},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1550411797901},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1550411797901},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1550411797901},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1550411797902},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1550411797902},{"_id":"public/css/main.css","hash":"97f6f26f8aff281610cd7c587c6cfb11e0096d0f","modified":1550411797902},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1550411797902},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1550411797902},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1550411797902},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1550411797907},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1550411797908},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1550411797908},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1550411797908},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1550411797908},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1550411797908},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1550411797908},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1550411797909},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1550411797932},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1550411797932},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1550411797940},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1550411797941},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1550411797984},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1550411797984},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1550411797985},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1550411797985},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1550411797986},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1550411797986},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1550411797986},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1550411797986},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1550411797994},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1550411797994},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1550411797994},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1550411797996},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1550411798001},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1550411798002},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1550411798053},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1550411798065},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1550411798069}],"Category":[],"Data":[],"Page":[{"title":"About","date":"2018-12-08T08:46:18.000Z","_content":"\n###  专注容器生态圈，喜欢钻研 kubernetes、docker、service mesh 等技术\n\n欢迎关注我的公众号：\n\n![kubeCon.jpg](https://upload-images.jianshu.io/upload_images/1262158-92e245b5b1b41335.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)\n","source":"about/index.md","raw":"---\ntitle: About\ndate: 2018-12-08 16:46:18\n---\n\n###  专注容器生态圈，喜欢钻研 kubernetes、docker、service mesh 等技术\n\n欢迎关注我的公众号：\n\n![kubeCon.jpg](https://upload-images.jianshu.io/upload_images/1262158-92e245b5b1b41335.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600)\n","updated":"2019-02-17T14:05:37.000Z","path":"about/index.html","_id":"cjs8z5ggs000122dvpdsqhb2h","comments":1,"layout":"page","content":"<h3 id=\"专注容器生态圈，喜欢钻研-kubernetes、docker、service-mesh-等技术\"><a href=\"#专注容器生态圈，喜欢钻研-kubernetes、docker、service-mesh-等技术\" class=\"headerlink\" title=\"专注容器生态圈，喜欢钻研 kubernetes、docker、service mesh 等技术\"></a>专注容器生态圈，喜欢钻研 kubernetes、docker、service mesh 等技术</h3><p>欢迎关注我的公众号：</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-92e245b5b1b41335.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600\" alt=\"kubeCon.jpg\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"专注容器生态圈，喜欢钻研-kubernetes、docker、service-mesh-等技术\"><a href=\"#专注容器生态圈，喜欢钻研-kubernetes、docker、service-mesh-等技术\" class=\"headerlink\" title=\"专注容器生态圈，喜欢钻研 kubernetes、docker、service mesh 等技术\"></a>专注容器生态圈，喜欢钻研 kubernetes、docker、service mesh 等技术</h3><p>欢迎关注我的公众号：</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-92e245b5b1b41335.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/600\" alt=\"kubeCon.jpg\"></p>\n"},{"title":"categories","date":"2018-12-08T08:52:49.000Z","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-12-08 16:52:49\n---\n","updated":"2018-12-08T10:05:26.000Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjs8z5ggw000322dvciqonwo2","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-12-08T08:46:27.000Z","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-12-08 16:46:27\n---\n","updated":"2018-12-08T10:05:26.000Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjs8z5gnz000g22dvcexjcqd0","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"etcd 备份与恢复","date":"2017-03-02T10:04:00.000Z","type":"etcd","_content":"\n**[etcd](https://github.com/coreos/etcd)** 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。\n\netcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。\n\n    # etcdctl --version\n    etcdctl version: 3.0.4\n    API version: 2\n\n\n官方指出 etcd v2 和 v3 的数据不能混合存放，[support backup of v2 and v3 stores](https://github.com/coreos/etcd/issues/7002) 。\n\n\n**特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复\n若使用 v2 备份数据时存在 v3 的数据则恢复失败**\n\n### 对于 API 2 备份与恢复方法   \n[官方 v2 admin guide](https://github.com/coreos/etcd/blob/master/Documentation/v2/admin_guide.md#disaster-recovery)\n\n\netcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中：\n* snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。\n\n* wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。\n\n\n    # etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup\n\n    # etcd -data-dir=/home/etcd_backup/  -force-new-cluster\n\n\n恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。\n\n### 对于 API 3 备份与恢复方法  \n[官方 v3 admin guide](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md)\n\n在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。\n\n在命令行设置：\n\n\t# export ETCDCTL_API=3\n\t\n备份数据：\n\n\t# etcdctl --endpoints localhost:2379 snapshot save snapshot.db\n\n恢复：\n\n\t# etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data\n\n> 恢复后的文件需要修改权限为 etcd:etcd\n> --name:重新指定一个数据目录，可以不指定，默认为 default.etcd\n> --data-dir：指定数据目录\n> 建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir\n\netcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 \n\n[官方文档翻译](https://skyao.gitbooks.io/leaning-etcd3/content/documentation/op-guide/recovery.html)\n","source":"_posts/etcd-backup.md","raw":"---\ntitle: etcd 备份与恢复\ndate: 2017-03-02 18:04:00\ntype: \"etcd\"\n\n---\n\n**[etcd](https://github.com/coreos/etcd)** 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。\n\netcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。\n\n    # etcdctl --version\n    etcdctl version: 3.0.4\n    API version: 2\n\n\n官方指出 etcd v2 和 v3 的数据不能混合存放，[support backup of v2 and v3 stores](https://github.com/coreos/etcd/issues/7002) 。\n\n\n**特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复\n若使用 v2 备份数据时存在 v3 的数据则恢复失败**\n\n### 对于 API 2 备份与恢复方法   \n[官方 v2 admin guide](https://github.com/coreos/etcd/blob/master/Documentation/v2/admin_guide.md#disaster-recovery)\n\n\netcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中：\n* snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。\n\n* wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。\n\n\n    # etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup\n\n    # etcd -data-dir=/home/etcd_backup/  -force-new-cluster\n\n\n恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。\n\n### 对于 API 3 备份与恢复方法  \n[官方 v3 admin guide](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md)\n\n在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。\n\n在命令行设置：\n\n\t# export ETCDCTL_API=3\n\t\n备份数据：\n\n\t# etcdctl --endpoints localhost:2379 snapshot save snapshot.db\n\n恢复：\n\n\t# etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data\n\n> 恢复后的文件需要修改权限为 etcd:etcd\n> --name:重新指定一个数据目录，可以不指定，默认为 default.etcd\n> --data-dir：指定数据目录\n> 建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir\n\netcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 \n\n[官方文档翻译](https://skyao.gitbooks.io/leaning-etcd3/content/documentation/op-guide/recovery.html)\n","slug":"etcd-backup","published":1,"updated":"2018-12-08T10:32:05.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5ggm000022dv8y8ng8no","content":"<p><strong><a href=\"https://github.com/coreos/etcd\" target=\"_blank\" rel=\"noopener\">etcd</a></strong> 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。</p>\n<p>etcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。</p>\n<pre><code># etcdctl --version\netcdctl version: 3.0.4\nAPI version: 2\n</code></pre><p>官方指出 etcd v2 和 v3 的数据不能混合存放，<a href=\"https://github.com/coreos/etcd/issues/7002\" target=\"_blank\" rel=\"noopener\">support backup of v2 and v3 stores</a> 。</p>\n<p><strong>特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复<br>若使用 v2 备份数据时存在 v3 的数据则恢复失败</strong></p>\n<h3 id=\"对于-API-2-备份与恢复方法\"><a href=\"#对于-API-2-备份与恢复方法\" class=\"headerlink\" title=\"对于 API 2 备份与恢复方法\"></a>对于 API 2 备份与恢复方法</h3><p><a href=\"https://github.com/coreos/etcd/blob/master/Documentation/v2/admin_guide.md#disaster-recovery\" target=\"_blank\" rel=\"noopener\">官方 v2 admin guide</a></p>\n<p>etcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中：</p>\n<ul>\n<li><p>snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。</p>\n</li>\n<li><p>wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。</p>\n</li>\n</ul>\n<pre><code># etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup\n\n# etcd -data-dir=/home/etcd_backup/  -force-new-cluster\n</code></pre><p>恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。</p>\n<h3 id=\"对于-API-3-备份与恢复方法\"><a href=\"#对于-API-3-备份与恢复方法\" class=\"headerlink\" title=\"对于 API 3 备份与恢复方法\"></a>对于 API 3 备份与恢复方法</h3><p><a href=\"https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">官方 v3 admin guide</a></p>\n<p>在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。</p>\n<p>在命令行设置：</p>\n<pre><code># export ETCDCTL_API=3\n</code></pre><p>备份数据：</p>\n<pre><code># etcdctl --endpoints localhost:2379 snapshot save snapshot.db\n</code></pre><p>恢复：</p>\n<pre><code># etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data\n</code></pre><blockquote>\n<p>恢复后的文件需要修改权限为 etcd:etcd<br>–name:重新指定一个数据目录，可以不指定，默认为 default.etcd<br>–data-dir：指定数据目录<br>建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir</p>\n</blockquote>\n<p>etcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 </p>\n<p><a href=\"https://skyao.gitbooks.io/leaning-etcd3/content/documentation/op-guide/recovery.html\" target=\"_blank\" rel=\"noopener\">官方文档翻译</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong><a href=\"https://github.com/coreos/etcd\" target=\"_blank\" rel=\"noopener\">etcd</a></strong> 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。</p>\n<p>etcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。</p>\n<pre><code># etcdctl --version\netcdctl version: 3.0.4\nAPI version: 2\n</code></pre><p>官方指出 etcd v2 和 v3 的数据不能混合存放，<a href=\"https://github.com/coreos/etcd/issues/7002\" target=\"_blank\" rel=\"noopener\">support backup of v2 and v3 stores</a> 。</p>\n<p><strong>特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复<br>若使用 v2 备份数据时存在 v3 的数据则恢复失败</strong></p>\n<h3 id=\"对于-API-2-备份与恢复方法\"><a href=\"#对于-API-2-备份与恢复方法\" class=\"headerlink\" title=\"对于 API 2 备份与恢复方法\"></a>对于 API 2 备份与恢复方法</h3><p><a href=\"https://github.com/coreos/etcd/blob/master/Documentation/v2/admin_guide.md#disaster-recovery\" target=\"_blank\" rel=\"noopener\">官方 v2 admin guide</a></p>\n<p>etcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中：</p>\n<ul>\n<li><p>snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。</p>\n</li>\n<li><p>wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。</p>\n</li>\n</ul>\n<pre><code># etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup\n\n# etcd -data-dir=/home/etcd_backup/  -force-new-cluster\n</code></pre><p>恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。</p>\n<h3 id=\"对于-API-3-备份与恢复方法\"><a href=\"#对于-API-3-备份与恢复方法\" class=\"headerlink\" title=\"对于 API 3 备份与恢复方法\"></a>对于 API 3 备份与恢复方法</h3><p><a href=\"https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">官方 v3 admin guide</a></p>\n<p>在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。</p>\n<p>在命令行设置：</p>\n<pre><code># export ETCDCTL_API=3\n</code></pre><p>备份数据：</p>\n<pre><code># etcdctl --endpoints localhost:2379 snapshot save snapshot.db\n</code></pre><p>恢复：</p>\n<pre><code># etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data\n</code></pre><blockquote>\n<p>恢复后的文件需要修改权限为 etcd:etcd<br>–name:重新指定一个数据目录，可以不指定，默认为 default.etcd<br>–data-dir：指定数据目录<br>建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir</p>\n</blockquote>\n<p>etcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 </p>\n<p><a href=\"https://skyao.gitbooks.io/leaning-etcd3/content/documentation/op-guide/recovery.html\" target=\"_blank\" rel=\"noopener\">官方文档翻译</a></p>\n"},{"title":"Docker 架构中的几个核心概念","date":"2018-12-05T12:57:00.000Z","type":"docker","_content":"\n\n## 一、Docker 开源之路\n\n2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。\n\n2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。\n\n从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。\n\n\n## 二、Docker 架构\n\n![docker 架构](https://upload-images.jianshu.io/upload_images/1262158-eee83eb356fccdb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n![docker 进程关系](https://upload-images.jianshu.io/upload_images/1262158-3f74443f956fa132.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### 三、核心概念\n\ndocker 1.13 版本中包含以下几个二进制文件。\n```\n$ docker --version\nDocker version 1.13.1, build 092cba3\n\n$ docker\ndocker             docker-containerd-ctr   dockerd      docker-proxy\ndocker-containerd  docker-containerd-shim  docker-init  docker-runc\n```\n\n#### 1、docker \ndocker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。\n\n#### 2、dockerd \ndockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面：\n\n- 接收并处理 docker client 发送的请求\n- 管理所有的 docker 容器\n\n有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。\n\n#### 3、containerd\n\ncontainerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有：\n- 容器生命周期管理\n- 日志管理\n- 镜像管理\n- 存储管理\n- 容器网络接口及网络管理\n\n#### 4、containerd-shim\n\ncontainerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是：\n\n- 它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC)\n- 即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的\n- 向 containerd 报告容器的退出状态\n\n有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。\n\n#### 5、runC\nrunC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。\n\n一个例子，使用 runC 运行 busybox 容器:\n```\n# mkdir /container\n# cd /container/\n# mkdir rootfs\n\n准备容器镜像的文件系统,从 busybox 镜像中提取\n# docker export $(docker create busybox) | tar -C rootfs -xvf -    \n# ls rootfs/\nbin  dev  etc  home  proc  root  sys  tmp  usr  var\n\n有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，\n包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成\n# docker-runc spec\n# ls\nconfig.json  rootfs\n# docker-runc run simplebusybox    #启动容器\n/ # ls\nbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/ # hostname\nrunc\n```\n---\n参考：\n[Use of containerd-shim in docker-architecture](https://groups.google.com/forum/#!topic/docker-dev/zaZFlvIx1_k)\n[从 docker 到 runC](https://www.cnblogs.com/sparkdev/p/9129334.html)\n[OCI 和 runc：容器标准化和 docker](http://cizixs.com/2017/11/05/oci-and-runc/)\n[Open Container Initiative](https://github.com/opencontainers)\n","source":"_posts/docker-introduces.md","raw":"---\ntitle: Docker 架构中的几个核心概念\ndate: 2018-12-05 20:57:00\ntype: \"docker\"\n\n---\n\n\n## 一、Docker 开源之路\n\n2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。\n\n2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。\n\n从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。\n\n\n## 二、Docker 架构\n\n![docker 架构](https://upload-images.jianshu.io/upload_images/1262158-eee83eb356fccdb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n![docker 进程关系](https://upload-images.jianshu.io/upload_images/1262158-3f74443f956fa132.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### 三、核心概念\n\ndocker 1.13 版本中包含以下几个二进制文件。\n```\n$ docker --version\nDocker version 1.13.1, build 092cba3\n\n$ docker\ndocker             docker-containerd-ctr   dockerd      docker-proxy\ndocker-containerd  docker-containerd-shim  docker-init  docker-runc\n```\n\n#### 1、docker \ndocker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。\n\n#### 2、dockerd \ndockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面：\n\n- 接收并处理 docker client 发送的请求\n- 管理所有的 docker 容器\n\n有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。\n\n#### 3、containerd\n\ncontainerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有：\n- 容器生命周期管理\n- 日志管理\n- 镜像管理\n- 存储管理\n- 容器网络接口及网络管理\n\n#### 4、containerd-shim\n\ncontainerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是：\n\n- 它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC)\n- 即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的\n- 向 containerd 报告容器的退出状态\n\n有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。\n\n#### 5、runC\nrunC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。\n\n一个例子，使用 runC 运行 busybox 容器:\n```\n# mkdir /container\n# cd /container/\n# mkdir rootfs\n\n准备容器镜像的文件系统,从 busybox 镜像中提取\n# docker export $(docker create busybox) | tar -C rootfs -xvf -    \n# ls rootfs/\nbin  dev  etc  home  proc  root  sys  tmp  usr  var\n\n有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，\n包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成\n# docker-runc spec\n# ls\nconfig.json  rootfs\n# docker-runc run simplebusybox    #启动容器\n/ # ls\nbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/ # hostname\nrunc\n```\n---\n参考：\n[Use of containerd-shim in docker-architecture](https://groups.google.com/forum/#!topic/docker-dev/zaZFlvIx1_k)\n[从 docker 到 runC](https://www.cnblogs.com/sparkdev/p/9129334.html)\n[OCI 和 runc：容器标准化和 docker](http://cizixs.com/2017/11/05/oci-and-runc/)\n[Open Container Initiative](https://github.com/opencontainers)\n","slug":"docker-introduces","published":1,"updated":"2018-12-08T10:37:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5ggt000222dvttusllxd","content":"<h2 id=\"一、Docker-开源之路\"><a href=\"#一、Docker-开源之路\" class=\"headerlink\" title=\"一、Docker 开源之路\"></a>一、Docker 开源之路</h2><p>2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。</p>\n<p>2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。</p>\n<p>从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。</p>\n<h2 id=\"二、Docker-架构\"><a href=\"#二、Docker-架构\" class=\"headerlink\" title=\"二、Docker 架构\"></a>二、Docker 架构</h2><p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-eee83eb356fccdb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"docker 架构\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-3f74443f956fa132.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"docker 进程关系\"></p>\n<h3 id=\"三、核心概念\"><a href=\"#三、核心概念\" class=\"headerlink\" title=\"三、核心概念\"></a>三、核心概念</h3><p>docker 1.13 版本中包含以下几个二进制文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker --version</span><br><span class=\"line\">Docker version 1.13.1, build 092cba3</span><br><span class=\"line\"></span><br><span class=\"line\">$ docker</span><br><span class=\"line\">docker             docker-containerd-ctr   dockerd      docker-proxy</span><br><span class=\"line\">docker-containerd  docker-containerd-shim  docker-init  docker-runc</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1、docker\"><a href=\"#1、docker\" class=\"headerlink\" title=\"1、docker\"></a>1、docker</h4><p>docker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。</p>\n<h4 id=\"2、dockerd\"><a href=\"#2、dockerd\" class=\"headerlink\" title=\"2、dockerd\"></a>2、dockerd</h4><p>dockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面：</p>\n<ul>\n<li>接收并处理 docker client 发送的请求</li>\n<li>管理所有的 docker 容器</li>\n</ul>\n<p>有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。</p>\n<h4 id=\"3、containerd\"><a href=\"#3、containerd\" class=\"headerlink\" title=\"3、containerd\"></a>3、containerd</h4><p>containerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有：</p>\n<ul>\n<li>容器生命周期管理</li>\n<li>日志管理</li>\n<li>镜像管理</li>\n<li>存储管理</li>\n<li>容器网络接口及网络管理</li>\n</ul>\n<h4 id=\"4、containerd-shim\"><a href=\"#4、containerd-shim\" class=\"headerlink\" title=\"4、containerd-shim\"></a>4、containerd-shim</h4><p>containerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是：</p>\n<ul>\n<li>它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC)</li>\n<li>即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的</li>\n<li>向 containerd 报告容器的退出状态</li>\n</ul>\n<p>有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。</p>\n<h4 id=\"5、runC\"><a href=\"#5、runC\" class=\"headerlink\" title=\"5、runC\"></a>5、runC</h4><p>runC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。</p>\n<p>一个例子，使用 runC 运行 busybox 容器:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># mkdir /container</span><br><span class=\"line\"># cd /container/</span><br><span class=\"line\"># mkdir rootfs</span><br><span class=\"line\"></span><br><span class=\"line\">准备容器镜像的文件系统,从 busybox 镜像中提取</span><br><span class=\"line\"># docker export $(docker create busybox) | tar -C rootfs -xvf -    </span><br><span class=\"line\"># ls rootfs/</span><br><span class=\"line\">bin  dev  etc  home  proc  root  sys  tmp  usr  var</span><br><span class=\"line\"></span><br><span class=\"line\">有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，</span><br><span class=\"line\">包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成</span><br><span class=\"line\"># docker-runc spec</span><br><span class=\"line\"># ls</span><br><span class=\"line\">config.json  rootfs</span><br><span class=\"line\"># docker-runc run simplebusybox    #启动容器</span><br><span class=\"line\">/ # ls</span><br><span class=\"line\">bin   dev   etc   home  proc  root  sys   tmp   usr   var</span><br><span class=\"line\">/ # hostname</span><br><span class=\"line\">runc</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>参考：<br><a href=\"https://groups.google.com/forum/#!topic/docker-dev/zaZFlvIx1_k\" target=\"_blank\" rel=\"noopener\">Use of containerd-shim in docker-architecture</a><br><a href=\"https://www.cnblogs.com/sparkdev/p/9129334.html\" target=\"_blank\" rel=\"noopener\">从 docker 到 runC</a><br><a href=\"http://cizixs.com/2017/11/05/oci-and-runc/\" target=\"_blank\" rel=\"noopener\">OCI 和 runc：容器标准化和 docker</a><br><a href=\"https://github.com/opencontainers\" target=\"_blank\" rel=\"noopener\">Open Container Initiative</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一、Docker-开源之路\"><a href=\"#一、Docker-开源之路\" class=\"headerlink\" title=\"一、Docker 开源之路\"></a>一、Docker 开源之路</h2><p>2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。</p>\n<p>2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。</p>\n<p>从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。</p>\n<h2 id=\"二、Docker-架构\"><a href=\"#二、Docker-架构\" class=\"headerlink\" title=\"二、Docker 架构\"></a>二、Docker 架构</h2><p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-eee83eb356fccdb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"docker 架构\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-3f74443f956fa132.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"docker 进程关系\"></p>\n<h3 id=\"三、核心概念\"><a href=\"#三、核心概念\" class=\"headerlink\" title=\"三、核心概念\"></a>三、核心概念</h3><p>docker 1.13 版本中包含以下几个二进制文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker --version</span><br><span class=\"line\">Docker version 1.13.1, build 092cba3</span><br><span class=\"line\"></span><br><span class=\"line\">$ docker</span><br><span class=\"line\">docker             docker-containerd-ctr   dockerd      docker-proxy</span><br><span class=\"line\">docker-containerd  docker-containerd-shim  docker-init  docker-runc</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1、docker\"><a href=\"#1、docker\" class=\"headerlink\" title=\"1、docker\"></a>1、docker</h4><p>docker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。</p>\n<h4 id=\"2、dockerd\"><a href=\"#2、dockerd\" class=\"headerlink\" title=\"2、dockerd\"></a>2、dockerd</h4><p>dockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面：</p>\n<ul>\n<li>接收并处理 docker client 发送的请求</li>\n<li>管理所有的 docker 容器</li>\n</ul>\n<p>有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。</p>\n<h4 id=\"3、containerd\"><a href=\"#3、containerd\" class=\"headerlink\" title=\"3、containerd\"></a>3、containerd</h4><p>containerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有：</p>\n<ul>\n<li>容器生命周期管理</li>\n<li>日志管理</li>\n<li>镜像管理</li>\n<li>存储管理</li>\n<li>容器网络接口及网络管理</li>\n</ul>\n<h4 id=\"4、containerd-shim\"><a href=\"#4、containerd-shim\" class=\"headerlink\" title=\"4、containerd-shim\"></a>4、containerd-shim</h4><p>containerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是：</p>\n<ul>\n<li>它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC)</li>\n<li>即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的</li>\n<li>向 containerd 报告容器的退出状态</li>\n</ul>\n<p>有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。</p>\n<h4 id=\"5、runC\"><a href=\"#5、runC\" class=\"headerlink\" title=\"5、runC\"></a>5、runC</h4><p>runC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。</p>\n<p>一个例子，使用 runC 运行 busybox 容器:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># mkdir /container</span><br><span class=\"line\"># cd /container/</span><br><span class=\"line\"># mkdir rootfs</span><br><span class=\"line\"></span><br><span class=\"line\">准备容器镜像的文件系统,从 busybox 镜像中提取</span><br><span class=\"line\"># docker export $(docker create busybox) | tar -C rootfs -xvf -    </span><br><span class=\"line\"># ls rootfs/</span><br><span class=\"line\">bin  dev  etc  home  proc  root  sys  tmp  usr  var</span><br><span class=\"line\"></span><br><span class=\"line\">有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，</span><br><span class=\"line\">包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成</span><br><span class=\"line\"># docker-runc spec</span><br><span class=\"line\"># ls</span><br><span class=\"line\">config.json  rootfs</span><br><span class=\"line\"># docker-runc run simplebusybox    #启动容器</span><br><span class=\"line\">/ # ls</span><br><span class=\"line\">bin   dev   etc   home  proc  root  sys   tmp   usr   var</span><br><span class=\"line\">/ # hostname</span><br><span class=\"line\">runc</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>参考：<br><a href=\"https://groups.google.com/forum/#!topic/docker-dev/zaZFlvIx1_k\" target=\"_blank\" rel=\"noopener\">Use of containerd-shim in docker-architecture</a><br><a href=\"https://www.cnblogs.com/sparkdev/p/9129334.html\" target=\"_blank\" rel=\"noopener\">从 docker 到 runC</a><br><a href=\"http://cizixs.com/2017/11/05/oci-and-runc/\" target=\"_blank\" rel=\"noopener\">OCI 和 runc：容器标准化和 docker</a><br><a href=\"https://github.com/opencontainers\" target=\"_blank\" rel=\"noopener\">Open Container Initiative</a></p>\n"},{"title":"k8s 中定时任务的实现","date":"2019-02-16T13:59:30.000Z","type":"wait","_content":"k8s 中有许多优秀的包都可以在平时的开发中借鉴与使用，比如，任务的定时轮询、高可用的实现、日志处理、缓存使用等都是独立的包，可以直接引用。本篇文章会介绍 k8s 中定时任务的实现，k8s 中定时任务都是通过 wait 包实现的，wait 包在 k8s 的多个组件中都有用到，以下是 wait 包在 kubelet 中的几处使用：\n\n\n```\t\t\nfunc run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh <-chan struct{}) (err error) {\n\t\t...\n\t\t// kubelet 每5分钟一次从 apiserver 获取证书\n\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t\n\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t...\n}\n\n...\n\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration,   kubeDeps *kubelet.Dependencies, enableServer bool) {\n    // 持续监听 pod 的变化\n    go wait.Until(func() {\n        k.Run(podCfg.Updates())\n    }, 0, wait.NeverStop)\n    ...\n}\n```\n\ngolang 中可以通过 time.Ticker 实现定时任务的执行，但在 k8s 中用了更原生的方式，使用 time.Timer 实现的。time.Ticker 和 time.Timer 的使用区别如下：\n\n- ticker 只要定义完成，从此刻开始计时，不需要任何其他的操作，每隔固定时间都会自动触发。\n- timer 定时器是到了固定时间后会执行一次，仅执行一次\n- 如果 timer 定时器要每隔间隔的时间执行，实现 ticker 的效果，使用 `func (t *Timer) Reset(d Duration) bool`\n \n一个示例：\n\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\nfunc main() {\n\tvar wg sync.WaitGroup\n\n\ttimer1 := time.NewTimer(2 * time.Second)\n\tticker1 := time.NewTicker(2 * time.Second)\n\n\twg.Add(1)\n\tgo func(t *time.Ticker) {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\t<-t.C\n\t\t\tfmt.Println(\"exec ticker\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\t}\n\t}(ticker1)\n\n\twg.Add(1)\n\tgo func(t *time.Timer) {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\t<-t.C\n\t\t\tfmt.Println(\"exec timer\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\t\tt.Reset(2 * time.Second)\n\t\t}\n\t}(timer1)\n\t\n\twg.Wait()\n}\n\n```\n\n### 一、wait 包中的核心代码\n\n\n核心代码（k8s.io/apimachinery/pkg/util/wait/wait.go）：\n\n\n```\nfunc JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh <-chan struct{}) {\n\tvar t *time.Timer\n\tvar sawTimeout bool\n\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tjitteredPeriod := period\n\t\tif jitterFactor > 0.0 {\n\t\t\tjitteredPeriod = Jitter(period, jitterFactor)\n\t\t}\n\n\t\tif !sliding {\n\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)\n\t\t}\n\n\t\tfunc() {\n\t\t\tdefer runtime.HandleCrash()\n\t\t\tf()\n\t\t}()\n\n\t\tif sliding {\n\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)\n\t\t}\n\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn\n\t\tcase <-t.C:\n\t\t\tsawTimeout = true\n\t\t}\n\t}\n}\n\n...\n\nfunc resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer {\n    if t == nil {\n        return time.NewTimer(d)\n    }\n    if !t.Stop() && !sawTimeout {\n        <-t.C\n    }\n    t.Reset(d)\n    return t\n}\n```\n\n几个关键点的说明：\n\n- 1、如果 sliding 为 true，则在 f() 运行之后计算周期。如果为 false，那么 period 包含 f() 的执行时间。\n- 2、在 golang 中 select 没有优先级选择，为了避免额外执行 f(),在每次循环开始后会先判断 stopCh chan。\n\nk8s 中 wait 包其实是对 time.Timer 做了一层封装实现。\n\n### 二、wait 包常用的方法\n\n##### 1、定期执行一个函数，永不停止，可以使用 Forever 方法：\n\nfunc Forever(f func(), period time.Duration)\n\n##### 2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：\n\nfunc Until(f func(), period time.Duration, stopCh <-chan struct{})\n\n上面的第三个参数 stopCh 就是用于退出无限循环的标志，停止的时候我们 close 掉这个 chan 就可以了。\n\n##### 3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：\n\nfunc Poll(interval, timeout time.Duration, condition ConditionFunc)\n\n这个函数会以 interval 为间隔，不断去检查 condition 条件是否为真，如果为真则可以继续后续处理；如果指定了 timeout 参数，则该函数也可以只常识指定的时间。\n\n##### 4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：\n\nPollUntil(interval time.Duration, condition ConditionFunc, stopCh <-chan struct{}) error\n\n此外还有 PollImmediate 、 PollInfinite 和 PollImmediateInfinite 方法。\n\n### 三、总结\n\n本篇文章主要讲了 k8s 中定时任务的实现与对应包（wait）中方法的使用。通过阅读 k8s 的源代码，可以发现 k8s 中许多功能的实现也都是我们需要在平时工作中用的，其大部分包的性能都是经过大规模考验的，通过使用其相关的工具包不仅能学到大量的编程技巧也能避免自己造轮子。\n\n","source":"_posts/k8s-crontab.md","raw":"---\ntitle: k8s 中定时任务的实现\ndate: 2019-02-16 21:59:30\ntags: [\"crontab\",\"wait\",\"k8s\"]\ntype: \"wait\"\n\n---\nk8s 中有许多优秀的包都可以在平时的开发中借鉴与使用，比如，任务的定时轮询、高可用的实现、日志处理、缓存使用等都是独立的包，可以直接引用。本篇文章会介绍 k8s 中定时任务的实现，k8s 中定时任务都是通过 wait 包实现的，wait 包在 k8s 的多个组件中都有用到，以下是 wait 包在 kubelet 中的几处使用：\n\n\n```\t\t\nfunc run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh <-chan struct{}) (err error) {\n\t\t...\n\t\t// kubelet 每5分钟一次从 apiserver 获取证书\n\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t\n\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t...\n}\n\n...\n\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration,   kubeDeps *kubelet.Dependencies, enableServer bool) {\n    // 持续监听 pod 的变化\n    go wait.Until(func() {\n        k.Run(podCfg.Updates())\n    }, 0, wait.NeverStop)\n    ...\n}\n```\n\ngolang 中可以通过 time.Ticker 实现定时任务的执行，但在 k8s 中用了更原生的方式，使用 time.Timer 实现的。time.Ticker 和 time.Timer 的使用区别如下：\n\n- ticker 只要定义完成，从此刻开始计时，不需要任何其他的操作，每隔固定时间都会自动触发。\n- timer 定时器是到了固定时间后会执行一次，仅执行一次\n- 如果 timer 定时器要每隔间隔的时间执行，实现 ticker 的效果，使用 `func (t *Timer) Reset(d Duration) bool`\n \n一个示例：\n\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\nfunc main() {\n\tvar wg sync.WaitGroup\n\n\ttimer1 := time.NewTimer(2 * time.Second)\n\tticker1 := time.NewTicker(2 * time.Second)\n\n\twg.Add(1)\n\tgo func(t *time.Ticker) {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\t<-t.C\n\t\t\tfmt.Println(\"exec ticker\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\t}\n\t}(ticker1)\n\n\twg.Add(1)\n\tgo func(t *time.Timer) {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\t<-t.C\n\t\t\tfmt.Println(\"exec timer\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\t\tt.Reset(2 * time.Second)\n\t\t}\n\t}(timer1)\n\t\n\twg.Wait()\n}\n\n```\n\n### 一、wait 包中的核心代码\n\n\n核心代码（k8s.io/apimachinery/pkg/util/wait/wait.go）：\n\n\n```\nfunc JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh <-chan struct{}) {\n\tvar t *time.Timer\n\tvar sawTimeout bool\n\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tjitteredPeriod := period\n\t\tif jitterFactor > 0.0 {\n\t\t\tjitteredPeriod = Jitter(period, jitterFactor)\n\t\t}\n\n\t\tif !sliding {\n\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)\n\t\t}\n\n\t\tfunc() {\n\t\t\tdefer runtime.HandleCrash()\n\t\t\tf()\n\t\t}()\n\n\t\tif sliding {\n\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)\n\t\t}\n\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn\n\t\tcase <-t.C:\n\t\t\tsawTimeout = true\n\t\t}\n\t}\n}\n\n...\n\nfunc resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer {\n    if t == nil {\n        return time.NewTimer(d)\n    }\n    if !t.Stop() && !sawTimeout {\n        <-t.C\n    }\n    t.Reset(d)\n    return t\n}\n```\n\n几个关键点的说明：\n\n- 1、如果 sliding 为 true，则在 f() 运行之后计算周期。如果为 false，那么 period 包含 f() 的执行时间。\n- 2、在 golang 中 select 没有优先级选择，为了避免额外执行 f(),在每次循环开始后会先判断 stopCh chan。\n\nk8s 中 wait 包其实是对 time.Timer 做了一层封装实现。\n\n### 二、wait 包常用的方法\n\n##### 1、定期执行一个函数，永不停止，可以使用 Forever 方法：\n\nfunc Forever(f func(), period time.Duration)\n\n##### 2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：\n\nfunc Until(f func(), period time.Duration, stopCh <-chan struct{})\n\n上面的第三个参数 stopCh 就是用于退出无限循环的标志，停止的时候我们 close 掉这个 chan 就可以了。\n\n##### 3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：\n\nfunc Poll(interval, timeout time.Duration, condition ConditionFunc)\n\n这个函数会以 interval 为间隔，不断去检查 condition 条件是否为真，如果为真则可以继续后续处理；如果指定了 timeout 参数，则该函数也可以只常识指定的时间。\n\n##### 4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：\n\nPollUntil(interval time.Duration, condition ConditionFunc, stopCh <-chan struct{}) error\n\n此外还有 PollImmediate 、 PollInfinite 和 PollImmediateInfinite 方法。\n\n### 三、总结\n\n本篇文章主要讲了 k8s 中定时任务的实现与对应包（wait）中方法的使用。通过阅读 k8s 的源代码，可以发现 k8s 中许多功能的实现也都是我们需要在平时工作中用的，其大部分包的性能都是经过大规模考验的，通过使用其相关的工具包不仅能学到大量的编程技巧也能避免自己造轮子。\n\n","slug":"k8s-crontab","published":1,"updated":"2019-02-16T14:16:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5ggx000422dvjlurvls6","content":"<p>k8s 中有许多优秀的包都可以在平时的开发中借鉴与使用，比如，任务的定时轮询、高可用的实现、日志处理、缓存使用等都是独立的包，可以直接引用。本篇文章会介绍 k8s 中定时任务的实现，k8s 中定时任务都是通过 wait 包实现的，wait 包在 k8s 的多个组件中都有用到，以下是 wait 包在 kubelet 中的几处使用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// kubelet 每5分钟一次从 apiserver 获取证书</span><br><span class=\"line\">\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration,   kubeDeps *kubelet.Dependencies, enableServer bool) &#123;</span><br><span class=\"line\">    // 持续监听 pod 的变化</span><br><span class=\"line\">    go wait.Until(func() &#123;</span><br><span class=\"line\">        k.Run(podCfg.Updates())</span><br><span class=\"line\">    &#125;, 0, wait.NeverStop)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>golang 中可以通过 time.Ticker 实现定时任务的执行，但在 k8s 中用了更原生的方式，使用 time.Timer 实现的。time.Ticker 和 time.Timer 的使用区别如下：</p>\n<ul>\n<li>ticker 只要定义完成，从此刻开始计时，不需要任何其他的操作，每隔固定时间都会自动触发。</li>\n<li>timer 定时器是到了固定时间后会执行一次，仅执行一次</li>\n<li>如果 timer 定时器要每隔间隔的时间执行，实现 ticker 的效果，使用 <code>func (t *Timer) Reset(d Duration) bool</code></li>\n</ul>\n<p>一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;fmt&quot;</span><br><span class=\"line\">\t&quot;sync&quot;</span><br><span class=\"line\">\t&quot;time&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tvar wg sync.WaitGroup</span><br><span class=\"line\"></span><br><span class=\"line\">\ttimer1 := time.NewTimer(2 * time.Second)</span><br><span class=\"line\">\tticker1 := time.NewTicker(2 * time.Second)</span><br><span class=\"line\"></span><br><span class=\"line\">\twg.Add(1)</span><br><span class=\"line\">\tgo func(t *time.Ticker) &#123;</span><br><span class=\"line\">\t\tdefer wg.Done()</span><br><span class=\"line\">\t\tfor &#123;</span><br><span class=\"line\">\t\t\t&lt;-t.C</span><br><span class=\"line\">\t\t\tfmt.Println(&quot;exec ticker&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;(ticker1)</span><br><span class=\"line\"></span><br><span class=\"line\">\twg.Add(1)</span><br><span class=\"line\">\tgo func(t *time.Timer) &#123;</span><br><span class=\"line\">\t\tdefer wg.Done()</span><br><span class=\"line\">\t\tfor &#123;</span><br><span class=\"line\">\t\t\t&lt;-t.C</span><br><span class=\"line\">\t\t\tfmt.Println(&quot;exec timer&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;))</span><br><span class=\"line\">\t\t\tt.Reset(2 * time.Second)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;(timer1)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\twg.Wait()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"一、wait-包中的核心代码\"><a href=\"#一、wait-包中的核心代码\" class=\"headerlink\" title=\"一、wait 包中的核心代码\"></a>一、wait 包中的核心代码</h3><p>核心代码（k8s.io/apimachinery/pkg/util/wait/wait.go）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh &lt;-chan struct&#123;&#125;) &#123;</span><br><span class=\"line\">\tvar t *time.Timer</span><br><span class=\"line\">\tvar sawTimeout bool</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tselect &#123;</span><br><span class=\"line\">\t\tcase &lt;-stopCh:</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\tdefault:</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tjitteredPeriod := period</span><br><span class=\"line\">\t\tif jitterFactor &gt; 0.0 &#123;</span><br><span class=\"line\">\t\t\tjitteredPeriod = Jitter(period, jitterFactor)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif !sliding &#123;</span><br><span class=\"line\">\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfunc() &#123;</span><br><span class=\"line\">\t\t\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t\t\tf()</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif sliding &#123;</span><br><span class=\"line\">\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tselect &#123;</span><br><span class=\"line\">\t\tcase &lt;-stopCh:</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\tcase &lt;-t.C:</span><br><span class=\"line\">\t\t\tsawTimeout = true</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">func resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer &#123;</span><br><span class=\"line\">    if t == nil &#123;</span><br><span class=\"line\">        return time.NewTimer(d)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if !t.Stop() &amp;&amp; !sawTimeout &#123;</span><br><span class=\"line\">        &lt;-t.C</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    t.Reset(d)</span><br><span class=\"line\">    return t</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>几个关键点的说明：</p>\n<ul>\n<li>1、如果 sliding 为 true，则在 f() 运行之后计算周期。如果为 false，那么 period 包含 f() 的执行时间。</li>\n<li>2、在 golang 中 select 没有优先级选择，为了避免额外执行 f(),在每次循环开始后会先判断 stopCh chan。</li>\n</ul>\n<p>k8s 中 wait 包其实是对 time.Timer 做了一层封装实现。</p>\n<h3 id=\"二、wait-包常用的方法\"><a href=\"#二、wait-包常用的方法\" class=\"headerlink\" title=\"二、wait 包常用的方法\"></a>二、wait 包常用的方法</h3><h5 id=\"1、定期执行一个函数，永不停止，可以使用-Forever-方法：\"><a href=\"#1、定期执行一个函数，永不停止，可以使用-Forever-方法：\" class=\"headerlink\" title=\"1、定期执行一个函数，永不停止，可以使用 Forever 方法：\"></a>1、定期执行一个函数，永不停止，可以使用 Forever 方法：</h5><p>func Forever(f func(), period time.Duration)</p>\n<h5 id=\"2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的-chan-即可，方法定义如下：\"><a href=\"#2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的-chan-即可，方法定义如下：\" class=\"headerlink\" title=\"2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：\"></a>2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：</h5><p>func Until(f func(), period time.Duration, stopCh &lt;-chan struct{})</p>\n<p>上面的第三个参数 stopCh 就是用于退出无限循环的标志，停止的时候我们 close 掉这个 chan 就可以了。</p>\n<h5 id=\"3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用-Poll-方法：\"><a href=\"#3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用-Poll-方法：\" class=\"headerlink\" title=\"3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：\"></a>3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：</h5><p>func Poll(interval, timeout time.Duration, condition ConditionFunc)</p>\n<p>这个函数会以 interval 为间隔，不断去检查 condition 条件是否为真，如果为真则可以继续后续处理；如果指定了 timeout 参数，则该函数也可以只常识指定的时间。</p>\n<h5 id=\"4、PollUntil-方法和上面的类似，但是没有-timeout-参数，多了一个-stopCh-参数，如下所示：\"><a href=\"#4、PollUntil-方法和上面的类似，但是没有-timeout-参数，多了一个-stopCh-参数，如下所示：\" class=\"headerlink\" title=\"4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：\"></a>4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：</h5><p>PollUntil(interval time.Duration, condition ConditionFunc, stopCh &lt;-chan struct{}) error</p>\n<p>此外还有 PollImmediate 、 PollInfinite 和 PollImmediateInfinite 方法。</p>\n<h3 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h3><p>本篇文章主要讲了 k8s 中定时任务的实现与对应包（wait）中方法的使用。通过阅读 k8s 的源代码，可以发现 k8s 中许多功能的实现也都是我们需要在平时工作中用的，其大部分包的性能都是经过大规模考验的，通过使用其相关的工具包不仅能学到大量的编程技巧也能避免自己造轮子。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>k8s 中有许多优秀的包都可以在平时的开发中借鉴与使用，比如，任务的定时轮询、高可用的实现、日志处理、缓存使用等都是独立的包，可以直接引用。本篇文章会介绍 k8s 中定时任务的实现，k8s 中定时任务都是通过 wait 包实现的，wait 包在 k8s 的多个组件中都有用到，以下是 wait 包在 kubelet 中的几处使用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// kubelet 每5分钟一次从 apiserver 获取证书</span><br><span class=\"line\">\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration,   kubeDeps *kubelet.Dependencies, enableServer bool) &#123;</span><br><span class=\"line\">    // 持续监听 pod 的变化</span><br><span class=\"line\">    go wait.Until(func() &#123;</span><br><span class=\"line\">        k.Run(podCfg.Updates())</span><br><span class=\"line\">    &#125;, 0, wait.NeverStop)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>golang 中可以通过 time.Ticker 实现定时任务的执行，但在 k8s 中用了更原生的方式，使用 time.Timer 实现的。time.Ticker 和 time.Timer 的使用区别如下：</p>\n<ul>\n<li>ticker 只要定义完成，从此刻开始计时，不需要任何其他的操作，每隔固定时间都会自动触发。</li>\n<li>timer 定时器是到了固定时间后会执行一次，仅执行一次</li>\n<li>如果 timer 定时器要每隔间隔的时间执行，实现 ticker 的效果，使用 <code>func (t *Timer) Reset(d Duration) bool</code></li>\n</ul>\n<p>一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;fmt&quot;</span><br><span class=\"line\">\t&quot;sync&quot;</span><br><span class=\"line\">\t&quot;time&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tvar wg sync.WaitGroup</span><br><span class=\"line\"></span><br><span class=\"line\">\ttimer1 := time.NewTimer(2 * time.Second)</span><br><span class=\"line\">\tticker1 := time.NewTicker(2 * time.Second)</span><br><span class=\"line\"></span><br><span class=\"line\">\twg.Add(1)</span><br><span class=\"line\">\tgo func(t *time.Ticker) &#123;</span><br><span class=\"line\">\t\tdefer wg.Done()</span><br><span class=\"line\">\t\tfor &#123;</span><br><span class=\"line\">\t\t\t&lt;-t.C</span><br><span class=\"line\">\t\t\tfmt.Println(&quot;exec ticker&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;(ticker1)</span><br><span class=\"line\"></span><br><span class=\"line\">\twg.Add(1)</span><br><span class=\"line\">\tgo func(t *time.Timer) &#123;</span><br><span class=\"line\">\t\tdefer wg.Done()</span><br><span class=\"line\">\t\tfor &#123;</span><br><span class=\"line\">\t\t\t&lt;-t.C</span><br><span class=\"line\">\t\t\tfmt.Println(&quot;exec timer&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;))</span><br><span class=\"line\">\t\t\tt.Reset(2 * time.Second)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;(timer1)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\twg.Wait()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"一、wait-包中的核心代码\"><a href=\"#一、wait-包中的核心代码\" class=\"headerlink\" title=\"一、wait 包中的核心代码\"></a>一、wait 包中的核心代码</h3><p>核心代码（k8s.io/apimachinery/pkg/util/wait/wait.go）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh &lt;-chan struct&#123;&#125;) &#123;</span><br><span class=\"line\">\tvar t *time.Timer</span><br><span class=\"line\">\tvar sawTimeout bool</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tselect &#123;</span><br><span class=\"line\">\t\tcase &lt;-stopCh:</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\tdefault:</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tjitteredPeriod := period</span><br><span class=\"line\">\t\tif jitterFactor &gt; 0.0 &#123;</span><br><span class=\"line\">\t\t\tjitteredPeriod = Jitter(period, jitterFactor)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif !sliding &#123;</span><br><span class=\"line\">\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfunc() &#123;</span><br><span class=\"line\">\t\t\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t\t\tf()</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif sliding &#123;</span><br><span class=\"line\">\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tselect &#123;</span><br><span class=\"line\">\t\tcase &lt;-stopCh:</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\tcase &lt;-t.C:</span><br><span class=\"line\">\t\t\tsawTimeout = true</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">func resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer &#123;</span><br><span class=\"line\">    if t == nil &#123;</span><br><span class=\"line\">        return time.NewTimer(d)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if !t.Stop() &amp;&amp; !sawTimeout &#123;</span><br><span class=\"line\">        &lt;-t.C</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    t.Reset(d)</span><br><span class=\"line\">    return t</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>几个关键点的说明：</p>\n<ul>\n<li>1、如果 sliding 为 true，则在 f() 运行之后计算周期。如果为 false，那么 period 包含 f() 的执行时间。</li>\n<li>2、在 golang 中 select 没有优先级选择，为了避免额外执行 f(),在每次循环开始后会先判断 stopCh chan。</li>\n</ul>\n<p>k8s 中 wait 包其实是对 time.Timer 做了一层封装实现。</p>\n<h3 id=\"二、wait-包常用的方法\"><a href=\"#二、wait-包常用的方法\" class=\"headerlink\" title=\"二、wait 包常用的方法\"></a>二、wait 包常用的方法</h3><h5 id=\"1、定期执行一个函数，永不停止，可以使用-Forever-方法：\"><a href=\"#1、定期执行一个函数，永不停止，可以使用-Forever-方法：\" class=\"headerlink\" title=\"1、定期执行一个函数，永不停止，可以使用 Forever 方法：\"></a>1、定期执行一个函数，永不停止，可以使用 Forever 方法：</h5><p>func Forever(f func(), period time.Duration)</p>\n<h5 id=\"2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的-chan-即可，方法定义如下：\"><a href=\"#2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的-chan-即可，方法定义如下：\" class=\"headerlink\" title=\"2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：\"></a>2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：</h5><p>func Until(f func(), period time.Duration, stopCh &lt;-chan struct{})</p>\n<p>上面的第三个参数 stopCh 就是用于退出无限循环的标志，停止的时候我们 close 掉这个 chan 就可以了。</p>\n<h5 id=\"3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用-Poll-方法：\"><a href=\"#3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用-Poll-方法：\" class=\"headerlink\" title=\"3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：\"></a>3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：</h5><p>func Poll(interval, timeout time.Duration, condition ConditionFunc)</p>\n<p>这个函数会以 interval 为间隔，不断去检查 condition 条件是否为真，如果为真则可以继续后续处理；如果指定了 timeout 参数，则该函数也可以只常识指定的时间。</p>\n<h5 id=\"4、PollUntil-方法和上面的类似，但是没有-timeout-参数，多了一个-stopCh-参数，如下所示：\"><a href=\"#4、PollUntil-方法和上面的类似，但是没有-timeout-参数，多了一个-stopCh-参数，如下所示：\" class=\"headerlink\" title=\"4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：\"></a>4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：</h5><p>PollUntil(interval time.Duration, condition ConditionFunc, stopCh &lt;-chan struct{}) error</p>\n<p>此外还有 PollImmediate 、 PollInfinite 和 PollImmediateInfinite 方法。</p>\n<h3 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h3><p>本篇文章主要讲了 k8s 中定时任务的实现与对应包（wait）中方法的使用。通过阅读 k8s 的源代码，可以发现 k8s 中许多功能的实现也都是我们需要在平时工作中用的，其大部分包的性能都是经过大规模考验的，通过使用其相关的工具包不仅能学到大量的编程技巧也能避免自己造轮子。</p>\n"},{"title":"kubernetes 中 kubeconfig 的用法","date":"2019-01-09T11:28:30.000Z","type":"kubeconfig","_content":"\n用于配置集群访问信息的文件叫作 kubeconfig 文件，在开启了 TLS 的集群中，每次与集群交互时都需要身份认证，生产环境一般使用证书进行认证，其认证所需要的信息会放在 kubeconfig 文件中。此外，k8s 的组件都可以使用 kubeconfig 连接 apiserver，[client-go ](https://github.com/kubernetes/client-go/blob/master/examples/create-update-delete-deployment/main.go)、operator、helm 等其他组件也使用 kubeconfig 访问 apiserver。\n\n## 一、kubeconfig 配置文件的生成\n\nkubeconfig 的一个示例：\n```\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: xxx\n    server: https://xxx:6443\n  name: cluster1\n- cluster:\n    certificate-authority-data: xxx\n    server: https://xxx:6443\n  name: cluster2\ncontexts:\n- context:\n    cluster: cluster1\n    user: kubelet\n  name: cluster1-context\n- context:\n    cluster: cluster2\n    user: kubelet\n  name: cluster2-context\ncurrent-context: cluster1-context\nkind: Config\npreferences: {}\nusers:\n- name: kubelet\n  user:\n    client-certificate-data: xxx\n    client-key-data: xxx\n```\n\napiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。\n\n#### 1、clusters模块\ncluster中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构。\n\n可以使用 `kubectl config set-cluster` 添加或修改 cluster 条目。\n\n#### 2、users 模块\nuser 定义用于向 kubernetes 集群进行身份验证的客户端凭据。\n\n可用凭证有 `client-certificate、client-key、token 和 username/password`。 \n`username/password` 和 `token` 是二者只能选择一个，但 `client-certificate` 和 `client-key` 可以分别与它们组合。\n\n可以使用 `kubectl config set-credentials` 添加或者修改 user 条目。\n\n#### 3、contexts 模块\n\ncontext 定义了一个命名的`cluster、user、namespace`元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。\n\n三个都是可选的；\n仅使用 cluster、user、namespace 之一指定上下文，或指定`none`。 \n\n未指定的值或在加载的 kubeconfig 中没有相应条目的命名值将被替换为默认值。\n加载和合并 kubeconfig 文件的规则很简单，但有很多，具体可以查看[加载和合并kubeconfig规则](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/)。\n\n可以使用`kubectl config set-context`添加或修改上下文条目。\n\n#### 4、current-context 模块\n\ncurrent-context 是作为`cluster、user、namespace`元组的 key，\n当 kubectl 从该文件中加载配置的时候会被默认使用。\n\n可以在 kubectl 命令行里覆盖这些值，通过分别传入`--context=CONTEXT、--cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE`。\n以上示例中若不指定 context 则默认使用 cluster1-context。\n```\nkubectl  get node --kubeconfig=./kubeconfig --context=cluster2-context\n```\n可以使用 `kubectl config use-context` 更改 current-context。\n\n#### 5、kubectl 生成 kubeconfig 的示例\n\nkubectl 可以快速生成 kubeconfig，以下是一个示例：\n```\n$ kubectl config set-credentials myself --username=admin --password=secret\n$ kubectl config set-cluster local-server --server=http://localhost:8080\n$ kubectl config set-context default-context --cluster=local-server --user=myself\n$ kubectl config use-context cluster-context\n$ kubectl config set contexts.default-context.namespace the-right-prefix\n$ kubectl config view\n```\n\n若使用手写 kubeconfig 的方式，推荐一个工具 [kubeval](https://github.com/garethr/kubeval)，可以校验 kubernetes yaml 或 json 格式的配置文件是否正确。\n\n## 二、使用 kubeconfig 文件配置 kuebctl 跨集群认证\n\nkubectl 作为操作 k8s 的一个客户端工具，只要为 kubectl 提供连接 apiserver 的配置(kubeconfig)，kubectl 可以在任何地方操作该集群，当然，若 kubeconfig 文件中配置多个集群，kubectl 也可以轻松地在多个集群之间切换。\n\nkubectl 加载配置文件的顺序：\n1、kubectl 默认连接本机的 8080 端口\n2、从 $HOME/.kube 目录下查找文件名为 config 的文件\n3、通过设置环境变量 KUBECONFIG 或者通过设置去指定其它 kubeconfig 文件\n```\n# 设置 KUBECONFIG 的环境变量\nexport KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig\n# 指定 kubeconfig 文件\nkubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig\n# 使用不同的 context 在多个集群之间切换\nkubectl  get node --kubeconfig=./kubeconfig --context=cluster1-context\n```\n开篇的示例就是多集群认证方式配置的一种。\n\n参考：\nhttps://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\nhttps://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\n","source":"_posts/kubeconfig.md","raw":"---\ntitle: kubernetes 中 kubeconfig 的用法\ndate: 2019-01-09 19:28:30\ntags: \"kubeconfig\"\ntype: \"kubeconfig\"\n\n---\n\n用于配置集群访问信息的文件叫作 kubeconfig 文件，在开启了 TLS 的集群中，每次与集群交互时都需要身份认证，生产环境一般使用证书进行认证，其认证所需要的信息会放在 kubeconfig 文件中。此外，k8s 的组件都可以使用 kubeconfig 连接 apiserver，[client-go ](https://github.com/kubernetes/client-go/blob/master/examples/create-update-delete-deployment/main.go)、operator、helm 等其他组件也使用 kubeconfig 访问 apiserver。\n\n## 一、kubeconfig 配置文件的生成\n\nkubeconfig 的一个示例：\n```\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: xxx\n    server: https://xxx:6443\n  name: cluster1\n- cluster:\n    certificate-authority-data: xxx\n    server: https://xxx:6443\n  name: cluster2\ncontexts:\n- context:\n    cluster: cluster1\n    user: kubelet\n  name: cluster1-context\n- context:\n    cluster: cluster2\n    user: kubelet\n  name: cluster2-context\ncurrent-context: cluster1-context\nkind: Config\npreferences: {}\nusers:\n- name: kubelet\n  user:\n    client-certificate-data: xxx\n    client-key-data: xxx\n```\n\napiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。\n\n#### 1、clusters模块\ncluster中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构。\n\n可以使用 `kubectl config set-cluster` 添加或修改 cluster 条目。\n\n#### 2、users 模块\nuser 定义用于向 kubernetes 集群进行身份验证的客户端凭据。\n\n可用凭证有 `client-certificate、client-key、token 和 username/password`。 \n`username/password` 和 `token` 是二者只能选择一个，但 `client-certificate` 和 `client-key` 可以分别与它们组合。\n\n可以使用 `kubectl config set-credentials` 添加或者修改 user 条目。\n\n#### 3、contexts 模块\n\ncontext 定义了一个命名的`cluster、user、namespace`元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。\n\n三个都是可选的；\n仅使用 cluster、user、namespace 之一指定上下文，或指定`none`。 \n\n未指定的值或在加载的 kubeconfig 中没有相应条目的命名值将被替换为默认值。\n加载和合并 kubeconfig 文件的规则很简单，但有很多，具体可以查看[加载和合并kubeconfig规则](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/)。\n\n可以使用`kubectl config set-context`添加或修改上下文条目。\n\n#### 4、current-context 模块\n\ncurrent-context 是作为`cluster、user、namespace`元组的 key，\n当 kubectl 从该文件中加载配置的时候会被默认使用。\n\n可以在 kubectl 命令行里覆盖这些值，通过分别传入`--context=CONTEXT、--cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE`。\n以上示例中若不指定 context 则默认使用 cluster1-context。\n```\nkubectl  get node --kubeconfig=./kubeconfig --context=cluster2-context\n```\n可以使用 `kubectl config use-context` 更改 current-context。\n\n#### 5、kubectl 生成 kubeconfig 的示例\n\nkubectl 可以快速生成 kubeconfig，以下是一个示例：\n```\n$ kubectl config set-credentials myself --username=admin --password=secret\n$ kubectl config set-cluster local-server --server=http://localhost:8080\n$ kubectl config set-context default-context --cluster=local-server --user=myself\n$ kubectl config use-context cluster-context\n$ kubectl config set contexts.default-context.namespace the-right-prefix\n$ kubectl config view\n```\n\n若使用手写 kubeconfig 的方式，推荐一个工具 [kubeval](https://github.com/garethr/kubeval)，可以校验 kubernetes yaml 或 json 格式的配置文件是否正确。\n\n## 二、使用 kubeconfig 文件配置 kuebctl 跨集群认证\n\nkubectl 作为操作 k8s 的一个客户端工具，只要为 kubectl 提供连接 apiserver 的配置(kubeconfig)，kubectl 可以在任何地方操作该集群，当然，若 kubeconfig 文件中配置多个集群，kubectl 也可以轻松地在多个集群之间切换。\n\nkubectl 加载配置文件的顺序：\n1、kubectl 默认连接本机的 8080 端口\n2、从 $HOME/.kube 目录下查找文件名为 config 的文件\n3、通过设置环境变量 KUBECONFIG 或者通过设置去指定其它 kubeconfig 文件\n```\n# 设置 KUBECONFIG 的环境变量\nexport KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig\n# 指定 kubeconfig 文件\nkubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig\n# 使用不同的 context 在多个集群之间切换\nkubectl  get node --kubeconfig=./kubeconfig --context=cluster1-context\n```\n开篇的示例就是多集群认证方式配置的一种。\n\n参考：\nhttps://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\nhttps://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\n","slug":"kubeconfig","published":1,"updated":"2019-01-09T11:31:44.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5ggz000522dveh68gwf4","content":"<p>用于配置集群访问信息的文件叫作 kubeconfig 文件，在开启了 TLS 的集群中，每次与集群交互时都需要身份认证，生产环境一般使用证书进行认证，其认证所需要的信息会放在 kubeconfig 文件中。此外，k8s 的组件都可以使用 kubeconfig 连接 apiserver，<a href=\"https://github.com/kubernetes/client-go/blob/master/examples/create-update-delete-deployment/main.go\" target=\"_blank\" rel=\"noopener\">client-go </a>、operator、helm 等其他组件也使用 kubeconfig 访问 apiserver。</p>\n<h2 id=\"一、kubeconfig-配置文件的生成\"><a href=\"#一、kubeconfig-配置文件的生成\" class=\"headerlink\" title=\"一、kubeconfig 配置文件的生成\"></a>一、kubeconfig 配置文件的生成</h2><p>kubeconfig 的一个示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    certificate-authority-data: xxx</span><br><span class=\"line\">    server: https://xxx:6443</span><br><span class=\"line\">  name: cluster1</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    certificate-authority-data: xxx</span><br><span class=\"line\">    server: https://xxx:6443</span><br><span class=\"line\">  name: cluster2</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: cluster1</span><br><span class=\"line\">    user: kubelet</span><br><span class=\"line\">  name: cluster1-context</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: cluster2</span><br><span class=\"line\">    user: kubelet</span><br><span class=\"line\">  name: cluster2-context</span><br><span class=\"line\">current-context: cluster1-context</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">users:</span><br><span class=\"line\">- name: kubelet</span><br><span class=\"line\">  user:</span><br><span class=\"line\">    client-certificate-data: xxx</span><br><span class=\"line\">    client-key-data: xxx</span><br></pre></td></tr></table></figure></p>\n<p>apiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。</p>\n<h4 id=\"1、clusters模块\"><a href=\"#1、clusters模块\" class=\"headerlink\" title=\"1、clusters模块\"></a>1、clusters模块</h4><p>cluster中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构。</p>\n<p>可以使用 <code>kubectl config set-cluster</code> 添加或修改 cluster 条目。</p>\n<h4 id=\"2、users-模块\"><a href=\"#2、users-模块\" class=\"headerlink\" title=\"2、users 模块\"></a>2、users 模块</h4><p>user 定义用于向 kubernetes 集群进行身份验证的客户端凭据。</p>\n<p>可用凭证有 <code>client-certificate、client-key、token 和 username/password</code>。<br><code>username/password</code> 和 <code>token</code> 是二者只能选择一个，但 <code>client-certificate</code> 和 <code>client-key</code> 可以分别与它们组合。</p>\n<p>可以使用 <code>kubectl config set-credentials</code> 添加或者修改 user 条目。</p>\n<h4 id=\"3、contexts-模块\"><a href=\"#3、contexts-模块\" class=\"headerlink\" title=\"3、contexts 模块\"></a>3、contexts 模块</h4><p>context 定义了一个命名的<code>cluster、user、namespace</code>元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。</p>\n<p>三个都是可选的；<br>仅使用 cluster、user、namespace 之一指定上下文，或指定<code>none</code>。 </p>\n<p>未指定的值或在加载的 kubeconfig 中没有相应条目的命名值将被替换为默认值。<br>加载和合并 kubeconfig 文件的规则很简单，但有很多，具体可以查看<a href=\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\" target=\"_blank\" rel=\"noopener\">加载和合并kubeconfig规则</a>。</p>\n<p>可以使用<code>kubectl config set-context</code>添加或修改上下文条目。</p>\n<h4 id=\"4、current-context-模块\"><a href=\"#4、current-context-模块\" class=\"headerlink\" title=\"4、current-context 模块\"></a>4、current-context 模块</h4><p>current-context 是作为<code>cluster、user、namespace</code>元组的 key，<br>当 kubectl 从该文件中加载配置的时候会被默认使用。</p>\n<p>可以在 kubectl 命令行里覆盖这些值，通过分别传入<code>--context=CONTEXT、--cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE</code>。<br>以上示例中若不指定 context 则默认使用 cluster1-context。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl  get node --kubeconfig=./kubeconfig --context=cluster2-context</span><br></pre></td></tr></table></figure></p>\n<p>可以使用 <code>kubectl config use-context</code> 更改 current-context。</p>\n<h4 id=\"5、kubectl-生成-kubeconfig-的示例\"><a href=\"#5、kubectl-生成-kubeconfig-的示例\" class=\"headerlink\" title=\"5、kubectl 生成 kubeconfig 的示例\"></a>5、kubectl 生成 kubeconfig 的示例</h4><p>kubectl 可以快速生成 kubeconfig，以下是一个示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl config set-credentials myself --username=admin --password=secret</span><br><span class=\"line\">$ kubectl config set-cluster local-server --server=http://localhost:8080</span><br><span class=\"line\">$ kubectl config set-context default-context --cluster=local-server --user=myself</span><br><span class=\"line\">$ kubectl config use-context cluster-context</span><br><span class=\"line\">$ kubectl config set contexts.default-context.namespace the-right-prefix</span><br><span class=\"line\">$ kubectl config view</span><br></pre></td></tr></table></figure></p>\n<p>若使用手写 kubeconfig 的方式，推荐一个工具 <a href=\"https://github.com/garethr/kubeval\" target=\"_blank\" rel=\"noopener\">kubeval</a>，可以校验 kubernetes yaml 或 json 格式的配置文件是否正确。</p>\n<h2 id=\"二、使用-kubeconfig-文件配置-kuebctl-跨集群认证\"><a href=\"#二、使用-kubeconfig-文件配置-kuebctl-跨集群认证\" class=\"headerlink\" title=\"二、使用 kubeconfig 文件配置 kuebctl 跨集群认证\"></a>二、使用 kubeconfig 文件配置 kuebctl 跨集群认证</h2><p>kubectl 作为操作 k8s 的一个客户端工具，只要为 kubectl 提供连接 apiserver 的配置(kubeconfig)，kubectl 可以在任何地方操作该集群，当然，若 kubeconfig 文件中配置多个集群，kubectl 也可以轻松地在多个集群之间切换。</p>\n<p>kubectl 加载配置文件的顺序：<br>1、kubectl 默认连接本机的 8080 端口<br>2、从 $HOME/.kube 目录下查找文件名为 config 的文件<br>3、通过设置环境变量 KUBECONFIG 或者通过设置去指定其它 kubeconfig 文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 设置 KUBECONFIG 的环境变量</span><br><span class=\"line\">export KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig</span><br><span class=\"line\"># 指定 kubeconfig 文件</span><br><span class=\"line\">kubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig</span><br><span class=\"line\"># 使用不同的 context 在多个集群之间切换</span><br><span class=\"line\">kubectl  get node --kubeconfig=./kubeconfig --context=cluster1-context</span><br></pre></td></tr></table></figure></p>\n<p>开篇的示例就是多集群认证方式配置的一种。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/</a><br><a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>用于配置集群访问信息的文件叫作 kubeconfig 文件，在开启了 TLS 的集群中，每次与集群交互时都需要身份认证，生产环境一般使用证书进行认证，其认证所需要的信息会放在 kubeconfig 文件中。此外，k8s 的组件都可以使用 kubeconfig 连接 apiserver，<a href=\"https://github.com/kubernetes/client-go/blob/master/examples/create-update-delete-deployment/main.go\" target=\"_blank\" rel=\"noopener\">client-go </a>、operator、helm 等其他组件也使用 kubeconfig 访问 apiserver。</p>\n<h2 id=\"一、kubeconfig-配置文件的生成\"><a href=\"#一、kubeconfig-配置文件的生成\" class=\"headerlink\" title=\"一、kubeconfig 配置文件的生成\"></a>一、kubeconfig 配置文件的生成</h2><p>kubeconfig 的一个示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    certificate-authority-data: xxx</span><br><span class=\"line\">    server: https://xxx:6443</span><br><span class=\"line\">  name: cluster1</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    certificate-authority-data: xxx</span><br><span class=\"line\">    server: https://xxx:6443</span><br><span class=\"line\">  name: cluster2</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: cluster1</span><br><span class=\"line\">    user: kubelet</span><br><span class=\"line\">  name: cluster1-context</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: cluster2</span><br><span class=\"line\">    user: kubelet</span><br><span class=\"line\">  name: cluster2-context</span><br><span class=\"line\">current-context: cluster1-context</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">users:</span><br><span class=\"line\">- name: kubelet</span><br><span class=\"line\">  user:</span><br><span class=\"line\">    client-certificate-data: xxx</span><br><span class=\"line\">    client-key-data: xxx</span><br></pre></td></tr></table></figure></p>\n<p>apiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。</p>\n<h4 id=\"1、clusters模块\"><a href=\"#1、clusters模块\" class=\"headerlink\" title=\"1、clusters模块\"></a>1、clusters模块</h4><p>cluster中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构。</p>\n<p>可以使用 <code>kubectl config set-cluster</code> 添加或修改 cluster 条目。</p>\n<h4 id=\"2、users-模块\"><a href=\"#2、users-模块\" class=\"headerlink\" title=\"2、users 模块\"></a>2、users 模块</h4><p>user 定义用于向 kubernetes 集群进行身份验证的客户端凭据。</p>\n<p>可用凭证有 <code>client-certificate、client-key、token 和 username/password</code>。<br><code>username/password</code> 和 <code>token</code> 是二者只能选择一个，但 <code>client-certificate</code> 和 <code>client-key</code> 可以分别与它们组合。</p>\n<p>可以使用 <code>kubectl config set-credentials</code> 添加或者修改 user 条目。</p>\n<h4 id=\"3、contexts-模块\"><a href=\"#3、contexts-模块\" class=\"headerlink\" title=\"3、contexts 模块\"></a>3、contexts 模块</h4><p>context 定义了一个命名的<code>cluster、user、namespace</code>元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。</p>\n<p>三个都是可选的；<br>仅使用 cluster、user、namespace 之一指定上下文，或指定<code>none</code>。 </p>\n<p>未指定的值或在加载的 kubeconfig 中没有相应条目的命名值将被替换为默认值。<br>加载和合并 kubeconfig 文件的规则很简单，但有很多，具体可以查看<a href=\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\" target=\"_blank\" rel=\"noopener\">加载和合并kubeconfig规则</a>。</p>\n<p>可以使用<code>kubectl config set-context</code>添加或修改上下文条目。</p>\n<h4 id=\"4、current-context-模块\"><a href=\"#4、current-context-模块\" class=\"headerlink\" title=\"4、current-context 模块\"></a>4、current-context 模块</h4><p>current-context 是作为<code>cluster、user、namespace</code>元组的 key，<br>当 kubectl 从该文件中加载配置的时候会被默认使用。</p>\n<p>可以在 kubectl 命令行里覆盖这些值，通过分别传入<code>--context=CONTEXT、--cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE</code>。<br>以上示例中若不指定 context 则默认使用 cluster1-context。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl  get node --kubeconfig=./kubeconfig --context=cluster2-context</span><br></pre></td></tr></table></figure></p>\n<p>可以使用 <code>kubectl config use-context</code> 更改 current-context。</p>\n<h4 id=\"5、kubectl-生成-kubeconfig-的示例\"><a href=\"#5、kubectl-生成-kubeconfig-的示例\" class=\"headerlink\" title=\"5、kubectl 生成 kubeconfig 的示例\"></a>5、kubectl 生成 kubeconfig 的示例</h4><p>kubectl 可以快速生成 kubeconfig，以下是一个示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl config set-credentials myself --username=admin --password=secret</span><br><span class=\"line\">$ kubectl config set-cluster local-server --server=http://localhost:8080</span><br><span class=\"line\">$ kubectl config set-context default-context --cluster=local-server --user=myself</span><br><span class=\"line\">$ kubectl config use-context cluster-context</span><br><span class=\"line\">$ kubectl config set contexts.default-context.namespace the-right-prefix</span><br><span class=\"line\">$ kubectl config view</span><br></pre></td></tr></table></figure></p>\n<p>若使用手写 kubeconfig 的方式，推荐一个工具 <a href=\"https://github.com/garethr/kubeval\" target=\"_blank\" rel=\"noopener\">kubeval</a>，可以校验 kubernetes yaml 或 json 格式的配置文件是否正确。</p>\n<h2 id=\"二、使用-kubeconfig-文件配置-kuebctl-跨集群认证\"><a href=\"#二、使用-kubeconfig-文件配置-kuebctl-跨集群认证\" class=\"headerlink\" title=\"二、使用 kubeconfig 文件配置 kuebctl 跨集群认证\"></a>二、使用 kubeconfig 文件配置 kuebctl 跨集群认证</h2><p>kubectl 作为操作 k8s 的一个客户端工具，只要为 kubectl 提供连接 apiserver 的配置(kubeconfig)，kubectl 可以在任何地方操作该集群，当然，若 kubeconfig 文件中配置多个集群，kubectl 也可以轻松地在多个集群之间切换。</p>\n<p>kubectl 加载配置文件的顺序：<br>1、kubectl 默认连接本机的 8080 端口<br>2、从 $HOME/.kube 目录下查找文件名为 config 的文件<br>3、通过设置环境变量 KUBECONFIG 或者通过设置去指定其它 kubeconfig 文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 设置 KUBECONFIG 的环境变量</span><br><span class=\"line\">export KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig</span><br><span class=\"line\"># 指定 kubeconfig 文件</span><br><span class=\"line\">kubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig</span><br><span class=\"line\"># 使用不同的 context 在多个集群之间切换</span><br><span class=\"line\">kubectl  get node --kubeconfig=./kubeconfig --context=cluster1-context</span><br></pre></td></tr></table></figure></p>\n<p>开篇的示例就是多集群认证方式配置的一种。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/</a><br><a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/</a></p>\n"},{"title":"kubernetes 常用 API","date":"2018-09-02T05:13:00.000Z","type":"kubernetes","_content":"\nkubectl  的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数  -v=9  可以看调用的相关 API，例：\n `$ kubectl get node -v=9` \n\n以下为 kubernetes 开发中常用的 API：\n![deployment 常用 API](https://upload-images.jianshu.io/upload_images/1262158-6fdb3babc9522929.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![statefulset 常用 API](https://upload-images.jianshu.io/upload_images/1262158-2562e12aaa019909.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![pod 常用 API](https://upload-images.jianshu.io/upload_images/1262158-d74728f38ba4361e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n![service 常用 API](https://upload-images.jianshu.io/upload_images/1262158-225076f08495b6d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![endpoints 常用 API](https://upload-images.jianshu.io/upload_images/1262158-71c815ad4fc45a65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![namespace 常用 API](https://upload-images.jianshu.io/upload_images/1262158-f79e0c4bb215bb40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![node 常用 API](https://upload-images.jianshu.io/upload_images/1262158-e67546fabc697d13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![pv 常用 API](https://upload-images.jianshu.io/upload_images/1262158-a0eca87df2960565.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n Markdown 表格显示过大，此仅以图片格式展示。\n\n","source":"_posts/kubernetes-api.md","raw":"---\ntitle: kubernetes 常用 API\ndate: 2018-09-02 13:13:00\ntype: \"kubernetes\"\n\n---\n\nkubectl  的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数  -v=9  可以看调用的相关 API，例：\n `$ kubectl get node -v=9` \n\n以下为 kubernetes 开发中常用的 API：\n![deployment 常用 API](https://upload-images.jianshu.io/upload_images/1262158-6fdb3babc9522929.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![statefulset 常用 API](https://upload-images.jianshu.io/upload_images/1262158-2562e12aaa019909.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![pod 常用 API](https://upload-images.jianshu.io/upload_images/1262158-d74728f38ba4361e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n![service 常用 API](https://upload-images.jianshu.io/upload_images/1262158-225076f08495b6d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![endpoints 常用 API](https://upload-images.jianshu.io/upload_images/1262158-71c815ad4fc45a65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![namespace 常用 API](https://upload-images.jianshu.io/upload_images/1262158-f79e0c4bb215bb40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![node 常用 API](https://upload-images.jianshu.io/upload_images/1262158-e67546fabc697d13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![pv 常用 API](https://upload-images.jianshu.io/upload_images/1262158-a0eca87df2960565.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n Markdown 表格显示过大，此仅以图片格式展示。\n\n","slug":"kubernetes-api","published":1,"updated":"2018-12-08T10:35:21.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5gh3000722dv7f1x6j7p","content":"<p>kubectl  的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数  -v=9  可以看调用的相关 API，例：<br> <code>$ kubectl get node -v=9</code> </p>\n<p>以下为 kubernetes 开发中常用的 API：<br><img src=\"https://upload-images.jianshu.io/upload_images/1262158-6fdb3babc9522929.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"deployment 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-2562e12aaa019909.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"statefulset 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-d74728f38ba4361e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"pod 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-225076f08495b6d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"service 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-71c815ad4fc45a65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"endpoints 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-f79e0c4bb215bb40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"namespace 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-e67546fabc697d13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"node 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-a0eca87df2960565.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"pv 常用 API\"></p>\n<p> Markdown 表格显示过大，此仅以图片格式展示。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>kubectl  的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数  -v=9  可以看调用的相关 API，例：<br> <code>$ kubectl get node -v=9</code> </p>\n<p>以下为 kubernetes 开发中常用的 API：<br><img src=\"https://upload-images.jianshu.io/upload_images/1262158-6fdb3babc9522929.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"deployment 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-2562e12aaa019909.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"statefulset 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-d74728f38ba4361e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"pod 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-225076f08495b6d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"service 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-71c815ad4fc45a65.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"endpoints 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-f79e0c4bb215bb40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"namespace 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-e67546fabc697d13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"node 常用 API\"></p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-a0eca87df2960565.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"pv 常用 API\"></p>\n<p> Markdown 表格显示过大，此仅以图片格式展示。</p>\n"},{"title":"etcd 启用 https","date":"2017-03-15T13:32:00.000Z","type":"etcd","_content":"* 1， 生成 TLS 秘钥对\n* 2，拷贝密钥对到所有节点\n* 3，配置 etcd 使用证书\n* 4，测试 etcd 是否正常\n* 5，配置 kube-apiserver 使用 CA 连接 etcd\n* 6，测试 kube-apiserver\n* 7，未解决的问题\n\nSSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。\n\n因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。\n\n[Generate self-signed certificates 官方参考文档](https://coreos.com/os/docs/latest/generate-self-signed-certificates.html)\n\n官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 [easy-rsa](https://github.com/OpenVPN/easy-rsa)。以下步骤遵循官方文档：\n\n## 1， 生成 TLS 秘钥对\n\n生成步骤：\n\n* 1，下载 cfssl\n* 2，初始化证书颁发机构\n* 3，配置 CA 选项\n* 4，生成服务器端证书\n* 5，生成对等证书\n* 6，生成客户端证书\n\n想深入了解 HTTPS 的看这里：\n\n* [聊聊HTTPS和SSL/TLS协议](http://www.techug.com/post/https-ssl-tls.html)\n* [数字证书CA及扫盲](http://blog.jobbole.com/104919/)\n* [互联网加密及OpenSSL介绍和简单使用](https://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8)\n* [SSL双向认证和单向认证的区别](http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html)\n\n##### 1，下载 cfssl\n\n    mkdir ~/bin\n    curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\n    curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\n    chmod +x ~/bin/{cfssl,cfssljson}\n    export PATH=$PATH:~/bin\n\n##### 2，初始化证书颁发机构\n\n```\nmkdir ~/cfssl\ncd ~/cfssl\ncfssl print-defaults config > ca-config.json\ncfssl print-defaults csr > ca-csr.json\n```\n\n证书类型介绍：\n\n* client certificate  用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。\n* server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。\n* peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。\n\n##### 3，配置 CA 选项\n\n```\n$ cat << EOF > ca-config.json\n\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"43800h\"\n        },\n        \"profiles\": {\n            \"server\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\"\n                ]\n            },\n            \"client\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"client auth\"\n                ]\n            },\n            \"peer\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ]\n            }\n        }\n    }\n}\n\n$ cat << EOF > ca-csr.json\n\n{\n    \"CN\": \"My own CA\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"US\",\n            \"L\": \"CA\",\n            \"O\": \"My Company Name\",\n            \"ST\": \"San Francisco\",\n            \"OU\": \"Org Unit 1\",\n            \"OU\": \"Org Unit 2\"\n        }\n    ]\n}\n\n生成 CA 证书：\n\n$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n\n将会生成以下几个文件：\n\nca-key.pem\nca.csr\nca.pem\n\n```\n> 请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。\n\n##### 4，生成服务器端证书\n```\n$ echo '{\"CN\":\"coreos1\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=\"10.93.81.17,127.0.0.1,server\" - | cfssljson -bare server\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\nserver-key.pem\nserver.csr\nserver.pem\n```\n\n##### 5，生成对等证书\n```\n$ echo '{\"CN\":\"member1\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=\"10.93.81.17,127.0.0.1,server,member1\" - | cfssljson -bare member1\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\n\nmember1-key.pem\nmember1.csr\nmember1.pem\n\n如果有多个 etcd 成员，重复此步为每个成员生成对等证书。\n\n```\n\n##### 6，生成客户端证书\n\n```\n$ echo '{\"CN\":\"client\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\n\nclient-key.pem\nclient.csr\nclient.pem\n\n```\n至此，所有证书都已生成完毕。\n\n\n## 2，拷贝密钥对到所有节点\n* 1，拷贝密钥对到所有节点\n* 2，更新系统证书库\n\n##### 1，拷贝密钥对到所有节点\n\n\n```\n$ mkdir -pv /etc/ssl/etcd/\n$ cp ~/cfssl/* /etc/ssl/etcd/\n$ chown -R etcd:etcd /etc/ssl/etcd\n$ chmod 600 /etc/ssl/etcd/*-key.pem\n$ cp ~/cfssl/ca.pem /etc/ssl/certs/\n```\n\n##### 2，更新系统证书库\n\n```\n$ yum install ca-certificates -y\n     \n$ update-ca-trust\n        \n```\n\n## 3，配置 etcd 使用证书\n\n```\n$ etcdctl version\netcdctl version: 3.1.3\nAPI version: 3.1\n\n$ cat  /etc/etcd/etcd.conf\n\nETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\n#监听URL，用于与其他节点通讯\nETCD_LISTEN_PEER_URLS=\"https://10.93.81.17:2380\"\n\n#告知客户端的URL, 也就是服务的URL\nETCD_LISTEN_CLIENT_URLS=\"https://10.93.81.17:2379,https://10.93.81.17:4001\"\n\n#表示监听其他节点同步信号的地址\nETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://10.93.81.17:2380\"\n\n#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求\nETCD_ADVERTISE_CLIENT_URLS=\"https://10.93.81.17:2379\"\n\n#启动参数配置\nETCD_NAME=\"node1\"\nETCD_INITIAL_CLUSTER=\"node1=https://10.93.81.17:2380\"\nETCD_INITIAL_CLUSTER_STATE=\"new\"\n\n#[security]\n\nETCD_CERT_FILE=\"/etc/ssl/etcd/server.pem\"\nETCD_KEY_FILE=\"/etc/ssl/etcd/server-key.pem\"\nETCD_TRUSTED_CA_FILE=\"/etc/ssl/etcd/ca.pem\"\nETCD_CLIENT_CERT_AUTH=\"true\"\nETCD_PEER_CERT_FILE=\"/etc/ssl/etcd/member1.pem\"\nETCD_PEER_KEY_FILE=\"/etc/ssl/etcd/member1-key.pem\"\nETCD_PEER_TRUSTED_CA_FILE=\"/etc/ssl/etcd/ca.pem\"\nETCD_PEER_CLIENT_CERT_AUTH=\"true\"\n#[logging]\nETCD_DEBUG=\"true\"\nETCD_LOG_PACKAGE_LEVELS=\"etcdserver=WARNING,security=DEBUG\"\n```\n\n\n## 4，测试 etcd 是否正常\n\n```\n$ systemctl restart  etcd\n\n如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。\n\n$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health\n{\"health\": \"true\"}\n\n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list\n     \n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar  \"hello world\"\n     \n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar\n```\n\n## 5，配置 kube-apiserver 使用 CA 连接 etcd\n\n```\n$ cp /etc/ssl/etcd/*  /var/run/kubernetes/\n    \n$ chown  -R kube.kube /var/run/kubernetes/\n\n在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：\n\n--cert-dir='/var/run/kubernetes/' --etcd-cafile='/var/run/kubernetes/ca.pem' --etcd-certfile='/var/run/kubernetes/client.pem' --etcd-keyfile='/var/run/kubernetes/client-key.pem'\n\n\n```\n\n## 6，测试 kube-apiserver \n\n```\n$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy\n\n$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy\n\n$ kubectl get node\n\n$ kubectl get cs\nNAME                 STATUS      MESSAGE                                                                   ERROR\nscheduler            Healthy     ok\ncontroller-manager   Healthy     ok\netcd-0               Unhealthy   Get https://10.93.81.17:2379/health: remote error: tls: bad certificate\n\n$ ./version.sh\netcdctl version: 3.1.3\nAPI version: 3.1\nKubernetes v1.6.0-beta.1\n\n```\n\n## 7，未解决的问题\n\n##### 1，使用  `kubectl get cs ` 查看会出现如上面所示的报错： \n```\netcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate\n```\n此问题有人提交 pr 但尚未被 merge，[etcd component status check should include credentials](https://github.com/kubernetes/kubernetes/pull/39716)\n\n##### 2，使用以下命令查看到的 2380 端口是未加密的\n```\n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list  \n\n2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated\n145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379\n```\n\n参考文档：\n\n* [kubernetes + etcd ssl 支持](https://www.addops.cn/post/tls-for-kubernetes-etcd.html)\n* [Security model](https://coreos.com/etcd/docs/latest/op-guide/security.html)\n* [Enabling HTTPS in an existing etcd cluster](https://coreos.com/etcd/docs/latest/etcd-live-http-to-https-migration.html)\n","source":"_posts/etcd-enable-https.md","raw":"---\ntitle: etcd 启用 https\ndate: 2017-03-15 21:32:00\ntype: \"etcd\"\n\n---\n* 1， 生成 TLS 秘钥对\n* 2，拷贝密钥对到所有节点\n* 3，配置 etcd 使用证书\n* 4，测试 etcd 是否正常\n* 5，配置 kube-apiserver 使用 CA 连接 etcd\n* 6，测试 kube-apiserver\n* 7，未解决的问题\n\nSSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。\n\n因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。\n\n[Generate self-signed certificates 官方参考文档](https://coreos.com/os/docs/latest/generate-self-signed-certificates.html)\n\n官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 [easy-rsa](https://github.com/OpenVPN/easy-rsa)。以下步骤遵循官方文档：\n\n## 1， 生成 TLS 秘钥对\n\n生成步骤：\n\n* 1，下载 cfssl\n* 2，初始化证书颁发机构\n* 3，配置 CA 选项\n* 4，生成服务器端证书\n* 5，生成对等证书\n* 6，生成客户端证书\n\n想深入了解 HTTPS 的看这里：\n\n* [聊聊HTTPS和SSL/TLS协议](http://www.techug.com/post/https-ssl-tls.html)\n* [数字证书CA及扫盲](http://blog.jobbole.com/104919/)\n* [互联网加密及OpenSSL介绍和简单使用](https://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8)\n* [SSL双向认证和单向认证的区别](http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html)\n\n##### 1，下载 cfssl\n\n    mkdir ~/bin\n    curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\n    curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\n    chmod +x ~/bin/{cfssl,cfssljson}\n    export PATH=$PATH:~/bin\n\n##### 2，初始化证书颁发机构\n\n```\nmkdir ~/cfssl\ncd ~/cfssl\ncfssl print-defaults config > ca-config.json\ncfssl print-defaults csr > ca-csr.json\n```\n\n证书类型介绍：\n\n* client certificate  用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。\n* server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。\n* peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。\n\n##### 3，配置 CA 选项\n\n```\n$ cat << EOF > ca-config.json\n\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"43800h\"\n        },\n        \"profiles\": {\n            \"server\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\"\n                ]\n            },\n            \"client\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"client auth\"\n                ]\n            },\n            \"peer\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ]\n            }\n        }\n    }\n}\n\n$ cat << EOF > ca-csr.json\n\n{\n    \"CN\": \"My own CA\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"US\",\n            \"L\": \"CA\",\n            \"O\": \"My Company Name\",\n            \"ST\": \"San Francisco\",\n            \"OU\": \"Org Unit 1\",\n            \"OU\": \"Org Unit 2\"\n        }\n    ]\n}\n\n生成 CA 证书：\n\n$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n\n将会生成以下几个文件：\n\nca-key.pem\nca.csr\nca.pem\n\n```\n> 请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。\n\n##### 4，生成服务器端证书\n```\n$ echo '{\"CN\":\"coreos1\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=\"10.93.81.17,127.0.0.1,server\" - | cfssljson -bare server\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\nserver-key.pem\nserver.csr\nserver.pem\n```\n\n##### 5，生成对等证书\n```\n$ echo '{\"CN\":\"member1\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=\"10.93.81.17,127.0.0.1,server,member1\" - | cfssljson -bare member1\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\n\nmember1-key.pem\nmember1.csr\nmember1.pem\n\n如果有多个 etcd 成员，重复此步为每个成员生成对等证书。\n\n```\n\n##### 6，生成客户端证书\n\n```\n$ echo '{\"CN\":\"client\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\n\nclient-key.pem\nclient.csr\nclient.pem\n\n```\n至此，所有证书都已生成完毕。\n\n\n## 2，拷贝密钥对到所有节点\n* 1，拷贝密钥对到所有节点\n* 2，更新系统证书库\n\n##### 1，拷贝密钥对到所有节点\n\n\n```\n$ mkdir -pv /etc/ssl/etcd/\n$ cp ~/cfssl/* /etc/ssl/etcd/\n$ chown -R etcd:etcd /etc/ssl/etcd\n$ chmod 600 /etc/ssl/etcd/*-key.pem\n$ cp ~/cfssl/ca.pem /etc/ssl/certs/\n```\n\n##### 2，更新系统证书库\n\n```\n$ yum install ca-certificates -y\n     \n$ update-ca-trust\n        \n```\n\n## 3，配置 etcd 使用证书\n\n```\n$ etcdctl version\netcdctl version: 3.1.3\nAPI version: 3.1\n\n$ cat  /etc/etcd/etcd.conf\n\nETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\n#监听URL，用于与其他节点通讯\nETCD_LISTEN_PEER_URLS=\"https://10.93.81.17:2380\"\n\n#告知客户端的URL, 也就是服务的URL\nETCD_LISTEN_CLIENT_URLS=\"https://10.93.81.17:2379,https://10.93.81.17:4001\"\n\n#表示监听其他节点同步信号的地址\nETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://10.93.81.17:2380\"\n\n#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求\nETCD_ADVERTISE_CLIENT_URLS=\"https://10.93.81.17:2379\"\n\n#启动参数配置\nETCD_NAME=\"node1\"\nETCD_INITIAL_CLUSTER=\"node1=https://10.93.81.17:2380\"\nETCD_INITIAL_CLUSTER_STATE=\"new\"\n\n#[security]\n\nETCD_CERT_FILE=\"/etc/ssl/etcd/server.pem\"\nETCD_KEY_FILE=\"/etc/ssl/etcd/server-key.pem\"\nETCD_TRUSTED_CA_FILE=\"/etc/ssl/etcd/ca.pem\"\nETCD_CLIENT_CERT_AUTH=\"true\"\nETCD_PEER_CERT_FILE=\"/etc/ssl/etcd/member1.pem\"\nETCD_PEER_KEY_FILE=\"/etc/ssl/etcd/member1-key.pem\"\nETCD_PEER_TRUSTED_CA_FILE=\"/etc/ssl/etcd/ca.pem\"\nETCD_PEER_CLIENT_CERT_AUTH=\"true\"\n#[logging]\nETCD_DEBUG=\"true\"\nETCD_LOG_PACKAGE_LEVELS=\"etcdserver=WARNING,security=DEBUG\"\n```\n\n\n## 4，测试 etcd 是否正常\n\n```\n$ systemctl restart  etcd\n\n如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。\n\n$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health\n{\"health\": \"true\"}\n\n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list\n     \n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar  \"hello world\"\n     \n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar\n```\n\n## 5，配置 kube-apiserver 使用 CA 连接 etcd\n\n```\n$ cp /etc/ssl/etcd/*  /var/run/kubernetes/\n    \n$ chown  -R kube.kube /var/run/kubernetes/\n\n在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：\n\n--cert-dir='/var/run/kubernetes/' --etcd-cafile='/var/run/kubernetes/ca.pem' --etcd-certfile='/var/run/kubernetes/client.pem' --etcd-keyfile='/var/run/kubernetes/client-key.pem'\n\n\n```\n\n## 6，测试 kube-apiserver \n\n```\n$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy\n\n$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy\n\n$ kubectl get node\n\n$ kubectl get cs\nNAME                 STATUS      MESSAGE                                                                   ERROR\nscheduler            Healthy     ok\ncontroller-manager   Healthy     ok\netcd-0               Unhealthy   Get https://10.93.81.17:2379/health: remote error: tls: bad certificate\n\n$ ./version.sh\netcdctl version: 3.1.3\nAPI version: 3.1\nKubernetes v1.6.0-beta.1\n\n```\n\n## 7，未解决的问题\n\n##### 1，使用  `kubectl get cs ` 查看会出现如上面所示的报错： \n```\netcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate\n```\n此问题有人提交 pr 但尚未被 merge，[etcd component status check should include credentials](https://github.com/kubernetes/kubernetes/pull/39716)\n\n##### 2，使用以下命令查看到的 2380 端口是未加密的\n```\n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list  \n\n2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated\n145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379\n```\n\n参考文档：\n\n* [kubernetes + etcd ssl 支持](https://www.addops.cn/post/tls-for-kubernetes-etcd.html)\n* [Security model](https://coreos.com/etcd/docs/latest/op-guide/security.html)\n* [Enabling HTTPS in an existing etcd cluster](https://coreos.com/etcd/docs/latest/etcd-live-http-to-https-migration.html)\n","slug":"etcd-enable-https","published":1,"updated":"2018-12-08T10:33:38.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5gnw000f22dvgo9z6jd3","content":"<ul>\n<li>1， 生成 TLS 秘钥对</li>\n<li>2，拷贝密钥对到所有节点</li>\n<li>3，配置 etcd 使用证书</li>\n<li>4，测试 etcd 是否正常</li>\n<li>5，配置 kube-apiserver 使用 CA 连接 etcd</li>\n<li>6，测试 kube-apiserver</li>\n<li>7，未解决的问题</li>\n</ul>\n<p>SSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。</p>\n<p>因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。</p>\n<p><a href=\"https://coreos.com/os/docs/latest/generate-self-signed-certificates.html\" target=\"_blank\" rel=\"noopener\">Generate self-signed certificates 官方参考文档</a></p>\n<p>官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 <a href=\"https://github.com/OpenVPN/easy-rsa\" target=\"_blank\" rel=\"noopener\">easy-rsa</a>。以下步骤遵循官方文档：</p>\n<h2 id=\"1，-生成-TLS-秘钥对\"><a href=\"#1，-生成-TLS-秘钥对\" class=\"headerlink\" title=\"1， 生成 TLS 秘钥对\"></a>1， 生成 TLS 秘钥对</h2><p>生成步骤：</p>\n<ul>\n<li>1，下载 cfssl</li>\n<li>2，初始化证书颁发机构</li>\n<li>3，配置 CA 选项</li>\n<li>4，生成服务器端证书</li>\n<li>5，生成对等证书</li>\n<li>6，生成客户端证书</li>\n</ul>\n<p>想深入了解 HTTPS 的看这里：</p>\n<ul>\n<li><a href=\"http://www.techug.com/post/https-ssl-tls.html\" target=\"_blank\" rel=\"noopener\">聊聊HTTPS和SSL/TLS协议</a></li>\n<li><a href=\"http://blog.jobbole.com/104919/\" target=\"_blank\" rel=\"noopener\">数字证书CA及扫盲</a></li>\n<li><a href=\"https://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8\" target=\"_blank\" rel=\"noopener\">互联网加密及OpenSSL介绍和简单使用</a></li>\n<li><a href=\"http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html\" target=\"_blank\" rel=\"noopener\">SSL双向认证和单向认证的区别</a></li>\n</ul>\n<h5 id=\"1，下载-cfssl\"><a href=\"#1，下载-cfssl\" class=\"headerlink\" title=\"1，下载 cfssl\"></a>1，下载 cfssl</h5><pre><code>mkdir ~/bin\ncurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x ~/bin/{cfssl,cfssljson}\nexport PATH=$PATH:~/bin\n</code></pre><h5 id=\"2，初始化证书颁发机构\"><a href=\"#2，初始化证书颁发机构\" class=\"headerlink\" title=\"2，初始化证书颁发机构\"></a>2，初始化证书颁发机构</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/cfssl</span><br><span class=\"line\">cd ~/cfssl</span><br><span class=\"line\">cfssl print-defaults config &gt; ca-config.json</span><br><span class=\"line\">cfssl print-defaults csr &gt; ca-csr.json</span><br></pre></td></tr></table></figure>\n<p>证书类型介绍：</p>\n<ul>\n<li>client certificate  用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。</li>\n<li>server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。</li>\n<li>peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。</li>\n</ul>\n<h5 id=\"3，配置-CA-选项\"><a href=\"#3，配置-CA-选项\" class=\"headerlink\" title=\"3，配置 CA 选项\"></a>3，配置 CA 选项</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-config.json</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;signing&quot;: &#123;</span><br><span class=\"line\">        &quot;default&quot;: &#123;</span><br><span class=\"line\">            &quot;expiry&quot;: &quot;43800h&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;profiles&quot;: &#123;</span><br><span class=\"line\">            &quot;server&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;server auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;client&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;client auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;peer&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;server auth&quot;,</span><br><span class=\"line\">                    &quot;client auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-csr.json</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;CN&quot;: &quot;My own CA&quot;,</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 2048</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;C&quot;: &quot;US&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;CA&quot;,</span><br><span class=\"line\">            &quot;O&quot;: &quot;My Company Name&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;San Francisco&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;Org Unit 1&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;Org Unit 2&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">生成 CA 证书：</span><br><span class=\"line\"></span><br><span class=\"line\">$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</span><br><span class=\"line\"></span><br><span class=\"line\">将会生成以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">ca-key.pem</span><br><span class=\"line\">ca.csr</span><br><span class=\"line\">ca.pem</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。</p>\n</blockquote>\n<h5 id=\"4，生成服务器端证书\"><a href=\"#4，生成服务器端证书\" class=\"headerlink\" title=\"4，生成服务器端证书\"></a>4，生成服务器端证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;coreos1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&quot;10.93.81.17,127.0.0.1,server&quot; - | cfssljson -bare server</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\">server-key.pem</span><br><span class=\"line\">server.csr</span><br><span class=\"line\">server.pem</span><br></pre></td></tr></table></figure>\n<h5 id=\"5，生成对等证书\"><a href=\"#5，生成对等证书\" class=\"headerlink\" title=\"5，生成对等证书\"></a>5，生成对等证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;member1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=&quot;10.93.81.17,127.0.0.1,server,member1&quot; - | cfssljson -bare member1</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">member1-key.pem</span><br><span class=\"line\">member1.csr</span><br><span class=\"line\">member1.pem</span><br><span class=\"line\"></span><br><span class=\"line\">如果有多个 etcd 成员，重复此步为每个成员生成对等证书。</span><br></pre></td></tr></table></figure>\n<h5 id=\"6，生成客户端证书\"><a href=\"#6，生成客户端证书\" class=\"headerlink\" title=\"6，生成客户端证书\"></a>6，生成客户端证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;client&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">client-key.pem</span><br><span class=\"line\">client.csr</span><br><span class=\"line\">client.pem</span><br></pre></td></tr></table></figure>\n<p>至此，所有证书都已生成完毕。</p>\n<h2 id=\"2，拷贝密钥对到所有节点\"><a href=\"#2，拷贝密钥对到所有节点\" class=\"headerlink\" title=\"2，拷贝密钥对到所有节点\"></a>2，拷贝密钥对到所有节点</h2><ul>\n<li>1，拷贝密钥对到所有节点</li>\n<li>2，更新系统证书库</li>\n</ul>\n<h5 id=\"1，拷贝密钥对到所有节点\"><a href=\"#1，拷贝密钥对到所有节点\" class=\"headerlink\" title=\"1，拷贝密钥对到所有节点\"></a>1，拷贝密钥对到所有节点</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -pv /etc/ssl/etcd/</span><br><span class=\"line\">$ cp ~/cfssl/* /etc/ssl/etcd/</span><br><span class=\"line\">$ chown -R etcd:etcd /etc/ssl/etcd</span><br><span class=\"line\">$ chmod 600 /etc/ssl/etcd/*-key.pem</span><br><span class=\"line\">$ cp ~/cfssl/ca.pem /etc/ssl/certs/</span><br></pre></td></tr></table></figure>\n<h5 id=\"2，更新系统证书库\"><a href=\"#2，更新系统证书库\" class=\"headerlink\" title=\"2，更新系统证书库\"></a>2，更新系统证书库</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yum install ca-certificates -y</span><br><span class=\"line\">     </span><br><span class=\"line\">$ update-ca-trust</span><br></pre></td></tr></table></figure>\n<h2 id=\"3，配置-etcd-使用证书\"><a href=\"#3，配置-etcd-使用证书\" class=\"headerlink\" title=\"3，配置 etcd 使用证书\"></a>3，配置 etcd 使用证书</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcdctl version</span><br><span class=\"line\">etcdctl version: 3.1.3</span><br><span class=\"line\">API version: 3.1</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat  /etc/etcd/etcd.conf</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;</span><br><span class=\"line\">#监听URL，用于与其他节点通讯</span><br><span class=\"line\">ETCD_LISTEN_PEER_URLS=&quot;https://10.93.81.17:2380&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#告知客户端的URL, 也就是服务的URL</span><br><span class=\"line\">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.93.81.17:2379,https://10.93.81.17:4001&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#表示监听其他节点同步信号的地址</span><br><span class=\"line\">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.93.81.17:2380&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求</span><br><span class=\"line\">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.93.81.17:2379&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#启动参数配置</span><br><span class=\"line\">ETCD_NAME=&quot;node1&quot;</span><br><span class=\"line\">ETCD_INITIAL_CLUSTER=&quot;node1=https://10.93.81.17:2380&quot;</span><br><span class=\"line\">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#[security]</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_CERT_FILE=&quot;/etc/ssl/etcd/server.pem&quot;</span><br><span class=\"line\">ETCD_KEY_FILE=&quot;/etc/ssl/etcd/server-key.pem&quot;</span><br><span class=\"line\">ETCD_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;</span><br><span class=\"line\">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class=\"line\">ETCD_PEER_CERT_FILE=&quot;/etc/ssl/etcd/member1.pem&quot;</span><br><span class=\"line\">ETCD_PEER_KEY_FILE=&quot;/etc/ssl/etcd/member1-key.pem&quot;</span><br><span class=\"line\">ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;</span><br><span class=\"line\">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class=\"line\">#[logging]</span><br><span class=\"line\">ETCD_DEBUG=&quot;true&quot;</span><br><span class=\"line\">ETCD_LOG_PACKAGE_LEVELS=&quot;etcdserver=WARNING,security=DEBUG&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4，测试-etcd-是否正常\"><a href=\"#4，测试-etcd-是否正常\" class=\"headerlink\" title=\"4，测试 etcd 是否正常\"></a>4，测试 etcd 是否正常</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart  etcd</span><br><span class=\"line\"></span><br><span class=\"line\">如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health</span><br><span class=\"line\">&#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list</span><br><span class=\"line\">     </span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar  &quot;hello world&quot;</span><br><span class=\"line\">     </span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar</span><br></pre></td></tr></table></figure>\n<h2 id=\"5，配置-kube-apiserver-使用-CA-连接-etcd\"><a href=\"#5，配置-kube-apiserver-使用-CA-连接-etcd\" class=\"headerlink\" title=\"5，配置 kube-apiserver 使用 CA 连接 etcd\"></a>5，配置 kube-apiserver 使用 CA 连接 etcd</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp /etc/ssl/etcd/*  /var/run/kubernetes/</span><br><span class=\"line\">    </span><br><span class=\"line\">$ chown  -R kube.kube /var/run/kubernetes/</span><br><span class=\"line\"></span><br><span class=\"line\">在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：</span><br><span class=\"line\"></span><br><span class=\"line\">--cert-dir=&apos;/var/run/kubernetes/&apos; --etcd-cafile=&apos;/var/run/kubernetes/ca.pem&apos; --etcd-certfile=&apos;/var/run/kubernetes/client.pem&apos; --etcd-keyfile=&apos;/var/run/kubernetes/client-key.pem&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"6，测试-kube-apiserver\"><a href=\"#6，测试-kube-apiserver\" class=\"headerlink\" title=\"6，测试 kube-apiserver\"></a>6，测试 kube-apiserver</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get node</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get cs</span><br><span class=\"line\">NAME                 STATUS      MESSAGE                                                                   ERROR</span><br><span class=\"line\">scheduler            Healthy     ok</span><br><span class=\"line\">controller-manager   Healthy     ok</span><br><span class=\"line\">etcd-0               Unhealthy   Get https://10.93.81.17:2379/health: remote error: tls: bad certificate</span><br><span class=\"line\"></span><br><span class=\"line\">$ ./version.sh</span><br><span class=\"line\">etcdctl version: 3.1.3</span><br><span class=\"line\">API version: 3.1</span><br><span class=\"line\">Kubernetes v1.6.0-beta.1</span><br></pre></td></tr></table></figure>\n<h2 id=\"7，未解决的问题\"><a href=\"#7，未解决的问题\" class=\"headerlink\" title=\"7，未解决的问题\"></a>7，未解决的问题</h2><h5 id=\"1，使用-kubectl-get-cs-查看会出现如上面所示的报错：\"><a href=\"#1，使用-kubectl-get-cs-查看会出现如上面所示的报错：\" class=\"headerlink\" title=\"1，使用  kubectl get cs 查看会出现如上面所示的报错：\"></a>1，使用  <code>kubectl get cs</code> 查看会出现如上面所示的报错：</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">etcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate</span><br></pre></td></tr></table></figure>\n<p>此问题有人提交 pr 但尚未被 merge，<a href=\"https://github.com/kubernetes/kubernetes/pull/39716\" target=\"_blank\" rel=\"noopener\">etcd component status check should include credentials</a></p>\n<h5 id=\"2，使用以下命令查看到的-2380-端口是未加密的\"><a href=\"#2，使用以下命令查看到的-2380-端口是未加密的\" class=\"headerlink\" title=\"2，使用以下命令查看到的 2380 端口是未加密的\"></a>2，使用以下命令查看到的 2380 端口是未加密的</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list  </span><br><span class=\"line\"></span><br><span class=\"line\">2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated</span><br><span class=\"line\">145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379</span><br></pre></td></tr></table></figure>\n<p>参考文档：</p>\n<ul>\n<li><a href=\"https://www.addops.cn/post/tls-for-kubernetes-etcd.html\" target=\"_blank\" rel=\"noopener\">kubernetes + etcd ssl 支持</a></li>\n<li><a href=\"https://coreos.com/etcd/docs/latest/op-guide/security.html\" target=\"_blank\" rel=\"noopener\">Security model</a></li>\n<li><a href=\"https://coreos.com/etcd/docs/latest/etcd-live-http-to-https-migration.html\" target=\"_blank\" rel=\"noopener\">Enabling HTTPS in an existing etcd cluster</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li>1， 生成 TLS 秘钥对</li>\n<li>2，拷贝密钥对到所有节点</li>\n<li>3，配置 etcd 使用证书</li>\n<li>4，测试 etcd 是否正常</li>\n<li>5，配置 kube-apiserver 使用 CA 连接 etcd</li>\n<li>6，测试 kube-apiserver</li>\n<li>7，未解决的问题</li>\n</ul>\n<p>SSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。</p>\n<p>因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。</p>\n<p><a href=\"https://coreos.com/os/docs/latest/generate-self-signed-certificates.html\" target=\"_blank\" rel=\"noopener\">Generate self-signed certificates 官方参考文档</a></p>\n<p>官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 <a href=\"https://github.com/OpenVPN/easy-rsa\" target=\"_blank\" rel=\"noopener\">easy-rsa</a>。以下步骤遵循官方文档：</p>\n<h2 id=\"1，-生成-TLS-秘钥对\"><a href=\"#1，-生成-TLS-秘钥对\" class=\"headerlink\" title=\"1， 生成 TLS 秘钥对\"></a>1， 生成 TLS 秘钥对</h2><p>生成步骤：</p>\n<ul>\n<li>1，下载 cfssl</li>\n<li>2，初始化证书颁发机构</li>\n<li>3，配置 CA 选项</li>\n<li>4，生成服务器端证书</li>\n<li>5，生成对等证书</li>\n<li>6，生成客户端证书</li>\n</ul>\n<p>想深入了解 HTTPS 的看这里：</p>\n<ul>\n<li><a href=\"http://www.techug.com/post/https-ssl-tls.html\" target=\"_blank\" rel=\"noopener\">聊聊HTTPS和SSL/TLS协议</a></li>\n<li><a href=\"http://blog.jobbole.com/104919/\" target=\"_blank\" rel=\"noopener\">数字证书CA及扫盲</a></li>\n<li><a href=\"https://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8\" target=\"_blank\" rel=\"noopener\">互联网加密及OpenSSL介绍和简单使用</a></li>\n<li><a href=\"http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html\" target=\"_blank\" rel=\"noopener\">SSL双向认证和单向认证的区别</a></li>\n</ul>\n<h5 id=\"1，下载-cfssl\"><a href=\"#1，下载-cfssl\" class=\"headerlink\" title=\"1，下载 cfssl\"></a>1，下载 cfssl</h5><pre><code>mkdir ~/bin\ncurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x ~/bin/{cfssl,cfssljson}\nexport PATH=$PATH:~/bin\n</code></pre><h5 id=\"2，初始化证书颁发机构\"><a href=\"#2，初始化证书颁发机构\" class=\"headerlink\" title=\"2，初始化证书颁发机构\"></a>2，初始化证书颁发机构</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/cfssl</span><br><span class=\"line\">cd ~/cfssl</span><br><span class=\"line\">cfssl print-defaults config &gt; ca-config.json</span><br><span class=\"line\">cfssl print-defaults csr &gt; ca-csr.json</span><br></pre></td></tr></table></figure>\n<p>证书类型介绍：</p>\n<ul>\n<li>client certificate  用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。</li>\n<li>server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。</li>\n<li>peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。</li>\n</ul>\n<h5 id=\"3，配置-CA-选项\"><a href=\"#3，配置-CA-选项\" class=\"headerlink\" title=\"3，配置 CA 选项\"></a>3，配置 CA 选项</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-config.json</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;signing&quot;: &#123;</span><br><span class=\"line\">        &quot;default&quot;: &#123;</span><br><span class=\"line\">            &quot;expiry&quot;: &quot;43800h&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;profiles&quot;: &#123;</span><br><span class=\"line\">            &quot;server&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;server auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;client&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;client auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;peer&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;server auth&quot;,</span><br><span class=\"line\">                    &quot;client auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-csr.json</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;CN&quot;: &quot;My own CA&quot;,</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 2048</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;C&quot;: &quot;US&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;CA&quot;,</span><br><span class=\"line\">            &quot;O&quot;: &quot;My Company Name&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;San Francisco&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;Org Unit 1&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;Org Unit 2&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">生成 CA 证书：</span><br><span class=\"line\"></span><br><span class=\"line\">$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</span><br><span class=\"line\"></span><br><span class=\"line\">将会生成以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">ca-key.pem</span><br><span class=\"line\">ca.csr</span><br><span class=\"line\">ca.pem</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。</p>\n</blockquote>\n<h5 id=\"4，生成服务器端证书\"><a href=\"#4，生成服务器端证书\" class=\"headerlink\" title=\"4，生成服务器端证书\"></a>4，生成服务器端证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;coreos1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&quot;10.93.81.17,127.0.0.1,server&quot; - | cfssljson -bare server</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\">server-key.pem</span><br><span class=\"line\">server.csr</span><br><span class=\"line\">server.pem</span><br></pre></td></tr></table></figure>\n<h5 id=\"5，生成对等证书\"><a href=\"#5，生成对等证书\" class=\"headerlink\" title=\"5，生成对等证书\"></a>5，生成对等证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;member1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=&quot;10.93.81.17,127.0.0.1,server,member1&quot; - | cfssljson -bare member1</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">member1-key.pem</span><br><span class=\"line\">member1.csr</span><br><span class=\"line\">member1.pem</span><br><span class=\"line\"></span><br><span class=\"line\">如果有多个 etcd 成员，重复此步为每个成员生成对等证书。</span><br></pre></td></tr></table></figure>\n<h5 id=\"6，生成客户端证书\"><a href=\"#6，生成客户端证书\" class=\"headerlink\" title=\"6，生成客户端证书\"></a>6，生成客户端证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;client&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">client-key.pem</span><br><span class=\"line\">client.csr</span><br><span class=\"line\">client.pem</span><br></pre></td></tr></table></figure>\n<p>至此，所有证书都已生成完毕。</p>\n<h2 id=\"2，拷贝密钥对到所有节点\"><a href=\"#2，拷贝密钥对到所有节点\" class=\"headerlink\" title=\"2，拷贝密钥对到所有节点\"></a>2，拷贝密钥对到所有节点</h2><ul>\n<li>1，拷贝密钥对到所有节点</li>\n<li>2，更新系统证书库</li>\n</ul>\n<h5 id=\"1，拷贝密钥对到所有节点\"><a href=\"#1，拷贝密钥对到所有节点\" class=\"headerlink\" title=\"1，拷贝密钥对到所有节点\"></a>1，拷贝密钥对到所有节点</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -pv /etc/ssl/etcd/</span><br><span class=\"line\">$ cp ~/cfssl/* /etc/ssl/etcd/</span><br><span class=\"line\">$ chown -R etcd:etcd /etc/ssl/etcd</span><br><span class=\"line\">$ chmod 600 /etc/ssl/etcd/*-key.pem</span><br><span class=\"line\">$ cp ~/cfssl/ca.pem /etc/ssl/certs/</span><br></pre></td></tr></table></figure>\n<h5 id=\"2，更新系统证书库\"><a href=\"#2，更新系统证书库\" class=\"headerlink\" title=\"2，更新系统证书库\"></a>2，更新系统证书库</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yum install ca-certificates -y</span><br><span class=\"line\">     </span><br><span class=\"line\">$ update-ca-trust</span><br></pre></td></tr></table></figure>\n<h2 id=\"3，配置-etcd-使用证书\"><a href=\"#3，配置-etcd-使用证书\" class=\"headerlink\" title=\"3，配置 etcd 使用证书\"></a>3，配置 etcd 使用证书</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcdctl version</span><br><span class=\"line\">etcdctl version: 3.1.3</span><br><span class=\"line\">API version: 3.1</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat  /etc/etcd/etcd.conf</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;</span><br><span class=\"line\">#监听URL，用于与其他节点通讯</span><br><span class=\"line\">ETCD_LISTEN_PEER_URLS=&quot;https://10.93.81.17:2380&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#告知客户端的URL, 也就是服务的URL</span><br><span class=\"line\">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.93.81.17:2379,https://10.93.81.17:4001&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#表示监听其他节点同步信号的地址</span><br><span class=\"line\">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.93.81.17:2380&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求</span><br><span class=\"line\">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.93.81.17:2379&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#启动参数配置</span><br><span class=\"line\">ETCD_NAME=&quot;node1&quot;</span><br><span class=\"line\">ETCD_INITIAL_CLUSTER=&quot;node1=https://10.93.81.17:2380&quot;</span><br><span class=\"line\">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#[security]</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_CERT_FILE=&quot;/etc/ssl/etcd/server.pem&quot;</span><br><span class=\"line\">ETCD_KEY_FILE=&quot;/etc/ssl/etcd/server-key.pem&quot;</span><br><span class=\"line\">ETCD_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;</span><br><span class=\"line\">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class=\"line\">ETCD_PEER_CERT_FILE=&quot;/etc/ssl/etcd/member1.pem&quot;</span><br><span class=\"line\">ETCD_PEER_KEY_FILE=&quot;/etc/ssl/etcd/member1-key.pem&quot;</span><br><span class=\"line\">ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;</span><br><span class=\"line\">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class=\"line\">#[logging]</span><br><span class=\"line\">ETCD_DEBUG=&quot;true&quot;</span><br><span class=\"line\">ETCD_LOG_PACKAGE_LEVELS=&quot;etcdserver=WARNING,security=DEBUG&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4，测试-etcd-是否正常\"><a href=\"#4，测试-etcd-是否正常\" class=\"headerlink\" title=\"4，测试 etcd 是否正常\"></a>4，测试 etcd 是否正常</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart  etcd</span><br><span class=\"line\"></span><br><span class=\"line\">如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health</span><br><span class=\"line\">&#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list</span><br><span class=\"line\">     </span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar  &quot;hello world&quot;</span><br><span class=\"line\">     </span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar</span><br></pre></td></tr></table></figure>\n<h2 id=\"5，配置-kube-apiserver-使用-CA-连接-etcd\"><a href=\"#5，配置-kube-apiserver-使用-CA-连接-etcd\" class=\"headerlink\" title=\"5，配置 kube-apiserver 使用 CA 连接 etcd\"></a>5，配置 kube-apiserver 使用 CA 连接 etcd</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp /etc/ssl/etcd/*  /var/run/kubernetes/</span><br><span class=\"line\">    </span><br><span class=\"line\">$ chown  -R kube.kube /var/run/kubernetes/</span><br><span class=\"line\"></span><br><span class=\"line\">在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：</span><br><span class=\"line\"></span><br><span class=\"line\">--cert-dir=&apos;/var/run/kubernetes/&apos; --etcd-cafile=&apos;/var/run/kubernetes/ca.pem&apos; --etcd-certfile=&apos;/var/run/kubernetes/client.pem&apos; --etcd-keyfile=&apos;/var/run/kubernetes/client-key.pem&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"6，测试-kube-apiserver\"><a href=\"#6，测试-kube-apiserver\" class=\"headerlink\" title=\"6，测试 kube-apiserver\"></a>6，测试 kube-apiserver</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get node</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get cs</span><br><span class=\"line\">NAME                 STATUS      MESSAGE                                                                   ERROR</span><br><span class=\"line\">scheduler            Healthy     ok</span><br><span class=\"line\">controller-manager   Healthy     ok</span><br><span class=\"line\">etcd-0               Unhealthy   Get https://10.93.81.17:2379/health: remote error: tls: bad certificate</span><br><span class=\"line\"></span><br><span class=\"line\">$ ./version.sh</span><br><span class=\"line\">etcdctl version: 3.1.3</span><br><span class=\"line\">API version: 3.1</span><br><span class=\"line\">Kubernetes v1.6.0-beta.1</span><br></pre></td></tr></table></figure>\n<h2 id=\"7，未解决的问题\"><a href=\"#7，未解决的问题\" class=\"headerlink\" title=\"7，未解决的问题\"></a>7，未解决的问题</h2><h5 id=\"1，使用-kubectl-get-cs-查看会出现如上面所示的报错：\"><a href=\"#1，使用-kubectl-get-cs-查看会出现如上面所示的报错：\" class=\"headerlink\" title=\"1，使用  kubectl get cs 查看会出现如上面所示的报错：\"></a>1，使用  <code>kubectl get cs</code> 查看会出现如上面所示的报错：</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">etcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate</span><br></pre></td></tr></table></figure>\n<p>此问题有人提交 pr 但尚未被 merge，<a href=\"https://github.com/kubernetes/kubernetes/pull/39716\" target=\"_blank\" rel=\"noopener\">etcd component status check should include credentials</a></p>\n<h5 id=\"2，使用以下命令查看到的-2380-端口是未加密的\"><a href=\"#2，使用以下命令查看到的-2380-端口是未加密的\" class=\"headerlink\" title=\"2，使用以下命令查看到的 2380 端口是未加密的\"></a>2，使用以下命令查看到的 2380 端口是未加密的</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list  </span><br><span class=\"line\"></span><br><span class=\"line\">2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated</span><br><span class=\"line\">145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379</span><br></pre></td></tr></table></figure>\n<p>参考文档：</p>\n<ul>\n<li><a href=\"https://www.addops.cn/post/tls-for-kubernetes-etcd.html\" target=\"_blank\" rel=\"noopener\">kubernetes + etcd ssl 支持</a></li>\n<li><a href=\"https://coreos.com/etcd/docs/latest/op-guide/security.html\" target=\"_blank\" rel=\"noopener\">Security model</a></li>\n<li><a href=\"https://coreos.com/etcd/docs/latest/etcd-live-http-to-https-migration.html\" target=\"_blank\" rel=\"noopener\">Enabling HTTPS in an existing etcd cluster</a></li>\n</ul>\n"},{"title":"kubernetes 审计日志功能","date":"2019-01-30T08:26:30.000Z","type":"audit","_content":"审计日志可以记录所有对 apiserver 接口的调用，让我们能够非常清晰的知道集群到底发生了什么事情，通过记录的日志可以查到所发生的事件、操作的用户和时间。kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。\n\n> kubernetes feature-gates 中的功能 Alpha 版本默认为 false，到 Beta 版本时默认为 true，所以 v1.8 会默认启用审计日志的功能。\n\n\n### 一、审计日志的策略\n\n#### 1、日志记录阶段\n\nkube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有：\n\n- RequestReceived - apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。\n- ResponseStarted - 在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。\n- ResponseComplete - 当响应 body 发送完并且不再发送数据。\n- Panic - 当有 panic 发生时生成。\n\n也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成。\n\n#### 2、日志记录级别\n\n当前支持的日志记录级别有：\n\n- None - 不记录日志。\n- Metadata - 只记录 Request 的一些 metadata (例如 user, timestamp, resource, verb 等)，但不记录 Request 或 Response 的body。\n- Request - 记录 Request 的 metadata 和 body。\n- RequestResponse - 最全记录方式，会记录所有的 metadata、Request 和 Response 的 body。\n\n#### 3、日志记录策略\n\n在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。\n\n- 一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段\n- 不要记录所有的资源，不要记录一个资源的所有子资源\n- 系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录\n- 对一些认证信息（secerts、configmaps、token 等）的 body 不记录 \n\nk8s 审计日志的一个示例：\n\n```\n{\n  \"kind\": \"EventList\",\n  \"apiVersion\": \"audit.k8s.io/v1beta1\",\n  \"Items\": [\n    {\n      \"Level\": \"Request\",\n      \"AuditID\": \"793e7ae2-5ca7-4ad3-a632-19708d2f8265\",\n      \"Stage\": \"RequestReceived\",\n      \"RequestURI\": \"/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0\",\n      \"Verb\": \"get\",\n      \"User\": {\n        \"Username\": \"system:unsecured\",\n        \"UID\": \"\",\n        \"Groups\": [\n          \"system:masters\",\n          \"system:authenticated\"\n        ],\n        \"Extra\": null\n      },\n      \"ImpersonatedUser\": null,\n      \"SourceIPs\": [\n        \"192.168.1.11\"\n      ],\n      \"UserAgent\": \"kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler\",\n      \"ObjectRef\": {\n        \"Resource\": \"pods\",\n        \"Namespace\": \"default\",\n        \"Name\": \"test-pre-sf-de7cc-0\",\n        \"UID\": \"\",\n        \"APIGroup\": \"\",\n        \"APIVersion\": \"v1\",\n        \"ResourceVersion\": \"\",\n        \"Subresource\": \"\"\n      },\n      \"ResponseStatus\": null,\n      \"RequestObject\": null,\n      \"ResponseObject\": null,\n      \"RequestReceivedTimestamp\": \"2019-01-11T06:51:43.528703Z\",\n      \"StageTimestamp\": \"2019-01-11T06:51:43.528703Z\",\n      \"Annotations\": null\n    }\n    ]\n}\n```\n\n### 二、启用审计日志\n\n当前的审计日志支持两种收集方式：保存为日志文件和调用自定义的 webhook，在 v1.13 中还支持动态的 webhook。\n\n#### 1、将审计日志以 json 格式保存到本地文件\n\napiserver 配置文件的 KUBE_API_ARGS 中需要添加如下参数：\n```\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json\n```\n\n日志保存到本地后再通过 fluentd 等其他组件进行收集。\n还有其他几个选项可以指定保留审计日志文件的最大天数、文件的最大数量、文件的大小等。\n\n#### 2、将审计日志打到后端指定的 webhook\n\n```\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig\n```\n\nwebhook 配置文件实际上是一个 kubeconfig，apiserver 会将审计日志发送 到指定的 webhook 后，webhook 接收到日志后可以再分发到 kafka 或其他组件进行收集。\n\n`audit-webhook-kubeconfig` 示例：\n```\napiVersion: v1\nclusters:\n- cluster:\n    server: http://127.0.0.1:8081/audit/webhook\n  name: metric\ncontexts:\n- context:\n    cluster: metric\n    user: \"\"\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers: []\n```\n\n前面提到过，apiserver 的每一个请求会记录三个阶段的审计日志，但是在实际中并不是需要所有的审计日志，官方也说明了启用审计日志会增加 apiserver 对内存的使用量。\n\n> Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration.\n\n\n`audit-policy.yaml` 配置示例：\n\n```\napiVersion: audit.k8s.io/v1\nkind: Policy\n# ResponseStarted 阶段不记录\nomitStages:\n  - \"ResponseStarted\"\nrules:\n  # 记录用户对 pod 和 statefulset 的操作\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      resources: [\"pods\",\"pods/status\"]\n    - group: \"apps\"\n      resources: [\"statefulsets\",\"statefulsets/scale\"]\n  # kube-controller-manager、kube-scheduler 等已经认证过身份的请求不需要记录\n  - level: None\n    userGroups: [\"system:authenticated\"]\n    nonResourceURLs:\n    - \"/api*\"\n    - \"/version\"\n  # 对 config、secret、token 等认证信息不记录请求体和返回体\n  - level: Metadata\n    resources:\n    - group: \"\" # core API group\n      resources: [\"secrets\", \"configmaps\"]\n```\n\n官方提供两个参考示例：\n\n- [Use fluentd to collect and distribute audit events from log file](https://kubernetes.io/zh/docs/tasks/debug-application-cluster/audit/#%E6%97%A5%E5%BF%97%E9%80%89%E6%8B%A9%E5%99%A8%E7%A4%BA%E4%BE%8B)\n- [Use logstash to collect and distribute audit events from webhook backend](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend)\n\n#### 3、subresource 说明\n\n\nkubernetes 每个资源对象都有 subresource,通过调用 master 的 api 可以获取 kubernetes 中所有的 resource 以及对应的 subresource,比如 pod 有 logs、exec 等 subresource。\n\n\n![](https://upload-images.jianshu.io/upload_images/1262158-4450a01f65b3f76d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n```\n获取所有 resource（ 1.10 之后使用）：\n$ curl  127.0.0.1:8080/openapi/v2\n```\n\n参考：[https://kubernetes.io/docs/concepts/overview/kubernetes-api/](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)\n\n### 三、webhook 的一个简单示例\n\n```\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/emicklei/go-restful\"\n\t\"github.com/gosoon/glog\"\n\t\"k8s.io/apiserver/pkg/apis/audit\"\n)\n\nfunc main() {\n\t// NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter)\n\tcontainer := restful.NewContainer()\n\tws := new(restful.WebService)\n\tws.Path(\"/audit\").\n\t\tConsumes(restful.MIME_JSON).\n\t\tProduces(restful.MIME_JSON)\n\tws.Route(ws.POST(\"/webhook\").To(AuditWebhook))\n\n\t//WebService ws2被添加到container2中\n\tcontainer.Add(ws)\n\tserver := &http.Server{\n\t\tAddr:    \":8081\",\n\t\tHandler: container,\n\t}\n\t//go consumer()\n\tlog.Fatal(server.ListenAndServe())\n}\n\nfunc AuditWebhook(req *restful.Request, resp *restful.Response) {\n\tbody, err := ioutil.ReadAll(req.Request.Body)\n\tif err != nil {\n\t\tglog.Errorf(\"read body err is: %v\", err)\n\t}\n\tvar eventList audit.EventList\n\terr = json.Unmarshal(body, &eventList)\n\tif err != nil {\n\t\tglog.Errorf(\"unmarshal failed with:%v,body is :\\n\", err, string(body))\n\t\treturn\n\t}\n\tfor _, event := range eventList.Items {\n\t\tjsonBytes, err := json.Marshal(event)\n\t\tif err != nil {\n\t\t\tglog.Infof(\"marshal failed with:%v,event is \\n %+v\", err, event)\n\t\t}\n\t\t// 消费日志\n\t\tasyncProducer(string(jsonBytes))\n\t}\n\tresp.AddHeader(\"Content-Type\", \"application/json\")\n\tresp.WriteEntity(\"success\")\n}\n```\n\n> 完整代码请参考：[https://github.com/gosoon/k8s-audit-webhook](https://github.com/gosoon/k8s-audit-webhook)\n\n\n### 四、总结\n\n本文主要介绍了 kubernetes 的日志审计功能，kubernetes 最近也被爆出多个安全漏洞，安全问题是每个团队不可忽视的，kubernetes 虽然被多数公司用作私有云，但日志审计也是不可或缺的。\n\n\n----\n\n参考：\n[https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n[ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/)\n[阿里云 Kubernetes 审计日志方案](https://yq.aliyun.com/articles/686982?utm_content=g_1000040449)\n\n","source":"_posts/k8s-audit-webhook.md","raw":"---\ntitle: kubernetes 审计日志功能\ndate: 2019-01-30 16:26:30\ntags: [\"audit\",\"log\"]\ntype: \"audit\"\n\n---\n审计日志可以记录所有对 apiserver 接口的调用，让我们能够非常清晰的知道集群到底发生了什么事情，通过记录的日志可以查到所发生的事件、操作的用户和时间。kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。\n\n> kubernetes feature-gates 中的功能 Alpha 版本默认为 false，到 Beta 版本时默认为 true，所以 v1.8 会默认启用审计日志的功能。\n\n\n### 一、审计日志的策略\n\n#### 1、日志记录阶段\n\nkube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有：\n\n- RequestReceived - apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。\n- ResponseStarted - 在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。\n- ResponseComplete - 当响应 body 发送完并且不再发送数据。\n- Panic - 当有 panic 发生时生成。\n\n也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成。\n\n#### 2、日志记录级别\n\n当前支持的日志记录级别有：\n\n- None - 不记录日志。\n- Metadata - 只记录 Request 的一些 metadata (例如 user, timestamp, resource, verb 等)，但不记录 Request 或 Response 的body。\n- Request - 记录 Request 的 metadata 和 body。\n- RequestResponse - 最全记录方式，会记录所有的 metadata、Request 和 Response 的 body。\n\n#### 3、日志记录策略\n\n在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。\n\n- 一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段\n- 不要记录所有的资源，不要记录一个资源的所有子资源\n- 系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录\n- 对一些认证信息（secerts、configmaps、token 等）的 body 不记录 \n\nk8s 审计日志的一个示例：\n\n```\n{\n  \"kind\": \"EventList\",\n  \"apiVersion\": \"audit.k8s.io/v1beta1\",\n  \"Items\": [\n    {\n      \"Level\": \"Request\",\n      \"AuditID\": \"793e7ae2-5ca7-4ad3-a632-19708d2f8265\",\n      \"Stage\": \"RequestReceived\",\n      \"RequestURI\": \"/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0\",\n      \"Verb\": \"get\",\n      \"User\": {\n        \"Username\": \"system:unsecured\",\n        \"UID\": \"\",\n        \"Groups\": [\n          \"system:masters\",\n          \"system:authenticated\"\n        ],\n        \"Extra\": null\n      },\n      \"ImpersonatedUser\": null,\n      \"SourceIPs\": [\n        \"192.168.1.11\"\n      ],\n      \"UserAgent\": \"kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler\",\n      \"ObjectRef\": {\n        \"Resource\": \"pods\",\n        \"Namespace\": \"default\",\n        \"Name\": \"test-pre-sf-de7cc-0\",\n        \"UID\": \"\",\n        \"APIGroup\": \"\",\n        \"APIVersion\": \"v1\",\n        \"ResourceVersion\": \"\",\n        \"Subresource\": \"\"\n      },\n      \"ResponseStatus\": null,\n      \"RequestObject\": null,\n      \"ResponseObject\": null,\n      \"RequestReceivedTimestamp\": \"2019-01-11T06:51:43.528703Z\",\n      \"StageTimestamp\": \"2019-01-11T06:51:43.528703Z\",\n      \"Annotations\": null\n    }\n    ]\n}\n```\n\n### 二、启用审计日志\n\n当前的审计日志支持两种收集方式：保存为日志文件和调用自定义的 webhook，在 v1.13 中还支持动态的 webhook。\n\n#### 1、将审计日志以 json 格式保存到本地文件\n\napiserver 配置文件的 KUBE_API_ARGS 中需要添加如下参数：\n```\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json\n```\n\n日志保存到本地后再通过 fluentd 等其他组件进行收集。\n还有其他几个选项可以指定保留审计日志文件的最大天数、文件的最大数量、文件的大小等。\n\n#### 2、将审计日志打到后端指定的 webhook\n\n```\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig\n```\n\nwebhook 配置文件实际上是一个 kubeconfig，apiserver 会将审计日志发送 到指定的 webhook 后，webhook 接收到日志后可以再分发到 kafka 或其他组件进行收集。\n\n`audit-webhook-kubeconfig` 示例：\n```\napiVersion: v1\nclusters:\n- cluster:\n    server: http://127.0.0.1:8081/audit/webhook\n  name: metric\ncontexts:\n- context:\n    cluster: metric\n    user: \"\"\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers: []\n```\n\n前面提到过，apiserver 的每一个请求会记录三个阶段的审计日志，但是在实际中并不是需要所有的审计日志，官方也说明了启用审计日志会增加 apiserver 对内存的使用量。\n\n> Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration.\n\n\n`audit-policy.yaml` 配置示例：\n\n```\napiVersion: audit.k8s.io/v1\nkind: Policy\n# ResponseStarted 阶段不记录\nomitStages:\n  - \"ResponseStarted\"\nrules:\n  # 记录用户对 pod 和 statefulset 的操作\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      resources: [\"pods\",\"pods/status\"]\n    - group: \"apps\"\n      resources: [\"statefulsets\",\"statefulsets/scale\"]\n  # kube-controller-manager、kube-scheduler 等已经认证过身份的请求不需要记录\n  - level: None\n    userGroups: [\"system:authenticated\"]\n    nonResourceURLs:\n    - \"/api*\"\n    - \"/version\"\n  # 对 config、secret、token 等认证信息不记录请求体和返回体\n  - level: Metadata\n    resources:\n    - group: \"\" # core API group\n      resources: [\"secrets\", \"configmaps\"]\n```\n\n官方提供两个参考示例：\n\n- [Use fluentd to collect and distribute audit events from log file](https://kubernetes.io/zh/docs/tasks/debug-application-cluster/audit/#%E6%97%A5%E5%BF%97%E9%80%89%E6%8B%A9%E5%99%A8%E7%A4%BA%E4%BE%8B)\n- [Use logstash to collect and distribute audit events from webhook backend](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend)\n\n#### 3、subresource 说明\n\n\nkubernetes 每个资源对象都有 subresource,通过调用 master 的 api 可以获取 kubernetes 中所有的 resource 以及对应的 subresource,比如 pod 有 logs、exec 等 subresource。\n\n\n![](https://upload-images.jianshu.io/upload_images/1262158-4450a01f65b3f76d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n```\n获取所有 resource（ 1.10 之后使用）：\n$ curl  127.0.0.1:8080/openapi/v2\n```\n\n参考：[https://kubernetes.io/docs/concepts/overview/kubernetes-api/](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)\n\n### 三、webhook 的一个简单示例\n\n```\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/emicklei/go-restful\"\n\t\"github.com/gosoon/glog\"\n\t\"k8s.io/apiserver/pkg/apis/audit\"\n)\n\nfunc main() {\n\t// NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter)\n\tcontainer := restful.NewContainer()\n\tws := new(restful.WebService)\n\tws.Path(\"/audit\").\n\t\tConsumes(restful.MIME_JSON).\n\t\tProduces(restful.MIME_JSON)\n\tws.Route(ws.POST(\"/webhook\").To(AuditWebhook))\n\n\t//WebService ws2被添加到container2中\n\tcontainer.Add(ws)\n\tserver := &http.Server{\n\t\tAddr:    \":8081\",\n\t\tHandler: container,\n\t}\n\t//go consumer()\n\tlog.Fatal(server.ListenAndServe())\n}\n\nfunc AuditWebhook(req *restful.Request, resp *restful.Response) {\n\tbody, err := ioutil.ReadAll(req.Request.Body)\n\tif err != nil {\n\t\tglog.Errorf(\"read body err is: %v\", err)\n\t}\n\tvar eventList audit.EventList\n\terr = json.Unmarshal(body, &eventList)\n\tif err != nil {\n\t\tglog.Errorf(\"unmarshal failed with:%v,body is :\\n\", err, string(body))\n\t\treturn\n\t}\n\tfor _, event := range eventList.Items {\n\t\tjsonBytes, err := json.Marshal(event)\n\t\tif err != nil {\n\t\t\tglog.Infof(\"marshal failed with:%v,event is \\n %+v\", err, event)\n\t\t}\n\t\t// 消费日志\n\t\tasyncProducer(string(jsonBytes))\n\t}\n\tresp.AddHeader(\"Content-Type\", \"application/json\")\n\tresp.WriteEntity(\"success\")\n}\n```\n\n> 完整代码请参考：[https://github.com/gosoon/k8s-audit-webhook](https://github.com/gosoon/k8s-audit-webhook)\n\n\n### 四、总结\n\n本文主要介绍了 kubernetes 的日志审计功能，kubernetes 最近也被爆出多个安全漏洞，安全问题是每个团队不可忽视的，kubernetes 虽然被多数公司用作私有云，但日志审计也是不可或缺的。\n\n\n----\n\n参考：\n[https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n[ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/)\n[阿里云 Kubernetes 审计日志方案](https://yq.aliyun.com/articles/686982?utm_content=g_1000040449)\n\n","slug":"k8s-audit-webhook","published":1,"updated":"2019-01-30T08:30:15.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5go5000h22dvbdp4izwa","content":"<p>审计日志可以记录所有对 apiserver 接口的调用，让我们能够非常清晰的知道集群到底发生了什么事情，通过记录的日志可以查到所发生的事件、操作的用户和时间。kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。</p>\n<blockquote>\n<p>kubernetes feature-gates 中的功能 Alpha 版本默认为 false，到 Beta 版本时默认为 true，所以 v1.8 会默认启用审计日志的功能。</p>\n</blockquote>\n<h3 id=\"一、审计日志的策略\"><a href=\"#一、审计日志的策略\" class=\"headerlink\" title=\"一、审计日志的策略\"></a>一、审计日志的策略</h3><h4 id=\"1、日志记录阶段\"><a href=\"#1、日志记录阶段\" class=\"headerlink\" title=\"1、日志记录阶段\"></a>1、日志记录阶段</h4><p>kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有：</p>\n<ul>\n<li>RequestReceived - apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。</li>\n<li>ResponseStarted - 在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。</li>\n<li>ResponseComplete - 当响应 body 发送完并且不再发送数据。</li>\n<li>Panic - 当有 panic 发生时生成。</li>\n</ul>\n<p>也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成。</p>\n<h4 id=\"2、日志记录级别\"><a href=\"#2、日志记录级别\" class=\"headerlink\" title=\"2、日志记录级别\"></a>2、日志记录级别</h4><p>当前支持的日志记录级别有：</p>\n<ul>\n<li>None - 不记录日志。</li>\n<li>Metadata - 只记录 Request 的一些 metadata (例如 user, timestamp, resource, verb 等)，但不记录 Request 或 Response 的body。</li>\n<li>Request - 记录 Request 的 metadata 和 body。</li>\n<li>RequestResponse - 最全记录方式，会记录所有的 metadata、Request 和 Response 的 body。</li>\n</ul>\n<h4 id=\"3、日志记录策略\"><a href=\"#3、日志记录策略\" class=\"headerlink\" title=\"3、日志记录策略\"></a>3、日志记录策略</h4><p>在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。</p>\n<ul>\n<li>一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段</li>\n<li>不要记录所有的资源，不要记录一个资源的所有子资源</li>\n<li>系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录</li>\n<li>对一些认证信息（secerts、configmaps、token 等）的 body 不记录 </li>\n</ul>\n<p>k8s 审计日志的一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;kind&quot;: &quot;EventList&quot;,</span><br><span class=\"line\">  &quot;apiVersion&quot;: &quot;audit.k8s.io/v1beta1&quot;,</span><br><span class=\"line\">  &quot;Items&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;Level&quot;: &quot;Request&quot;,</span><br><span class=\"line\">      &quot;AuditID&quot;: &quot;793e7ae2-5ca7-4ad3-a632-19708d2f8265&quot;,</span><br><span class=\"line\">      &quot;Stage&quot;: &quot;RequestReceived&quot;,</span><br><span class=\"line\">      &quot;RequestURI&quot;: &quot;/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0&quot;,</span><br><span class=\"line\">      &quot;Verb&quot;: &quot;get&quot;,</span><br><span class=\"line\">      &quot;User&quot;: &#123;</span><br><span class=\"line\">        &quot;Username&quot;: &quot;system:unsecured&quot;,</span><br><span class=\"line\">        &quot;UID&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;Groups&quot;: [</span><br><span class=\"line\">          &quot;system:masters&quot;,</span><br><span class=\"line\">          &quot;system:authenticated&quot;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;Extra&quot;: null</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;ImpersonatedUser&quot;: null,</span><br><span class=\"line\">      &quot;SourceIPs&quot;: [</span><br><span class=\"line\">        &quot;192.168.1.11&quot;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;UserAgent&quot;: &quot;kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler&quot;,</span><br><span class=\"line\">      &quot;ObjectRef&quot;: &#123;</span><br><span class=\"line\">        &quot;Resource&quot;: &quot;pods&quot;,</span><br><span class=\"line\">        &quot;Namespace&quot;: &quot;default&quot;,</span><br><span class=\"line\">        &quot;Name&quot;: &quot;test-pre-sf-de7cc-0&quot;,</span><br><span class=\"line\">        &quot;UID&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;APIGroup&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;APIVersion&quot;: &quot;v1&quot;,</span><br><span class=\"line\">        &quot;ResourceVersion&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;Subresource&quot;: &quot;&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;ResponseStatus&quot;: null,</span><br><span class=\"line\">      &quot;RequestObject&quot;: null,</span><br><span class=\"line\">      &quot;ResponseObject&quot;: null,</span><br><span class=\"line\">      &quot;RequestReceivedTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;,</span><br><span class=\"line\">      &quot;StageTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;,</span><br><span class=\"line\">      &quot;Annotations&quot;: null</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"二、启用审计日志\"><a href=\"#二、启用审计日志\" class=\"headerlink\" title=\"二、启用审计日志\"></a>二、启用审计日志</h3><p>当前的审计日志支持两种收集方式：保存为日志文件和调用自定义的 webhook，在 v1.13 中还支持动态的 webhook。</p>\n<h4 id=\"1、将审计日志以-json-格式保存到本地文件\"><a href=\"#1、将审计日志以-json-格式保存到本地文件\" class=\"headerlink\" title=\"1、将审计日志以 json 格式保存到本地文件\"></a>1、将审计日志以 json 格式保存到本地文件</h4><p>apiserver 配置文件的 KUBE_API_ARGS 中需要添加如下参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json</span><br></pre></td></tr></table></figure></p>\n<p>日志保存到本地后再通过 fluentd 等其他组件进行收集。<br>还有其他几个选项可以指定保留审计日志文件的最大天数、文件的最大数量、文件的大小等。</p>\n<h4 id=\"2、将审计日志打到后端指定的-webhook\"><a href=\"#2、将审计日志打到后端指定的-webhook\" class=\"headerlink\" title=\"2、将审计日志打到后端指定的 webhook\"></a>2、将审计日志打到后端指定的 webhook</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig</span><br></pre></td></tr></table></figure>\n<p>webhook 配置文件实际上是一个 kubeconfig，apiserver 会将审计日志发送 到指定的 webhook 后，webhook 接收到日志后可以再分发到 kafka 或其他组件进行收集。</p>\n<p><code>audit-webhook-kubeconfig</code> 示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    server: http://127.0.0.1:8081/audit/webhook</span><br><span class=\"line\">  name: metric</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: metric</span><br><span class=\"line\">    user: &quot;&quot;</span><br><span class=\"line\">  name: default-context</span><br><span class=\"line\">current-context: default-context</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">users: []</span><br></pre></td></tr></table></figure></p>\n<p>前面提到过，apiserver 的每一个请求会记录三个阶段的审计日志，但是在实际中并不是需要所有的审计日志，官方也说明了启用审计日志会增加 apiserver 对内存的使用量。</p>\n<blockquote>\n<p>Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration.</p>\n</blockquote>\n<p><code>audit-policy.yaml</code> 配置示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: audit.k8s.io/v1</span><br><span class=\"line\">kind: Policy</span><br><span class=\"line\"># ResponseStarted 阶段不记录</span><br><span class=\"line\">omitStages:</span><br><span class=\"line\">  - &quot;ResponseStarted&quot;</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  # 记录用户对 pod 和 statefulset 的操作</span><br><span class=\"line\">  - level: RequestResponse</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">    - group: &quot;&quot;</span><br><span class=\"line\">      resources: [&quot;pods&quot;,&quot;pods/status&quot;]</span><br><span class=\"line\">    - group: &quot;apps&quot;</span><br><span class=\"line\">      resources: [&quot;statefulsets&quot;,&quot;statefulsets/scale&quot;]</span><br><span class=\"line\">  # kube-controller-manager、kube-scheduler 等已经认证过身份的请求不需要记录</span><br><span class=\"line\">  - level: None</span><br><span class=\"line\">    userGroups: [&quot;system:authenticated&quot;]</span><br><span class=\"line\">    nonResourceURLs:</span><br><span class=\"line\">    - &quot;/api*&quot;</span><br><span class=\"line\">    - &quot;/version&quot;</span><br><span class=\"line\">  # 对 config、secret、token 等认证信息不记录请求体和返回体</span><br><span class=\"line\">  - level: Metadata</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">    - group: &quot;&quot; # core API group</span><br><span class=\"line\">      resources: [&quot;secrets&quot;, &quot;configmaps&quot;]</span><br></pre></td></tr></table></figure>\n<p>官方提供两个参考示例：</p>\n<ul>\n<li><a href=\"https://kubernetes.io/zh/docs/tasks/debug-application-cluster/audit/#%E6%97%A5%E5%BF%97%E9%80%89%E6%8B%A9%E5%99%A8%E7%A4%BA%E4%BE%8B\" target=\"_blank\" rel=\"noopener\">Use fluentd to collect and distribute audit events from log file</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend\" target=\"_blank\" rel=\"noopener\">Use logstash to collect and distribute audit events from webhook backend</a></li>\n</ul>\n<h4 id=\"3、subresource-说明\"><a href=\"#3、subresource-说明\" class=\"headerlink\" title=\"3、subresource 说明\"></a>3、subresource 说明</h4><p>kubernetes 每个资源对象都有 subresource,通过调用 master 的 api 可以获取 kubernetes 中所有的 resource 以及对应的 subresource,比如 pod 有 logs、exec 等 subresource。</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-4450a01f65b3f76d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">获取所有 resource（ 1.10 之后使用）：</span><br><span class=\"line\">$ curl  127.0.0.1:8080/openapi/v2</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://kubernetes.io/docs/concepts/overview/kubernetes-api/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/overview/kubernetes-api/</a></p>\n<h3 id=\"三、webhook-的一个简单示例\"><a href=\"#三、webhook-的一个简单示例\" class=\"headerlink\" title=\"三、webhook 的一个简单示例\"></a>三、webhook 的一个简单示例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;encoding/json&quot;</span><br><span class=\"line\">\t&quot;io/ioutil&quot;</span><br><span class=\"line\">\t&quot;log&quot;</span><br><span class=\"line\">\t&quot;net/http&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&quot;github.com/emicklei/go-restful&quot;</span><br><span class=\"line\">\t&quot;github.com/gosoon/glog&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apiserver/pkg/apis/audit&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\t// NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter)</span><br><span class=\"line\">\tcontainer := restful.NewContainer()</span><br><span class=\"line\">\tws := new(restful.WebService)</span><br><span class=\"line\">\tws.Path(&quot;/audit&quot;).</span><br><span class=\"line\">\t\tConsumes(restful.MIME_JSON).</span><br><span class=\"line\">\t\tProduces(restful.MIME_JSON)</span><br><span class=\"line\">\tws.Route(ws.POST(&quot;/webhook&quot;).To(AuditWebhook))</span><br><span class=\"line\"></span><br><span class=\"line\">\t//WebService ws2被添加到container2中</span><br><span class=\"line\">\tcontainer.Add(ws)</span><br><span class=\"line\">\tserver := &amp;http.Server&#123;</span><br><span class=\"line\">\t\tAddr:    &quot;:8081&quot;,</span><br><span class=\"line\">\t\tHandler: container,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t//go consumer()</span><br><span class=\"line\">\tlog.Fatal(server.ListenAndServe())</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func AuditWebhook(req *restful.Request, resp *restful.Response) &#123;</span><br><span class=\"line\">\tbody, err := ioutil.ReadAll(req.Request.Body)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;read body err is: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tvar eventList audit.EventList</span><br><span class=\"line\">\terr = json.Unmarshal(body, &amp;eventList)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;unmarshal failed with:%v,body is :\\n&quot;, err, string(body))</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tfor _, event := range eventList.Items &#123;</span><br><span class=\"line\">\t\tjsonBytes, err := json.Marshal(event)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Infof(&quot;marshal failed with:%v,event is \\n %+v&quot;, err, event)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 消费日志</span><br><span class=\"line\">\t\tasyncProducer(string(jsonBytes))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tresp.AddHeader(&quot;Content-Type&quot;, &quot;application/json&quot;)</span><br><span class=\"line\">\tresp.WriteEntity(&quot;success&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>完整代码请参考：<a href=\"https://github.com/gosoon/k8s-audit-webhook\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/k8s-audit-webhook</a></p>\n</blockquote>\n<h3 id=\"四、总结\"><a href=\"#四、总结\" class=\"headerlink\" title=\"四、总结\"></a>四、总结</h3><p>本文主要介绍了 kubernetes 的日志审计功能，kubernetes 最近也被爆出多个安全漏洞，安全问题是每个团队不可忽视的，kubernetes 虽然被多数公司用作私有云，但日志审计也是不可或缺的。</p>\n<hr>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</a><br><a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/audit/\" target=\"_blank\" rel=\"noopener\">ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/</a><br><a href=\"https://yq.aliyun.com/articles/686982?utm_content=g_1000040449\" target=\"_blank\" rel=\"noopener\">阿里云 Kubernetes 审计日志方案</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>审计日志可以记录所有对 apiserver 接口的调用，让我们能够非常清晰的知道集群到底发生了什么事情，通过记录的日志可以查到所发生的事件、操作的用户和时间。kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。</p>\n<blockquote>\n<p>kubernetes feature-gates 中的功能 Alpha 版本默认为 false，到 Beta 版本时默认为 true，所以 v1.8 会默认启用审计日志的功能。</p>\n</blockquote>\n<h3 id=\"一、审计日志的策略\"><a href=\"#一、审计日志的策略\" class=\"headerlink\" title=\"一、审计日志的策略\"></a>一、审计日志的策略</h3><h4 id=\"1、日志记录阶段\"><a href=\"#1、日志记录阶段\" class=\"headerlink\" title=\"1、日志记录阶段\"></a>1、日志记录阶段</h4><p>kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有：</p>\n<ul>\n<li>RequestReceived - apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。</li>\n<li>ResponseStarted - 在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。</li>\n<li>ResponseComplete - 当响应 body 发送完并且不再发送数据。</li>\n<li>Panic - 当有 panic 发生时生成。</li>\n</ul>\n<p>也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成。</p>\n<h4 id=\"2、日志记录级别\"><a href=\"#2、日志记录级别\" class=\"headerlink\" title=\"2、日志记录级别\"></a>2、日志记录级别</h4><p>当前支持的日志记录级别有：</p>\n<ul>\n<li>None - 不记录日志。</li>\n<li>Metadata - 只记录 Request 的一些 metadata (例如 user, timestamp, resource, verb 等)，但不记录 Request 或 Response 的body。</li>\n<li>Request - 记录 Request 的 metadata 和 body。</li>\n<li>RequestResponse - 最全记录方式，会记录所有的 metadata、Request 和 Response 的 body。</li>\n</ul>\n<h4 id=\"3、日志记录策略\"><a href=\"#3、日志记录策略\" class=\"headerlink\" title=\"3、日志记录策略\"></a>3、日志记录策略</h4><p>在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。</p>\n<ul>\n<li>一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段</li>\n<li>不要记录所有的资源，不要记录一个资源的所有子资源</li>\n<li>系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录</li>\n<li>对一些认证信息（secerts、configmaps、token 等）的 body 不记录 </li>\n</ul>\n<p>k8s 审计日志的一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;kind&quot;: &quot;EventList&quot;,</span><br><span class=\"line\">  &quot;apiVersion&quot;: &quot;audit.k8s.io/v1beta1&quot;,</span><br><span class=\"line\">  &quot;Items&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;Level&quot;: &quot;Request&quot;,</span><br><span class=\"line\">      &quot;AuditID&quot;: &quot;793e7ae2-5ca7-4ad3-a632-19708d2f8265&quot;,</span><br><span class=\"line\">      &quot;Stage&quot;: &quot;RequestReceived&quot;,</span><br><span class=\"line\">      &quot;RequestURI&quot;: &quot;/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0&quot;,</span><br><span class=\"line\">      &quot;Verb&quot;: &quot;get&quot;,</span><br><span class=\"line\">      &quot;User&quot;: &#123;</span><br><span class=\"line\">        &quot;Username&quot;: &quot;system:unsecured&quot;,</span><br><span class=\"line\">        &quot;UID&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;Groups&quot;: [</span><br><span class=\"line\">          &quot;system:masters&quot;,</span><br><span class=\"line\">          &quot;system:authenticated&quot;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;Extra&quot;: null</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;ImpersonatedUser&quot;: null,</span><br><span class=\"line\">      &quot;SourceIPs&quot;: [</span><br><span class=\"line\">        &quot;192.168.1.11&quot;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;UserAgent&quot;: &quot;kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler&quot;,</span><br><span class=\"line\">      &quot;ObjectRef&quot;: &#123;</span><br><span class=\"line\">        &quot;Resource&quot;: &quot;pods&quot;,</span><br><span class=\"line\">        &quot;Namespace&quot;: &quot;default&quot;,</span><br><span class=\"line\">        &quot;Name&quot;: &quot;test-pre-sf-de7cc-0&quot;,</span><br><span class=\"line\">        &quot;UID&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;APIGroup&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;APIVersion&quot;: &quot;v1&quot;,</span><br><span class=\"line\">        &quot;ResourceVersion&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;Subresource&quot;: &quot;&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;ResponseStatus&quot;: null,</span><br><span class=\"line\">      &quot;RequestObject&quot;: null,</span><br><span class=\"line\">      &quot;ResponseObject&quot;: null,</span><br><span class=\"line\">      &quot;RequestReceivedTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;,</span><br><span class=\"line\">      &quot;StageTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;,</span><br><span class=\"line\">      &quot;Annotations&quot;: null</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"二、启用审计日志\"><a href=\"#二、启用审计日志\" class=\"headerlink\" title=\"二、启用审计日志\"></a>二、启用审计日志</h3><p>当前的审计日志支持两种收集方式：保存为日志文件和调用自定义的 webhook，在 v1.13 中还支持动态的 webhook。</p>\n<h4 id=\"1、将审计日志以-json-格式保存到本地文件\"><a href=\"#1、将审计日志以-json-格式保存到本地文件\" class=\"headerlink\" title=\"1、将审计日志以 json 格式保存到本地文件\"></a>1、将审计日志以 json 格式保存到本地文件</h4><p>apiserver 配置文件的 KUBE_API_ARGS 中需要添加如下参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json</span><br></pre></td></tr></table></figure></p>\n<p>日志保存到本地后再通过 fluentd 等其他组件进行收集。<br>还有其他几个选项可以指定保留审计日志文件的最大天数、文件的最大数量、文件的大小等。</p>\n<h4 id=\"2、将审计日志打到后端指定的-webhook\"><a href=\"#2、将审计日志打到后端指定的-webhook\" class=\"headerlink\" title=\"2、将审计日志打到后端指定的 webhook\"></a>2、将审计日志打到后端指定的 webhook</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig</span><br></pre></td></tr></table></figure>\n<p>webhook 配置文件实际上是一个 kubeconfig，apiserver 会将审计日志发送 到指定的 webhook 后，webhook 接收到日志后可以再分发到 kafka 或其他组件进行收集。</p>\n<p><code>audit-webhook-kubeconfig</code> 示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    server: http://127.0.0.1:8081/audit/webhook</span><br><span class=\"line\">  name: metric</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: metric</span><br><span class=\"line\">    user: &quot;&quot;</span><br><span class=\"line\">  name: default-context</span><br><span class=\"line\">current-context: default-context</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">users: []</span><br></pre></td></tr></table></figure></p>\n<p>前面提到过，apiserver 的每一个请求会记录三个阶段的审计日志，但是在实际中并不是需要所有的审计日志，官方也说明了启用审计日志会增加 apiserver 对内存的使用量。</p>\n<blockquote>\n<p>Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration.</p>\n</blockquote>\n<p><code>audit-policy.yaml</code> 配置示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: audit.k8s.io/v1</span><br><span class=\"line\">kind: Policy</span><br><span class=\"line\"># ResponseStarted 阶段不记录</span><br><span class=\"line\">omitStages:</span><br><span class=\"line\">  - &quot;ResponseStarted&quot;</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  # 记录用户对 pod 和 statefulset 的操作</span><br><span class=\"line\">  - level: RequestResponse</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">    - group: &quot;&quot;</span><br><span class=\"line\">      resources: [&quot;pods&quot;,&quot;pods/status&quot;]</span><br><span class=\"line\">    - group: &quot;apps&quot;</span><br><span class=\"line\">      resources: [&quot;statefulsets&quot;,&quot;statefulsets/scale&quot;]</span><br><span class=\"line\">  # kube-controller-manager、kube-scheduler 等已经认证过身份的请求不需要记录</span><br><span class=\"line\">  - level: None</span><br><span class=\"line\">    userGroups: [&quot;system:authenticated&quot;]</span><br><span class=\"line\">    nonResourceURLs:</span><br><span class=\"line\">    - &quot;/api*&quot;</span><br><span class=\"line\">    - &quot;/version&quot;</span><br><span class=\"line\">  # 对 config、secret、token 等认证信息不记录请求体和返回体</span><br><span class=\"line\">  - level: Metadata</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">    - group: &quot;&quot; # core API group</span><br><span class=\"line\">      resources: [&quot;secrets&quot;, &quot;configmaps&quot;]</span><br></pre></td></tr></table></figure>\n<p>官方提供两个参考示例：</p>\n<ul>\n<li><a href=\"https://kubernetes.io/zh/docs/tasks/debug-application-cluster/audit/#%E6%97%A5%E5%BF%97%E9%80%89%E6%8B%A9%E5%99%A8%E7%A4%BA%E4%BE%8B\" target=\"_blank\" rel=\"noopener\">Use fluentd to collect and distribute audit events from log file</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend\" target=\"_blank\" rel=\"noopener\">Use logstash to collect and distribute audit events from webhook backend</a></li>\n</ul>\n<h4 id=\"3、subresource-说明\"><a href=\"#3、subresource-说明\" class=\"headerlink\" title=\"3、subresource 说明\"></a>3、subresource 说明</h4><p>kubernetes 每个资源对象都有 subresource,通过调用 master 的 api 可以获取 kubernetes 中所有的 resource 以及对应的 subresource,比如 pod 有 logs、exec 等 subresource。</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-4450a01f65b3f76d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">获取所有 resource（ 1.10 之后使用）：</span><br><span class=\"line\">$ curl  127.0.0.1:8080/openapi/v2</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://kubernetes.io/docs/concepts/overview/kubernetes-api/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/overview/kubernetes-api/</a></p>\n<h3 id=\"三、webhook-的一个简单示例\"><a href=\"#三、webhook-的一个简单示例\" class=\"headerlink\" title=\"三、webhook 的一个简单示例\"></a>三、webhook 的一个简单示例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;encoding/json&quot;</span><br><span class=\"line\">\t&quot;io/ioutil&quot;</span><br><span class=\"line\">\t&quot;log&quot;</span><br><span class=\"line\">\t&quot;net/http&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&quot;github.com/emicklei/go-restful&quot;</span><br><span class=\"line\">\t&quot;github.com/gosoon/glog&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apiserver/pkg/apis/audit&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\t// NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter)</span><br><span class=\"line\">\tcontainer := restful.NewContainer()</span><br><span class=\"line\">\tws := new(restful.WebService)</span><br><span class=\"line\">\tws.Path(&quot;/audit&quot;).</span><br><span class=\"line\">\t\tConsumes(restful.MIME_JSON).</span><br><span class=\"line\">\t\tProduces(restful.MIME_JSON)</span><br><span class=\"line\">\tws.Route(ws.POST(&quot;/webhook&quot;).To(AuditWebhook))</span><br><span class=\"line\"></span><br><span class=\"line\">\t//WebService ws2被添加到container2中</span><br><span class=\"line\">\tcontainer.Add(ws)</span><br><span class=\"line\">\tserver := &amp;http.Server&#123;</span><br><span class=\"line\">\t\tAddr:    &quot;:8081&quot;,</span><br><span class=\"line\">\t\tHandler: container,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t//go consumer()</span><br><span class=\"line\">\tlog.Fatal(server.ListenAndServe())</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func AuditWebhook(req *restful.Request, resp *restful.Response) &#123;</span><br><span class=\"line\">\tbody, err := ioutil.ReadAll(req.Request.Body)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;read body err is: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tvar eventList audit.EventList</span><br><span class=\"line\">\terr = json.Unmarshal(body, &amp;eventList)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;unmarshal failed with:%v,body is :\\n&quot;, err, string(body))</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tfor _, event := range eventList.Items &#123;</span><br><span class=\"line\">\t\tjsonBytes, err := json.Marshal(event)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Infof(&quot;marshal failed with:%v,event is \\n %+v&quot;, err, event)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 消费日志</span><br><span class=\"line\">\t\tasyncProducer(string(jsonBytes))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tresp.AddHeader(&quot;Content-Type&quot;, &quot;application/json&quot;)</span><br><span class=\"line\">\tresp.WriteEntity(&quot;success&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>完整代码请参考：<a href=\"https://github.com/gosoon/k8s-audit-webhook\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/k8s-audit-webhook</a></p>\n</blockquote>\n<h3 id=\"四、总结\"><a href=\"#四、总结\" class=\"headerlink\" title=\"四、总结\"></a>四、总结</h3><p>本文主要介绍了 kubernetes 的日志审计功能，kubernetes 最近也被爆出多个安全漏洞，安全问题是每个团队不可忽视的，kubernetes 虽然被多数公司用作私有云，但日志审计也是不可或缺的。</p>\n<hr>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</a><br><a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/audit/\" target=\"_blank\" rel=\"noopener\">ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/</a><br><a href=\"https://yq.aliyun.com/articles/686982?utm_content=g_1000040449\" target=\"_blank\" rel=\"noopener\">阿里云 Kubernetes 审计日志方案</a></p>\n"},{"title":"kubeadm 安装 kubernetes","date":"2019-01-17T02:11:30.000Z","type":"kubeadm","_content":"\nkubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中，当前版本为 GA，暂不建议用于部署生产环境，其先进的设计理念可以借鉴。\n\n## 一、kubeadm 原理介绍\n\nkubeadm 会在初始化的机器上首先部署 kubelet 服务，kubelet 创建 pod 的方式有三种，其中一种就是监控指定目下（/etc/kubernetes/manifests）容器状态的变化然后进行相应的操作。kubeadm 启动 kubelet 后会在 /etc/kubernetes/manifests 目录下创建出 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 四个组件 static pod 的 yaml 文件，此时 kubelet 监测到该目录下有 yaml 文件便会将其创建为对应的 pod，最终 kube-apiserver、kube-controller-manager、kube-scheduler 以及 etcd 会以 static pod 的方式运行。\n\n\n> 本次安装 kubernetes 版本：v1.12.0\n\n当前宿主机系统与内核版本：\n```\n$ uname -r\n3.10.0-514.16.1.el7.x86_64\n\n$ cat /etc/redhat-release\nCentOS Linux release 7.2.1511 (Core)\n```\n## 二、安装前的准备工作\n```\n# 关闭swap\n$ sudo swapoff -a\n\n# 关闭selinux\n$ sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux \n$ setenforce 0\n\n# 关闭防火墙\n$ systemctl disable firewalld.service && systemctl stop firewalld.service\n\n# 配置转发相关参数\n$ cat << EOF >> /etc/sysctl.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nvm.swappiness=0\nEOF \n$ sysctl -p\n```\n\n## 三、安装 Docker CE \n \n> 本次安装的 docker 版本：docker-ce-18.06.1.ce\n\n```\n# Install Docker CE\n## Set up the repository\n### Install required packages.\nyum install yum-utils device-mapper-persistent-data lvm2\n\n### Add docker repository.\nyum-config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/centos/docker-ce.repo\n\n## Install docker ce.\nyum update && yum install docker-ce-18.06.1.ce\n\n## Create /etc/docker directory.\nmkdir /etc/docker\n\n# Setup daemon.\ncat > /etc/docker/daemon.json <<EOF\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"100m\"\n  },\n  \"storage-driver\": \"overlay2\",\n  \"storage-opts\": [\n    \"overlay2.override_kernel_check=true\"\n  ]\n}\nEOF\n\nmkdir -p /etc/systemd/system/docker.service.d\n\n# Restart docker.\nsystemctl daemon-reload\nsystemctl restart docker\n```\n参考：https://kubernetes.io/docs/setup/cri/\n\n## 四、安装 kubernetes master 组件\n\n使用 kubeadm 初始化集群：\n```\n$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16\n[init] using Kubernetes version: v1.12.0\n[preflight] running pre-flight checks\n[preflight/images] Pulling images required for setting up a Kubernetes cluster\n[preflight/images] This might take a minute or two, depending on the speed of your internet connection\n[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'\n[kubelet] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[preflight] Activating the kubelet service\n[certificates] Using the existing front-proxy-client certificate and key.\n[certificates] Using the existing etcd/server certificate and key.\n[certificates] Using the existing etcd/peer certificate and key.\n[certificates] Using the existing etcd/healthcheck-client certificate and key.\n[certificates] Using the existing apiserver-etcd-client certificate and key.\n[certificates] Using the existing apiserver certificate and key.\n[certificates] Using the existing apiserver-kubelet-client certificate and key.\n[certificates] valid certificates and keys now exist in \"/etc/kubernetes/pki\"\n[certificates] Using the existing sa key.\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/admin.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/kubelet.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/controller-manager.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/scheduler.conf\"\n[controlplane] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"\n[etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\"\n[init] waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\"\n[init] this might take a minute or longer if the control plane images have to be pulled\n[apiclient] All control plane components are healthy after 14.002350 seconds\n[uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config-1.12\" in namespace kube-system with the configuration for the kubelets in the cluster\n[markmaster] Marking the node 192.168.1.110 as master by adding the label \"node-role.kubernetes.io/master=''\"\n[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"192.168.1.110\" as an annotation\n[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt\n[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstraptoken] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes master has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23\n\n```\n\n这个命令会自动执行以下步骤：\n\n- 系统状态检查\n- 生成 token\n- 生成自签名 CA 和 client 端证书\n- 生成 kubeconfig 用于 kubelet 连接 API server\n- 为 Master 组件生成 Static Pod manifests，并放到 /etc/kubernetes/manifests 目录中\n- 配置 RBAC 并设置 Master node 只运行控制平面组件\n- 创建附加服务，比如 kube-proxy 和 CoreDNS\n\n\n配置 kubetl 认证信息： \n```\n $ mkdir -p $HOME/.kube\n $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n $ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n将本机作为 node 加入到 master 中：\n```\n$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash\n```\nkubeadm 默认 master 节点不作为 node 节点使用，初始化完成后会给 master 节点打上 taint 标签，若单机部署，使用以下命令去掉 taint 标签：\n```\n$ kubectl taint nodes --all node-role.kubernetes.io/master-\n```\n\n查看各组件是否正常运行：\n\n```\n$ kubectl get pod -n kube-system\nNAME                                     READY   STATUS             RESTARTS   AGE\ncoredns-99b9bb8bd-pgh5t                  1/1     Running            0          48m\netcd                                     1/1     Running            2          48m\nkube-apiserver                           1/1     Running            1          48m\nkube-controller-manager                  1/1     Running            0          49m\nkube-flannel-ds-amd64-b5rjg              1/1     Running            0          31m\nkube-proxy-c8ktg                         1/1     Running            0          48m\nkube-scheduler                           1/1     Running            2          48m\n```\n\n## 五、安装 kubernetes 网络\n\nkubernetes 本身是不提供网络方案的，但是有很多开源组件可以帮助我们打通容器和容器之间的网络，实现 Kubernetes 要求的网络模型。从实现原理上来说大致分为以下两种：\n- overlay 网络，通过封包解包的方式构造一个隧道，代表的方案有 flannel(udp/vxlan）、weave、calico(ipip)，openvswitch 等\n- 通过路由来实现(更改 iptables 等手段)，flannel(host-gw)，calico(bgp)，macvlan 等\n\n当然每种方案都有自己适合的场景，flannel 和 calico 是两种最常见的网络方案，我们要根据自己的实际需要进行选择。此次安装选择 flannel 网络：\n\n\n此操作也会为 flannel 创建对应的 RBAC 规则，flannel 会以 daemonset 的方式创建出来：\n```\n$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```\n\n创建一个 pod 验证集群是否正常：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n```\n\n## 六、kubeadm 其他相关的操作\n\n1、删除安装:\n```\n$ kubeadm reset\n```\n2、版本升级\n```\n# 查看可升级的版本\n$ kubeadm upgrade plan\n\n# 升级至指定版本\n$ kubeadm upgrade apply [version]\n```\n>  1. 要执行升级，需要先将 kubeadm 升级到对应的版本；\n>  2. kubeadm 并不负责 kubelet 的升级，需要在升级完 master 组件后，手工对 kubelet 进行升级。\n\n\n## 七、创建过程中的一些 case 记录\n\n##### 1、flannel 容器启动报错：pod cidr not assgned\n\n需要在  /etc/kubernetes/manifests/kube-controller-manager.yaml 文件中添加以下配置：\n\n--allocate-node-cidrs=true\n--cluster-cidr=10.244.0.0/16\n\n参考：https://github.com/coreos/flannel/issues/728\n\n##### 2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory\n\n```\n$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line\n$ grub2-mkconfig -o /boot/grub2/grub.cfg\n$ shutdown -r now\n```\n参考：https://github.com/containernetworking/cni/issues/569\n\n##### 3、kubeadm 证书有效期问题\n\n默认情况下，kubeadm 会生成集群运行所需的所有证书，我们也可以通过提供自己的证书来覆盖此行为。要做到这一点，必须把它们放在 --cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录（默认目录为 /etc/kubernetes/pki），如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。所以只要我们自己提供一个有效期很长的证书去覆盖掉默认的证书就可以来避免这个的问题。\n\n##### 4、kubeadm join 时 token 无法生效\n\ntoken 的失效为24小时，若忘记或者 token 过期可以使用 `kubeadm token create` 重新生成 token。\n\n## 八、总结\n本篇文章讲述了使用 kubeadm 来搭建一个 kubernetes 集群，kubeadm 暂时还不建议用于生产环境，若部署生产环境请使用二进制文件。kubeadm 搭建出的集群还是有很多不完善的地方，比如，集群 master 组件的参数配置问题，官方默认的并不会满足需求，有许多参数需要根据实际情况进行修改。\n\n\n参考：\n[Creating a single master cluster with kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/)\n[kubeadm 工作原理](https://github.com/feiskyer/kubernetes-handbook/blob/master/components/kubeadm.md)\n[DockOne微信分享（一六三）：Kubernetes官方集群部署工具kubeadm原理解析](http://dockone.io/article/4645)\n[centos7.2 安装k8s v1.11.0](https://segmentfault.com/a/1190000015787725)\n\n","source":"_posts/kubeadm.md","raw":"---\ntitle: kubeadm 安装 kubernetes\ndate: 2019-01-17 10:11:30\ntags: \"kubeadm\"\ntype: \"kubeadm\"\n\n---\n\nkubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中，当前版本为 GA，暂不建议用于部署生产环境，其先进的设计理念可以借鉴。\n\n## 一、kubeadm 原理介绍\n\nkubeadm 会在初始化的机器上首先部署 kubelet 服务，kubelet 创建 pod 的方式有三种，其中一种就是监控指定目下（/etc/kubernetes/manifests）容器状态的变化然后进行相应的操作。kubeadm 启动 kubelet 后会在 /etc/kubernetes/manifests 目录下创建出 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 四个组件 static pod 的 yaml 文件，此时 kubelet 监测到该目录下有 yaml 文件便会将其创建为对应的 pod，最终 kube-apiserver、kube-controller-manager、kube-scheduler 以及 etcd 会以 static pod 的方式运行。\n\n\n> 本次安装 kubernetes 版本：v1.12.0\n\n当前宿主机系统与内核版本：\n```\n$ uname -r\n3.10.0-514.16.1.el7.x86_64\n\n$ cat /etc/redhat-release\nCentOS Linux release 7.2.1511 (Core)\n```\n## 二、安装前的准备工作\n```\n# 关闭swap\n$ sudo swapoff -a\n\n# 关闭selinux\n$ sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux \n$ setenforce 0\n\n# 关闭防火墙\n$ systemctl disable firewalld.service && systemctl stop firewalld.service\n\n# 配置转发相关参数\n$ cat << EOF >> /etc/sysctl.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nvm.swappiness=0\nEOF \n$ sysctl -p\n```\n\n## 三、安装 Docker CE \n \n> 本次安装的 docker 版本：docker-ce-18.06.1.ce\n\n```\n# Install Docker CE\n## Set up the repository\n### Install required packages.\nyum install yum-utils device-mapper-persistent-data lvm2\n\n### Add docker repository.\nyum-config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/centos/docker-ce.repo\n\n## Install docker ce.\nyum update && yum install docker-ce-18.06.1.ce\n\n## Create /etc/docker directory.\nmkdir /etc/docker\n\n# Setup daemon.\ncat > /etc/docker/daemon.json <<EOF\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"100m\"\n  },\n  \"storage-driver\": \"overlay2\",\n  \"storage-opts\": [\n    \"overlay2.override_kernel_check=true\"\n  ]\n}\nEOF\n\nmkdir -p /etc/systemd/system/docker.service.d\n\n# Restart docker.\nsystemctl daemon-reload\nsystemctl restart docker\n```\n参考：https://kubernetes.io/docs/setup/cri/\n\n## 四、安装 kubernetes master 组件\n\n使用 kubeadm 初始化集群：\n```\n$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16\n[init] using Kubernetes version: v1.12.0\n[preflight] running pre-flight checks\n[preflight/images] Pulling images required for setting up a Kubernetes cluster\n[preflight/images] This might take a minute or two, depending on the speed of your internet connection\n[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'\n[kubelet] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[preflight] Activating the kubelet service\n[certificates] Using the existing front-proxy-client certificate and key.\n[certificates] Using the existing etcd/server certificate and key.\n[certificates] Using the existing etcd/peer certificate and key.\n[certificates] Using the existing etcd/healthcheck-client certificate and key.\n[certificates] Using the existing apiserver-etcd-client certificate and key.\n[certificates] Using the existing apiserver certificate and key.\n[certificates] Using the existing apiserver-kubelet-client certificate and key.\n[certificates] valid certificates and keys now exist in \"/etc/kubernetes/pki\"\n[certificates] Using the existing sa key.\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/admin.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/kubelet.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/controller-manager.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/scheduler.conf\"\n[controlplane] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"\n[etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\"\n[init] waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\"\n[init] this might take a minute or longer if the control plane images have to be pulled\n[apiclient] All control plane components are healthy after 14.002350 seconds\n[uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config-1.12\" in namespace kube-system with the configuration for the kubelets in the cluster\n[markmaster] Marking the node 192.168.1.110 as master by adding the label \"node-role.kubernetes.io/master=''\"\n[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"192.168.1.110\" as an annotation\n[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt\n[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstraptoken] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes master has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23\n\n```\n\n这个命令会自动执行以下步骤：\n\n- 系统状态检查\n- 生成 token\n- 生成自签名 CA 和 client 端证书\n- 生成 kubeconfig 用于 kubelet 连接 API server\n- 为 Master 组件生成 Static Pod manifests，并放到 /etc/kubernetes/manifests 目录中\n- 配置 RBAC 并设置 Master node 只运行控制平面组件\n- 创建附加服务，比如 kube-proxy 和 CoreDNS\n\n\n配置 kubetl 认证信息： \n```\n $ mkdir -p $HOME/.kube\n $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n $ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n将本机作为 node 加入到 master 中：\n```\n$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash\n```\nkubeadm 默认 master 节点不作为 node 节点使用，初始化完成后会给 master 节点打上 taint 标签，若单机部署，使用以下命令去掉 taint 标签：\n```\n$ kubectl taint nodes --all node-role.kubernetes.io/master-\n```\n\n查看各组件是否正常运行：\n\n```\n$ kubectl get pod -n kube-system\nNAME                                     READY   STATUS             RESTARTS   AGE\ncoredns-99b9bb8bd-pgh5t                  1/1     Running            0          48m\netcd                                     1/1     Running            2          48m\nkube-apiserver                           1/1     Running            1          48m\nkube-controller-manager                  1/1     Running            0          49m\nkube-flannel-ds-amd64-b5rjg              1/1     Running            0          31m\nkube-proxy-c8ktg                         1/1     Running            0          48m\nkube-scheduler                           1/1     Running            2          48m\n```\n\n## 五、安装 kubernetes 网络\n\nkubernetes 本身是不提供网络方案的，但是有很多开源组件可以帮助我们打通容器和容器之间的网络，实现 Kubernetes 要求的网络模型。从实现原理上来说大致分为以下两种：\n- overlay 网络，通过封包解包的方式构造一个隧道，代表的方案有 flannel(udp/vxlan）、weave、calico(ipip)，openvswitch 等\n- 通过路由来实现(更改 iptables 等手段)，flannel(host-gw)，calico(bgp)，macvlan 等\n\n当然每种方案都有自己适合的场景，flannel 和 calico 是两种最常见的网络方案，我们要根据自己的实际需要进行选择。此次安装选择 flannel 网络：\n\n\n此操作也会为 flannel 创建对应的 RBAC 规则，flannel 会以 daemonset 的方式创建出来：\n```\n$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```\n\n创建一个 pod 验证集群是否正常：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n```\n\n## 六、kubeadm 其他相关的操作\n\n1、删除安装:\n```\n$ kubeadm reset\n```\n2、版本升级\n```\n# 查看可升级的版本\n$ kubeadm upgrade plan\n\n# 升级至指定版本\n$ kubeadm upgrade apply [version]\n```\n>  1. 要执行升级，需要先将 kubeadm 升级到对应的版本；\n>  2. kubeadm 并不负责 kubelet 的升级，需要在升级完 master 组件后，手工对 kubelet 进行升级。\n\n\n## 七、创建过程中的一些 case 记录\n\n##### 1、flannel 容器启动报错：pod cidr not assgned\n\n需要在  /etc/kubernetes/manifests/kube-controller-manager.yaml 文件中添加以下配置：\n\n--allocate-node-cidrs=true\n--cluster-cidr=10.244.0.0/16\n\n参考：https://github.com/coreos/flannel/issues/728\n\n##### 2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory\n\n```\n$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line\n$ grub2-mkconfig -o /boot/grub2/grub.cfg\n$ shutdown -r now\n```\n参考：https://github.com/containernetworking/cni/issues/569\n\n##### 3、kubeadm 证书有效期问题\n\n默认情况下，kubeadm 会生成集群运行所需的所有证书，我们也可以通过提供自己的证书来覆盖此行为。要做到这一点，必须把它们放在 --cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录（默认目录为 /etc/kubernetes/pki），如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。所以只要我们自己提供一个有效期很长的证书去覆盖掉默认的证书就可以来避免这个的问题。\n\n##### 4、kubeadm join 时 token 无法生效\n\ntoken 的失效为24小时，若忘记或者 token 过期可以使用 `kubeadm token create` 重新生成 token。\n\n## 八、总结\n本篇文章讲述了使用 kubeadm 来搭建一个 kubernetes 集群，kubeadm 暂时还不建议用于生产环境，若部署生产环境请使用二进制文件。kubeadm 搭建出的集群还是有很多不完善的地方，比如，集群 master 组件的参数配置问题，官方默认的并不会满足需求，有许多参数需要根据实际情况进行修改。\n\n\n参考：\n[Creating a single master cluster with kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/)\n[kubeadm 工作原理](https://github.com/feiskyer/kubernetes-handbook/blob/master/components/kubeadm.md)\n[DockOne微信分享（一六三）：Kubernetes官方集群部署工具kubeadm原理解析](http://dockone.io/article/4645)\n[centos7.2 安装k8s v1.11.0](https://segmentfault.com/a/1190000015787725)\n\n","slug":"kubeadm","published":1,"updated":"2019-01-17T02:46:52.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5goc000i22dvlp4oh4ej","content":"<p>kubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中，当前版本为 GA，暂不建议用于部署生产环境，其先进的设计理念可以借鉴。</p>\n<h2 id=\"一、kubeadm-原理介绍\"><a href=\"#一、kubeadm-原理介绍\" class=\"headerlink\" title=\"一、kubeadm 原理介绍\"></a>一、kubeadm 原理介绍</h2><p>kubeadm 会在初始化的机器上首先部署 kubelet 服务，kubelet 创建 pod 的方式有三种，其中一种就是监控指定目下（/etc/kubernetes/manifests）容器状态的变化然后进行相应的操作。kubeadm 启动 kubelet 后会在 /etc/kubernetes/manifests 目录下创建出 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 四个组件 static pod 的 yaml 文件，此时 kubelet 监测到该目录下有 yaml 文件便会将其创建为对应的 pod，最终 kube-apiserver、kube-controller-manager、kube-scheduler 以及 etcd 会以 static pod 的方式运行。</p>\n<blockquote>\n<p>本次安装 kubernetes 版本：v1.12.0</p>\n</blockquote>\n<p>当前宿主机系统与内核版本：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ uname -r</span><br><span class=\"line\">3.10.0-514.16.1.el7.x86_64</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat /etc/redhat-release</span><br><span class=\"line\">CentOS Linux release 7.2.1511 (Core)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"二、安装前的准备工作\"><a href=\"#二、安装前的准备工作\" class=\"headerlink\" title=\"二、安装前的准备工作\"></a>二、安装前的准备工作</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 关闭swap</span><br><span class=\"line\">$ sudo swapoff -a</span><br><span class=\"line\"></span><br><span class=\"line\"># 关闭selinux</span><br><span class=\"line\">$ sed -i &apos;s/SELINUX=permissive/SELINUX=disabled/&apos; /etc/sysconfig/selinux </span><br><span class=\"line\">$ setenforce 0</span><br><span class=\"line\"></span><br><span class=\"line\"># 关闭防火墙</span><br><span class=\"line\">$ systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.service</span><br><span class=\"line\"></span><br><span class=\"line\"># 配置转发相关参数</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class=\"line\">net.bridge.bridge-nf-call-iptables = 1</span><br><span class=\"line\">vm.swappiness=0</span><br><span class=\"line\">EOF </span><br><span class=\"line\">$ sysctl -p</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、安装-Docker-CE\"><a href=\"#三、安装-Docker-CE\" class=\"headerlink\" title=\"三、安装 Docker CE\"></a>三、安装 Docker CE</h2><blockquote>\n<p>本次安装的 docker 版本：docker-ce-18.06.1.ce</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Install Docker CE</span><br><span class=\"line\">## Set up the repository</span><br><span class=\"line\">### Install required packages.</span><br><span class=\"line\">yum install yum-utils device-mapper-persistent-data lvm2</span><br><span class=\"line\"></span><br><span class=\"line\">### Add docker repository.</span><br><span class=\"line\">yum-config-manager \\</span><br><span class=\"line\">    --add-repo \\</span><br><span class=\"line\">    https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class=\"line\"></span><br><span class=\"line\">## Install docker ce.</span><br><span class=\"line\">yum update &amp;&amp; yum install docker-ce-18.06.1.ce</span><br><span class=\"line\"></span><br><span class=\"line\">## Create /etc/docker directory.</span><br><span class=\"line\">mkdir /etc/docker</span><br><span class=\"line\"></span><br><span class=\"line\"># Setup daemon.</span><br><span class=\"line\">cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class=\"line\">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class=\"line\">  &quot;log-opts&quot;: &#123;</span><br><span class=\"line\">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class=\"line\">  &quot;storage-opts&quot;: [</span><br><span class=\"line\">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">mkdir -p /etc/systemd/system/docker.service.d</span><br><span class=\"line\"></span><br><span class=\"line\"># Restart docker.</span><br><span class=\"line\">systemctl daemon-reload</span><br><span class=\"line\">systemctl restart docker</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://kubernetes.io/docs/setup/cri/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/setup/cri/</a></p>\n<h2 id=\"四、安装-kubernetes-master-组件\"><a href=\"#四、安装-kubernetes-master-组件\" class=\"headerlink\" title=\"四、安装 kubernetes master 组件\"></a>四、安装 kubernetes master 组件</h2><p>使用 kubeadm 初始化集群：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16</span><br><span class=\"line\">[init] using Kubernetes version: v1.12.0</span><br><span class=\"line\">[preflight] running pre-flight checks</span><br><span class=\"line\">[preflight/images] Pulling images required for setting up a Kubernetes cluster</span><br><span class=\"line\">[preflight/images] This might take a minute or two, depending on the speed of your internet connection</span><br><span class=\"line\">[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class=\"line\">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class=\"line\">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class=\"line\">[preflight] Activating the kubelet service</span><br><span class=\"line\">[certificates] Using the existing front-proxy-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/server certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/peer certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/healthcheck-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver-etcd-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver-kubelet-client certificate and key.</span><br><span class=\"line\">[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class=\"line\">[certificates] Using the existing sa key.</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/kubelet.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/controller-manager.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/scheduler.conf&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class=\"line\">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class=\"line\">[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;</span><br><span class=\"line\">[init] this might take a minute or longer if the control plane images have to be pulled</span><br><span class=\"line\">[apiclient] All control plane components are healthy after 14.002350 seconds</span><br><span class=\"line\">[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class=\"line\">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class=\"line\">[markmaster] Marking the node 192.168.1.110 as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class=\"line\">[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class=\"line\">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;192.168.1.110&quot; as an annotation</span><br><span class=\"line\">[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class=\"line\">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class=\"line\">[addons] Applied essential addon: CoreDNS</span><br><span class=\"line\">[addons] Applied essential addon: kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">Your Kubernetes master has initialized successfully!</span><br><span class=\"line\"></span><br><span class=\"line\">To start using your cluster, you need to run the following as a regular user:</span><br><span class=\"line\"></span><br><span class=\"line\">  mkdir -p $HOME/.kube</span><br><span class=\"line\">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class=\"line\"></span><br><span class=\"line\">You should now deploy a pod network to the cluster.</span><br><span class=\"line\">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class=\"line\">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class=\"line\"></span><br><span class=\"line\">You can now join any number of machines by running the following on each node</span><br><span class=\"line\">as root:</span><br><span class=\"line\"></span><br><span class=\"line\">  kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23</span><br></pre></td></tr></table></figure></p>\n<p>这个命令会自动执行以下步骤：</p>\n<ul>\n<li>系统状态检查</li>\n<li>生成 token</li>\n<li>生成自签名 CA 和 client 端证书</li>\n<li>生成 kubeconfig 用于 kubelet 连接 API server</li>\n<li>为 Master 组件生成 Static Pod manifests，并放到 /etc/kubernetes/manifests 目录中</li>\n<li>配置 RBAC 并设置 Master node 只运行控制平面组件</li>\n<li>创建附加服务，比如 kube-proxy 和 CoreDNS</li>\n</ul>\n<p>配置 kubetl 认证信息：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -p $HOME/.kube</span><br><span class=\"line\">$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure></p>\n<p>将本机作为 node 加入到 master 中：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash</span><br></pre></td></tr></table></figure></p>\n<p>kubeadm 默认 master 节点不作为 node 节点使用，初始化完成后会给 master 节点打上 taint 标签，若单机部署，使用以下命令去掉 taint 标签：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl taint nodes --all node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure></p>\n<p>查看各组件是否正常运行：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pod -n kube-system</span><br><span class=\"line\">NAME                                     READY   STATUS             RESTARTS   AGE</span><br><span class=\"line\">coredns-99b9bb8bd-pgh5t                  1/1     Running            0          48m</span><br><span class=\"line\">etcd                                     1/1     Running            2          48m</span><br><span class=\"line\">kube-apiserver                           1/1     Running            1          48m</span><br><span class=\"line\">kube-controller-manager                  1/1     Running            0          49m</span><br><span class=\"line\">kube-flannel-ds-amd64-b5rjg              1/1     Running            0          31m</span><br><span class=\"line\">kube-proxy-c8ktg                         1/1     Running            0          48m</span><br><span class=\"line\">kube-scheduler                           1/1     Running            2          48m</span><br></pre></td></tr></table></figure>\n<h2 id=\"五、安装-kubernetes-网络\"><a href=\"#五、安装-kubernetes-网络\" class=\"headerlink\" title=\"五、安装 kubernetes 网络\"></a>五、安装 kubernetes 网络</h2><p>kubernetes 本身是不提供网络方案的，但是有很多开源组件可以帮助我们打通容器和容器之间的网络，实现 Kubernetes 要求的网络模型。从实现原理上来说大致分为以下两种：</p>\n<ul>\n<li>overlay 网络，通过封包解包的方式构造一个隧道，代表的方案有 flannel(udp/vxlan）、weave、calico(ipip)，openvswitch 等</li>\n<li>通过路由来实现(更改 iptables 等手段)，flannel(host-gw)，calico(bgp)，macvlan 等</li>\n</ul>\n<p>当然每种方案都有自己适合的场景，flannel 和 calico 是两种最常见的网络方案，我们要根据自己的实际需要进行选择。此次安装选择 flannel 网络：</p>\n<p>此操作也会为 flannel 创建对应的 RBAC 规则，flannel 会以 daemonset 的方式创建出来：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure></p>\n<p>创建一个 pod 验证集群是否正常：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: nginx</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    name: nginx</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">  - name: nginx</span><br><span class=\"line\">    image: nginx</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">    - containerPort: 80</span><br></pre></td></tr></table></figure>\n<h2 id=\"六、kubeadm-其他相关的操作\"><a href=\"#六、kubeadm-其他相关的操作\" class=\"headerlink\" title=\"六、kubeadm 其他相关的操作\"></a>六、kubeadm 其他相关的操作</h2><p>1、删除安装:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm reset</span><br></pre></td></tr></table></figure></p>\n<p>2、版本升级<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 查看可升级的版本</span><br><span class=\"line\">$ kubeadm upgrade plan</span><br><span class=\"line\"></span><br><span class=\"line\"># 升级至指定版本</span><br><span class=\"line\">$ kubeadm upgrade apply [version]</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<ol>\n<li>要执行升级，需要先将 kubeadm 升级到对应的版本；</li>\n<li>kubeadm 并不负责 kubelet 的升级，需要在升级完 master 组件后，手工对 kubelet 进行升级。</li>\n</ol>\n</blockquote>\n<h2 id=\"七、创建过程中的一些-case-记录\"><a href=\"#七、创建过程中的一些-case-记录\" class=\"headerlink\" title=\"七、创建过程中的一些 case 记录\"></a>七、创建过程中的一些 case 记录</h2><h5 id=\"1、flannel-容器启动报错：pod-cidr-not-assgned\"><a href=\"#1、flannel-容器启动报错：pod-cidr-not-assgned\" class=\"headerlink\" title=\"1、flannel 容器启动报错：pod cidr not assgned\"></a>1、flannel 容器启动报错：pod cidr not assgned</h5><p>需要在  /etc/kubernetes/manifests/kube-controller-manager.yaml 文件中添加以下配置：</p>\n<p>–allocate-node-cidrs=true<br>–cluster-cidr=10.244.0.0/16</p>\n<p>参考：<a href=\"https://github.com/coreos/flannel/issues/728\" target=\"_blank\" rel=\"noopener\">https://github.com/coreos/flannel/issues/728</a></p>\n<h5 id=\"2、coredns-容器启动失败报错：-proc-sys-net-ipv6-conf-eth0-accept-dad-no-such-file-or-directory\"><a href=\"#2、coredns-容器启动失败报错：-proc-sys-net-ipv6-conf-eth0-accept-dad-no-such-file-or-directory\" class=\"headerlink\" title=\"2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory\"></a>2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line</span><br><span class=\"line\">$ grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class=\"line\">$ shutdown -r now</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://github.com/containernetworking/cni/issues/569\" target=\"_blank\" rel=\"noopener\">https://github.com/containernetworking/cni/issues/569</a></p>\n<h5 id=\"3、kubeadm-证书有效期问题\"><a href=\"#3、kubeadm-证书有效期问题\" class=\"headerlink\" title=\"3、kubeadm 证书有效期问题\"></a>3、kubeadm 证书有效期问题</h5><p>默认情况下，kubeadm 会生成集群运行所需的所有证书，我们也可以通过提供自己的证书来覆盖此行为。要做到这一点，必须把它们放在 –cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录（默认目录为 /etc/kubernetes/pki），如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。所以只要我们自己提供一个有效期很长的证书去覆盖掉默认的证书就可以来避免这个的问题。</p>\n<h5 id=\"4、kubeadm-join-时-token-无法生效\"><a href=\"#4、kubeadm-join-时-token-无法生效\" class=\"headerlink\" title=\"4、kubeadm join 时 token 无法生效\"></a>4、kubeadm join 时 token 无法生效</h5><p>token 的失效为24小时，若忘记或者 token 过期可以使用 <code>kubeadm token create</code> 重新生成 token。</p>\n<h2 id=\"八、总结\"><a href=\"#八、总结\" class=\"headerlink\" title=\"八、总结\"></a>八、总结</h2><p>本篇文章讲述了使用 kubeadm 来搭建一个 kubernetes 集群，kubeadm 暂时还不建议用于生产环境，若部署生产环境请使用二进制文件。kubeadm 搭建出的集群还是有很多不完善的地方，比如，集群 master 组件的参数配置问题，官方默认的并不会满足需求，有许多参数需要根据实际情况进行修改。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\" target=\"_blank\" rel=\"noopener\">Creating a single master cluster with kubeadm</a><br><a href=\"https://github.com/feiskyer/kubernetes-handbook/blob/master/components/kubeadm.md\" target=\"_blank\" rel=\"noopener\">kubeadm 工作原理</a><br><a href=\"http://dockone.io/article/4645\" target=\"_blank\" rel=\"noopener\">DockOne微信分享（一六三）：Kubernetes官方集群部署工具kubeadm原理解析</a><br><a href=\"https://segmentfault.com/a/1190000015787725\" target=\"_blank\" rel=\"noopener\">centos7.2 安装k8s v1.11.0</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>kubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中，当前版本为 GA，暂不建议用于部署生产环境，其先进的设计理念可以借鉴。</p>\n<h2 id=\"一、kubeadm-原理介绍\"><a href=\"#一、kubeadm-原理介绍\" class=\"headerlink\" title=\"一、kubeadm 原理介绍\"></a>一、kubeadm 原理介绍</h2><p>kubeadm 会在初始化的机器上首先部署 kubelet 服务，kubelet 创建 pod 的方式有三种，其中一种就是监控指定目下（/etc/kubernetes/manifests）容器状态的变化然后进行相应的操作。kubeadm 启动 kubelet 后会在 /etc/kubernetes/manifests 目录下创建出 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 四个组件 static pod 的 yaml 文件，此时 kubelet 监测到该目录下有 yaml 文件便会将其创建为对应的 pod，最终 kube-apiserver、kube-controller-manager、kube-scheduler 以及 etcd 会以 static pod 的方式运行。</p>\n<blockquote>\n<p>本次安装 kubernetes 版本：v1.12.0</p>\n</blockquote>\n<p>当前宿主机系统与内核版本：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ uname -r</span><br><span class=\"line\">3.10.0-514.16.1.el7.x86_64</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat /etc/redhat-release</span><br><span class=\"line\">CentOS Linux release 7.2.1511 (Core)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"二、安装前的准备工作\"><a href=\"#二、安装前的准备工作\" class=\"headerlink\" title=\"二、安装前的准备工作\"></a>二、安装前的准备工作</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 关闭swap</span><br><span class=\"line\">$ sudo swapoff -a</span><br><span class=\"line\"></span><br><span class=\"line\"># 关闭selinux</span><br><span class=\"line\">$ sed -i &apos;s/SELINUX=permissive/SELINUX=disabled/&apos; /etc/sysconfig/selinux </span><br><span class=\"line\">$ setenforce 0</span><br><span class=\"line\"></span><br><span class=\"line\"># 关闭防火墙</span><br><span class=\"line\">$ systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.service</span><br><span class=\"line\"></span><br><span class=\"line\"># 配置转发相关参数</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class=\"line\">net.bridge.bridge-nf-call-iptables = 1</span><br><span class=\"line\">vm.swappiness=0</span><br><span class=\"line\">EOF </span><br><span class=\"line\">$ sysctl -p</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、安装-Docker-CE\"><a href=\"#三、安装-Docker-CE\" class=\"headerlink\" title=\"三、安装 Docker CE\"></a>三、安装 Docker CE</h2><blockquote>\n<p>本次安装的 docker 版本：docker-ce-18.06.1.ce</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Install Docker CE</span><br><span class=\"line\">## Set up the repository</span><br><span class=\"line\">### Install required packages.</span><br><span class=\"line\">yum install yum-utils device-mapper-persistent-data lvm2</span><br><span class=\"line\"></span><br><span class=\"line\">### Add docker repository.</span><br><span class=\"line\">yum-config-manager \\</span><br><span class=\"line\">    --add-repo \\</span><br><span class=\"line\">    https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class=\"line\"></span><br><span class=\"line\">## Install docker ce.</span><br><span class=\"line\">yum update &amp;&amp; yum install docker-ce-18.06.1.ce</span><br><span class=\"line\"></span><br><span class=\"line\">## Create /etc/docker directory.</span><br><span class=\"line\">mkdir /etc/docker</span><br><span class=\"line\"></span><br><span class=\"line\"># Setup daemon.</span><br><span class=\"line\">cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class=\"line\">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class=\"line\">  &quot;log-opts&quot;: &#123;</span><br><span class=\"line\">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class=\"line\">  &quot;storage-opts&quot;: [</span><br><span class=\"line\">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">mkdir -p /etc/systemd/system/docker.service.d</span><br><span class=\"line\"></span><br><span class=\"line\"># Restart docker.</span><br><span class=\"line\">systemctl daemon-reload</span><br><span class=\"line\">systemctl restart docker</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://kubernetes.io/docs/setup/cri/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/setup/cri/</a></p>\n<h2 id=\"四、安装-kubernetes-master-组件\"><a href=\"#四、安装-kubernetes-master-组件\" class=\"headerlink\" title=\"四、安装 kubernetes master 组件\"></a>四、安装 kubernetes master 组件</h2><p>使用 kubeadm 初始化集群：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16</span><br><span class=\"line\">[init] using Kubernetes version: v1.12.0</span><br><span class=\"line\">[preflight] running pre-flight checks</span><br><span class=\"line\">[preflight/images] Pulling images required for setting up a Kubernetes cluster</span><br><span class=\"line\">[preflight/images] This might take a minute or two, depending on the speed of your internet connection</span><br><span class=\"line\">[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class=\"line\">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class=\"line\">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class=\"line\">[preflight] Activating the kubelet service</span><br><span class=\"line\">[certificates] Using the existing front-proxy-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/server certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/peer certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/healthcheck-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver-etcd-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver-kubelet-client certificate and key.</span><br><span class=\"line\">[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class=\"line\">[certificates] Using the existing sa key.</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/kubelet.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/controller-manager.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/scheduler.conf&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class=\"line\">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class=\"line\">[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;</span><br><span class=\"line\">[init] this might take a minute or longer if the control plane images have to be pulled</span><br><span class=\"line\">[apiclient] All control plane components are healthy after 14.002350 seconds</span><br><span class=\"line\">[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class=\"line\">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class=\"line\">[markmaster] Marking the node 192.168.1.110 as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class=\"line\">[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class=\"line\">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;192.168.1.110&quot; as an annotation</span><br><span class=\"line\">[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class=\"line\">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class=\"line\">[addons] Applied essential addon: CoreDNS</span><br><span class=\"line\">[addons] Applied essential addon: kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">Your Kubernetes master has initialized successfully!</span><br><span class=\"line\"></span><br><span class=\"line\">To start using your cluster, you need to run the following as a regular user:</span><br><span class=\"line\"></span><br><span class=\"line\">  mkdir -p $HOME/.kube</span><br><span class=\"line\">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class=\"line\"></span><br><span class=\"line\">You should now deploy a pod network to the cluster.</span><br><span class=\"line\">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class=\"line\">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class=\"line\"></span><br><span class=\"line\">You can now join any number of machines by running the following on each node</span><br><span class=\"line\">as root:</span><br><span class=\"line\"></span><br><span class=\"line\">  kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23</span><br></pre></td></tr></table></figure></p>\n<p>这个命令会自动执行以下步骤：</p>\n<ul>\n<li>系统状态检查</li>\n<li>生成 token</li>\n<li>生成自签名 CA 和 client 端证书</li>\n<li>生成 kubeconfig 用于 kubelet 连接 API server</li>\n<li>为 Master 组件生成 Static Pod manifests，并放到 /etc/kubernetes/manifests 目录中</li>\n<li>配置 RBAC 并设置 Master node 只运行控制平面组件</li>\n<li>创建附加服务，比如 kube-proxy 和 CoreDNS</li>\n</ul>\n<p>配置 kubetl 认证信息：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -p $HOME/.kube</span><br><span class=\"line\">$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure></p>\n<p>将本机作为 node 加入到 master 中：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash</span><br></pre></td></tr></table></figure></p>\n<p>kubeadm 默认 master 节点不作为 node 节点使用，初始化完成后会给 master 节点打上 taint 标签，若单机部署，使用以下命令去掉 taint 标签：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl taint nodes --all node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure></p>\n<p>查看各组件是否正常运行：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pod -n kube-system</span><br><span class=\"line\">NAME                                     READY   STATUS             RESTARTS   AGE</span><br><span class=\"line\">coredns-99b9bb8bd-pgh5t                  1/1     Running            0          48m</span><br><span class=\"line\">etcd                                     1/1     Running            2          48m</span><br><span class=\"line\">kube-apiserver                           1/1     Running            1          48m</span><br><span class=\"line\">kube-controller-manager                  1/1     Running            0          49m</span><br><span class=\"line\">kube-flannel-ds-amd64-b5rjg              1/1     Running            0          31m</span><br><span class=\"line\">kube-proxy-c8ktg                         1/1     Running            0          48m</span><br><span class=\"line\">kube-scheduler                           1/1     Running            2          48m</span><br></pre></td></tr></table></figure>\n<h2 id=\"五、安装-kubernetes-网络\"><a href=\"#五、安装-kubernetes-网络\" class=\"headerlink\" title=\"五、安装 kubernetes 网络\"></a>五、安装 kubernetes 网络</h2><p>kubernetes 本身是不提供网络方案的，但是有很多开源组件可以帮助我们打通容器和容器之间的网络，实现 Kubernetes 要求的网络模型。从实现原理上来说大致分为以下两种：</p>\n<ul>\n<li>overlay 网络，通过封包解包的方式构造一个隧道，代表的方案有 flannel(udp/vxlan）、weave、calico(ipip)，openvswitch 等</li>\n<li>通过路由来实现(更改 iptables 等手段)，flannel(host-gw)，calico(bgp)，macvlan 等</li>\n</ul>\n<p>当然每种方案都有自己适合的场景，flannel 和 calico 是两种最常见的网络方案，我们要根据自己的实际需要进行选择。此次安装选择 flannel 网络：</p>\n<p>此操作也会为 flannel 创建对应的 RBAC 规则，flannel 会以 daemonset 的方式创建出来：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure></p>\n<p>创建一个 pod 验证集群是否正常：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: nginx</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    name: nginx</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">  - name: nginx</span><br><span class=\"line\">    image: nginx</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">    - containerPort: 80</span><br></pre></td></tr></table></figure>\n<h2 id=\"六、kubeadm-其他相关的操作\"><a href=\"#六、kubeadm-其他相关的操作\" class=\"headerlink\" title=\"六、kubeadm 其他相关的操作\"></a>六、kubeadm 其他相关的操作</h2><p>1、删除安装:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm reset</span><br></pre></td></tr></table></figure></p>\n<p>2、版本升级<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 查看可升级的版本</span><br><span class=\"line\">$ kubeadm upgrade plan</span><br><span class=\"line\"></span><br><span class=\"line\"># 升级至指定版本</span><br><span class=\"line\">$ kubeadm upgrade apply [version]</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<ol>\n<li>要执行升级，需要先将 kubeadm 升级到对应的版本；</li>\n<li>kubeadm 并不负责 kubelet 的升级，需要在升级完 master 组件后，手工对 kubelet 进行升级。</li>\n</ol>\n</blockquote>\n<h2 id=\"七、创建过程中的一些-case-记录\"><a href=\"#七、创建过程中的一些-case-记录\" class=\"headerlink\" title=\"七、创建过程中的一些 case 记录\"></a>七、创建过程中的一些 case 记录</h2><h5 id=\"1、flannel-容器启动报错：pod-cidr-not-assgned\"><a href=\"#1、flannel-容器启动报错：pod-cidr-not-assgned\" class=\"headerlink\" title=\"1、flannel 容器启动报错：pod cidr not assgned\"></a>1、flannel 容器启动报错：pod cidr not assgned</h5><p>需要在  /etc/kubernetes/manifests/kube-controller-manager.yaml 文件中添加以下配置：</p>\n<p>–allocate-node-cidrs=true<br>–cluster-cidr=10.244.0.0/16</p>\n<p>参考：<a href=\"https://github.com/coreos/flannel/issues/728\" target=\"_blank\" rel=\"noopener\">https://github.com/coreos/flannel/issues/728</a></p>\n<h5 id=\"2、coredns-容器启动失败报错：-proc-sys-net-ipv6-conf-eth0-accept-dad-no-such-file-or-directory\"><a href=\"#2、coredns-容器启动失败报错：-proc-sys-net-ipv6-conf-eth0-accept-dad-no-such-file-or-directory\" class=\"headerlink\" title=\"2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory\"></a>2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line</span><br><span class=\"line\">$ grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class=\"line\">$ shutdown -r now</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://github.com/containernetworking/cni/issues/569\" target=\"_blank\" rel=\"noopener\">https://github.com/containernetworking/cni/issues/569</a></p>\n<h5 id=\"3、kubeadm-证书有效期问题\"><a href=\"#3、kubeadm-证书有效期问题\" class=\"headerlink\" title=\"3、kubeadm 证书有效期问题\"></a>3、kubeadm 证书有效期问题</h5><p>默认情况下，kubeadm 会生成集群运行所需的所有证书，我们也可以通过提供自己的证书来覆盖此行为。要做到这一点，必须把它们放在 –cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录（默认目录为 /etc/kubernetes/pki），如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。所以只要我们自己提供一个有效期很长的证书去覆盖掉默认的证书就可以来避免这个的问题。</p>\n<h5 id=\"4、kubeadm-join-时-token-无法生效\"><a href=\"#4、kubeadm-join-时-token-无法生效\" class=\"headerlink\" title=\"4、kubeadm join 时 token 无法生效\"></a>4、kubeadm join 时 token 无法生效</h5><p>token 的失效为24小时，若忘记或者 token 过期可以使用 <code>kubeadm token create</code> 重新生成 token。</p>\n<h2 id=\"八、总结\"><a href=\"#八、总结\" class=\"headerlink\" title=\"八、总结\"></a>八、总结</h2><p>本篇文章讲述了使用 kubeadm 来搭建一个 kubernetes 集群，kubeadm 暂时还不建议用于生产环境，若部署生产环境请使用二进制文件。kubeadm 搭建出的集群还是有很多不完善的地方，比如，集群 master 组件的参数配置问题，官方默认的并不会满足需求，有许多参数需要根据实际情况进行修改。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\" target=\"_blank\" rel=\"noopener\">Creating a single master cluster with kubeadm</a><br><a href=\"https://github.com/feiskyer/kubernetes-handbook/blob/master/components/kubeadm.md\" target=\"_blank\" rel=\"noopener\">kubeadm 工作原理</a><br><a href=\"http://dockone.io/article/4645\" target=\"_blank\" rel=\"noopener\">DockOne微信分享（一六三）：Kubernetes官方集群部署工具kubeadm原理解析</a><br><a href=\"https://segmentfault.com/a/1190000015787725\" target=\"_blank\" rel=\"noopener\">centos7.2 安装k8s v1.11.0</a></p>\n"},{"title":"kubelet 架构浅析","date":"2018-12-16T09:35:30.000Z","type":"kubelet","_content":"\n## 一、概要\nkubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。\n\nkubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。\n\n## 二、kubelet 的主要功能\n\n1、kubelet  默认监听四个端口，分别为 10250 、10255、10248、4194。\n\n```\nLISTEN     0      128          *:10250                    *:*                   users:((\"kubelet\",pid=48500,fd=28))\nLISTEN     0      128          *:10255                    *:*                   users:((\"kubelet\",pid=48500,fd=26))\nLISTEN     0      128          *:4194                     *:*                   users:((\"kubelet\",pid=48500,fd=13))\nLISTEN     0      128    127.0.0.1:10248                    *:*                   users:((\"kubelet\",pid=48500,fd=23))\n```\n\n- 10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。\n\n- 10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 `--healthz-port` 和 `--healthz-bind-address` 来指定监听的地址和端口。\n```\n  $ curl http://127.0.0.1:10248/healthz\n  ok\n```\n- 4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 http://localhost:4194 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 `--cadvisor-port` 可以指定启动的端口。\n\n```\n  $ curl  http://127.0.0.1:4194/metrics\n```\n\n- 10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。\n``` \n  //  获取 pod 的接口，与 apiserver 的 \n  // http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName=  接口类似\n  $ curl  http://127.0.0.1:10255/pods\n\n  // 节点信息接口,提供磁盘、网络、CPU、内存等信息\n  $ curl http://127.0.0.1:10255/spec/\n```\n\n2、kubelet 主要功能：\n\n- pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。\n\n- 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。\n\n- 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。\n\n\n## 三、kubelet 组件中的模块\n\n ![kubelet 组件中的模块](https://upload-images.jianshu.io/upload_images/1262158-91b13623d3cf45ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n上图展示了 kubelet 组件中的模块以及模块间的划分。\n\n- 1、PLEG(Pod Lifecycle Event Generator）\nPLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。\n\n- 2、cAdvisor \ncAdvisor（https://github.com/google/cadvisor）是 google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。\n\n- 3、OOMWatcher \n系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。\n\n- 4、probeManager \nprobeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。\nlivenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。\nreadinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 \n\n- 5、statusManager \nstatusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。\n\n- 6、containerRefManager\n容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。\n\n- 7、evictionManager \n当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 `--eviction-hard=` 来决定 evict 的策略值。\n\n- 8、imageGC \nimageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 `--image-gc-high-threshold` 和 `--image-gc-low-threshold` 来设置。\n\n- 9、containerGC \ncontainerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。\n\n- 10、imageManager \n调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。\n\n- 11、volumeManager \n负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。\n\n- 12、containerManager \n负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 `--cgroups-per-qos` 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 `true`，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。\n\n- 13、runtimeManager \ncontainerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 `--container-runtime` 来定义是使用docker 还是 rkt，默认是 `docker`。\n\n- 14、podManager \npodManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 \n\n\n在 v1.12 中，kubelet 组件有18个 manager：\n\n```\ncertificateManager\ncgroupManager\ncontainerManager\ncpuManager\nnodeContainerManager\nconfigmapManager\ncontainerReferenceManager\nevictionManager\nnvidiaGpuManager\nimageGCManager\nkuberuntimeManager\nhostportManager\npodManager\nproberManager\nsecretManager\nstatusManager\nvolumeManager\t\ntokenManager\n```\n\n其中比较重要的模块后面会进行一一分析。\n\n\n参考：\n[微软资深工程师详解 K8S 容器运行时](https://juejin.im/entry/5bc71b3d6fb9a05cf23029b8)\n[kubernetes 简介： kubelet 和 pod](https://cizixs.com/2016/10/25/kubernetes-intro-kubelet/)\n[Kubelet 组件解析](https://blog.csdn.net/jettery/article/details/78891733)\n\n\n\n","source":"_posts/kubelet-modules.md","raw":"---\ntitle: kubelet 架构浅析\ndate: 2018-12-16 17:35:30\ntags: \"kubelet\"\ntype: \"kubelet\"\n\n---\n\n## 一、概要\nkubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。\n\nkubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。\n\n## 二、kubelet 的主要功能\n\n1、kubelet  默认监听四个端口，分别为 10250 、10255、10248、4194。\n\n```\nLISTEN     0      128          *:10250                    *:*                   users:((\"kubelet\",pid=48500,fd=28))\nLISTEN     0      128          *:10255                    *:*                   users:((\"kubelet\",pid=48500,fd=26))\nLISTEN     0      128          *:4194                     *:*                   users:((\"kubelet\",pid=48500,fd=13))\nLISTEN     0      128    127.0.0.1:10248                    *:*                   users:((\"kubelet\",pid=48500,fd=23))\n```\n\n- 10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。\n\n- 10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 `--healthz-port` 和 `--healthz-bind-address` 来指定监听的地址和端口。\n```\n  $ curl http://127.0.0.1:10248/healthz\n  ok\n```\n- 4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 http://localhost:4194 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 `--cadvisor-port` 可以指定启动的端口。\n\n```\n  $ curl  http://127.0.0.1:4194/metrics\n```\n\n- 10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。\n``` \n  //  获取 pod 的接口，与 apiserver 的 \n  // http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName=  接口类似\n  $ curl  http://127.0.0.1:10255/pods\n\n  // 节点信息接口,提供磁盘、网络、CPU、内存等信息\n  $ curl http://127.0.0.1:10255/spec/\n```\n\n2、kubelet 主要功能：\n\n- pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。\n\n- 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。\n\n- 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。\n\n\n## 三、kubelet 组件中的模块\n\n ![kubelet 组件中的模块](https://upload-images.jianshu.io/upload_images/1262158-91b13623d3cf45ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n上图展示了 kubelet 组件中的模块以及模块间的划分。\n\n- 1、PLEG(Pod Lifecycle Event Generator）\nPLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。\n\n- 2、cAdvisor \ncAdvisor（https://github.com/google/cadvisor）是 google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。\n\n- 3、OOMWatcher \n系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。\n\n- 4、probeManager \nprobeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。\nlivenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。\nreadinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 \n\n- 5、statusManager \nstatusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。\n\n- 6、containerRefManager\n容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。\n\n- 7、evictionManager \n当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 `--eviction-hard=` 来决定 evict 的策略值。\n\n- 8、imageGC \nimageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 `--image-gc-high-threshold` 和 `--image-gc-low-threshold` 来设置。\n\n- 9、containerGC \ncontainerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。\n\n- 10、imageManager \n调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。\n\n- 11、volumeManager \n负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。\n\n- 12、containerManager \n负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 `--cgroups-per-qos` 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 `true`，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。\n\n- 13、runtimeManager \ncontainerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 `--container-runtime` 来定义是使用docker 还是 rkt，默认是 `docker`。\n\n- 14、podManager \npodManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 \n\n\n在 v1.12 中，kubelet 组件有18个 manager：\n\n```\ncertificateManager\ncgroupManager\ncontainerManager\ncpuManager\nnodeContainerManager\nconfigmapManager\ncontainerReferenceManager\nevictionManager\nnvidiaGpuManager\nimageGCManager\nkuberuntimeManager\nhostportManager\npodManager\nproberManager\nsecretManager\nstatusManager\nvolumeManager\t\ntokenManager\n```\n\n其中比较重要的模块后面会进行一一分析。\n\n\n参考：\n[微软资深工程师详解 K8S 容器运行时](https://juejin.im/entry/5bc71b3d6fb9a05cf23029b8)\n[kubernetes 简介： kubelet 和 pod](https://cizixs.com/2016/10/25/kubernetes-intro-kubelet/)\n[Kubelet 组件解析](https://blog.csdn.net/jettery/article/details/78891733)\n\n\n\n","slug":"kubelet-modules","published":1,"updated":"2018-12-16T09:47:50.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5goe000k22dvkj677cfa","content":"<h2 id=\"一、概要\"><a href=\"#一、概要\" class=\"headerlink\" title=\"一、概要\"></a>一、概要</h2><p>kubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。</p>\n<p>kubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。</p>\n<h2 id=\"二、kubelet-的主要功能\"><a href=\"#二、kubelet-的主要功能\" class=\"headerlink\" title=\"二、kubelet 的主要功能\"></a>二、kubelet 的主要功能</h2><p>1、kubelet  默认监听四个端口，分别为 10250 、10255、10248、4194。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LISTEN     0      128          *:10250                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=28))</span><br><span class=\"line\">LISTEN     0      128          *:10255                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=26))</span><br><span class=\"line\">LISTEN     0      128          *:4194                     *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=13))</span><br><span class=\"line\">LISTEN     0      128    127.0.0.1:10248                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=23))</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。</p>\n</li>\n<li><p>10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 <code>--healthz-port</code> 和 <code>--healthz-bind-address</code> 来指定监听的地址和端口。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://127.0.0.1:10248/healthz</span><br><span class=\"line\">ok</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 <a href=\"http://localhost:4194\" target=\"_blank\" rel=\"noopener\">http://localhost:4194</a> 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 <code>--cadvisor-port</code> 可以指定启动的端口。</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl  http://127.0.0.1:4194/metrics</span><br></pre></td></tr></table></figure>\n<ul>\n<li>10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//  获取 pod 的接口，与 apiserver 的 </span><br><span class=\"line\">// http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName=  接口类似</span><br><span class=\"line\">$ curl  http://127.0.0.1:10255/pods</span><br><span class=\"line\"></span><br><span class=\"line\">// 节点信息接口,提供磁盘、网络、CPU、内存等信息</span><br><span class=\"line\">$ curl http://127.0.0.1:10255/spec/</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>2、kubelet 主要功能：</p>\n<ul>\n<li><p>pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。</p>\n</li>\n<li><p>容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。</p>\n</li>\n<li><p>容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。</p>\n</li>\n</ul>\n<h2 id=\"三、kubelet-组件中的模块\"><a href=\"#三、kubelet-组件中的模块\" class=\"headerlink\" title=\"三、kubelet 组件中的模块\"></a>三、kubelet 组件中的模块</h2><p> <img src=\"https://upload-images.jianshu.io/upload_images/1262158-91b13623d3cf45ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubelet 组件中的模块\"></p>\n<p>上图展示了 kubelet 组件中的模块以及模块间的划分。</p>\n<ul>\n<li><p>1、PLEG(Pod Lifecycle Event Generator）<br>PLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。</p>\n</li>\n<li><p>2、cAdvisor<br>cAdvisor（<a href=\"https://github.com/google/cadvisor）是\" target=\"_blank\" rel=\"noopener\">https://github.com/google/cadvisor）是</a> google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。</p>\n</li>\n<li><p>3、OOMWatcher<br>系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。</p>\n</li>\n<li><p>4、probeManager<br>probeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。<br>livenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。<br>readinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 </p>\n</li>\n<li><p>5、statusManager<br>statusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。</p>\n</li>\n<li><p>6、containerRefManager<br>容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。</p>\n</li>\n<li><p>7、evictionManager<br>当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 <code>--eviction-hard=</code> 来决定 evict 的策略值。</p>\n</li>\n<li><p>8、imageGC<br>imageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 <code>--image-gc-high-threshold</code> 和 <code>--image-gc-low-threshold</code> 来设置。</p>\n</li>\n<li><p>9、containerGC<br>containerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。</p>\n</li>\n<li><p>10、imageManager<br>调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。</p>\n</li>\n<li><p>11、volumeManager<br>负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。</p>\n</li>\n<li><p>12、containerManager<br>负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 <code>--cgroups-per-qos</code> 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 <code>true</code>，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。</p>\n</li>\n<li><p>13、runtimeManager<br>containerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 <code>--container-runtime</code> 来定义是使用docker 还是 rkt，默认是 <code>docker</code>。</p>\n</li>\n<li><p>14、podManager<br>podManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 </p>\n</li>\n</ul>\n<p>在 v1.12 中，kubelet 组件有18个 manager：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">certificateManager</span><br><span class=\"line\">cgroupManager</span><br><span class=\"line\">containerManager</span><br><span class=\"line\">cpuManager</span><br><span class=\"line\">nodeContainerManager</span><br><span class=\"line\">configmapManager</span><br><span class=\"line\">containerReferenceManager</span><br><span class=\"line\">evictionManager</span><br><span class=\"line\">nvidiaGpuManager</span><br><span class=\"line\">imageGCManager</span><br><span class=\"line\">kuberuntimeManager</span><br><span class=\"line\">hostportManager</span><br><span class=\"line\">podManager</span><br><span class=\"line\">proberManager</span><br><span class=\"line\">secretManager</span><br><span class=\"line\">statusManager</span><br><span class=\"line\">volumeManager\t</span><br><span class=\"line\">tokenManager</span><br></pre></td></tr></table></figure>\n<p>其中比较重要的模块后面会进行一一分析。</p>\n<p>参考：<br><a href=\"https://juejin.im/entry/5bc71b3d6fb9a05cf23029b8\" target=\"_blank\" rel=\"noopener\">微软资深工程师详解 K8S 容器运行时</a><br><a href=\"https://cizixs.com/2016/10/25/kubernetes-intro-kubelet/\" target=\"_blank\" rel=\"noopener\">kubernetes 简介： kubelet 和 pod</a><br><a href=\"https://blog.csdn.net/jettery/article/details/78891733\" target=\"_blank\" rel=\"noopener\">Kubelet 组件解析</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一、概要\"><a href=\"#一、概要\" class=\"headerlink\" title=\"一、概要\"></a>一、概要</h2><p>kubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。</p>\n<p>kubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。</p>\n<h2 id=\"二、kubelet-的主要功能\"><a href=\"#二、kubelet-的主要功能\" class=\"headerlink\" title=\"二、kubelet 的主要功能\"></a>二、kubelet 的主要功能</h2><p>1、kubelet  默认监听四个端口，分别为 10250 、10255、10248、4194。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LISTEN     0      128          *:10250                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=28))</span><br><span class=\"line\">LISTEN     0      128          *:10255                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=26))</span><br><span class=\"line\">LISTEN     0      128          *:4194                     *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=13))</span><br><span class=\"line\">LISTEN     0      128    127.0.0.1:10248                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=23))</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。</p>\n</li>\n<li><p>10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 <code>--healthz-port</code> 和 <code>--healthz-bind-address</code> 来指定监听的地址和端口。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://127.0.0.1:10248/healthz</span><br><span class=\"line\">ok</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 <a href=\"http://localhost:4194\" target=\"_blank\" rel=\"noopener\">http://localhost:4194</a> 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 <code>--cadvisor-port</code> 可以指定启动的端口。</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl  http://127.0.0.1:4194/metrics</span><br></pre></td></tr></table></figure>\n<ul>\n<li>10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//  获取 pod 的接口，与 apiserver 的 </span><br><span class=\"line\">// http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName=  接口类似</span><br><span class=\"line\">$ curl  http://127.0.0.1:10255/pods</span><br><span class=\"line\"></span><br><span class=\"line\">// 节点信息接口,提供磁盘、网络、CPU、内存等信息</span><br><span class=\"line\">$ curl http://127.0.0.1:10255/spec/</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>2、kubelet 主要功能：</p>\n<ul>\n<li><p>pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。</p>\n</li>\n<li><p>容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。</p>\n</li>\n<li><p>容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。</p>\n</li>\n</ul>\n<h2 id=\"三、kubelet-组件中的模块\"><a href=\"#三、kubelet-组件中的模块\" class=\"headerlink\" title=\"三、kubelet 组件中的模块\"></a>三、kubelet 组件中的模块</h2><p> <img src=\"https://upload-images.jianshu.io/upload_images/1262158-91b13623d3cf45ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubelet 组件中的模块\"></p>\n<p>上图展示了 kubelet 组件中的模块以及模块间的划分。</p>\n<ul>\n<li><p>1、PLEG(Pod Lifecycle Event Generator）<br>PLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。</p>\n</li>\n<li><p>2、cAdvisor<br>cAdvisor（<a href=\"https://github.com/google/cadvisor）是\" target=\"_blank\" rel=\"noopener\">https://github.com/google/cadvisor）是</a> google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。</p>\n</li>\n<li><p>3、OOMWatcher<br>系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。</p>\n</li>\n<li><p>4、probeManager<br>probeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。<br>livenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。<br>readinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 </p>\n</li>\n<li><p>5、statusManager<br>statusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。</p>\n</li>\n<li><p>6、containerRefManager<br>容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。</p>\n</li>\n<li><p>7、evictionManager<br>当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 <code>--eviction-hard=</code> 来决定 evict 的策略值。</p>\n</li>\n<li><p>8、imageGC<br>imageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 <code>--image-gc-high-threshold</code> 和 <code>--image-gc-low-threshold</code> 来设置。</p>\n</li>\n<li><p>9、containerGC<br>containerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。</p>\n</li>\n<li><p>10、imageManager<br>调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。</p>\n</li>\n<li><p>11、volumeManager<br>负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。</p>\n</li>\n<li><p>12、containerManager<br>负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 <code>--cgroups-per-qos</code> 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 <code>true</code>，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。</p>\n</li>\n<li><p>13、runtimeManager<br>containerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 <code>--container-runtime</code> 来定义是使用docker 还是 rkt，默认是 <code>docker</code>。</p>\n</li>\n<li><p>14、podManager<br>podManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 </p>\n</li>\n</ul>\n<p>在 v1.12 中，kubelet 组件有18个 manager：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">certificateManager</span><br><span class=\"line\">cgroupManager</span><br><span class=\"line\">containerManager</span><br><span class=\"line\">cpuManager</span><br><span class=\"line\">nodeContainerManager</span><br><span class=\"line\">configmapManager</span><br><span class=\"line\">containerReferenceManager</span><br><span class=\"line\">evictionManager</span><br><span class=\"line\">nvidiaGpuManager</span><br><span class=\"line\">imageGCManager</span><br><span class=\"line\">kuberuntimeManager</span><br><span class=\"line\">hostportManager</span><br><span class=\"line\">podManager</span><br><span class=\"line\">proberManager</span><br><span class=\"line\">secretManager</span><br><span class=\"line\">statusManager</span><br><span class=\"line\">volumeManager\t</span><br><span class=\"line\">tokenManager</span><br></pre></td></tr></table></figure>\n<p>其中比较重要的模块后面会进行一一分析。</p>\n<p>参考：<br><a href=\"https://juejin.im/entry/5bc71b3d6fb9a05cf23029b8\" target=\"_blank\" rel=\"noopener\">微软资深工程师详解 K8S 容器运行时</a><br><a href=\"https://cizixs.com/2016/10/25/kubernetes-intro-kubelet/\" target=\"_blank\" rel=\"noopener\">kubernetes 简介： kubelet 和 pod</a><br><a href=\"https://blog.csdn.net/jettery/article/details/78891733\" target=\"_blank\" rel=\"noopener\">Kubelet 组件解析</a></p>\n"},{"title":"kubelet 创建 pod 的流程","date":"2019-01-03T00:15:30.000Z","type":"kubelet","_content":"上篇文章介绍了 [kubelet 的启动流程](http://blog.tianfeiyu.com/2018/12/23/kubelet_init/)，本篇文章主要介绍 kubelet 创建 pod 的流程。\n \n> kubernetes 版本： v1.12 \n\n![kubelet 工作原理](https://upload-images.jianshu.io/upload_images/1262158-78c8272f4b617c92.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nkubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。\n\n\n本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。\n\n## kubelet 创建 pod 的流程\n\n![kubelet 创建 pod 的流程](https://upload-images.jianshu.io/upload_images/1262158-bb6b29bac1ca6c9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### 1、kubelet 的控制循环（syncLoop）\n\nsyncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。\n\n\n```\nfunc (kl *Kubelet) syncLoop(updates <-chan kubetypes.PodUpdate, handler SyncHandler) {\n\tglog.Info(\"Starting kubelet main sync loop.\")\n\n\t// syncTicker 每秒检测一次是否有需要同步的 pod workers\n\tsyncTicker := time.NewTicker(time.Second)\n\tdefer syncTicker.Stop()\n\t// 每两秒检测一次是否有需要清理的 pod\n\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\n\tdefer housekeepingTicker.Stop()\n\t// pod 的生命周期变化\n\tplegCh := kl.pleg.Watch()\n\tconst (\n\t\tbase   = 100 * time.Millisecond\n\t\tmax    = 5 * time.Second\n\t\tfactor = 2\n\t)\n\tduration := base\n\tfor {\n\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {\n\t\t\ttime.Sleep(duration)\n\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))\n\t\t\tcontinue\n\t\t}\n        ...\n\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t\t// 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface，\n\t\t// 在该文件开头处定义\n\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {\n\t\t\tbreak\n\t\t}\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t}\n}\n```\n\n \n\n### 2、监听 pod 变化（syncLoopIteration） \n\nsyncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息：\n\n- configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。\n- syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态\n- houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作\n- plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。\n- livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作\n\n\n```\nfunc (kl *Kubelet) syncLoopIteration(configCh <-chan kubetypes.PodUpdate, handler SyncHandler,\n\tsyncCh <-chan time.Time, housekeepingCh <-chan time.Time, plegCh <-chan *pleg.PodLifecycleEvent) bool {\n\tselect {\n\tcase u, open := <-configCh:\n\t\tif !open {\n\t\t\tglog.Errorf(\"Update channel is closed. Exiting the sync loop.\")\n\t\t\treturn false\n\t\t}\n\n\t\tswitch u.Op {\n\t\tcase kubetypes.ADD:\n\t\t\t...\n\t\tcase kubetypes.UPDATE:\n\t\t\t...\n\t\tcase kubetypes.REMOVE:\n\t\t\t...\n\t\tcase kubetypes.RECONCILE:\n\t\t\t...\n\t\tcase kubetypes.DELETE:\n\t\t\t...\n\t\tcase kubetypes.RESTORE:\n\t\t\t...\n\t\tcase kubetypes.SET:\n\t\t\t...\n\t\t}\n\t\t...\n\tcase e := <-plegCh:\n\t\t...\n\tcase <-syncCh:\n\t\t...\n\tcase update := <-kl.livenessManager.Updates():\n\t\t...\n\tcase <-housekeepingCh:\n\t\t...\n\t}\n\treturn true\n}\n```\n\n### 3、处理新增 pod（HandlePodAddtions）\n\n对于事件中的每个 pod，执行以下操作：\n\n- 1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理\n- 2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了\n- 3、如果是 mirror pod 调用其单独的方法\n- 4、验证 pod 是否能在该节点运行，如果不可以直接拒绝\n- 5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理\n- 6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测\n\n```\nfunc (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {\n\tstart := kl.clock.Now()\n\t// 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理\n\tsort.Sort(sliceutils.PodsByCreationTime(pods))\n\tfor _, pod := range pods {\n\t\tif kl.dnsConfigurer != nil && kl.dnsConfigurer.ResolverConfig != \"\" {\n\t\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()\n\t\t}\n\t\texistingPods := kl.podManager.GetPods()\n\t\t// 把 pod 加入到 podManager 中\n\t\tkl.podManager.AddPod(pod)\n\n\t\t// 判断是否是 mirror pod（即 static pod）\n\t\tif kubepod.IsMirrorPod(pod) {\n\t\t\tkl.handleMirrorPod(pod, start)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !kl.podIsTerminated(pod) {\n\t\t\tactivePods := kl.filterOutTerminatedPods(existingPods)\n\t\t\t// 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间)\n\t\t\t// Check if we can admit the pod; if not, reject it.\n\t\t\tif ok, reason, message := kl.canAdmitPod(activePods, pod); !ok {\n\t\t\t\tkl.rejectPod(pod, reason, message)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t\n\t\tmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)\n\t\t// 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法.\n\t\tkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)\n\t\t// 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测\n\t\tkl.probeManager.AddPod(pod)\n\t}\n}\n```\n\n> static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。\n\n\n### 4、下发任务（dispatchWork）\n\ndispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。\n\n```\nfunc (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) {\n\tif kl.podIsTerminated(pod) {\n\t\tif pod.DeletionTimestamp != nil {\n\t\t\tkl.statusManager.TerminatePod(pod)\n\t\t}\n\t\treturn\n\t}\n\t// 落实在 podWorkers 中\n\tkl.podWorkers.UpdatePod(&UpdatePodOptions{\n\t\tPod:        pod,\n\t\tMirrorPod:  mirrorPod,\n\t\tUpdateType: syncType,\n\t\tOnCompleteFunc: func(err error) {\n\t\t\tif err != nil {\n\t\t\t\tmetrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start))\n\t\t\t}\n\t\t},\n\t})\n\tif syncType == kubetypes.SyncPodCreate {\n\t\tmetrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))\n\t}\n}\n```\n\n\n### 5、更新事件的 channel（UpdatePod）\n\npodWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。\n\n\n```\nfunc (p *podWorkers) UpdatePod(options *UpdatePodOptions) {\n\tpod := options.Pod\n\tuid := pod.UID\n\tvar podUpdates chan UpdatePodOptions\n\tvar exists bool\n\n\tp.podLock.Lock()\n\tdefer p.podLock.Unlock()\n\n\t// 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel\n\tif podUpdates, exists = p.podUpdates[uid]; !exists {\n\t\t// 创建 channel\n\t\tpodUpdates = make(chan UpdatePodOptions, 1)\n\t\tp.podUpdates[uid] = podUpdates\n\n\t\t// 启动 goroutine\n\t\tgo func() {\n\t\t\tdefer runtime.HandleCrash()\n\t\t\tp.managePodLoop(podUpdates)\n\t\t}()\n\t}\n\t// 下发更新事件\n\tif !p.isWorking[pod.UID] {\n\t\tp.isWorking[pod.UID] = true\n\t\tpodUpdates <- *options\n\t} else {\n\t\tupdate, found := p.lastUndeliveredWorkUpdate[pod.UID]\n\t\tif !found || update.UpdateType != kubetypes.SyncPodKill {\n\t\t\tp.lastUndeliveredWorkUpdate[pod.UID] = *options\n\t\t}\n\t}\n}\n```\n\n### 6、调用 syncPodFn 方法同步 pod（managePodLoop）\nmanagePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情:\n\n- 将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync\n- 将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。\n\n\n```\nfunc (p *podWorkers) managePodLoop(podUpdates <-chan UpdatePodOptions) {\n\tvar lastSyncTime time.Time\n\tfor update := range podUpdates {\n\t\terr := func() error {\n\t\t\tpodUID := update.Pod.UID\n\t\t\tstatus, err := p.podCache.GetNewerThan(podUID, lastSyncTime)\n\t\t\tif err != nil {\n\t\t\t\t...\n\t\t\t}\n\t\t\terr = p.syncPodFn(syncPodOptions{\n\t\t\t\tmirrorPod:      update.MirrorPod,\n\t\t\t\tpod:            update.Pod,\n\t\t\t\tpodStatus:      status,\n\t\t\t\tkillPodOptions: update.KillPodOptions,\n\t\t\t\tupdateType:     update.UpdateType,\n\t\t\t})\n\t\t\tlastSyncTime = time.Now()\n\t\t\treturn err\n\t\t}()\n\t\tif update.OnCompleteFunc != nil {\n\t\t\tupdate.OnCompleteFunc(err)\n\t\t}\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\tp.wrapUp(update.Pod.UID, err)\n\t}\n}\n```\n\n### 7、完成创建容器前的准备工作（SyncPod）\n\n在这个方法中，主要完成以下几件事情：\n\n- 如果是删除 pod，立即执行并返回\n- 同步 podStatus 到 kubelet.statusManager\n- 检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息\n- 创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup\n- 如果是 static Pod，就创建或者更新对应的 mirrorPod\n- 创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据\n- 然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。\n- 调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑\n\n这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。\n\n```\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error {\n\t// pull out the required options\n\tpod := o.pod\n\tmirrorPod := o.mirrorPod\n\tpodStatus := o.podStatus\n\tupdateType := o.updateType\n\n\t// 是否为 删除 pod\n\tif updateType == kubetypes.SyncPodKill {\n\t\t...\n\t}\n    ...\n\t// 检查 pod 是否能运行在本节点\n\trunnable := kl.canRunPod(pod)\n\tif !runnable.Admit {\n\t\t...\n\t}\n\n\t// 更新 pod 状态\n\tkl.statusManager.SetPodStatus(pod, apiPodStatus)\n\n\t// 如果 pod 非 running 状态则直接 kill 掉\n\tif !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed {\n\t\t...\n\t}\n\n\t// 加载网络插件\n\tif rs := kl.runtimeState.networkErrors(); len(rs) != 0 && !kubecontainer.IsHostNetworkPod(pod) {\n\t\t...\n\t}\n\n\tpcm := kl.containerManager.NewPodContainerManager()\n\tif !kl.podIsTerminated(pod) {\n\t\t...\n\t\t// 创建并更新 pod 的 cgroups\n\t\tif !(podKilled && pod.Spec.RestartPolicy == v1.RestartPolicyNever) {\n\t\t\tif !pcm.Exists(pod) {\n\t\t\t\t...\n\t\t\t}\n\t\t}\n\t}\n\n\t// 为 static pod 创建对应的 mirror pod\n\tif kubepod.IsStaticPod(pod) {\n\t\t...\n\t}\n\n\t// 创建数据目录\n\tif err := kl.makePodDataDirs(pod); err != nil {\n\t\t...\n\t}\n\n\t// 挂载 volume\n\tif !kl.podIsTerminated(pod) {\n\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\t// 获取 secret 信息\n\tpullSecrets := kl.getPullSecretsForPod(pod)\n\n\t// 调用 containerRuntime 的 SyncPod 方法开始创建容器\n\tresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)\n\tkl.reasonCache.Update(pod.UID, result)\n\tif err := result.Error(); err != nil {\n\t\t...\n\t}\n\n\treturn nil\n}\n```\n\n\n### 8、创建容器\n\ncontainerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。\nsyncPod 主要执行以下几个操作：\n- 1、计算 sandbox 和 container 是否发生变化\n- 2、创建 sandbox 容器\n- 3、启动 init 容器\n- 4、启动业务容器\n\ninitContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。\n\n```\nfunc (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {\n\t// 1、计算 sandbox 和 container 是否发生变化\n\tpodContainerChanges := m.computePodActions(pod, podStatus)\n\tif podContainerChanges.CreateSandbox {\n\t\tref, err := ref.GetReference(legacyscheme.Scheme, pod)\n\t\tif err != nil {\n\t\t\tglog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err)\n\t\t}\n\t\t...\n\t}\n\n\t// 2、kill 掉 sandbox 已经改变的 pod\n\tif podContainerChanges.KillPod {\n\t\t...\n\t} else {\n\t\t// 3、kill 掉非 running 状态的 containers\n\t\t...\n\t\tfor containerID, containerInfo := range podContainerChanges.ContainersToKill {\n\t\t\t...\n\t\t\tif err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil {\n\t\t\t\t...\n\t\t\t}\n\t\t}\n\t}\n\n\tm.pruneInitContainersBeforeStart(pod, podStatus)\n\tpodIP := \"\"\n\tif podStatus != nil {\n\t\tpodIP = podStatus.IP\n\t}\n\n\t// 4、创建 sandbox \n\tpodSandboxID := podContainerChanges.SandboxID\n\tif podContainerChanges.CreateSandbox {\n\t\tpodSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\t...\n\t\tpodSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\t// 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置\n\t\tif !kubecontainer.IsHostNetworkPod(pod) {\n\t\t\tpodIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus)\n\t\t\tglog.V(4).Infof(\"Determined the ip %q for pod %q after sandbox changed\", podIP, format.Pod(pod))\n\t\t}\n\t}\n\n\tconfigPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)\n\tresult.AddSyncResult(configPodSandboxResult)\n\t// 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等)\n\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)\n\t...\n\n\t// 5、启动 init container\n\tif container := podContainerChanges.NextInitContainerToStart; container != nil {\n\t\t...\n\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\t// 6、启动业务容器\n\tfor _, idx := range podContainerChanges.ContainersToStart {\n\t\t...\n\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\t\n\treturn\n}\n```\n\n\n### 9、启动容器\n\n最终由 startContainer 完成容器的启动，其主要有以下几个步骤：\n\n- 1、拉取镜像\n- 2、生成业务容器的配置信息\n- 3、调用 docker api 创建容器\n- 4、启动容器\n- 5、执行 post start hook\n\n\n```\nfunc (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) {\n\t// 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。\n\timageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)\n\tif err != nil {\n\t\t...\n\t}\n\n\tref, err := kubecontainer.GenerateContainerRef(pod, container)\n\tif err != nil {\n\t\t...\n\t}\n\n\t// 设置 RestartCount \n\trestartCount := 0\n\tcontainerStatus := podStatus.FindContainerStatusByName(container.Name)\n\tif containerStatus != nil {\n\t\trestartCount = containerStatus.RestartCount + 1\n\t}\n\n\t// 2、生成业务容器的配置信息\n\tcontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType)\n\tif cleanupAction != nil {\n\t\tdefer cleanupAction()\n\t}\n\t...\n\n\t// 3、通过 client.CreateContainer 调用 docker api 创建业务容器\n\tcontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)\n\tif err != nil {\n\t\t...\n\t}\n\terr = m.internalLifecycle.PreStartContainer(pod, container, containerID)\n\tif err != nil {\n\t\t...\n\t}\n\t...\n\n\t// 3、启动业务容器\n\terr = m.runtimeService.StartContainer(containerID)\n\tif err != nil {\n\t\t...\n\t}\n\n\tcontainerMeta := containerConfig.GetMetadata()\n\tsandboxMeta := podSandboxConfig.GetMetadata()\n\tlegacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name,\n\t\tsandboxMeta.Namespace)\n\tcontainerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath)\n\tif _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) {\n\t\tif err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil {\n\t\t\tglog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\",\n\t\t\t\tlegacySymlink, containerID, containerLog, err)\n\t\t}\n\t}\n\n\t// 4、执行 post start hook\n\tif container.Lifecycle != nil && container.Lifecycle.PostStart != nil {\n\t\tkubeContainerID := kubecontainer.ContainerID{\n\t\t\tType: m.runtimeName,\n\t\t\tID:   containerID,\n\t\t}\n\t\t// runner.Run 这个方法的主要作用就是在业务容器起来的时候，\n\t\t// 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。\n\t\t// 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。\n\t\tmsg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)\n\t\tif handlerErr != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\treturn \"\", nil\n}\n```\n\n\n## 总结\n\n本文主要讲述了 kubelet 从监听到容器调度至本节点再到创建容器的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。\n\n\n参考：\n[k8s源码分析-kubelet](https://sycki.com/articles/kubernetes/k8s-code-kubelet)\n[Kubelet源码分析(一):启动流程分析](https://segmentfault.com/a/1190000008267351)\n[kubelet 源码分析：pod 新建流程](http://cizixs.com/2017/06/07/kubelet-source-code-analysis-part-2/)\n[kubelet创建Pod流程解析](https://fatsheep9146.github.io/2018/07/22/kubelet%E5%88%9B%E5%BB%BAPod%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/)\n[Kubelet: Pod Lifecycle Event Generator (PLEG) Design-\tproposals](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md)\n\n","source":"_posts/kubelet_create_pod.md","raw":"---\ntitle: kubelet 创建 pod 的流程\ndate: 2019-01-03 08:15:30\ntags: \"kubelet\"\ntype: \"kubelet\"\n\n---\n上篇文章介绍了 [kubelet 的启动流程](http://blog.tianfeiyu.com/2018/12/23/kubelet_init/)，本篇文章主要介绍 kubelet 创建 pod 的流程。\n \n> kubernetes 版本： v1.12 \n\n![kubelet 工作原理](https://upload-images.jianshu.io/upload_images/1262158-78c8272f4b617c92.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\nkubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。\n\n\n本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。\n\n## kubelet 创建 pod 的流程\n\n![kubelet 创建 pod 的流程](https://upload-images.jianshu.io/upload_images/1262158-bb6b29bac1ca6c9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### 1、kubelet 的控制循环（syncLoop）\n\nsyncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。\n\n\n```\nfunc (kl *Kubelet) syncLoop(updates <-chan kubetypes.PodUpdate, handler SyncHandler) {\n\tglog.Info(\"Starting kubelet main sync loop.\")\n\n\t// syncTicker 每秒检测一次是否有需要同步的 pod workers\n\tsyncTicker := time.NewTicker(time.Second)\n\tdefer syncTicker.Stop()\n\t// 每两秒检测一次是否有需要清理的 pod\n\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\n\tdefer housekeepingTicker.Stop()\n\t// pod 的生命周期变化\n\tplegCh := kl.pleg.Watch()\n\tconst (\n\t\tbase   = 100 * time.Millisecond\n\t\tmax    = 5 * time.Second\n\t\tfactor = 2\n\t)\n\tduration := base\n\tfor {\n\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {\n\t\t\ttime.Sleep(duration)\n\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))\n\t\t\tcontinue\n\t\t}\n        ...\n\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t\t// 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface，\n\t\t// 在该文件开头处定义\n\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {\n\t\t\tbreak\n\t\t}\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t}\n}\n```\n\n \n\n### 2、监听 pod 变化（syncLoopIteration） \n\nsyncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息：\n\n- configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。\n- syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态\n- houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作\n- plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。\n- livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作\n\n\n```\nfunc (kl *Kubelet) syncLoopIteration(configCh <-chan kubetypes.PodUpdate, handler SyncHandler,\n\tsyncCh <-chan time.Time, housekeepingCh <-chan time.Time, plegCh <-chan *pleg.PodLifecycleEvent) bool {\n\tselect {\n\tcase u, open := <-configCh:\n\t\tif !open {\n\t\t\tglog.Errorf(\"Update channel is closed. Exiting the sync loop.\")\n\t\t\treturn false\n\t\t}\n\n\t\tswitch u.Op {\n\t\tcase kubetypes.ADD:\n\t\t\t...\n\t\tcase kubetypes.UPDATE:\n\t\t\t...\n\t\tcase kubetypes.REMOVE:\n\t\t\t...\n\t\tcase kubetypes.RECONCILE:\n\t\t\t...\n\t\tcase kubetypes.DELETE:\n\t\t\t...\n\t\tcase kubetypes.RESTORE:\n\t\t\t...\n\t\tcase kubetypes.SET:\n\t\t\t...\n\t\t}\n\t\t...\n\tcase e := <-plegCh:\n\t\t...\n\tcase <-syncCh:\n\t\t...\n\tcase update := <-kl.livenessManager.Updates():\n\t\t...\n\tcase <-housekeepingCh:\n\t\t...\n\t}\n\treturn true\n}\n```\n\n### 3、处理新增 pod（HandlePodAddtions）\n\n对于事件中的每个 pod，执行以下操作：\n\n- 1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理\n- 2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了\n- 3、如果是 mirror pod 调用其单独的方法\n- 4、验证 pod 是否能在该节点运行，如果不可以直接拒绝\n- 5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理\n- 6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测\n\n```\nfunc (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {\n\tstart := kl.clock.Now()\n\t// 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理\n\tsort.Sort(sliceutils.PodsByCreationTime(pods))\n\tfor _, pod := range pods {\n\t\tif kl.dnsConfigurer != nil && kl.dnsConfigurer.ResolverConfig != \"\" {\n\t\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()\n\t\t}\n\t\texistingPods := kl.podManager.GetPods()\n\t\t// 把 pod 加入到 podManager 中\n\t\tkl.podManager.AddPod(pod)\n\n\t\t// 判断是否是 mirror pod（即 static pod）\n\t\tif kubepod.IsMirrorPod(pod) {\n\t\t\tkl.handleMirrorPod(pod, start)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !kl.podIsTerminated(pod) {\n\t\t\tactivePods := kl.filterOutTerminatedPods(existingPods)\n\t\t\t// 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间)\n\t\t\t// Check if we can admit the pod; if not, reject it.\n\t\t\tif ok, reason, message := kl.canAdmitPod(activePods, pod); !ok {\n\t\t\t\tkl.rejectPod(pod, reason, message)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t\n\t\tmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)\n\t\t// 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法.\n\t\tkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)\n\t\t// 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测\n\t\tkl.probeManager.AddPod(pod)\n\t}\n}\n```\n\n> static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。\n\n\n### 4、下发任务（dispatchWork）\n\ndispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。\n\n```\nfunc (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) {\n\tif kl.podIsTerminated(pod) {\n\t\tif pod.DeletionTimestamp != nil {\n\t\t\tkl.statusManager.TerminatePod(pod)\n\t\t}\n\t\treturn\n\t}\n\t// 落实在 podWorkers 中\n\tkl.podWorkers.UpdatePod(&UpdatePodOptions{\n\t\tPod:        pod,\n\t\tMirrorPod:  mirrorPod,\n\t\tUpdateType: syncType,\n\t\tOnCompleteFunc: func(err error) {\n\t\t\tif err != nil {\n\t\t\t\tmetrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start))\n\t\t\t}\n\t\t},\n\t})\n\tif syncType == kubetypes.SyncPodCreate {\n\t\tmetrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))\n\t}\n}\n```\n\n\n### 5、更新事件的 channel（UpdatePod）\n\npodWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。\n\n\n```\nfunc (p *podWorkers) UpdatePod(options *UpdatePodOptions) {\n\tpod := options.Pod\n\tuid := pod.UID\n\tvar podUpdates chan UpdatePodOptions\n\tvar exists bool\n\n\tp.podLock.Lock()\n\tdefer p.podLock.Unlock()\n\n\t// 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel\n\tif podUpdates, exists = p.podUpdates[uid]; !exists {\n\t\t// 创建 channel\n\t\tpodUpdates = make(chan UpdatePodOptions, 1)\n\t\tp.podUpdates[uid] = podUpdates\n\n\t\t// 启动 goroutine\n\t\tgo func() {\n\t\t\tdefer runtime.HandleCrash()\n\t\t\tp.managePodLoop(podUpdates)\n\t\t}()\n\t}\n\t// 下发更新事件\n\tif !p.isWorking[pod.UID] {\n\t\tp.isWorking[pod.UID] = true\n\t\tpodUpdates <- *options\n\t} else {\n\t\tupdate, found := p.lastUndeliveredWorkUpdate[pod.UID]\n\t\tif !found || update.UpdateType != kubetypes.SyncPodKill {\n\t\t\tp.lastUndeliveredWorkUpdate[pod.UID] = *options\n\t\t}\n\t}\n}\n```\n\n### 6、调用 syncPodFn 方法同步 pod（managePodLoop）\nmanagePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情:\n\n- 将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync\n- 将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。\n\n\n```\nfunc (p *podWorkers) managePodLoop(podUpdates <-chan UpdatePodOptions) {\n\tvar lastSyncTime time.Time\n\tfor update := range podUpdates {\n\t\terr := func() error {\n\t\t\tpodUID := update.Pod.UID\n\t\t\tstatus, err := p.podCache.GetNewerThan(podUID, lastSyncTime)\n\t\t\tif err != nil {\n\t\t\t\t...\n\t\t\t}\n\t\t\terr = p.syncPodFn(syncPodOptions{\n\t\t\t\tmirrorPod:      update.MirrorPod,\n\t\t\t\tpod:            update.Pod,\n\t\t\t\tpodStatus:      status,\n\t\t\t\tkillPodOptions: update.KillPodOptions,\n\t\t\t\tupdateType:     update.UpdateType,\n\t\t\t})\n\t\t\tlastSyncTime = time.Now()\n\t\t\treturn err\n\t\t}()\n\t\tif update.OnCompleteFunc != nil {\n\t\t\tupdate.OnCompleteFunc(err)\n\t\t}\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\tp.wrapUp(update.Pod.UID, err)\n\t}\n}\n```\n\n### 7、完成创建容器前的准备工作（SyncPod）\n\n在这个方法中，主要完成以下几件事情：\n\n- 如果是删除 pod，立即执行并返回\n- 同步 podStatus 到 kubelet.statusManager\n- 检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息\n- 创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup\n- 如果是 static Pod，就创建或者更新对应的 mirrorPod\n- 创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据\n- 然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。\n- 调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑\n\n这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。\n\n```\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error {\n\t// pull out the required options\n\tpod := o.pod\n\tmirrorPod := o.mirrorPod\n\tpodStatus := o.podStatus\n\tupdateType := o.updateType\n\n\t// 是否为 删除 pod\n\tif updateType == kubetypes.SyncPodKill {\n\t\t...\n\t}\n    ...\n\t// 检查 pod 是否能运行在本节点\n\trunnable := kl.canRunPod(pod)\n\tif !runnable.Admit {\n\t\t...\n\t}\n\n\t// 更新 pod 状态\n\tkl.statusManager.SetPodStatus(pod, apiPodStatus)\n\n\t// 如果 pod 非 running 状态则直接 kill 掉\n\tif !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed {\n\t\t...\n\t}\n\n\t// 加载网络插件\n\tif rs := kl.runtimeState.networkErrors(); len(rs) != 0 && !kubecontainer.IsHostNetworkPod(pod) {\n\t\t...\n\t}\n\n\tpcm := kl.containerManager.NewPodContainerManager()\n\tif !kl.podIsTerminated(pod) {\n\t\t...\n\t\t// 创建并更新 pod 的 cgroups\n\t\tif !(podKilled && pod.Spec.RestartPolicy == v1.RestartPolicyNever) {\n\t\t\tif !pcm.Exists(pod) {\n\t\t\t\t...\n\t\t\t}\n\t\t}\n\t}\n\n\t// 为 static pod 创建对应的 mirror pod\n\tif kubepod.IsStaticPod(pod) {\n\t\t...\n\t}\n\n\t// 创建数据目录\n\tif err := kl.makePodDataDirs(pod); err != nil {\n\t\t...\n\t}\n\n\t// 挂载 volume\n\tif !kl.podIsTerminated(pod) {\n\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\t// 获取 secret 信息\n\tpullSecrets := kl.getPullSecretsForPod(pod)\n\n\t// 调用 containerRuntime 的 SyncPod 方法开始创建容器\n\tresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)\n\tkl.reasonCache.Update(pod.UID, result)\n\tif err := result.Error(); err != nil {\n\t\t...\n\t}\n\n\treturn nil\n}\n```\n\n\n### 8、创建容器\n\ncontainerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。\nsyncPod 主要执行以下几个操作：\n- 1、计算 sandbox 和 container 是否发生变化\n- 2、创建 sandbox 容器\n- 3、启动 init 容器\n- 4、启动业务容器\n\ninitContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。\n\n```\nfunc (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {\n\t// 1、计算 sandbox 和 container 是否发生变化\n\tpodContainerChanges := m.computePodActions(pod, podStatus)\n\tif podContainerChanges.CreateSandbox {\n\t\tref, err := ref.GetReference(legacyscheme.Scheme, pod)\n\t\tif err != nil {\n\t\t\tglog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err)\n\t\t}\n\t\t...\n\t}\n\n\t// 2、kill 掉 sandbox 已经改变的 pod\n\tif podContainerChanges.KillPod {\n\t\t...\n\t} else {\n\t\t// 3、kill 掉非 running 状态的 containers\n\t\t...\n\t\tfor containerID, containerInfo := range podContainerChanges.ContainersToKill {\n\t\t\t...\n\t\t\tif err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil {\n\t\t\t\t...\n\t\t\t}\n\t\t}\n\t}\n\n\tm.pruneInitContainersBeforeStart(pod, podStatus)\n\tpodIP := \"\"\n\tif podStatus != nil {\n\t\tpodIP = podStatus.IP\n\t}\n\n\t// 4、创建 sandbox \n\tpodSandboxID := podContainerChanges.SandboxID\n\tif podContainerChanges.CreateSandbox {\n\t\tpodSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\t...\n\t\tpodSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\t// 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置\n\t\tif !kubecontainer.IsHostNetworkPod(pod) {\n\t\t\tpodIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus)\n\t\t\tglog.V(4).Infof(\"Determined the ip %q for pod %q after sandbox changed\", podIP, format.Pod(pod))\n\t\t}\n\t}\n\n\tconfigPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)\n\tresult.AddSyncResult(configPodSandboxResult)\n\t// 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等)\n\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)\n\t...\n\n\t// 5、启动 init container\n\tif container := podContainerChanges.NextInitContainerToStart; container != nil {\n\t\t...\n\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\t// 6、启动业务容器\n\tfor _, idx := range podContainerChanges.ContainersToStart {\n\t\t...\n\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\t\n\treturn\n}\n```\n\n\n### 9、启动容器\n\n最终由 startContainer 完成容器的启动，其主要有以下几个步骤：\n\n- 1、拉取镜像\n- 2、生成业务容器的配置信息\n- 3、调用 docker api 创建容器\n- 4、启动容器\n- 5、执行 post start hook\n\n\n```\nfunc (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) {\n\t// 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。\n\timageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)\n\tif err != nil {\n\t\t...\n\t}\n\n\tref, err := kubecontainer.GenerateContainerRef(pod, container)\n\tif err != nil {\n\t\t...\n\t}\n\n\t// 设置 RestartCount \n\trestartCount := 0\n\tcontainerStatus := podStatus.FindContainerStatusByName(container.Name)\n\tif containerStatus != nil {\n\t\trestartCount = containerStatus.RestartCount + 1\n\t}\n\n\t// 2、生成业务容器的配置信息\n\tcontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType)\n\tif cleanupAction != nil {\n\t\tdefer cleanupAction()\n\t}\n\t...\n\n\t// 3、通过 client.CreateContainer 调用 docker api 创建业务容器\n\tcontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)\n\tif err != nil {\n\t\t...\n\t}\n\terr = m.internalLifecycle.PreStartContainer(pod, container, containerID)\n\tif err != nil {\n\t\t...\n\t}\n\t...\n\n\t// 3、启动业务容器\n\terr = m.runtimeService.StartContainer(containerID)\n\tif err != nil {\n\t\t...\n\t}\n\n\tcontainerMeta := containerConfig.GetMetadata()\n\tsandboxMeta := podSandboxConfig.GetMetadata()\n\tlegacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name,\n\t\tsandboxMeta.Namespace)\n\tcontainerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath)\n\tif _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) {\n\t\tif err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil {\n\t\t\tglog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\",\n\t\t\t\tlegacySymlink, containerID, containerLog, err)\n\t\t}\n\t}\n\n\t// 4、执行 post start hook\n\tif container.Lifecycle != nil && container.Lifecycle.PostStart != nil {\n\t\tkubeContainerID := kubecontainer.ContainerID{\n\t\t\tType: m.runtimeName,\n\t\t\tID:   containerID,\n\t\t}\n\t\t// runner.Run 这个方法的主要作用就是在业务容器起来的时候，\n\t\t// 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。\n\t\t// 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。\n\t\tmsg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)\n\t\tif handlerErr != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\treturn \"\", nil\n}\n```\n\n\n## 总结\n\n本文主要讲述了 kubelet 从监听到容器调度至本节点再到创建容器的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。\n\n\n参考：\n[k8s源码分析-kubelet](https://sycki.com/articles/kubernetes/k8s-code-kubelet)\n[Kubelet源码分析(一):启动流程分析](https://segmentfault.com/a/1190000008267351)\n[kubelet 源码分析：pod 新建流程](http://cizixs.com/2017/06/07/kubelet-source-code-analysis-part-2/)\n[kubelet创建Pod流程解析](https://fatsheep9146.github.io/2018/07/22/kubelet%E5%88%9B%E5%BB%BAPod%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/)\n[Kubelet: Pod Lifecycle Event Generator (PLEG) Design-\tproposals](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md)\n\n","slug":"kubelet_create_pod","published":1,"updated":"2019-01-03T01:15:06.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5gpx000s22dvjf8k5z8e","content":"<p>上篇文章介绍了 <a href=\"http://blog.tianfeiyu.com/2018/12/23/kubelet_init/\" target=\"_blank\" rel=\"noopener\">kubelet 的启动流程</a>，本篇文章主要介绍 kubelet 创建 pod 的流程。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-78c8272f4b617c92.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubelet 工作原理\"></p>\n<p>kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。</p>\n<p>本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。</p>\n<h2 id=\"kubelet-创建-pod-的流程\"><a href=\"#kubelet-创建-pod-的流程\" class=\"headerlink\" title=\"kubelet 创建 pod 的流程\"></a>kubelet 创建 pod 的流程</h2><p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-bb6b29bac1ca6c9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubelet 创建 pod 的流程\"></p>\n<h3 id=\"1、kubelet-的控制循环（syncLoop）\"><a href=\"#1、kubelet-的控制循环（syncLoop）\" class=\"headerlink\" title=\"1、kubelet 的控制循环（syncLoop）\"></a>1、kubelet 的控制循环（syncLoop）</h3><p>syncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123;</span><br><span class=\"line\">\tglog.Info(&quot;Starting kubelet main sync loop.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// syncTicker 每秒检测一次是否有需要同步的 pod workers</span><br><span class=\"line\">\tsyncTicker := time.NewTicker(time.Second)</span><br><span class=\"line\">\tdefer syncTicker.Stop()</span><br><span class=\"line\">\t// 每两秒检测一次是否有需要清理的 pod</span><br><span class=\"line\">\thousekeepingTicker := time.NewTicker(housekeepingPeriod)</span><br><span class=\"line\">\tdefer housekeepingTicker.Stop()</span><br><span class=\"line\">\t// pod 的生命周期变化</span><br><span class=\"line\">\tplegCh := kl.pleg.Watch()</span><br><span class=\"line\">\tconst (</span><br><span class=\"line\">\t\tbase   = 100 * time.Millisecond</span><br><span class=\"line\">\t\tmax    = 5 * time.Second</span><br><span class=\"line\">\t\tfactor = 2</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tduration := base</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123;</span><br><span class=\"line\">\t\t\ttime.Sleep(duration)</span><br><span class=\"line\">\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t\t// 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface，</span><br><span class=\"line\">\t\t// 在该文件开头处定义</span><br><span class=\"line\">\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123;</span><br><span class=\"line\">\t\t\tbreak</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2、监听-pod-变化（syncLoopIteration）\"><a href=\"#2、监听-pod-变化（syncLoopIteration）\" class=\"headerlink\" title=\"2、监听 pod 变化（syncLoopIteration）\"></a>2、监听 pod 变化（syncLoopIteration）</h3><p>syncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息：</p>\n<ul>\n<li>configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。</li>\n<li>syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态</li>\n<li>houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作</li>\n<li>plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。</li>\n<li>livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler,</span><br><span class=\"line\">\tsyncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123;</span><br><span class=\"line\">\tselect &#123;</span><br><span class=\"line\">\tcase u, open := &lt;-configCh:</span><br><span class=\"line\">\t\tif !open &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Update channel is closed. Exiting the sync loop.&quot;)</span><br><span class=\"line\">\t\t\treturn false</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tswitch u.Op &#123;</span><br><span class=\"line\">\t\tcase kubetypes.ADD:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.UPDATE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.REMOVE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.RECONCILE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.DELETE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.RESTORE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.SET:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase e := &lt;-plegCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase &lt;-syncCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase update := &lt;-kl.livenessManager.Updates():</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase &lt;-housekeepingCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn true</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3、处理新增-pod（HandlePodAddtions）\"><a href=\"#3、处理新增-pod（HandlePodAddtions）\" class=\"headerlink\" title=\"3、处理新增 pod（HandlePodAddtions）\"></a>3、处理新增 pod（HandlePodAddtions）</h3><p>对于事件中的每个 pod，执行以下操作：</p>\n<ul>\n<li>1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理</li>\n<li>2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了</li>\n<li>3、如果是 mirror pod 调用其单独的方法</li>\n<li>4、验证 pod 是否能在该节点运行，如果不可以直接拒绝</li>\n<li>5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理</li>\n<li>6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) &#123;</span><br><span class=\"line\">\tstart := kl.clock.Now()</span><br><span class=\"line\">\t// 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理</span><br><span class=\"line\">\tsort.Sort(sliceutils.PodsByCreationTime(pods))</span><br><span class=\"line\">\tfor _, pod := range pods &#123;</span><br><span class=\"line\">\t\tif kl.dnsConfigurer != nil &amp;&amp; kl.dnsConfigurer.ResolverConfig != &quot;&quot; &#123;</span><br><span class=\"line\">\t\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\texistingPods := kl.podManager.GetPods()</span><br><span class=\"line\">\t\t// 把 pod 加入到 podManager 中</span><br><span class=\"line\">\t\tkl.podManager.AddPod(pod)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 判断是否是 mirror pod（即 static pod）</span><br><span class=\"line\">\t\tif kubepod.IsMirrorPod(pod) &#123;</span><br><span class=\"line\">\t\t\tkl.handleMirrorPod(pod, start)</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\t\tactivePods := kl.filterOutTerminatedPods(existingPods)</span><br><span class=\"line\">\t\t\t// 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间)</span><br><span class=\"line\">\t\t\t// Check if we can admit the pod; if not, reject it.</span><br><span class=\"line\">\t\t\tif ok, reason, message := kl.canAdmitPod(activePods, pod); !ok &#123;</span><br><span class=\"line\">\t\t\t\tkl.rejectPod(pod, reason, message)</span><br><span class=\"line\">\t\t\t\tcontinue</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)</span><br><span class=\"line\">\t\t// 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法.</span><br><span class=\"line\">\t\tkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)</span><br><span class=\"line\">\t\t// 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测</span><br><span class=\"line\">\t\tkl.probeManager.AddPod(pod)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。</p>\n</blockquote>\n<h3 id=\"4、下发任务（dispatchWork）\"><a href=\"#4、下发任务（dispatchWork）\" class=\"headerlink\" title=\"4、下发任务（dispatchWork）\"></a>4、下发任务（dispatchWork）</h3><p>dispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) &#123;</span><br><span class=\"line\">\tif kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\tif pod.DeletionTimestamp != nil &#123;</span><br><span class=\"line\">\t\t\tkl.statusManager.TerminatePod(pod)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 落实在 podWorkers 中</span><br><span class=\"line\">\tkl.podWorkers.UpdatePod(&amp;UpdatePodOptions&#123;</span><br><span class=\"line\">\t\tPod:        pod,</span><br><span class=\"line\">\t\tMirrorPod:  mirrorPod,</span><br><span class=\"line\">\t\tUpdateType: syncType,</span><br><span class=\"line\">\t\tOnCompleteFunc: func(err error) &#123;</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tmetrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start))</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t&#125;)</span><br><span class=\"line\">\tif syncType == kubetypes.SyncPodCreate &#123;</span><br><span class=\"line\">\t\tmetrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5、更新事件的-channel（UpdatePod）\"><a href=\"#5、更新事件的-channel（UpdatePod）\" class=\"headerlink\" title=\"5、更新事件的 channel（UpdatePod）\"></a>5、更新事件的 channel（UpdatePod）</h3><p>podWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (p *podWorkers) UpdatePod(options *UpdatePodOptions) &#123;</span><br><span class=\"line\">\tpod := options.Pod</span><br><span class=\"line\">\tuid := pod.UID</span><br><span class=\"line\">\tvar podUpdates chan UpdatePodOptions</span><br><span class=\"line\">\tvar exists bool</span><br><span class=\"line\"></span><br><span class=\"line\">\tp.podLock.Lock()</span><br><span class=\"line\">\tdefer p.podLock.Unlock()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel</span><br><span class=\"line\">\tif podUpdates, exists = p.podUpdates[uid]; !exists &#123;</span><br><span class=\"line\">\t\t// 创建 channel</span><br><span class=\"line\">\t\tpodUpdates = make(chan UpdatePodOptions, 1)</span><br><span class=\"line\">\t\tp.podUpdates[uid] = podUpdates</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 启动 goroutine</span><br><span class=\"line\">\t\tgo func() &#123;</span><br><span class=\"line\">\t\t\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t\t\tp.managePodLoop(podUpdates)</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 下发更新事件</span><br><span class=\"line\">\tif !p.isWorking[pod.UID] &#123;</span><br><span class=\"line\">\t\tp.isWorking[pod.UID] = true</span><br><span class=\"line\">\t\tpodUpdates &lt;- *options</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\tupdate, found := p.lastUndeliveredWorkUpdate[pod.UID]</span><br><span class=\"line\">\t\tif !found || update.UpdateType != kubetypes.SyncPodKill &#123;</span><br><span class=\"line\">\t\t\tp.lastUndeliveredWorkUpdate[pod.UID] = *options</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"6、调用-syncPodFn-方法同步-pod（managePodLoop）\"><a href=\"#6、调用-syncPodFn-方法同步-pod（managePodLoop）\" class=\"headerlink\" title=\"6、调用 syncPodFn 方法同步 pod（managePodLoop）\"></a>6、调用 syncPodFn 方法同步 pod（managePodLoop）</h3><p>managePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情:</p>\n<ul>\n<li>将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync</li>\n<li>将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (p *podWorkers) managePodLoop(podUpdates &lt;-chan UpdatePodOptions) &#123;</span><br><span class=\"line\">\tvar lastSyncTime time.Time</span><br><span class=\"line\">\tfor update := range podUpdates &#123;</span><br><span class=\"line\">\t\terr := func() error &#123;</span><br><span class=\"line\">\t\t\tpodUID := update.Pod.UID</span><br><span class=\"line\">\t\t\tstatus, err := p.podCache.GetNewerThan(podUID, lastSyncTime)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\terr = p.syncPodFn(syncPodOptions&#123;</span><br><span class=\"line\">\t\t\t\tmirrorPod:      update.MirrorPod,</span><br><span class=\"line\">\t\t\t\tpod:            update.Pod,</span><br><span class=\"line\">\t\t\t\tpodStatus:      status,</span><br><span class=\"line\">\t\t\t\tkillPodOptions: update.KillPodOptions,</span><br><span class=\"line\">\t\t\t\tupdateType:     update.UpdateType,</span><br><span class=\"line\">\t\t\t&#125;)</span><br><span class=\"line\">\t\t\tlastSyncTime = time.Now()</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\">\t\tif update.OnCompleteFunc != nil &#123;</span><br><span class=\"line\">\t\t\tupdate.OnCompleteFunc(err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tp.wrapUp(update.Pod.UID, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"7、完成创建容器前的准备工作（SyncPod）\"><a href=\"#7、完成创建容器前的准备工作（SyncPod）\" class=\"headerlink\" title=\"7、完成创建容器前的准备工作（SyncPod）\"></a>7、完成创建容器前的准备工作（SyncPod）</h3><p>在这个方法中，主要完成以下几件事情：</p>\n<ul>\n<li>如果是删除 pod，立即执行并返回</li>\n<li>同步 podStatus 到 kubelet.statusManager</li>\n<li>检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息</li>\n<li>创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup</li>\n<li>如果是 static Pod，就创建或者更新对应的 mirrorPod</li>\n<li>创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据</li>\n<li>然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。</li>\n<li>调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑</li>\n</ul>\n<p>这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncPod(o syncPodOptions) error &#123;</span><br><span class=\"line\">\t// pull out the required options</span><br><span class=\"line\">\tpod := o.pod</span><br><span class=\"line\">\tmirrorPod := o.mirrorPod</span><br><span class=\"line\">\tpodStatus := o.podStatus</span><br><span class=\"line\">\tupdateType := o.updateType</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 是否为 删除 pod</span><br><span class=\"line\">\tif updateType == kubetypes.SyncPodKill &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\t// 检查 pod 是否能运行在本节点</span><br><span class=\"line\">\trunnable := kl.canRunPod(pod)</span><br><span class=\"line\">\tif !runnable.Admit &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 更新 pod 状态</span><br><span class=\"line\">\tkl.statusManager.SetPodStatus(pod, apiPodStatus)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 如果 pod 非 running 状态则直接 kill 掉</span><br><span class=\"line\">\tif !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 加载网络插件</span><br><span class=\"line\">\tif rs := kl.runtimeState.networkErrors(); len(rs) != 0 &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpcm := kl.containerManager.NewPodContainerManager()</span><br><span class=\"line\">\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// 创建并更新 pod 的 cgroups</span><br><span class=\"line\">\t\tif !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) &#123;</span><br><span class=\"line\">\t\t\tif !pcm.Exists(pod) &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 为 static pod 创建对应的 mirror pod</span><br><span class=\"line\">\tif kubepod.IsStaticPod(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建数据目录</span><br><span class=\"line\">\tif err := kl.makePodDataDirs(pod); err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 挂载 volume</span><br><span class=\"line\">\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 获取 secret 信息</span><br><span class=\"line\">\tpullSecrets := kl.getPullSecretsForPod(pod)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 调用 containerRuntime 的 SyncPod 方法开始创建容器</span><br><span class=\"line\">\tresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)</span><br><span class=\"line\">\tkl.reasonCache.Update(pod.UID, result)</span><br><span class=\"line\">\tif err := result.Error(); err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"8、创建容器\"><a href=\"#8、创建容器\" class=\"headerlink\" title=\"8、创建容器\"></a>8、创建容器</h3><p>containerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。<br>syncPod 主要执行以下几个操作：</p>\n<ul>\n<li>1、计算 sandbox 和 container 是否发生变化</li>\n<li>2、创建 sandbox 容器</li>\n<li>3、启动 init 容器</li>\n<li>4、启动业务容器</li>\n</ul>\n<p>initContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123;</span><br><span class=\"line\">\t// 1、计算 sandbox 和 container 是否发生变化</span><br><span class=\"line\">\tpodContainerChanges := m.computePodActions(pod, podStatus)</span><br><span class=\"line\">\tif podContainerChanges.CreateSandbox &#123;</span><br><span class=\"line\">\t\tref, err := ref.GetReference(legacyscheme.Scheme, pod)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Couldn&apos;t make a ref to pod %q: &apos;%v&apos;&quot;, format.Pod(pod), err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、kill 掉 sandbox 已经改变的 pod</span><br><span class=\"line\">\tif podContainerChanges.KillPod &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\t// 3、kill 掉非 running 状态的 containers</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tfor containerID, containerInfo := range podContainerChanges.ContainersToKill &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\tif err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tm.pruneInitContainersBeforeStart(pod, podStatus)</span><br><span class=\"line\">\tpodIP := &quot;&quot;</span><br><span class=\"line\">\tif podStatus != nil &#123;</span><br><span class=\"line\">\t\tpodIP = podStatus.IP</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 4、创建 sandbox </span><br><span class=\"line\">\tpodSandboxID := podContainerChanges.SandboxID</span><br><span class=\"line\">\tif podContainerChanges.CreateSandbox &#123;</span><br><span class=\"line\">\t\tpodSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tpodSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置</span><br><span class=\"line\">\t\tif !kubecontainer.IsHostNetworkPod(pod) &#123;</span><br><span class=\"line\">\t\t\tpodIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus)</span><br><span class=\"line\">\t\t\tglog.V(4).Infof(&quot;Determined the ip %q for pod %q after sandbox changed&quot;, podIP, format.Pod(pod))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tconfigPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)</span><br><span class=\"line\">\tresult.AddSyncResult(configPodSandboxResult)</span><br><span class=\"line\">\t// 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等)</span><br><span class=\"line\">\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 5、启动 init container</span><br><span class=\"line\">\tif container := podContainerChanges.NextInitContainerToStart; container != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 6、启动业务容器</span><br><span class=\"line\">\tfor _, idx := range podContainerChanges.ContainersToStart &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\treturn</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"9、启动容器\"><a href=\"#9、启动容器\" class=\"headerlink\" title=\"9、启动容器\"></a>9、启动容器</h3><p>最终由 startContainer 完成容器的启动，其主要有以下几个步骤：</p>\n<ul>\n<li>1、拉取镜像</li>\n<li>2、生成业务容器的配置信息</li>\n<li>3、调用 docker api 创建容器</li>\n<li>4、启动容器</li>\n<li>5、执行 post start hook</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) &#123;</span><br><span class=\"line\">\t// 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。</span><br><span class=\"line\">\timageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tref, err := kubecontainer.GenerateContainerRef(pod, container)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 设置 RestartCount </span><br><span class=\"line\">\trestartCount := 0</span><br><span class=\"line\">\tcontainerStatus := podStatus.FindContainerStatusByName(container.Name)</span><br><span class=\"line\">\tif containerStatus != nil &#123;</span><br><span class=\"line\">\t\trestartCount = containerStatus.RestartCount + 1</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、生成业务容器的配置信息</span><br><span class=\"line\">\tcontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType)</span><br><span class=\"line\">\tif cleanupAction != nil &#123;</span><br><span class=\"line\">\t\tdefer cleanupAction()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、通过 client.CreateContainer 调用 docker api 创建业务容器</span><br><span class=\"line\">\tcontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\terr = m.internalLifecycle.PreStartContainer(pod, container, containerID)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、启动业务容器</span><br><span class=\"line\">\terr = m.runtimeService.StartContainer(containerID)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tcontainerMeta := containerConfig.GetMetadata()</span><br><span class=\"line\">\tsandboxMeta := podSandboxConfig.GetMetadata()</span><br><span class=\"line\">\tlegacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name,</span><br><span class=\"line\">\t\tsandboxMeta.Namespace)</span><br><span class=\"line\">\tcontainerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath)</span><br><span class=\"line\">\tif _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) &#123;</span><br><span class=\"line\">\t\tif err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Failed to create legacy symbolic link %q to container %q log %q: %v&quot;,</span><br><span class=\"line\">\t\t\t\tlegacySymlink, containerID, containerLog, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 4、执行 post start hook</span><br><span class=\"line\">\tif container.Lifecycle != nil &amp;&amp; container.Lifecycle.PostStart != nil &#123;</span><br><span class=\"line\">\t\tkubeContainerID := kubecontainer.ContainerID&#123;</span><br><span class=\"line\">\t\t\tType: m.runtimeName,</span><br><span class=\"line\">\t\t\tID:   containerID,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// runner.Run 这个方法的主要作用就是在业务容器起来的时候，</span><br><span class=\"line\">\t\t// 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。</span><br><span class=\"line\">\t\t// 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。</span><br><span class=\"line\">\t\tmsg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)</span><br><span class=\"line\">\t\tif handlerErr != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn &quot;&quot;, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲述了 kubelet 从监听到容器调度至本节点再到创建容器的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。</p>\n<p>参考：<br><a href=\"https://sycki.com/articles/kubernetes/k8s-code-kubelet\" target=\"_blank\" rel=\"noopener\">k8s源码分析-kubelet</a><br><a href=\"https://segmentfault.com/a/1190000008267351\" target=\"_blank\" rel=\"noopener\">Kubelet源码分析(一):启动流程分析</a><br><a href=\"http://cizixs.com/2017/06/07/kubelet-source-code-analysis-part-2/\" target=\"_blank\" rel=\"noopener\">kubelet 源码分析：pod 新建流程</a><br><a href=\"https://fatsheep9146.github.io/2018/07/22/kubelet%E5%88%9B%E5%BB%BAPod%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">kubelet创建Pod流程解析</a><br><a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md\" target=\"_blank\" rel=\"noopener\">Kubelet: Pod Lifecycle Event Generator (PLEG) Design-    proposals</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>上篇文章介绍了 <a href=\"http://blog.tianfeiyu.com/2018/12/23/kubelet_init/\" target=\"_blank\" rel=\"noopener\">kubelet 的启动流程</a>，本篇文章主要介绍 kubelet 创建 pod 的流程。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-78c8272f4b617c92.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubelet 工作原理\"></p>\n<p>kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。</p>\n<p>本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。</p>\n<h2 id=\"kubelet-创建-pod-的流程\"><a href=\"#kubelet-创建-pod-的流程\" class=\"headerlink\" title=\"kubelet 创建 pod 的流程\"></a>kubelet 创建 pod 的流程</h2><p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-bb6b29bac1ca6c9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubelet 创建 pod 的流程\"></p>\n<h3 id=\"1、kubelet-的控制循环（syncLoop）\"><a href=\"#1、kubelet-的控制循环（syncLoop）\" class=\"headerlink\" title=\"1、kubelet 的控制循环（syncLoop）\"></a>1、kubelet 的控制循环（syncLoop）</h3><p>syncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123;</span><br><span class=\"line\">\tglog.Info(&quot;Starting kubelet main sync loop.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// syncTicker 每秒检测一次是否有需要同步的 pod workers</span><br><span class=\"line\">\tsyncTicker := time.NewTicker(time.Second)</span><br><span class=\"line\">\tdefer syncTicker.Stop()</span><br><span class=\"line\">\t// 每两秒检测一次是否有需要清理的 pod</span><br><span class=\"line\">\thousekeepingTicker := time.NewTicker(housekeepingPeriod)</span><br><span class=\"line\">\tdefer housekeepingTicker.Stop()</span><br><span class=\"line\">\t// pod 的生命周期变化</span><br><span class=\"line\">\tplegCh := kl.pleg.Watch()</span><br><span class=\"line\">\tconst (</span><br><span class=\"line\">\t\tbase   = 100 * time.Millisecond</span><br><span class=\"line\">\t\tmax    = 5 * time.Second</span><br><span class=\"line\">\t\tfactor = 2</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tduration := base</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123;</span><br><span class=\"line\">\t\t\ttime.Sleep(duration)</span><br><span class=\"line\">\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t\t// 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface，</span><br><span class=\"line\">\t\t// 在该文件开头处定义</span><br><span class=\"line\">\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123;</span><br><span class=\"line\">\t\t\tbreak</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2、监听-pod-变化（syncLoopIteration）\"><a href=\"#2、监听-pod-变化（syncLoopIteration）\" class=\"headerlink\" title=\"2、监听 pod 变化（syncLoopIteration）\"></a>2、监听 pod 变化（syncLoopIteration）</h3><p>syncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息：</p>\n<ul>\n<li>configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。</li>\n<li>syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态</li>\n<li>houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作</li>\n<li>plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。</li>\n<li>livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler,</span><br><span class=\"line\">\tsyncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123;</span><br><span class=\"line\">\tselect &#123;</span><br><span class=\"line\">\tcase u, open := &lt;-configCh:</span><br><span class=\"line\">\t\tif !open &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Update channel is closed. Exiting the sync loop.&quot;)</span><br><span class=\"line\">\t\t\treturn false</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tswitch u.Op &#123;</span><br><span class=\"line\">\t\tcase kubetypes.ADD:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.UPDATE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.REMOVE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.RECONCILE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.DELETE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.RESTORE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.SET:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase e := &lt;-plegCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase &lt;-syncCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase update := &lt;-kl.livenessManager.Updates():</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase &lt;-housekeepingCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn true</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3、处理新增-pod（HandlePodAddtions）\"><a href=\"#3、处理新增-pod（HandlePodAddtions）\" class=\"headerlink\" title=\"3、处理新增 pod（HandlePodAddtions）\"></a>3、处理新增 pod（HandlePodAddtions）</h3><p>对于事件中的每个 pod，执行以下操作：</p>\n<ul>\n<li>1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理</li>\n<li>2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了</li>\n<li>3、如果是 mirror pod 调用其单独的方法</li>\n<li>4、验证 pod 是否能在该节点运行，如果不可以直接拒绝</li>\n<li>5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理</li>\n<li>6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) &#123;</span><br><span class=\"line\">\tstart := kl.clock.Now()</span><br><span class=\"line\">\t// 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理</span><br><span class=\"line\">\tsort.Sort(sliceutils.PodsByCreationTime(pods))</span><br><span class=\"line\">\tfor _, pod := range pods &#123;</span><br><span class=\"line\">\t\tif kl.dnsConfigurer != nil &amp;&amp; kl.dnsConfigurer.ResolverConfig != &quot;&quot; &#123;</span><br><span class=\"line\">\t\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\texistingPods := kl.podManager.GetPods()</span><br><span class=\"line\">\t\t// 把 pod 加入到 podManager 中</span><br><span class=\"line\">\t\tkl.podManager.AddPod(pod)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 判断是否是 mirror pod（即 static pod）</span><br><span class=\"line\">\t\tif kubepod.IsMirrorPod(pod) &#123;</span><br><span class=\"line\">\t\t\tkl.handleMirrorPod(pod, start)</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\t\tactivePods := kl.filterOutTerminatedPods(existingPods)</span><br><span class=\"line\">\t\t\t// 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间)</span><br><span class=\"line\">\t\t\t// Check if we can admit the pod; if not, reject it.</span><br><span class=\"line\">\t\t\tif ok, reason, message := kl.canAdmitPod(activePods, pod); !ok &#123;</span><br><span class=\"line\">\t\t\t\tkl.rejectPod(pod, reason, message)</span><br><span class=\"line\">\t\t\t\tcontinue</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)</span><br><span class=\"line\">\t\t// 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法.</span><br><span class=\"line\">\t\tkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)</span><br><span class=\"line\">\t\t// 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测</span><br><span class=\"line\">\t\tkl.probeManager.AddPod(pod)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。</p>\n</blockquote>\n<h3 id=\"4、下发任务（dispatchWork）\"><a href=\"#4、下发任务（dispatchWork）\" class=\"headerlink\" title=\"4、下发任务（dispatchWork）\"></a>4、下发任务（dispatchWork）</h3><p>dispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) &#123;</span><br><span class=\"line\">\tif kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\tif pod.DeletionTimestamp != nil &#123;</span><br><span class=\"line\">\t\t\tkl.statusManager.TerminatePod(pod)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 落实在 podWorkers 中</span><br><span class=\"line\">\tkl.podWorkers.UpdatePod(&amp;UpdatePodOptions&#123;</span><br><span class=\"line\">\t\tPod:        pod,</span><br><span class=\"line\">\t\tMirrorPod:  mirrorPod,</span><br><span class=\"line\">\t\tUpdateType: syncType,</span><br><span class=\"line\">\t\tOnCompleteFunc: func(err error) &#123;</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tmetrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start))</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t&#125;)</span><br><span class=\"line\">\tif syncType == kubetypes.SyncPodCreate &#123;</span><br><span class=\"line\">\t\tmetrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5、更新事件的-channel（UpdatePod）\"><a href=\"#5、更新事件的-channel（UpdatePod）\" class=\"headerlink\" title=\"5、更新事件的 channel（UpdatePod）\"></a>5、更新事件的 channel（UpdatePod）</h3><p>podWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (p *podWorkers) UpdatePod(options *UpdatePodOptions) &#123;</span><br><span class=\"line\">\tpod := options.Pod</span><br><span class=\"line\">\tuid := pod.UID</span><br><span class=\"line\">\tvar podUpdates chan UpdatePodOptions</span><br><span class=\"line\">\tvar exists bool</span><br><span class=\"line\"></span><br><span class=\"line\">\tp.podLock.Lock()</span><br><span class=\"line\">\tdefer p.podLock.Unlock()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel</span><br><span class=\"line\">\tif podUpdates, exists = p.podUpdates[uid]; !exists &#123;</span><br><span class=\"line\">\t\t// 创建 channel</span><br><span class=\"line\">\t\tpodUpdates = make(chan UpdatePodOptions, 1)</span><br><span class=\"line\">\t\tp.podUpdates[uid] = podUpdates</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 启动 goroutine</span><br><span class=\"line\">\t\tgo func() &#123;</span><br><span class=\"line\">\t\t\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t\t\tp.managePodLoop(podUpdates)</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 下发更新事件</span><br><span class=\"line\">\tif !p.isWorking[pod.UID] &#123;</span><br><span class=\"line\">\t\tp.isWorking[pod.UID] = true</span><br><span class=\"line\">\t\tpodUpdates &lt;- *options</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\tupdate, found := p.lastUndeliveredWorkUpdate[pod.UID]</span><br><span class=\"line\">\t\tif !found || update.UpdateType != kubetypes.SyncPodKill &#123;</span><br><span class=\"line\">\t\t\tp.lastUndeliveredWorkUpdate[pod.UID] = *options</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"6、调用-syncPodFn-方法同步-pod（managePodLoop）\"><a href=\"#6、调用-syncPodFn-方法同步-pod（managePodLoop）\" class=\"headerlink\" title=\"6、调用 syncPodFn 方法同步 pod（managePodLoop）\"></a>6、调用 syncPodFn 方法同步 pod（managePodLoop）</h3><p>managePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情:</p>\n<ul>\n<li>将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync</li>\n<li>将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (p *podWorkers) managePodLoop(podUpdates &lt;-chan UpdatePodOptions) &#123;</span><br><span class=\"line\">\tvar lastSyncTime time.Time</span><br><span class=\"line\">\tfor update := range podUpdates &#123;</span><br><span class=\"line\">\t\terr := func() error &#123;</span><br><span class=\"line\">\t\t\tpodUID := update.Pod.UID</span><br><span class=\"line\">\t\t\tstatus, err := p.podCache.GetNewerThan(podUID, lastSyncTime)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\terr = p.syncPodFn(syncPodOptions&#123;</span><br><span class=\"line\">\t\t\t\tmirrorPod:      update.MirrorPod,</span><br><span class=\"line\">\t\t\t\tpod:            update.Pod,</span><br><span class=\"line\">\t\t\t\tpodStatus:      status,</span><br><span class=\"line\">\t\t\t\tkillPodOptions: update.KillPodOptions,</span><br><span class=\"line\">\t\t\t\tupdateType:     update.UpdateType,</span><br><span class=\"line\">\t\t\t&#125;)</span><br><span class=\"line\">\t\t\tlastSyncTime = time.Now()</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\">\t\tif update.OnCompleteFunc != nil &#123;</span><br><span class=\"line\">\t\t\tupdate.OnCompleteFunc(err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tp.wrapUp(update.Pod.UID, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"7、完成创建容器前的准备工作（SyncPod）\"><a href=\"#7、完成创建容器前的准备工作（SyncPod）\" class=\"headerlink\" title=\"7、完成创建容器前的准备工作（SyncPod）\"></a>7、完成创建容器前的准备工作（SyncPod）</h3><p>在这个方法中，主要完成以下几件事情：</p>\n<ul>\n<li>如果是删除 pod，立即执行并返回</li>\n<li>同步 podStatus 到 kubelet.statusManager</li>\n<li>检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息</li>\n<li>创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup</li>\n<li>如果是 static Pod，就创建或者更新对应的 mirrorPod</li>\n<li>创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据</li>\n<li>然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。</li>\n<li>调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑</li>\n</ul>\n<p>这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncPod(o syncPodOptions) error &#123;</span><br><span class=\"line\">\t// pull out the required options</span><br><span class=\"line\">\tpod := o.pod</span><br><span class=\"line\">\tmirrorPod := o.mirrorPod</span><br><span class=\"line\">\tpodStatus := o.podStatus</span><br><span class=\"line\">\tupdateType := o.updateType</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 是否为 删除 pod</span><br><span class=\"line\">\tif updateType == kubetypes.SyncPodKill &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\t// 检查 pod 是否能运行在本节点</span><br><span class=\"line\">\trunnable := kl.canRunPod(pod)</span><br><span class=\"line\">\tif !runnable.Admit &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 更新 pod 状态</span><br><span class=\"line\">\tkl.statusManager.SetPodStatus(pod, apiPodStatus)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 如果 pod 非 running 状态则直接 kill 掉</span><br><span class=\"line\">\tif !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 加载网络插件</span><br><span class=\"line\">\tif rs := kl.runtimeState.networkErrors(); len(rs) != 0 &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpcm := kl.containerManager.NewPodContainerManager()</span><br><span class=\"line\">\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// 创建并更新 pod 的 cgroups</span><br><span class=\"line\">\t\tif !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) &#123;</span><br><span class=\"line\">\t\t\tif !pcm.Exists(pod) &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 为 static pod 创建对应的 mirror pod</span><br><span class=\"line\">\tif kubepod.IsStaticPod(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建数据目录</span><br><span class=\"line\">\tif err := kl.makePodDataDirs(pod); err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 挂载 volume</span><br><span class=\"line\">\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 获取 secret 信息</span><br><span class=\"line\">\tpullSecrets := kl.getPullSecretsForPod(pod)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 调用 containerRuntime 的 SyncPod 方法开始创建容器</span><br><span class=\"line\">\tresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)</span><br><span class=\"line\">\tkl.reasonCache.Update(pod.UID, result)</span><br><span class=\"line\">\tif err := result.Error(); err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"8、创建容器\"><a href=\"#8、创建容器\" class=\"headerlink\" title=\"8、创建容器\"></a>8、创建容器</h3><p>containerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。<br>syncPod 主要执行以下几个操作：</p>\n<ul>\n<li>1、计算 sandbox 和 container 是否发生变化</li>\n<li>2、创建 sandbox 容器</li>\n<li>3、启动 init 容器</li>\n<li>4、启动业务容器</li>\n</ul>\n<p>initContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123;</span><br><span class=\"line\">\t// 1、计算 sandbox 和 container 是否发生变化</span><br><span class=\"line\">\tpodContainerChanges := m.computePodActions(pod, podStatus)</span><br><span class=\"line\">\tif podContainerChanges.CreateSandbox &#123;</span><br><span class=\"line\">\t\tref, err := ref.GetReference(legacyscheme.Scheme, pod)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Couldn&apos;t make a ref to pod %q: &apos;%v&apos;&quot;, format.Pod(pod), err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、kill 掉 sandbox 已经改变的 pod</span><br><span class=\"line\">\tif podContainerChanges.KillPod &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\t// 3、kill 掉非 running 状态的 containers</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tfor containerID, containerInfo := range podContainerChanges.ContainersToKill &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\tif err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tm.pruneInitContainersBeforeStart(pod, podStatus)</span><br><span class=\"line\">\tpodIP := &quot;&quot;</span><br><span class=\"line\">\tif podStatus != nil &#123;</span><br><span class=\"line\">\t\tpodIP = podStatus.IP</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 4、创建 sandbox </span><br><span class=\"line\">\tpodSandboxID := podContainerChanges.SandboxID</span><br><span class=\"line\">\tif podContainerChanges.CreateSandbox &#123;</span><br><span class=\"line\">\t\tpodSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tpodSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置</span><br><span class=\"line\">\t\tif !kubecontainer.IsHostNetworkPod(pod) &#123;</span><br><span class=\"line\">\t\t\tpodIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus)</span><br><span class=\"line\">\t\t\tglog.V(4).Infof(&quot;Determined the ip %q for pod %q after sandbox changed&quot;, podIP, format.Pod(pod))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tconfigPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)</span><br><span class=\"line\">\tresult.AddSyncResult(configPodSandboxResult)</span><br><span class=\"line\">\t// 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等)</span><br><span class=\"line\">\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 5、启动 init container</span><br><span class=\"line\">\tif container := podContainerChanges.NextInitContainerToStart; container != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 6、启动业务容器</span><br><span class=\"line\">\tfor _, idx := range podContainerChanges.ContainersToStart &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\treturn</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"9、启动容器\"><a href=\"#9、启动容器\" class=\"headerlink\" title=\"9、启动容器\"></a>9、启动容器</h3><p>最终由 startContainer 完成容器的启动，其主要有以下几个步骤：</p>\n<ul>\n<li>1、拉取镜像</li>\n<li>2、生成业务容器的配置信息</li>\n<li>3、调用 docker api 创建容器</li>\n<li>4、启动容器</li>\n<li>5、执行 post start hook</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) &#123;</span><br><span class=\"line\">\t// 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。</span><br><span class=\"line\">\timageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tref, err := kubecontainer.GenerateContainerRef(pod, container)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 设置 RestartCount </span><br><span class=\"line\">\trestartCount := 0</span><br><span class=\"line\">\tcontainerStatus := podStatus.FindContainerStatusByName(container.Name)</span><br><span class=\"line\">\tif containerStatus != nil &#123;</span><br><span class=\"line\">\t\trestartCount = containerStatus.RestartCount + 1</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、生成业务容器的配置信息</span><br><span class=\"line\">\tcontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType)</span><br><span class=\"line\">\tif cleanupAction != nil &#123;</span><br><span class=\"line\">\t\tdefer cleanupAction()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、通过 client.CreateContainer 调用 docker api 创建业务容器</span><br><span class=\"line\">\tcontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\terr = m.internalLifecycle.PreStartContainer(pod, container, containerID)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、启动业务容器</span><br><span class=\"line\">\terr = m.runtimeService.StartContainer(containerID)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tcontainerMeta := containerConfig.GetMetadata()</span><br><span class=\"line\">\tsandboxMeta := podSandboxConfig.GetMetadata()</span><br><span class=\"line\">\tlegacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name,</span><br><span class=\"line\">\t\tsandboxMeta.Namespace)</span><br><span class=\"line\">\tcontainerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath)</span><br><span class=\"line\">\tif _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) &#123;</span><br><span class=\"line\">\t\tif err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Failed to create legacy symbolic link %q to container %q log %q: %v&quot;,</span><br><span class=\"line\">\t\t\t\tlegacySymlink, containerID, containerLog, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 4、执行 post start hook</span><br><span class=\"line\">\tif container.Lifecycle != nil &amp;&amp; container.Lifecycle.PostStart != nil &#123;</span><br><span class=\"line\">\t\tkubeContainerID := kubecontainer.ContainerID&#123;</span><br><span class=\"line\">\t\t\tType: m.runtimeName,</span><br><span class=\"line\">\t\t\tID:   containerID,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// runner.Run 这个方法的主要作用就是在业务容器起来的时候，</span><br><span class=\"line\">\t\t// 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。</span><br><span class=\"line\">\t\t// 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。</span><br><span class=\"line\">\t\tmsg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)</span><br><span class=\"line\">\t\tif handlerErr != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn &quot;&quot;, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲述了 kubelet 从监听到容器调度至本节点再到创建容器的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。</p>\n<p>参考：<br><a href=\"https://sycki.com/articles/kubernetes/k8s-code-kubelet\" target=\"_blank\" rel=\"noopener\">k8s源码分析-kubelet</a><br><a href=\"https://segmentfault.com/a/1190000008267351\" target=\"_blank\" rel=\"noopener\">Kubelet源码分析(一):启动流程分析</a><br><a href=\"http://cizixs.com/2017/06/07/kubelet-source-code-analysis-part-2/\" target=\"_blank\" rel=\"noopener\">kubelet 源码分析：pod 新建流程</a><br><a href=\"https://fatsheep9146.github.io/2018/07/22/kubelet%E5%88%9B%E5%BB%BAPod%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">kubelet创建Pod流程解析</a><br><a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md\" target=\"_blank\" rel=\"noopener\">Kubelet: Pod Lifecycle Event Generator (PLEG) Design-    proposals</a></p>\n"},{"title":"kubernetes 学习笔记","date":"2017-02-12T14:58:00.000Z","type":"kubernetes","_content":"\n1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 **kubernetes + docker** 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 `API` 进行某些操作。\n\n\n## 1，kubernetes 搭建\n\n[kubernetes](https://github.com/kubernetes/kubernetes) 是 google 的一个开源软件，其社区活跃量远超 **Mesos，Coreos** 的，若想深入学习建议参考**《kubernetes 权威指南》**，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。\n\n\n![kubernetes 架构图](http://upload-images.jianshu.io/upload_images/1262158-e050e035d6fa64ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\nkubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 **gitbook** 上面某大神的一本书 一 [etcd3学习笔记](https://skyao.gitbooks.io/leaning-etcd3/content/documentation/leaning/)。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。\n\n> 注意：内核版本 > 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 \n\netcd 节点：\n\t\n\t# yum install -y etcd \n\t# systemctl start etcd  \n\nmaster 节点：\n\n\t# yum install -y kubernetes-master\n\t# systemctl start kube-apiserver \n\t# systemctl start kube-controller-manager \n\t# systemctl start kube-scheduler \n\nminion 节点：\n\n\t# yum install -y kubernetes  docker\n\t# systemctl start kubelet \n\t# systemctl start kube-proxy \n\t# systemctl start docker \n\t\n\n## 2，kubernetes 版本升级\n\n以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。\n\n下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 [kubernetes-server-linux-amd64.tar.gz](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#downloads-for-v160-alpha1)  这个包，下载你所要更新到的版本。\n\n**升级步骤**：\n\n- 1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上\n- 2，备份二进制程序（kubectl,kube-proxy）\n- 3，将解压后的二进制程序覆盖以前的版本\n- 4，最后重新启动服务\n\n\t\n\t# systemctl stop docker\n\t# which kubectl kube-proxy \n\t/usr/bin/kubectl\n\t/usr/bin/kube-proxy\n\n\t# cp /usr/bin/{kubectl,kube-proxy} /tmp/\n\t# yes | cp bin/{kubectl,kube-proxy} /usr/bin/\n\t\n\t# systemctl status {kubectl,kube-proxy}\n\n\t# systemctl start docker \n\n\n## 3，kubeconfig 使用\n\n若你使用的 kubelet 版本为 1.4，使用 `systemctl status kubelet`  会看到这样一句话：\n\n\t--api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml\n\n使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 [官方文档](http://kubernetes.io/docs/user-guide/kubeconfig-file/) 有格式说明， 本人已将其翻译，翻译文档见下文。\n\n**kubeconfig** 文件示例：\n\n\t\tapiVersion: v1\n\t\tclusters:\n\t\t- cluster:\n\t\t    server: http://localhost:8080\n\t\t  name: local-server\n\t\tcontexts:\n\t\t- context:\n\t\t    cluster: local-server\n\t\t    namespace: the-right-prefix\n\t\t    user: myself\n\t\t  name: default-context\n\t\tcurrent-context: default-context\n\t\tkind: Config\n\t\tpreferences: {}\n\t\tusers:\n\t\t- name: myself\n\t\t  user:\n\t\t    password: secret\n\t\t    username: admin\n\n---\n\n\t   # kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true\n\n\nkubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。\n\nrequire-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。\n\n关于 kubeconfig 的一个 **issue**，[Kubelet won't read apiserver from kubeconfig](https://github.com/kubernetes/kubernetes/issues/36745)。\n\n**升级步骤**，当然前提是你的 kubelet 版本已经到了 1.5：\n\n* 1，关闭 kubelet、kube-proxy 服务；\n* 2，注释掉 `/etc/kubernetes/kubelet` 文件中下面这一行:\n\n    `KUBELET_API_SERVER=\"--api-servers=http://127.0.0.1:8080\"`\n\n然后在 **KUBELET_ARGS** 中添加： \n\n\t--kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true\n\n这里的路径是你 yaml 文件放置的路径。 \n\n- 3，重新启动刚关掉的两个服务\n\n---\n## 4，以下为 [kubeconfig 配置官方文档](https://kubernetes.io/docs/user-guide/kubeconfig-file/)的翻译\n\n### kubernetes 中的验证对于不同的群体可以使用不同的方法.\n\n* 运行 kubelet 可能有的一种认证方式（即证书）。\n* 用户可能有不同的认证方式（即 token）。\n* 管理员可以为每个用户提供一个证书列表。\n* 可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 \n\n因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。\n\n此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。\n\n如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。\n\n### 相关讨论\n\n\thttp://issue.k8s.io/1755\n\n### kubeconfig 文件的组件 \n\nkubeconfig 文件示例：\n\n\tcurrent-context: federal-context\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    api-version: v1\n\t    server: http://cow.org:8080\n\t  name: cow-cluster\n\t- cluster:\n\t    certificate-authority: path/to/my/cafile\n\t    server: https://horse.org:4443\n\t  name: horse-cluster\n\t- cluster:\n\t    insecure-skip-tls-verify: true\n\t    server: https://pig.org:443\n\t  name: pig-cluster\n\tcontexts:\n\t- context:\n\t    cluster: horse-cluster\n\t    namespace: chisel-ns\n\t    user: green-user\n\t  name: federal-context\n\t- context:\n\t    cluster: pig-cluster\n\t    namespace: saw-ns\n\t    user: black-user\n\t  name: queen-anne-context\n\tkind: Config\n\tpreferences:\n\t  colors: true\n\tusers:\n\t- name: blue-user\n\t  user:\n\t    token: blue-token\n\t- name: green-user\n\t  user:\n\t    client-certificate: path/to/my/client/cert\n\t    client-key: path/to/my/client/key\n\n### 组件的解释\n\n#### cluster\n\n\tclusters:\n\t- cluster:\n\t    certificate-authority: path/to/my/cafile\n\t    server: https://horse.org:4443\n\t  name: horse-cluster\n\t- cluster:\n\t    insecure-skip-tls-verify: true\n\t    server: https://pig.org:443\n\t  name: pig-cluster\n\n\ncluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。\n\n#### user\n\n\tusers:\n\t- name: blue-user\n\t  user:\n\t    token: blue-token\n\t- name: green-user\n\t  user:\n\t    client-certificate: path/to/my/client/cert\n\t    client-key: path/to/my/client/key\n\n用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。\n\n### context\n\n\tcontexts:\n\t- context:\n\t    cluster: horse-cluster\n\t    namespace: chisel-ns\n\t    user: green-user\n\t  name: federal-context\n\ncontext 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。\n三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。\n\n#### current-context\n\n\tcurrent-context: federal-context\n\ncurrent-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 --context=CONTEXT, --cluster=CLUSTER, --user=USER, and/or --namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。\n\n#### 杂项\n\n\tapiVersion: v1\n\tkind: Config\n\tpreferences:\n\t  colors: true\n\napiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。\npreferences 指定选项(和当前未使用的) kubectl preferences.\n\n### 查看 kubeconfig 文件\n\nkubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 --minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。\n\n### 创建你的 kubeconfig 文件\n\n注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。\n\n在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。\n\n因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改...\n\n以上文件可能对应于使用--token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样：\n\t\n\tblue-user,blue-user,1\n\tmister-red,mister-red,2\n\n此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。\n\n* 由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。\n* 类似地，我们也可以选择改变 current-context 的值为 “blue-user”。\n* \n在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过\n\n\n### 加载和合并规则\n\n加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建：\n\n1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：\n如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。\n\n否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。\n\n对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。\n\n2，此链中第一个被匹配的 context 将被使用：\n\n* 1，命令行参数 - 命令行选项中 context 的值\n* 2，合并文件中的 current-context\n* 3，此段允许为空\n\n3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）：\n\n* 1，命令行参数 - user 是用户名，cluster 是集群名\n* 2，如果存在 context 则使用\n* 3，允许为空\n\n4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）：\n\n* 1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify\n* 2，如果存在集群信息并且该属性的值存在，则使用它。\n* 3，如果没有 server 位置则出错。\n\n5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法：\n\n* 1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段\n* 2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。\n* 3，如果两者有冲突则失败\n\n6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。\n\n7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。\n\nkubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。\n\n\n### 通过 kubectl config <subcommand> 操作 kubeconfig\n\n为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。\n\n例如：\n\n\t$ kubectl config set-credentials myself --username=admin --password=secret\n\t$ kubectl config set-cluster local-server --server=http://localhost:8080\n\t$ kubectl config set-context default-context --cluster=local-server --user=myself\n\t$ kubectl config use-context default-context\n\t$ kubectl config set contexts.default-context.namespace the-right-prefix\n\t$ kubectl config view\n\n输出：\n\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    server: http://localhost:8080\n\t  name: local-server\n\tcontexts:\n\t- context:\n\t    cluster: local-server\n\t    namespace: the-right-prefix\n\t    user: myself\n\t  name: default-context\n\tcurrent-context: default-context\n\tkind: Config\n\tpreferences: {}\n\tusers:\n\t- name: myself\n\t  user:\n\t    password: secret\n\t    username: admin\n\n一个 kubeconfig 文件类似这样：\n\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    server: http://localhost:8080\n\t  name: local-server\n\tcontexts:\n\t- context:\n\t    cluster: local-server\n\t    namespace: the-right-prefix\n\t    user: myself\n\t  name: default-context\n\tcurrent-context: default-context\n\tkind: Config\n\tpreferences: {}\n\tusers:\n\t- name: myself\n\t  user:\n\t    password: secret\n\t    username: admin\n\n示例文件的命令操作：\n\n\t$ kubectl config set preferences.colors true\n\t$ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1\n\t$ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile\n\t$ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true\n\t$ kubectl config set-credentials blue-user --token=blue-token\n\t$ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key\n\t$ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns\n\t$ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns\n\t$ kubectl config use-context federal-context\n\n最后的总结：\n\n所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了：\n\n* 仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证\n* 将上面的代码段替换为你集群的 api-server endpoint 的信息。\n* 确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。\n","source":"_posts/kubernetes-learn.md","raw":"---\ntitle: kubernetes 学习笔记\ndate: 2017-02-12 22:58:00\ntype: \"kubernetes\"\n\n---\n\n1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 **kubernetes + docker** 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 `API` 进行某些操作。\n\n\n## 1，kubernetes 搭建\n\n[kubernetes](https://github.com/kubernetes/kubernetes) 是 google 的一个开源软件，其社区活跃量远超 **Mesos，Coreos** 的，若想深入学习建议参考**《kubernetes 权威指南》**，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。\n\n\n![kubernetes 架构图](http://upload-images.jianshu.io/upload_images/1262158-e050e035d6fa64ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\nkubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 **gitbook** 上面某大神的一本书 一 [etcd3学习笔记](https://skyao.gitbooks.io/leaning-etcd3/content/documentation/leaning/)。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。\n\n> 注意：内核版本 > 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 \n\netcd 节点：\n\t\n\t# yum install -y etcd \n\t# systemctl start etcd  \n\nmaster 节点：\n\n\t# yum install -y kubernetes-master\n\t# systemctl start kube-apiserver \n\t# systemctl start kube-controller-manager \n\t# systemctl start kube-scheduler \n\nminion 节点：\n\n\t# yum install -y kubernetes  docker\n\t# systemctl start kubelet \n\t# systemctl start kube-proxy \n\t# systemctl start docker \n\t\n\n## 2，kubernetes 版本升级\n\n以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。\n\n下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 [kubernetes-server-linux-amd64.tar.gz](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#downloads-for-v160-alpha1)  这个包，下载你所要更新到的版本。\n\n**升级步骤**：\n\n- 1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上\n- 2，备份二进制程序（kubectl,kube-proxy）\n- 3，将解压后的二进制程序覆盖以前的版本\n- 4，最后重新启动服务\n\n\t\n\t# systemctl stop docker\n\t# which kubectl kube-proxy \n\t/usr/bin/kubectl\n\t/usr/bin/kube-proxy\n\n\t# cp /usr/bin/{kubectl,kube-proxy} /tmp/\n\t# yes | cp bin/{kubectl,kube-proxy} /usr/bin/\n\t\n\t# systemctl status {kubectl,kube-proxy}\n\n\t# systemctl start docker \n\n\n## 3，kubeconfig 使用\n\n若你使用的 kubelet 版本为 1.4，使用 `systemctl status kubelet`  会看到这样一句话：\n\n\t--api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml\n\n使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 [官方文档](http://kubernetes.io/docs/user-guide/kubeconfig-file/) 有格式说明， 本人已将其翻译，翻译文档见下文。\n\n**kubeconfig** 文件示例：\n\n\t\tapiVersion: v1\n\t\tclusters:\n\t\t- cluster:\n\t\t    server: http://localhost:8080\n\t\t  name: local-server\n\t\tcontexts:\n\t\t- context:\n\t\t    cluster: local-server\n\t\t    namespace: the-right-prefix\n\t\t    user: myself\n\t\t  name: default-context\n\t\tcurrent-context: default-context\n\t\tkind: Config\n\t\tpreferences: {}\n\t\tusers:\n\t\t- name: myself\n\t\t  user:\n\t\t    password: secret\n\t\t    username: admin\n\n---\n\n\t   # kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true\n\n\nkubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。\n\nrequire-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。\n\n关于 kubeconfig 的一个 **issue**，[Kubelet won't read apiserver from kubeconfig](https://github.com/kubernetes/kubernetes/issues/36745)。\n\n**升级步骤**，当然前提是你的 kubelet 版本已经到了 1.5：\n\n* 1，关闭 kubelet、kube-proxy 服务；\n* 2，注释掉 `/etc/kubernetes/kubelet` 文件中下面这一行:\n\n    `KUBELET_API_SERVER=\"--api-servers=http://127.0.0.1:8080\"`\n\n然后在 **KUBELET_ARGS** 中添加： \n\n\t--kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true\n\n这里的路径是你 yaml 文件放置的路径。 \n\n- 3，重新启动刚关掉的两个服务\n\n---\n## 4，以下为 [kubeconfig 配置官方文档](https://kubernetes.io/docs/user-guide/kubeconfig-file/)的翻译\n\n### kubernetes 中的验证对于不同的群体可以使用不同的方法.\n\n* 运行 kubelet 可能有的一种认证方式（即证书）。\n* 用户可能有不同的认证方式（即 token）。\n* 管理员可以为每个用户提供一个证书列表。\n* 可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 \n\n因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。\n\n此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。\n\n如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。\n\n### 相关讨论\n\n\thttp://issue.k8s.io/1755\n\n### kubeconfig 文件的组件 \n\nkubeconfig 文件示例：\n\n\tcurrent-context: federal-context\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    api-version: v1\n\t    server: http://cow.org:8080\n\t  name: cow-cluster\n\t- cluster:\n\t    certificate-authority: path/to/my/cafile\n\t    server: https://horse.org:4443\n\t  name: horse-cluster\n\t- cluster:\n\t    insecure-skip-tls-verify: true\n\t    server: https://pig.org:443\n\t  name: pig-cluster\n\tcontexts:\n\t- context:\n\t    cluster: horse-cluster\n\t    namespace: chisel-ns\n\t    user: green-user\n\t  name: federal-context\n\t- context:\n\t    cluster: pig-cluster\n\t    namespace: saw-ns\n\t    user: black-user\n\t  name: queen-anne-context\n\tkind: Config\n\tpreferences:\n\t  colors: true\n\tusers:\n\t- name: blue-user\n\t  user:\n\t    token: blue-token\n\t- name: green-user\n\t  user:\n\t    client-certificate: path/to/my/client/cert\n\t    client-key: path/to/my/client/key\n\n### 组件的解释\n\n#### cluster\n\n\tclusters:\n\t- cluster:\n\t    certificate-authority: path/to/my/cafile\n\t    server: https://horse.org:4443\n\t  name: horse-cluster\n\t- cluster:\n\t    insecure-skip-tls-verify: true\n\t    server: https://pig.org:443\n\t  name: pig-cluster\n\n\ncluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。\n\n#### user\n\n\tusers:\n\t- name: blue-user\n\t  user:\n\t    token: blue-token\n\t- name: green-user\n\t  user:\n\t    client-certificate: path/to/my/client/cert\n\t    client-key: path/to/my/client/key\n\n用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。\n\n### context\n\n\tcontexts:\n\t- context:\n\t    cluster: horse-cluster\n\t    namespace: chisel-ns\n\t    user: green-user\n\t  name: federal-context\n\ncontext 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。\n三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。\n\n#### current-context\n\n\tcurrent-context: federal-context\n\ncurrent-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 --context=CONTEXT, --cluster=CLUSTER, --user=USER, and/or --namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。\n\n#### 杂项\n\n\tapiVersion: v1\n\tkind: Config\n\tpreferences:\n\t  colors: true\n\napiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。\npreferences 指定选项(和当前未使用的) kubectl preferences.\n\n### 查看 kubeconfig 文件\n\nkubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 --minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。\n\n### 创建你的 kubeconfig 文件\n\n注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。\n\n在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。\n\n因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改...\n\n以上文件可能对应于使用--token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样：\n\t\n\tblue-user,blue-user,1\n\tmister-red,mister-red,2\n\n此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。\n\n* 由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。\n* 类似地，我们也可以选择改变 current-context 的值为 “blue-user”。\n* \n在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过\n\n\n### 加载和合并规则\n\n加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建：\n\n1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：\n如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。\n\n否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。\n\n对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。\n\n2，此链中第一个被匹配的 context 将被使用：\n\n* 1，命令行参数 - 命令行选项中 context 的值\n* 2，合并文件中的 current-context\n* 3，此段允许为空\n\n3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）：\n\n* 1，命令行参数 - user 是用户名，cluster 是集群名\n* 2，如果存在 context 则使用\n* 3，允许为空\n\n4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）：\n\n* 1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify\n* 2，如果存在集群信息并且该属性的值存在，则使用它。\n* 3，如果没有 server 位置则出错。\n\n5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法：\n\n* 1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段\n* 2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。\n* 3，如果两者有冲突则失败\n\n6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。\n\n7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。\n\nkubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。\n\n\n### 通过 kubectl config <subcommand> 操作 kubeconfig\n\n为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。\n\n例如：\n\n\t$ kubectl config set-credentials myself --username=admin --password=secret\n\t$ kubectl config set-cluster local-server --server=http://localhost:8080\n\t$ kubectl config set-context default-context --cluster=local-server --user=myself\n\t$ kubectl config use-context default-context\n\t$ kubectl config set contexts.default-context.namespace the-right-prefix\n\t$ kubectl config view\n\n输出：\n\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    server: http://localhost:8080\n\t  name: local-server\n\tcontexts:\n\t- context:\n\t    cluster: local-server\n\t    namespace: the-right-prefix\n\t    user: myself\n\t  name: default-context\n\tcurrent-context: default-context\n\tkind: Config\n\tpreferences: {}\n\tusers:\n\t- name: myself\n\t  user:\n\t    password: secret\n\t    username: admin\n\n一个 kubeconfig 文件类似这样：\n\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    server: http://localhost:8080\n\t  name: local-server\n\tcontexts:\n\t- context:\n\t    cluster: local-server\n\t    namespace: the-right-prefix\n\t    user: myself\n\t  name: default-context\n\tcurrent-context: default-context\n\tkind: Config\n\tpreferences: {}\n\tusers:\n\t- name: myself\n\t  user:\n\t    password: secret\n\t    username: admin\n\n示例文件的命令操作：\n\n\t$ kubectl config set preferences.colors true\n\t$ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1\n\t$ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile\n\t$ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true\n\t$ kubectl config set-credentials blue-user --token=blue-token\n\t$ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key\n\t$ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns\n\t$ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns\n\t$ kubectl config use-context federal-context\n\n最后的总结：\n\n所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了：\n\n* 仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证\n* 将上面的代码段替换为你集群的 api-server endpoint 的信息。\n* 确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。\n","slug":"kubernetes-learn","published":1,"updated":"2018-12-08T10:29:00.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5gpx000t22dvsva7cco7","content":"<p>1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 <strong>kubernetes + docker</strong> 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 <code>API</code> 进行某些操作。</p>\n<h2 id=\"1，kubernetes-搭建\"><a href=\"#1，kubernetes-搭建\" class=\"headerlink\" title=\"1，kubernetes 搭建\"></a>1，kubernetes 搭建</h2><p><a href=\"https://github.com/kubernetes/kubernetes\" target=\"_blank\" rel=\"noopener\">kubernetes</a> 是 google 的一个开源软件，其社区活跃量远超 <strong>Mesos，Coreos</strong> 的，若想深入学习建议参考<strong>《kubernetes 权威指南》</strong>，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1262158-e050e035d6fa64ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubernetes 架构图\"></p>\n<p>kubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 <strong>gitbook</strong> 上面某大神的一本书 一 <a href=\"https://skyao.gitbooks.io/leaning-etcd3/content/documentation/leaning/\" target=\"_blank\" rel=\"noopener\">etcd3学习笔记</a>。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。</p>\n<blockquote>\n<p>注意：内核版本 &gt; 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 </p>\n</blockquote>\n<p>etcd 节点：</p>\n<pre><code># yum install -y etcd \n# systemctl start etcd  \n</code></pre><p>master 节点：</p>\n<pre><code># yum install -y kubernetes-master\n# systemctl start kube-apiserver \n# systemctl start kube-controller-manager \n# systemctl start kube-scheduler \n</code></pre><p>minion 节点：</p>\n<pre><code># yum install -y kubernetes  docker\n# systemctl start kubelet \n# systemctl start kube-proxy \n# systemctl start docker \n</code></pre><h2 id=\"2，kubernetes-版本升级\"><a href=\"#2，kubernetes-版本升级\" class=\"headerlink\" title=\"2，kubernetes 版本升级\"></a>2，kubernetes 版本升级</h2><p>以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。</p>\n<p>下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#downloads-for-v160-alpha1\" target=\"_blank\" rel=\"noopener\">kubernetes-server-linux-amd64.tar.gz</a>  这个包，下载你所要更新到的版本。</p>\n<p><strong>升级步骤</strong>：</p>\n<ul>\n<li>1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上</li>\n<li>2，备份二进制程序（kubectl,kube-proxy）</li>\n<li>3，将解压后的二进制程序覆盖以前的版本</li>\n<li>4，最后重新启动服务</li>\n</ul>\n<pre><code># systemctl stop docker\n# which kubectl kube-proxy \n/usr/bin/kubectl\n/usr/bin/kube-proxy\n\n# cp /usr/bin/{kubectl,kube-proxy} /tmp/\n# yes | cp bin/{kubectl,kube-proxy} /usr/bin/\n\n# systemctl status {kubectl,kube-proxy}\n\n# systemctl start docker \n</code></pre><h2 id=\"3，kubeconfig-使用\"><a href=\"#3，kubeconfig-使用\" class=\"headerlink\" title=\"3，kubeconfig 使用\"></a>3，kubeconfig 使用</h2><p>若你使用的 kubelet 版本为 1.4，使用 <code>systemctl status kubelet</code>  会看到这样一句话：</p>\n<pre><code>--api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml\n</code></pre><p>使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 <a href=\"http://kubernetes.io/docs/user-guide/kubeconfig-file/\" target=\"_blank\" rel=\"noopener\">官方文档</a> 有格式说明， 本人已将其翻译，翻译文档见下文。</p>\n<p><strong>kubeconfig</strong> 文件示例：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><hr>\n<pre><code># kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true\n</code></pre><p>kubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。</p>\n<p>require-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。</p>\n<p>关于 kubeconfig 的一个 <strong>issue</strong>，<a href=\"https://github.com/kubernetes/kubernetes/issues/36745\" target=\"_blank\" rel=\"noopener\">Kubelet won’t read apiserver from kubeconfig</a>。</p>\n<p><strong>升级步骤</strong>，当然前提是你的 kubelet 版本已经到了 1.5：</p>\n<ul>\n<li>1，关闭 kubelet、kube-proxy 服务；</li>\n<li><p>2，注释掉 <code>/etc/kubernetes/kubelet</code> 文件中下面这一行:</p>\n<p>  <code>KUBELET_API_SERVER=&quot;--api-servers=http://127.0.0.1:8080&quot;</code></p>\n</li>\n</ul>\n<p>然后在 <strong>KUBELET_ARGS</strong> 中添加： </p>\n<pre><code>--kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true\n</code></pre><p>这里的路径是你 yaml 文件放置的路径。 </p>\n<ul>\n<li>3，重新启动刚关掉的两个服务</li>\n</ul>\n<hr>\n<h2 id=\"4，以下为-kubeconfig-配置官方文档的翻译\"><a href=\"#4，以下为-kubeconfig-配置官方文档的翻译\" class=\"headerlink\" title=\"4，以下为 kubeconfig 配置官方文档的翻译\"></a>4，以下为 <a href=\"https://kubernetes.io/docs/user-guide/kubeconfig-file/\" target=\"_blank\" rel=\"noopener\">kubeconfig 配置官方文档</a>的翻译</h2><h3 id=\"kubernetes-中的验证对于不同的群体可以使用不同的方法\"><a href=\"#kubernetes-中的验证对于不同的群体可以使用不同的方法\" class=\"headerlink\" title=\"kubernetes 中的验证对于不同的群体可以使用不同的方法.\"></a>kubernetes 中的验证对于不同的群体可以使用不同的方法.</h3><ul>\n<li>运行 kubelet 可能有的一种认证方式（即证书）。</li>\n<li>用户可能有不同的认证方式（即 token）。</li>\n<li>管理员可以为每个用户提供一个证书列表。</li>\n<li>可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 </li>\n</ul>\n<p>因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。</p>\n<p>此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。</p>\n<p>如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。</p>\n<h3 id=\"相关讨论\"><a href=\"#相关讨论\" class=\"headerlink\" title=\"相关讨论\"></a>相关讨论</h3><pre><code>http://issue.k8s.io/1755\n</code></pre><h3 id=\"kubeconfig-文件的组件\"><a href=\"#kubeconfig-文件的组件\" class=\"headerlink\" title=\"kubeconfig 文件的组件\"></a>kubeconfig 文件的组件</h3><p>kubeconfig 文件示例：</p>\n<pre><code>current-context: federal-context\napiVersion: v1\nclusters:\n- cluster:\n    api-version: v1\n    server: http://cow.org:8080\n  name: cow-cluster\n- cluster:\n    certificate-authority: path/to/my/cafile\n    server: https://horse.org:4443\n  name: horse-cluster\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://pig.org:443\n  name: pig-cluster\ncontexts:\n- context:\n    cluster: horse-cluster\n    namespace: chisel-ns\n    user: green-user\n  name: federal-context\n- context:\n    cluster: pig-cluster\n    namespace: saw-ns\n    user: black-user\n  name: queen-anne-context\nkind: Config\npreferences:\n  colors: true\nusers:\n- name: blue-user\n  user:\n    token: blue-token\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert\n    client-key: path/to/my/client/key\n</code></pre><h3 id=\"组件的解释\"><a href=\"#组件的解释\" class=\"headerlink\" title=\"组件的解释\"></a>组件的解释</h3><h4 id=\"cluster\"><a href=\"#cluster\" class=\"headerlink\" title=\"cluster\"></a>cluster</h4><pre><code>clusters:\n- cluster:\n    certificate-authority: path/to/my/cafile\n    server: https://horse.org:4443\n  name: horse-cluster\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://pig.org:443\n  name: pig-cluster\n</code></pre><p>cluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。</p>\n<h4 id=\"user\"><a href=\"#user\" class=\"headerlink\" title=\"user\"></a>user</h4><pre><code>users:\n- name: blue-user\n  user:\n    token: blue-token\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert\n    client-key: path/to/my/client/key\n</code></pre><p>用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。</p>\n<h3 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h3><pre><code>contexts:\n- context:\n    cluster: horse-cluster\n    namespace: chisel-ns\n    user: green-user\n  name: federal-context\n</code></pre><p>context 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。<br>三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。</p>\n<h4 id=\"current-context\"><a href=\"#current-context\" class=\"headerlink\" title=\"current-context\"></a>current-context</h4><pre><code>current-context: federal-context\n</code></pre><p>current-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 –context=CONTEXT, –cluster=CLUSTER, –user=USER, and/or –namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。</p>\n<h4 id=\"杂项\"><a href=\"#杂项\" class=\"headerlink\" title=\"杂项\"></a>杂项</h4><pre><code>apiVersion: v1\nkind: Config\npreferences:\n  colors: true\n</code></pre><p>apiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。<br>preferences 指定选项(和当前未使用的) kubectl preferences.</p>\n<h3 id=\"查看-kubeconfig-文件\"><a href=\"#查看-kubeconfig-文件\" class=\"headerlink\" title=\"查看 kubeconfig 文件\"></a>查看 kubeconfig 文件</h3><p>kubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 –minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。</p>\n<h3 id=\"创建你的-kubeconfig-文件\"><a href=\"#创建你的-kubeconfig-文件\" class=\"headerlink\" title=\"创建你的 kubeconfig 文件\"></a>创建你的 kubeconfig 文件</h3><p>注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。</p>\n<p>在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。</p>\n<p>因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改…</p>\n<p>以上文件可能对应于使用–token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样：</p>\n<pre><code>blue-user,blue-user,1\nmister-red,mister-red,2\n</code></pre><p>此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。</p>\n<ul>\n<li>由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。</li>\n<li>类似地，我们也可以选择改变 current-context 的值为 “blue-user”。</li>\n<li>在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过</li>\n</ul>\n<h3 id=\"加载和合并规则\"><a href=\"#加载和合并规则\" class=\"headerlink\" title=\"加载和合并规则\"></a>加载和合并规则</h3><p>加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建：</p>\n<p>1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：<br>如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。</p>\n<p>否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。</p>\n<p>对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。</p>\n<p>2，此链中第一个被匹配的 context 将被使用：</p>\n<ul>\n<li>1，命令行参数 - 命令行选项中 context 的值</li>\n<li>2，合并文件中的 current-context</li>\n<li>3，此段允许为空</li>\n</ul>\n<p>3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）：</p>\n<ul>\n<li>1，命令行参数 - user 是用户名，cluster 是集群名</li>\n<li>2，如果存在 context 则使用</li>\n<li>3，允许为空</li>\n</ul>\n<p>4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）：</p>\n<ul>\n<li>1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify</li>\n<li>2，如果存在集群信息并且该属性的值存在，则使用它。</li>\n<li>3，如果没有 server 位置则出错。</li>\n</ul>\n<p>5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法：</p>\n<ul>\n<li>1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段</li>\n<li>2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。</li>\n<li>3，如果两者有冲突则失败</li>\n</ul>\n<p>6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。</p>\n<p>7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。</p>\n<p>kubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。</p>\n<h3 id=\"通过-kubectl-config-操作-kubeconfig\"><a href=\"#通过-kubectl-config-操作-kubeconfig\" class=\"headerlink\" title=\"通过 kubectl config  操作 kubeconfig\"></a>通过 kubectl config <subcommand> 操作 kubeconfig</subcommand></h3><p>为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。</p>\n<p>例如：</p>\n<pre><code>$ kubectl config set-credentials myself --username=admin --password=secret\n$ kubectl config set-cluster local-server --server=http://localhost:8080\n$ kubectl config set-context default-context --cluster=local-server --user=myself\n$ kubectl config use-context default-context\n$ kubectl config set contexts.default-context.namespace the-right-prefix\n$ kubectl config view\n</code></pre><p>输出：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><p>一个 kubeconfig 文件类似这样：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><p>示例文件的命令操作：</p>\n<pre><code>$ kubectl config set preferences.colors true\n$ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1\n$ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile\n$ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true\n$ kubectl config set-credentials blue-user --token=blue-token\n$ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key\n$ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns\n$ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns\n$ kubectl config use-context federal-context\n</code></pre><p>最后的总结：</p>\n<p>所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了：</p>\n<ul>\n<li>仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证</li>\n<li>将上面的代码段替换为你集群的 api-server endpoint 的信息。</li>\n<li>确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 <strong>kubernetes + docker</strong> 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 <code>API</code> 进行某些操作。</p>\n<h2 id=\"1，kubernetes-搭建\"><a href=\"#1，kubernetes-搭建\" class=\"headerlink\" title=\"1，kubernetes 搭建\"></a>1，kubernetes 搭建</h2><p><a href=\"https://github.com/kubernetes/kubernetes\" target=\"_blank\" rel=\"noopener\">kubernetes</a> 是 google 的一个开源软件，其社区活跃量远超 <strong>Mesos，Coreos</strong> 的，若想深入学习建议参考<strong>《kubernetes 权威指南》</strong>，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。</p>\n<p><img src=\"http://upload-images.jianshu.io/upload_images/1262158-e050e035d6fa64ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubernetes 架构图\"></p>\n<p>kubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 <strong>gitbook</strong> 上面某大神的一本书 一 <a href=\"https://skyao.gitbooks.io/leaning-etcd3/content/documentation/leaning/\" target=\"_blank\" rel=\"noopener\">etcd3学习笔记</a>。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。</p>\n<blockquote>\n<p>注意：内核版本 &gt; 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 </p>\n</blockquote>\n<p>etcd 节点：</p>\n<pre><code># yum install -y etcd \n# systemctl start etcd  \n</code></pre><p>master 节点：</p>\n<pre><code># yum install -y kubernetes-master\n# systemctl start kube-apiserver \n# systemctl start kube-controller-manager \n# systemctl start kube-scheduler \n</code></pre><p>minion 节点：</p>\n<pre><code># yum install -y kubernetes  docker\n# systemctl start kubelet \n# systemctl start kube-proxy \n# systemctl start docker \n</code></pre><h2 id=\"2，kubernetes-版本升级\"><a href=\"#2，kubernetes-版本升级\" class=\"headerlink\" title=\"2，kubernetes 版本升级\"></a>2，kubernetes 版本升级</h2><p>以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。</p>\n<p>下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#downloads-for-v160-alpha1\" target=\"_blank\" rel=\"noopener\">kubernetes-server-linux-amd64.tar.gz</a>  这个包，下载你所要更新到的版本。</p>\n<p><strong>升级步骤</strong>：</p>\n<ul>\n<li>1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上</li>\n<li>2，备份二进制程序（kubectl,kube-proxy）</li>\n<li>3，将解压后的二进制程序覆盖以前的版本</li>\n<li>4，最后重新启动服务</li>\n</ul>\n<pre><code># systemctl stop docker\n# which kubectl kube-proxy \n/usr/bin/kubectl\n/usr/bin/kube-proxy\n\n# cp /usr/bin/{kubectl,kube-proxy} /tmp/\n# yes | cp bin/{kubectl,kube-proxy} /usr/bin/\n\n# systemctl status {kubectl,kube-proxy}\n\n# systemctl start docker \n</code></pre><h2 id=\"3，kubeconfig-使用\"><a href=\"#3，kubeconfig-使用\" class=\"headerlink\" title=\"3，kubeconfig 使用\"></a>3，kubeconfig 使用</h2><p>若你使用的 kubelet 版本为 1.4，使用 <code>systemctl status kubelet</code>  会看到这样一句话：</p>\n<pre><code>--api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml\n</code></pre><p>使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 <a href=\"http://kubernetes.io/docs/user-guide/kubeconfig-file/\" target=\"_blank\" rel=\"noopener\">官方文档</a> 有格式说明， 本人已将其翻译，翻译文档见下文。</p>\n<p><strong>kubeconfig</strong> 文件示例：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><hr>\n<pre><code># kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true\n</code></pre><p>kubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。</p>\n<p>require-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。</p>\n<p>关于 kubeconfig 的一个 <strong>issue</strong>，<a href=\"https://github.com/kubernetes/kubernetes/issues/36745\" target=\"_blank\" rel=\"noopener\">Kubelet won’t read apiserver from kubeconfig</a>。</p>\n<p><strong>升级步骤</strong>，当然前提是你的 kubelet 版本已经到了 1.5：</p>\n<ul>\n<li>1，关闭 kubelet、kube-proxy 服务；</li>\n<li><p>2，注释掉 <code>/etc/kubernetes/kubelet</code> 文件中下面这一行:</p>\n<p>  <code>KUBELET_API_SERVER=&quot;--api-servers=http://127.0.0.1:8080&quot;</code></p>\n</li>\n</ul>\n<p>然后在 <strong>KUBELET_ARGS</strong> 中添加： </p>\n<pre><code>--kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true\n</code></pre><p>这里的路径是你 yaml 文件放置的路径。 </p>\n<ul>\n<li>3，重新启动刚关掉的两个服务</li>\n</ul>\n<hr>\n<h2 id=\"4，以下为-kubeconfig-配置官方文档的翻译\"><a href=\"#4，以下为-kubeconfig-配置官方文档的翻译\" class=\"headerlink\" title=\"4，以下为 kubeconfig 配置官方文档的翻译\"></a>4，以下为 <a href=\"https://kubernetes.io/docs/user-guide/kubeconfig-file/\" target=\"_blank\" rel=\"noopener\">kubeconfig 配置官方文档</a>的翻译</h2><h3 id=\"kubernetes-中的验证对于不同的群体可以使用不同的方法\"><a href=\"#kubernetes-中的验证对于不同的群体可以使用不同的方法\" class=\"headerlink\" title=\"kubernetes 中的验证对于不同的群体可以使用不同的方法.\"></a>kubernetes 中的验证对于不同的群体可以使用不同的方法.</h3><ul>\n<li>运行 kubelet 可能有的一种认证方式（即证书）。</li>\n<li>用户可能有不同的认证方式（即 token）。</li>\n<li>管理员可以为每个用户提供一个证书列表。</li>\n<li>可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 </li>\n</ul>\n<p>因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。</p>\n<p>此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。</p>\n<p>如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。</p>\n<h3 id=\"相关讨论\"><a href=\"#相关讨论\" class=\"headerlink\" title=\"相关讨论\"></a>相关讨论</h3><pre><code>http://issue.k8s.io/1755\n</code></pre><h3 id=\"kubeconfig-文件的组件\"><a href=\"#kubeconfig-文件的组件\" class=\"headerlink\" title=\"kubeconfig 文件的组件\"></a>kubeconfig 文件的组件</h3><p>kubeconfig 文件示例：</p>\n<pre><code>current-context: federal-context\napiVersion: v1\nclusters:\n- cluster:\n    api-version: v1\n    server: http://cow.org:8080\n  name: cow-cluster\n- cluster:\n    certificate-authority: path/to/my/cafile\n    server: https://horse.org:4443\n  name: horse-cluster\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://pig.org:443\n  name: pig-cluster\ncontexts:\n- context:\n    cluster: horse-cluster\n    namespace: chisel-ns\n    user: green-user\n  name: federal-context\n- context:\n    cluster: pig-cluster\n    namespace: saw-ns\n    user: black-user\n  name: queen-anne-context\nkind: Config\npreferences:\n  colors: true\nusers:\n- name: blue-user\n  user:\n    token: blue-token\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert\n    client-key: path/to/my/client/key\n</code></pre><h3 id=\"组件的解释\"><a href=\"#组件的解释\" class=\"headerlink\" title=\"组件的解释\"></a>组件的解释</h3><h4 id=\"cluster\"><a href=\"#cluster\" class=\"headerlink\" title=\"cluster\"></a>cluster</h4><pre><code>clusters:\n- cluster:\n    certificate-authority: path/to/my/cafile\n    server: https://horse.org:4443\n  name: horse-cluster\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://pig.org:443\n  name: pig-cluster\n</code></pre><p>cluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。</p>\n<h4 id=\"user\"><a href=\"#user\" class=\"headerlink\" title=\"user\"></a>user</h4><pre><code>users:\n- name: blue-user\n  user:\n    token: blue-token\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert\n    client-key: path/to/my/client/key\n</code></pre><p>用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。</p>\n<h3 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h3><pre><code>contexts:\n- context:\n    cluster: horse-cluster\n    namespace: chisel-ns\n    user: green-user\n  name: federal-context\n</code></pre><p>context 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。<br>三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。</p>\n<h4 id=\"current-context\"><a href=\"#current-context\" class=\"headerlink\" title=\"current-context\"></a>current-context</h4><pre><code>current-context: federal-context\n</code></pre><p>current-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 –context=CONTEXT, –cluster=CLUSTER, –user=USER, and/or –namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。</p>\n<h4 id=\"杂项\"><a href=\"#杂项\" class=\"headerlink\" title=\"杂项\"></a>杂项</h4><pre><code>apiVersion: v1\nkind: Config\npreferences:\n  colors: true\n</code></pre><p>apiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。<br>preferences 指定选项(和当前未使用的) kubectl preferences.</p>\n<h3 id=\"查看-kubeconfig-文件\"><a href=\"#查看-kubeconfig-文件\" class=\"headerlink\" title=\"查看 kubeconfig 文件\"></a>查看 kubeconfig 文件</h3><p>kubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 –minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。</p>\n<h3 id=\"创建你的-kubeconfig-文件\"><a href=\"#创建你的-kubeconfig-文件\" class=\"headerlink\" title=\"创建你的 kubeconfig 文件\"></a>创建你的 kubeconfig 文件</h3><p>注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。</p>\n<p>在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。</p>\n<p>因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改…</p>\n<p>以上文件可能对应于使用–token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样：</p>\n<pre><code>blue-user,blue-user,1\nmister-red,mister-red,2\n</code></pre><p>此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。</p>\n<ul>\n<li>由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。</li>\n<li>类似地，我们也可以选择改变 current-context 的值为 “blue-user”。</li>\n<li>在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过</li>\n</ul>\n<h3 id=\"加载和合并规则\"><a href=\"#加载和合并规则\" class=\"headerlink\" title=\"加载和合并规则\"></a>加载和合并规则</h3><p>加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建：</p>\n<p>1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：<br>如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。</p>\n<p>否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。</p>\n<p>对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。</p>\n<p>2，此链中第一个被匹配的 context 将被使用：</p>\n<ul>\n<li>1，命令行参数 - 命令行选项中 context 的值</li>\n<li>2，合并文件中的 current-context</li>\n<li>3，此段允许为空</li>\n</ul>\n<p>3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）：</p>\n<ul>\n<li>1，命令行参数 - user 是用户名，cluster 是集群名</li>\n<li>2，如果存在 context 则使用</li>\n<li>3，允许为空</li>\n</ul>\n<p>4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）：</p>\n<ul>\n<li>1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify</li>\n<li>2，如果存在集群信息并且该属性的值存在，则使用它。</li>\n<li>3，如果没有 server 位置则出错。</li>\n</ul>\n<p>5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法：</p>\n<ul>\n<li>1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段</li>\n<li>2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。</li>\n<li>3，如果两者有冲突则失败</li>\n</ul>\n<p>6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。</p>\n<p>7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。</p>\n<p>kubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。</p>\n<h3 id=\"通过-kubectl-config-操作-kubeconfig\"><a href=\"#通过-kubectl-config-操作-kubeconfig\" class=\"headerlink\" title=\"通过 kubectl config  操作 kubeconfig\"></a>通过 kubectl config <subcommand> 操作 kubeconfig</subcommand></h3><p>为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。</p>\n<p>例如：</p>\n<pre><code>$ kubectl config set-credentials myself --username=admin --password=secret\n$ kubectl config set-cluster local-server --server=http://localhost:8080\n$ kubectl config set-context default-context --cluster=local-server --user=myself\n$ kubectl config use-context default-context\n$ kubectl config set contexts.default-context.namespace the-right-prefix\n$ kubectl config view\n</code></pre><p>输出：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><p>一个 kubeconfig 文件类似这样：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><p>示例文件的命令操作：</p>\n<pre><code>$ kubectl config set preferences.colors true\n$ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1\n$ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile\n$ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true\n$ kubectl config set-credentials blue-user --token=blue-token\n$ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key\n$ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns\n$ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns\n$ kubectl config use-context federal-context\n</code></pre><p>最后的总结：</p>\n<p>所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了：</p>\n<ul>\n<li>仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证</li>\n<li>将上面的代码段替换为你集群的 api-server endpoint 的信息。</li>\n<li>确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。</li>\n</ul>\n"},{"title":"kubelet 启动流程分析","date":"2018-12-23T13:22:30.000Z","type":"kubelet","_content":"\n上篇文章（[kubelet 架构浅析](https://blog.tianfeiyu.com/2018/12/16/kubelet-modules/) ）已经介绍过 kubelet 在整个集群架构中的功能以及自身各模块的用途，本篇文章主要介绍 kubelet 的启动流程。\n\n > kubernetes 版本： v1.12 \n\n\n## kubelet 启动流程\n\nkubelet 代码结构:\n\n```\n➜  kubernetes git:(release-1.12) ✗ tree cmd/kubelet\ncmd/kubelet\n├── BUILD\n├── OWNERS\n├── app\n│   ├── BUILD\n│   ├── OWNERS\n│   ├── auth.go\n│   ├── init_others.go\n│   ├── init_windows.go\n│   ├── options\n│   │   ├── BUILD\n│   │   ├── container_runtime.go\n│   │   ├── globalflags.go\n│   │   ├── globalflags_linux.go\n│   │   ├── globalflags_other.go\n│   │   ├── options.go\n│   │   ├── options_test.go\n│   │   ├── osflags_others.go\n│   │   └── osflags_windows.go\n│   ├── plugins.go\n│   ├── server.go\n│   ├── server_linux.go\n│   ├── server_test.go\n│   └── server_unsupported.go\n└── kubelet.go\n\n2 directories, 22 files\n```\n\n![kubelet 启动流程时序图](https://upload-images.jianshu.io/upload_images/1262158-bdeb34a5cdda93d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### 1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）\n\n```\nfunc main() {\n\trand.Seed(time.Now().UTC().UnixNano())\n\n\tcommand := app.NewKubeletCommand(server.SetupSignalHandler())\n\tlogs.InitLogs()\n\tdefer logs.FlushLogs()\n\n\tif err := command.Execute(); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"%v\\n\", err)\n\t\tos.Exit(1)\n\t}\n}\n```\n\n#### 2、初始化 kubelet 配置（cmd/kubelet/app/server.go）\n\nNewKubeletCommand() 函数主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。  \n\n```\n// NewKubeletCommand creates a *cobra.Command object with default parameters\nfunc NewKubeletCommand(stopCh <-chan struct{}) *cobra.Command {\n    cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)\n    cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc)\n    // Kubelet配置分两部分:\n    // KubeletFlag: 指那些不允许在 kubelet 运行时进行修改的配置集，或者不能在集群中各个 Nodes 之间共享的配置集。\n    // KubeletConfiguration: 指可以在集群中各个Nodes之间共享的配置集，可以进行动态配置。\n    kubeletFlags := options.NewKubeletFlags()\n\tkubeletConfig, err := options.NewKubeletConfiguration()\n\t...\n\tcmd := &cobra.Command{\n\t\t...\n\t\tRun: func(cmd *cobra.Command, args []string) {\n\t\t\t// 读取 kubelet 配置文件\n\t\t\tif configFile := kubeletFlags.KubeletConfigFile; len(configFile) > 0 {\n\t\t\t\tkubeletConfig, err = loadConfigFile(configFile)\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Fatal(err)\n\t\t\t\t}\n\t\t\t\t...\n\t\t\t}\n\t\t\t// 校验 kubelet 参数\n\t\t\tif err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t\t...\n\t\t\t// 此处初始化了 kubeletDeps\n\t\t\tkubeletDeps, err := UnsecuredDependencies(kubeletServer)\n\t\t\tif err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t\t...\n\t\t\t// 启动程序\n\t\t\tif err := Run(kubeletServer, kubeletDeps, stopCh); err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t},\n\t}\n    ...\n\treturn cmd\n}\n```\nkubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为。主要包括监控功能（cadvisor），cgroup 管理功能（containerManager）等。\n\nNewKubeletCommand() 会调用 Run() 函数，Run() 中主要调用 run() 函数进行一些准备事项。\n\n#### 3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）\n\nrun() 函数的主要功能：\n\n- 1、创建 kubeClient，evnetClient 用来和 apiserver 通信。创建 heartbeatClient 向 apiserver 上报心跳状态。\n- 2、为 kubeDeps 设定一些默认值。\n- 3、启动监听 Healthz 端口的 http server，默认端口是 10248。\n\n```\nfunc run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh <-chan struct{}) (err error) {\n\t...\n\t// 判断 kubelet 的启动模式\n\tif standaloneMode {\n\t...\n\t} else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil {\n\t\t...\n\t\t// 创建对象 kubeClient\n\t\tkubeClient, err = clientset.NewForConfig(clientConfig)\n\n\t\t...\n        // 创建对象 evnetClient\n\t\teventClient, err = v1core.NewForConfig(&eventClientConfig)\n\t\t...\n\t\t// heartbeatClient 上报状态\n\t\theartbeatClient, err = clientset.NewForConfig(&heartbeatClientConfig)\n\t\t...\n\t}\n\n\t// 为 kubeDeps 设定一些默认值\n\tif kubeDeps.Auth == nil {\n\t\t\tauth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tkubeDeps.Auth = auth\n\t\t}\n\n\t\tif kubeDeps.CAdvisorInterface == nil {\n\t\t\timageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)\n\t\t\tkubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// \n\tif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil {\n\t\t\treturn err\n\t}\n\t...\n\t// 启动监听 Healthz 端口的 http server  \n\tif s.HealthzPort > 0 {\n\t\thealthz.DefaultHealthz()\n\t\tgo wait.Until(func() {\n\t\t\terr := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil)\n\t\t\tif err != nil {\n\t\t\t\tglog.Errorf(\"Starting health server failed: %v\", err)\n\t\t\t}\n\t\t}, 5*time.Second, wait.NeverStop)\n\t}\n\t...\n}\n```\nkubelet 对 pod 资源的获取方式有三种：第一种是通过文件获得，文件一般放在 /etc/kubernetes/manifests 目录下面；第二种也是通过文件过得，只不过文件是通过 URL 获取的；第三种是通过 watch kube-apiserver 获取。其中前两种模式下，我们称 kubelet 运行在 standalone 模式下，运行在 standalone 模式下的 kubelet 一般用于调试某些功能。\n\nrun() 中调用 RunKubelet() 函数进行后续操作。\n\n#### 4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）\n\nRunKubelet()  主要功能：\n\n- 1、初始化 kubelet 组件中的各个模块，创建出 kubelet 对象。\n- 2、启动垃圾回收服务。\n\n```\nfunc RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error {\n    ...\n\n \t// 初始化 kubelet 内部模块\n\tk, err := CreateAndInitKubelet(&kubeServer.KubeletConfiguration,\n\t\tkubeDeps,\n\t\t&kubeServer.ContainerRuntimeOptions,\n\t\tkubeServer.ContainerRuntime,\n\t\tkubeServer.RuntimeCgroups,\n\t\tkubeServer.HostnameOverride,\n\t\tkubeServer.NodeIP,\n\t\tkubeServer.ProviderID,\n\t\tkubeServer.CloudProvider,\n\t\tkubeServer.CertDirectory,\n\t\tkubeServer.RootDirectory,\n\t\tkubeServer.RegisterNode,\n\t\tkubeServer.RegisterWithTaints,\n\t\tkubeServer.AllowedUnsafeSysctls,\n\t\tkubeServer.RemoteRuntimeEndpoint,\n\t\tkubeServer.RemoteImageEndpoint,\n\t\tkubeServer.ExperimentalMounterPath,\n\t\tkubeServer.ExperimentalKernelMemcgNotification,\n\t\tkubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount,\n\t\tkubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold,\n\t\tkubeServer.MinimumGCAge,\n\t\tkubeServer.MaxPerPodContainerCount,\n\t\tkubeServer.MaxContainerCount,\n\t\tkubeServer.MasterServiceNamespace,\n\t\tkubeServer.RegisterSchedulable,\n\t\tkubeServer.NonMasqueradeCIDR,\n\t\tkubeServer.KeepTerminatedPodVolumes,\n\t\tkubeServer.NodeLabels,\n\t\tkubeServer.SeccompProfileRoot,\n\t\tkubeServer.BootstrapCheckpointPath,\n\t\tkubeServer.NodeStatusMaxImages)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create kubelet: %v\", err)\n\t}\n\n\t...\n\tif runOnce {\n\t\tif _, err := k.RunOnce(podCfg.Updates()); err != nil {\n\t\t\treturn fmt.Errorf(\"runonce failed: %v\", err)\n\t\t}\n\t\tglog.Infof(\"Started kubelet as runonce\")\n\t} else {\n        // \n\t\tstartKubelet(k, podCfg, &kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer)\n\t\tglog.Infof(\"Started kubelet\")\n\t}\n\n}\n```\n\n\n```\nfunc CreateAndInitKubelet(...){\n\t// NewMainKubelet 实例化一个 kubelet 对象，并对 kubelet 内部各个模块进行初始化\n\tk, err = kubelet.NewMainKubelet(kubeCfg,\n\t\tkubeDeps,\n\t\tcrOptions,\n\t\tcontainerRuntime,\n\t\truntimeCgroups,\n\t\thostnameOverride,\n\t\tnodeIP,\n\t\tproviderID,\n\t\tcloudProvider,\n\t\tcertDirectory,\n\t\trootDirectory,\n\t\tregisterNode,\n\t\tregisterWithTaints,\n\t\tallowedUnsafeSysctls,\n\t\tremoteRuntimeEndpoint,\n\t\tremoteImageEndpoint,\n\t\texperimentalMounterPath,\n\t\texperimentalKernelMemcgNotification,\n\t\texperimentalCheckNodeCapabilitiesBeforeMount,\n\t\texperimentalNodeAllocatableIgnoreEvictionThreshold,\n\t\tminimumGCAge,\n\t\tmaxPerPodContainerCount,\n\t\tmaxContainerCount,\n\t\tmasterServiceNamespace,\n\t\tregisterSchedulable,\n\t\tnonMasqueradeCIDR,\n\t\tkeepTerminatedPodVolumes,\n\t\tnodeLabels,\n\t\tseccompProfileRoot,\n\t\tbootstrapCheckpointPath,\n\t\tnodeStatusMaxImages)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// 通知 apiserver kubelet 启动了\n\tk.BirthCry()\n\t// 启动垃圾回收服务\n\tk.StartGarbageCollection()\n\n\treturn k, nil\n\n}\n```\n\n```\nfunc NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...){\n    ...\n\tif kubeDeps.PodConfig == nil {\n\t\tvar err error\n\t\t// 初始化 makePodSourceConfig，监听 pod 元数据的来源(FILE, URL, api-server)，将不同 source 的 pod configuration 合并到一个结构中\n\t\tkubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n    \n    // kubelet 服务端口，默认 10250\n\tdaemonEndpoints := &v1.NodeDaemonEndpoints{\n\t\tKubeletEndpoint: v1.DaemonEndpoint{Port: kubeCfg.Port},\n\t}\n\n\t// 使用 reflector 把 ListWatch 得到的服务信息实时同步到 serviceStore 对象中\n\tserviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})\n\tif kubeDeps.KubeClient != nil {\n\t\tserviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"services\", metav1.NamespaceAll, fields.Everything())\n\t\tr := cache.NewReflector(serviceLW, &v1.Service{}, serviceIndexer, 0)\n\t\tgo r.Run(wait.NeverStop)\n\t}\n\tserviceLister := corelisters.NewServiceLister(serviceIndexer)\n\n\t// 使用 reflector 把 ListWatch 得到的节点信息实时同步到  nodeStore 对象中\n\tnodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{})\n\tif kubeDeps.KubeClient != nil {\n\t\tfieldSelector := fields.Set{api.ObjectNameField: string(nodeName)}.AsSelector()\n\t\tnodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"nodes\", metav1.NamespaceAll, fieldSelector)\n\t\tr := cache.NewReflector(nodeLW, &v1.Node{}, nodeIndexer, 0)\n\t\tgo r.Run(wait.NeverStop)\n\t}\n\tnodeInfo := &predicates.CachedNodeInfo{NodeLister: corelisters.NewNodeLister(nodeIndexer)}\n\n\t...\n\t// node 资源不足时的驱逐策略的设定\n\tthresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tevictionConfig := eviction.Config{\n\t\tPressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,\n\t\tMaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),\n\t\tThresholds:               thresholds,\n\t\tKernelMemcgNotification:  experimentalKernelMemcgNotification,\n\t\tPodCgroupRoot:            kubeDeps.ContainerManager.GetPodCgroupRoot(),\n\t}\n    ...\n    // 容器引用的管理\n\tcontainerRefManager := kubecontainer.NewRefManager()\n    // oom 监控\n\toomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder)\n\n\t// 根据配置信息和各种对象创建 Kubelet 实例\n\tklet := &Kubelet{\n\t\thostname:                       hostname,\n\t\thostnameOverridden:             len(hostnameOverride) > 0,\n\t\tnodeName:                       nodeName,\n\t\t...\n\t}\n\t\n\t// 从 cAdvisor 获取当前机器的信息\n\tmachineInfo, err := klet.cadvisor.MachineInfo()\n\n\t// 对 pod 的管理（如: 增删改等）\n\tklet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager)\n\n\t// 容器运行时管理\n\truntime, err := kuberuntime.NewKubeGenericRuntimeManager(...)\n\n\t// pleg\n\tklet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock{})\n\n\t// 创建 containerGC 对象，进行周期性的容器清理工作\n\tcontainerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)\n\n\t// 创建 imageManager 管理镜像\n\timageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage)\n\t\n\t// statusManager 实时检测节点上 pod 的状态，并更新到 apiserver 对应的 pod\n\tklet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)\n\n\t// 探针管理\n\tklet.probeManager = prober.NewManager(...)\n\n    // token 管理\n\ttokenManager := token.NewManager(kubeDeps.KubeClient)\n\n\t// 磁盘管理\n\tklet.volumeManager = volumemanager.NewVolumeManager()\n\t\n\t// 将 syncPod() 注入到 podWorkers 中\n\tklet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)\n\n\t// 容器驱逐策略管理\n\tevictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock)\n    ...\n}\n```\nRunKubelet 最后会调用 startKubelet() 进行后续的操作。\n\n#### 5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）\nstartKubelet()  的主要功能：\n\n- 1、以 goroutine 方式启动 kubelet 中的各个模块。\n- 2、启动 kubelet http server。\n\n\n```\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) {\n\tgo wait.Until(func() {\n\t\t// 以 goroutine 方式启动 kubelet 中的各个模块\n\t\tk.Run(podCfg.Updates())\n\t}, 0, wait.NeverStop)\n\n\t// 启动 kubelet http server\t\n\tif enableServer {\n\t\tgo k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling)\n\n\t}\n\tif kubeCfg.ReadOnlyPort > 0 {\n\t\tgo k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort))\n\t}\n}\n```\n\n```\n// Run starts the kubelet reacting to config updates\nfunc (kl *Kubelet) Run(updates <-chan kubetypes.PodUpdate) {\n\tif kl.logServer == nil {\n\t\tkl.logServer = http.StripPrefix(\"/logs/\", http.FileServer(http.Dir(\"/var/log/\")))\n\t}\n\tif kl.kubeClient == nil {\n\t\tglog.Warning(\"No api server defined - no node status update will be sent.\")\n\t}\n\n\t// Start the cloud provider sync manager\n\tif kl.cloudResourceSyncManager != nil {\n\t\tgo kl.cloudResourceSyncManager.Run(wait.NeverStop)\n\t}\n\n\tif err := kl.initializeModules(); err != nil {\n\t\tkl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())\n\t\tglog.Fatal(err)\n\t}\n\n\t// Start volume manager\n\tgo kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)\n\n\tif kl.kubeClient != nil {\n\t\t// Start syncing node status immediately, this may set up things the runtime needs to run.\n\t\tgo wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)\n\t\tgo kl.fastStatusUpdateOnce()\n\n\t\t// start syncing lease\n\t\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) {\n\t\t\tgo kl.nodeLeaseController.Run(wait.NeverStop)\n\t\t}\n\t}\n\tgo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)\n\n\t// Start loop to sync iptables util rules\n\tif kl.makeIPTablesUtilChains {\n\t\tgo wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop)\n\t}\n\n\t// Start a goroutine responsible for killing pods (that are not properly\n\t// handled by pod workers).\n\tgo wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)\n\n\t// Start component sync loops.\n\tkl.statusManager.Start()\n\tkl.probeManager.Start()\n\n\t// Start syncing RuntimeClasses if enabled.\n\tif kl.runtimeClassManager != nil {\n\t\tgo kl.runtimeClassManager.Run(wait.NeverStop)\n\t}\n\n\t// Start the pod lifecycle event generator.\n\tkl.pleg.Start()\n\n\tkl.syncLoop(updates, kl)\n}\n```\nsyncLoop 是 kubelet 的主循环方法，它从不同的管道(FILE,URL, API-SERVER)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 Pod 处于期望的状态。\n\n\n```\nfunc (kl *Kubelet) syncLoop(updates <-chan kubetypes.PodUpdate, handler SyncHandler) {\n\tglog.Info(\"Starting kubelet main sync loop.\")\n\n\t// syncTicker 每秒检测一次是否有需要同步的 pod workers\n\tsyncTicker := time.NewTicker(time.Second)\n\tdefer syncTicker.Stop()\n\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\n\tdefer housekeepingTicker.Stop()\n\tplegCh := kl.pleg.Watch()\n\tconst (\n\t\tbase   = 100 * time.Millisecond\n\t\tmax    = 5 * time.Second\n\t\tfactor = 2\n\t)\n\tduration := base\n\tfor {\n\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {\n\t\t\tglog.Infof(\"skipping pod synchronization - %v\", rs)\n\t\t\t// exponential backoff\n\t\t\ttime.Sleep(duration)\n\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))\n\t\t\tcontinue\n\t\t}\n\t\t// reset backoff if we have a success\n\t\tduration = base\n\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t\t// \n\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {\n\t\t\tbreak\n\t\t}\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t}\n}\n```\n\nsyncLoopIteration()  方法对多个管道进行遍历，如果 pod 发生变化，则会调用相应的 Handler，在 Handler 中通过调用 dispatchWork 分发任务。\n\n\n## 总结\n\n本篇文章主要讲述了 kubelet 组件从加载配置到初始化内部的各个模块再到启动 kubelet 服务的整个流程，上面的时序图能清楚的看到函数之间的调用关系，但是其中每个组件具体的工作方式以及组件之间的交互方式还不得而知，后面会一探究竟。\n\n参考：\n[kubernetes node components – kubelet](http://www.sel.zju.edu.cn/?p=595)\n[Kubelet 源码分析(一):启动流程分析](https://segmentfault.com/a/1190000008267351)\n[kubelet 源码分析：启动流程](https://cizixs.com/2017/06/06/kubelet-source-code-analysis-part-1/)\n[kubernetes 的 kubelet 的工作过程](https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/05/02/Kubernetes-kubelet.html)\n[kubelet 内部实现解析](https://fatsheep9146.github.io/2018/07/08/kubelet%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/)\n","source":"_posts/kubelet_init.md","raw":"---\ntitle: kubelet 启动流程分析\ndate: 2018-12-23 21:22:30\ntags: \"kubelet\"\ntype: \"kubelet\"\n\n---\n\n上篇文章（[kubelet 架构浅析](https://blog.tianfeiyu.com/2018/12/16/kubelet-modules/) ）已经介绍过 kubelet 在整个集群架构中的功能以及自身各模块的用途，本篇文章主要介绍 kubelet 的启动流程。\n\n > kubernetes 版本： v1.12 \n\n\n## kubelet 启动流程\n\nkubelet 代码结构:\n\n```\n➜  kubernetes git:(release-1.12) ✗ tree cmd/kubelet\ncmd/kubelet\n├── BUILD\n├── OWNERS\n├── app\n│   ├── BUILD\n│   ├── OWNERS\n│   ├── auth.go\n│   ├── init_others.go\n│   ├── init_windows.go\n│   ├── options\n│   │   ├── BUILD\n│   │   ├── container_runtime.go\n│   │   ├── globalflags.go\n│   │   ├── globalflags_linux.go\n│   │   ├── globalflags_other.go\n│   │   ├── options.go\n│   │   ├── options_test.go\n│   │   ├── osflags_others.go\n│   │   └── osflags_windows.go\n│   ├── plugins.go\n│   ├── server.go\n│   ├── server_linux.go\n│   ├── server_test.go\n│   └── server_unsupported.go\n└── kubelet.go\n\n2 directories, 22 files\n```\n\n![kubelet 启动流程时序图](https://upload-images.jianshu.io/upload_images/1262158-bdeb34a5cdda93d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n#### 1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）\n\n```\nfunc main() {\n\trand.Seed(time.Now().UTC().UnixNano())\n\n\tcommand := app.NewKubeletCommand(server.SetupSignalHandler())\n\tlogs.InitLogs()\n\tdefer logs.FlushLogs()\n\n\tif err := command.Execute(); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"%v\\n\", err)\n\t\tos.Exit(1)\n\t}\n}\n```\n\n#### 2、初始化 kubelet 配置（cmd/kubelet/app/server.go）\n\nNewKubeletCommand() 函数主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。  \n\n```\n// NewKubeletCommand creates a *cobra.Command object with default parameters\nfunc NewKubeletCommand(stopCh <-chan struct{}) *cobra.Command {\n    cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)\n    cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc)\n    // Kubelet配置分两部分:\n    // KubeletFlag: 指那些不允许在 kubelet 运行时进行修改的配置集，或者不能在集群中各个 Nodes 之间共享的配置集。\n    // KubeletConfiguration: 指可以在集群中各个Nodes之间共享的配置集，可以进行动态配置。\n    kubeletFlags := options.NewKubeletFlags()\n\tkubeletConfig, err := options.NewKubeletConfiguration()\n\t...\n\tcmd := &cobra.Command{\n\t\t...\n\t\tRun: func(cmd *cobra.Command, args []string) {\n\t\t\t// 读取 kubelet 配置文件\n\t\t\tif configFile := kubeletFlags.KubeletConfigFile; len(configFile) > 0 {\n\t\t\t\tkubeletConfig, err = loadConfigFile(configFile)\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Fatal(err)\n\t\t\t\t}\n\t\t\t\t...\n\t\t\t}\n\t\t\t// 校验 kubelet 参数\n\t\t\tif err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t\t...\n\t\t\t// 此处初始化了 kubeletDeps\n\t\t\tkubeletDeps, err := UnsecuredDependencies(kubeletServer)\n\t\t\tif err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t\t...\n\t\t\t// 启动程序\n\t\t\tif err := Run(kubeletServer, kubeletDeps, stopCh); err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t},\n\t}\n    ...\n\treturn cmd\n}\n```\nkubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为。主要包括监控功能（cadvisor），cgroup 管理功能（containerManager）等。\n\nNewKubeletCommand() 会调用 Run() 函数，Run() 中主要调用 run() 函数进行一些准备事项。\n\n#### 3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）\n\nrun() 函数的主要功能：\n\n- 1、创建 kubeClient，evnetClient 用来和 apiserver 通信。创建 heartbeatClient 向 apiserver 上报心跳状态。\n- 2、为 kubeDeps 设定一些默认值。\n- 3、启动监听 Healthz 端口的 http server，默认端口是 10248。\n\n```\nfunc run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh <-chan struct{}) (err error) {\n\t...\n\t// 判断 kubelet 的启动模式\n\tif standaloneMode {\n\t...\n\t} else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil {\n\t\t...\n\t\t// 创建对象 kubeClient\n\t\tkubeClient, err = clientset.NewForConfig(clientConfig)\n\n\t\t...\n        // 创建对象 evnetClient\n\t\teventClient, err = v1core.NewForConfig(&eventClientConfig)\n\t\t...\n\t\t// heartbeatClient 上报状态\n\t\theartbeatClient, err = clientset.NewForConfig(&heartbeatClientConfig)\n\t\t...\n\t}\n\n\t// 为 kubeDeps 设定一些默认值\n\tif kubeDeps.Auth == nil {\n\t\t\tauth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tkubeDeps.Auth = auth\n\t\t}\n\n\t\tif kubeDeps.CAdvisorInterface == nil {\n\t\t\timageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)\n\t\t\tkubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// \n\tif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil {\n\t\t\treturn err\n\t}\n\t...\n\t// 启动监听 Healthz 端口的 http server  \n\tif s.HealthzPort > 0 {\n\t\thealthz.DefaultHealthz()\n\t\tgo wait.Until(func() {\n\t\t\terr := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil)\n\t\t\tif err != nil {\n\t\t\t\tglog.Errorf(\"Starting health server failed: %v\", err)\n\t\t\t}\n\t\t}, 5*time.Second, wait.NeverStop)\n\t}\n\t...\n}\n```\nkubelet 对 pod 资源的获取方式有三种：第一种是通过文件获得，文件一般放在 /etc/kubernetes/manifests 目录下面；第二种也是通过文件过得，只不过文件是通过 URL 获取的；第三种是通过 watch kube-apiserver 获取。其中前两种模式下，我们称 kubelet 运行在 standalone 模式下，运行在 standalone 模式下的 kubelet 一般用于调试某些功能。\n\nrun() 中调用 RunKubelet() 函数进行后续操作。\n\n#### 4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）\n\nRunKubelet()  主要功能：\n\n- 1、初始化 kubelet 组件中的各个模块，创建出 kubelet 对象。\n- 2、启动垃圾回收服务。\n\n```\nfunc RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error {\n    ...\n\n \t// 初始化 kubelet 内部模块\n\tk, err := CreateAndInitKubelet(&kubeServer.KubeletConfiguration,\n\t\tkubeDeps,\n\t\t&kubeServer.ContainerRuntimeOptions,\n\t\tkubeServer.ContainerRuntime,\n\t\tkubeServer.RuntimeCgroups,\n\t\tkubeServer.HostnameOverride,\n\t\tkubeServer.NodeIP,\n\t\tkubeServer.ProviderID,\n\t\tkubeServer.CloudProvider,\n\t\tkubeServer.CertDirectory,\n\t\tkubeServer.RootDirectory,\n\t\tkubeServer.RegisterNode,\n\t\tkubeServer.RegisterWithTaints,\n\t\tkubeServer.AllowedUnsafeSysctls,\n\t\tkubeServer.RemoteRuntimeEndpoint,\n\t\tkubeServer.RemoteImageEndpoint,\n\t\tkubeServer.ExperimentalMounterPath,\n\t\tkubeServer.ExperimentalKernelMemcgNotification,\n\t\tkubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount,\n\t\tkubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold,\n\t\tkubeServer.MinimumGCAge,\n\t\tkubeServer.MaxPerPodContainerCount,\n\t\tkubeServer.MaxContainerCount,\n\t\tkubeServer.MasterServiceNamespace,\n\t\tkubeServer.RegisterSchedulable,\n\t\tkubeServer.NonMasqueradeCIDR,\n\t\tkubeServer.KeepTerminatedPodVolumes,\n\t\tkubeServer.NodeLabels,\n\t\tkubeServer.SeccompProfileRoot,\n\t\tkubeServer.BootstrapCheckpointPath,\n\t\tkubeServer.NodeStatusMaxImages)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create kubelet: %v\", err)\n\t}\n\n\t...\n\tif runOnce {\n\t\tif _, err := k.RunOnce(podCfg.Updates()); err != nil {\n\t\t\treturn fmt.Errorf(\"runonce failed: %v\", err)\n\t\t}\n\t\tglog.Infof(\"Started kubelet as runonce\")\n\t} else {\n        // \n\t\tstartKubelet(k, podCfg, &kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer)\n\t\tglog.Infof(\"Started kubelet\")\n\t}\n\n}\n```\n\n\n```\nfunc CreateAndInitKubelet(...){\n\t// NewMainKubelet 实例化一个 kubelet 对象，并对 kubelet 内部各个模块进行初始化\n\tk, err = kubelet.NewMainKubelet(kubeCfg,\n\t\tkubeDeps,\n\t\tcrOptions,\n\t\tcontainerRuntime,\n\t\truntimeCgroups,\n\t\thostnameOverride,\n\t\tnodeIP,\n\t\tproviderID,\n\t\tcloudProvider,\n\t\tcertDirectory,\n\t\trootDirectory,\n\t\tregisterNode,\n\t\tregisterWithTaints,\n\t\tallowedUnsafeSysctls,\n\t\tremoteRuntimeEndpoint,\n\t\tremoteImageEndpoint,\n\t\texperimentalMounterPath,\n\t\texperimentalKernelMemcgNotification,\n\t\texperimentalCheckNodeCapabilitiesBeforeMount,\n\t\texperimentalNodeAllocatableIgnoreEvictionThreshold,\n\t\tminimumGCAge,\n\t\tmaxPerPodContainerCount,\n\t\tmaxContainerCount,\n\t\tmasterServiceNamespace,\n\t\tregisterSchedulable,\n\t\tnonMasqueradeCIDR,\n\t\tkeepTerminatedPodVolumes,\n\t\tnodeLabels,\n\t\tseccompProfileRoot,\n\t\tbootstrapCheckpointPath,\n\t\tnodeStatusMaxImages)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// 通知 apiserver kubelet 启动了\n\tk.BirthCry()\n\t// 启动垃圾回收服务\n\tk.StartGarbageCollection()\n\n\treturn k, nil\n\n}\n```\n\n```\nfunc NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...){\n    ...\n\tif kubeDeps.PodConfig == nil {\n\t\tvar err error\n\t\t// 初始化 makePodSourceConfig，监听 pod 元数据的来源(FILE, URL, api-server)，将不同 source 的 pod configuration 合并到一个结构中\n\t\tkubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n    \n    // kubelet 服务端口，默认 10250\n\tdaemonEndpoints := &v1.NodeDaemonEndpoints{\n\t\tKubeletEndpoint: v1.DaemonEndpoint{Port: kubeCfg.Port},\n\t}\n\n\t// 使用 reflector 把 ListWatch 得到的服务信息实时同步到 serviceStore 对象中\n\tserviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})\n\tif kubeDeps.KubeClient != nil {\n\t\tserviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"services\", metav1.NamespaceAll, fields.Everything())\n\t\tr := cache.NewReflector(serviceLW, &v1.Service{}, serviceIndexer, 0)\n\t\tgo r.Run(wait.NeverStop)\n\t}\n\tserviceLister := corelisters.NewServiceLister(serviceIndexer)\n\n\t// 使用 reflector 把 ListWatch 得到的节点信息实时同步到  nodeStore 对象中\n\tnodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{})\n\tif kubeDeps.KubeClient != nil {\n\t\tfieldSelector := fields.Set{api.ObjectNameField: string(nodeName)}.AsSelector()\n\t\tnodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"nodes\", metav1.NamespaceAll, fieldSelector)\n\t\tr := cache.NewReflector(nodeLW, &v1.Node{}, nodeIndexer, 0)\n\t\tgo r.Run(wait.NeverStop)\n\t}\n\tnodeInfo := &predicates.CachedNodeInfo{NodeLister: corelisters.NewNodeLister(nodeIndexer)}\n\n\t...\n\t// node 资源不足时的驱逐策略的设定\n\tthresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tevictionConfig := eviction.Config{\n\t\tPressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,\n\t\tMaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),\n\t\tThresholds:               thresholds,\n\t\tKernelMemcgNotification:  experimentalKernelMemcgNotification,\n\t\tPodCgroupRoot:            kubeDeps.ContainerManager.GetPodCgroupRoot(),\n\t}\n    ...\n    // 容器引用的管理\n\tcontainerRefManager := kubecontainer.NewRefManager()\n    // oom 监控\n\toomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder)\n\n\t// 根据配置信息和各种对象创建 Kubelet 实例\n\tklet := &Kubelet{\n\t\thostname:                       hostname,\n\t\thostnameOverridden:             len(hostnameOverride) > 0,\n\t\tnodeName:                       nodeName,\n\t\t...\n\t}\n\t\n\t// 从 cAdvisor 获取当前机器的信息\n\tmachineInfo, err := klet.cadvisor.MachineInfo()\n\n\t// 对 pod 的管理（如: 增删改等）\n\tklet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager)\n\n\t// 容器运行时管理\n\truntime, err := kuberuntime.NewKubeGenericRuntimeManager(...)\n\n\t// pleg\n\tklet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock{})\n\n\t// 创建 containerGC 对象，进行周期性的容器清理工作\n\tcontainerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)\n\n\t// 创建 imageManager 管理镜像\n\timageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage)\n\t\n\t// statusManager 实时检测节点上 pod 的状态，并更新到 apiserver 对应的 pod\n\tklet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)\n\n\t// 探针管理\n\tklet.probeManager = prober.NewManager(...)\n\n    // token 管理\n\ttokenManager := token.NewManager(kubeDeps.KubeClient)\n\n\t// 磁盘管理\n\tklet.volumeManager = volumemanager.NewVolumeManager()\n\t\n\t// 将 syncPod() 注入到 podWorkers 中\n\tklet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)\n\n\t// 容器驱逐策略管理\n\tevictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock)\n    ...\n}\n```\nRunKubelet 最后会调用 startKubelet() 进行后续的操作。\n\n#### 5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）\nstartKubelet()  的主要功能：\n\n- 1、以 goroutine 方式启动 kubelet 中的各个模块。\n- 2、启动 kubelet http server。\n\n\n```\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) {\n\tgo wait.Until(func() {\n\t\t// 以 goroutine 方式启动 kubelet 中的各个模块\n\t\tk.Run(podCfg.Updates())\n\t}, 0, wait.NeverStop)\n\n\t// 启动 kubelet http server\t\n\tif enableServer {\n\t\tgo k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling)\n\n\t}\n\tif kubeCfg.ReadOnlyPort > 0 {\n\t\tgo k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort))\n\t}\n}\n```\n\n```\n// Run starts the kubelet reacting to config updates\nfunc (kl *Kubelet) Run(updates <-chan kubetypes.PodUpdate) {\n\tif kl.logServer == nil {\n\t\tkl.logServer = http.StripPrefix(\"/logs/\", http.FileServer(http.Dir(\"/var/log/\")))\n\t}\n\tif kl.kubeClient == nil {\n\t\tglog.Warning(\"No api server defined - no node status update will be sent.\")\n\t}\n\n\t// Start the cloud provider sync manager\n\tif kl.cloudResourceSyncManager != nil {\n\t\tgo kl.cloudResourceSyncManager.Run(wait.NeverStop)\n\t}\n\n\tif err := kl.initializeModules(); err != nil {\n\t\tkl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())\n\t\tglog.Fatal(err)\n\t}\n\n\t// Start volume manager\n\tgo kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)\n\n\tif kl.kubeClient != nil {\n\t\t// Start syncing node status immediately, this may set up things the runtime needs to run.\n\t\tgo wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)\n\t\tgo kl.fastStatusUpdateOnce()\n\n\t\t// start syncing lease\n\t\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) {\n\t\t\tgo kl.nodeLeaseController.Run(wait.NeverStop)\n\t\t}\n\t}\n\tgo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)\n\n\t// Start loop to sync iptables util rules\n\tif kl.makeIPTablesUtilChains {\n\t\tgo wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop)\n\t}\n\n\t// Start a goroutine responsible for killing pods (that are not properly\n\t// handled by pod workers).\n\tgo wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)\n\n\t// Start component sync loops.\n\tkl.statusManager.Start()\n\tkl.probeManager.Start()\n\n\t// Start syncing RuntimeClasses if enabled.\n\tif kl.runtimeClassManager != nil {\n\t\tgo kl.runtimeClassManager.Run(wait.NeverStop)\n\t}\n\n\t// Start the pod lifecycle event generator.\n\tkl.pleg.Start()\n\n\tkl.syncLoop(updates, kl)\n}\n```\nsyncLoop 是 kubelet 的主循环方法，它从不同的管道(FILE,URL, API-SERVER)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 Pod 处于期望的状态。\n\n\n```\nfunc (kl *Kubelet) syncLoop(updates <-chan kubetypes.PodUpdate, handler SyncHandler) {\n\tglog.Info(\"Starting kubelet main sync loop.\")\n\n\t// syncTicker 每秒检测一次是否有需要同步的 pod workers\n\tsyncTicker := time.NewTicker(time.Second)\n\tdefer syncTicker.Stop()\n\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\n\tdefer housekeepingTicker.Stop()\n\tplegCh := kl.pleg.Watch()\n\tconst (\n\t\tbase   = 100 * time.Millisecond\n\t\tmax    = 5 * time.Second\n\t\tfactor = 2\n\t)\n\tduration := base\n\tfor {\n\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {\n\t\t\tglog.Infof(\"skipping pod synchronization - %v\", rs)\n\t\t\t// exponential backoff\n\t\t\ttime.Sleep(duration)\n\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))\n\t\t\tcontinue\n\t\t}\n\t\t// reset backoff if we have a success\n\t\tduration = base\n\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t\t// \n\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {\n\t\t\tbreak\n\t\t}\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t}\n}\n```\n\nsyncLoopIteration()  方法对多个管道进行遍历，如果 pod 发生变化，则会调用相应的 Handler，在 Handler 中通过调用 dispatchWork 分发任务。\n\n\n## 总结\n\n本篇文章主要讲述了 kubelet 组件从加载配置到初始化内部的各个模块再到启动 kubelet 服务的整个流程，上面的时序图能清楚的看到函数之间的调用关系，但是其中每个组件具体的工作方式以及组件之间的交互方式还不得而知，后面会一探究竟。\n\n参考：\n[kubernetes node components – kubelet](http://www.sel.zju.edu.cn/?p=595)\n[Kubelet 源码分析(一):启动流程分析](https://segmentfault.com/a/1190000008267351)\n[kubelet 源码分析：启动流程](https://cizixs.com/2017/06/06/kubelet-source-code-analysis-part-1/)\n[kubernetes 的 kubelet 的工作过程](https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/05/02/Kubernetes-kubelet.html)\n[kubelet 内部实现解析](https://fatsheep9146.github.io/2018/07/08/kubelet%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/)\n","slug":"kubelet_init","published":1,"updated":"2019-01-01T06:55:23.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjs8z5gpy000v22dv5523nl78","content":"<p>上篇文章（<a href=\"https://blog.tianfeiyu.com/2018/12/16/kubelet-modules/\" target=\"_blank\" rel=\"noopener\">kubelet 架构浅析</a> ）已经介绍过 kubelet 在整个集群架构中的功能以及自身各模块的用途，本篇文章主要介绍 kubelet 的启动流程。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<h2 id=\"kubelet-启动流程\"><a href=\"#kubelet-启动流程\" class=\"headerlink\" title=\"kubelet 启动流程\"></a>kubelet 启动流程</h2><p>kubelet 代码结构:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  kubernetes git:(release-1.12) ✗ tree cmd/kubelet</span><br><span class=\"line\">cmd/kubelet</span><br><span class=\"line\">├── BUILD</span><br><span class=\"line\">├── OWNERS</span><br><span class=\"line\">├── app</span><br><span class=\"line\">│   ├── BUILD</span><br><span class=\"line\">│   ├── OWNERS</span><br><span class=\"line\">│   ├── auth.go</span><br><span class=\"line\">│   ├── init_others.go</span><br><span class=\"line\">│   ├── init_windows.go</span><br><span class=\"line\">│   ├── options</span><br><span class=\"line\">│   │   ├── BUILD</span><br><span class=\"line\">│   │   ├── container_runtime.go</span><br><span class=\"line\">│   │   ├── globalflags.go</span><br><span class=\"line\">│   │   ├── globalflags_linux.go</span><br><span class=\"line\">│   │   ├── globalflags_other.go</span><br><span class=\"line\">│   │   ├── options.go</span><br><span class=\"line\">│   │   ├── options_test.go</span><br><span class=\"line\">│   │   ├── osflags_others.go</span><br><span class=\"line\">│   │   └── osflags_windows.go</span><br><span class=\"line\">│   ├── plugins.go</span><br><span class=\"line\">│   ├── server.go</span><br><span class=\"line\">│   ├── server_linux.go</span><br><span class=\"line\">│   ├── server_test.go</span><br><span class=\"line\">│   └── server_unsupported.go</span><br><span class=\"line\">└── kubelet.go</span><br><span class=\"line\"></span><br><span class=\"line\">2 directories, 22 files</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-bdeb34a5cdda93d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubelet 启动流程时序图\"></p>\n<h4 id=\"1、kubelet-入口函数-main（cmd-kubelet-kubelet-go）\"><a href=\"#1、kubelet-入口函数-main（cmd-kubelet-kubelet-go）\" class=\"headerlink\" title=\"1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）\"></a>1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\trand.Seed(time.Now().UTC().UnixNano())</span><br><span class=\"line\"></span><br><span class=\"line\">\tcommand := app.NewKubeletCommand(server.SetupSignalHandler())</span><br><span class=\"line\">\tlogs.InitLogs()</span><br><span class=\"line\">\tdefer logs.FlushLogs()</span><br><span class=\"line\"></span><br><span class=\"line\">\tif err := command.Execute(); err != nil &#123;</span><br><span class=\"line\">\t\tfmt.Fprintf(os.Stderr, &quot;%v\\n&quot;, err)</span><br><span class=\"line\">\t\tos.Exit(1)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"2、初始化-kubelet-配置（cmd-kubelet-app-server-go）\"><a href=\"#2、初始化-kubelet-配置（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"2、初始化 kubelet 配置（cmd/kubelet/app/server.go）\"></a>2、初始化 kubelet 配置（cmd/kubelet/app/server.go）</h4><p>NewKubeletCommand() 函数主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// NewKubeletCommand creates a *cobra.Command object with default parameters</span><br><span class=\"line\">func NewKubeletCommand(stopCh &lt;-chan struct&#123;&#125;) *cobra.Command &#123;</span><br><span class=\"line\">    cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)</span><br><span class=\"line\">    cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc)</span><br><span class=\"line\">    // Kubelet配置分两部分:</span><br><span class=\"line\">    // KubeletFlag: 指那些不允许在 kubelet 运行时进行修改的配置集，或者不能在集群中各个 Nodes 之间共享的配置集。</span><br><span class=\"line\">    // KubeletConfiguration: 指可以在集群中各个Nodes之间共享的配置集，可以进行动态配置。</span><br><span class=\"line\">    kubeletFlags := options.NewKubeletFlags()</span><br><span class=\"line\">\tkubeletConfig, err := options.NewKubeletConfiguration()</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\tcmd := &amp;cobra.Command&#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tRun: func(cmd *cobra.Command, args []string) &#123;</span><br><span class=\"line\">\t\t\t// 读取 kubelet 配置文件</span><br><span class=\"line\">\t\t\tif configFile := kubeletFlags.KubeletConfigFile; len(configFile) &gt; 0 &#123;</span><br><span class=\"line\">\t\t\t\tkubeletConfig, err = loadConfigFile(configFile)</span><br><span class=\"line\">\t\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t// 校验 kubelet 参数</span><br><span class=\"line\">\t\t\tif err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\t// 此处初始化了 kubeletDeps</span><br><span class=\"line\">\t\t\tkubeletDeps, err := UnsecuredDependencies(kubeletServer)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\t// 启动程序</span><br><span class=\"line\">\t\t\tif err := Run(kubeletServer, kubeletDeps, stopCh); err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\treturn cmd</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为。主要包括监控功能（cadvisor），cgroup 管理功能（containerManager）等。</p>\n<p>NewKubeletCommand() 会调用 Run() 函数，Run() 中主要调用 run() 函数进行一些准备事项。</p>\n<h4 id=\"3、创建和-apiserver-通信的对象（cmd-kubelet-app-server-go）\"><a href=\"#3、创建和-apiserver-通信的对象（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）\"></a>3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）</h4><p>run() 函数的主要功能：</p>\n<ul>\n<li>1、创建 kubeClient，evnetClient 用来和 apiserver 通信。创建 heartbeatClient 向 apiserver 上报心跳状态。</li>\n<li>2、为 kubeDeps 设定一些默认值。</li>\n<li>3、启动监听 Healthz 端口的 http server，默认端口是 10248。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// 判断 kubelet 的启动模式</span><br><span class=\"line\">\tif standaloneMode &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t&#125; else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// 创建对象 kubeClient</span><br><span class=\"line\">\t\tkubeClient, err = clientset.NewForConfig(clientConfig)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">        // 创建对象 evnetClient</span><br><span class=\"line\">\t\teventClient, err = v1core.NewForConfig(&amp;eventClientConfig)</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// heartbeatClient 上报状态</span><br><span class=\"line\">\t\theartbeatClient, err = clientset.NewForConfig(&amp;heartbeatClientConfig)</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 为 kubeDeps 设定一些默认值</span><br><span class=\"line\">\tif kubeDeps.Auth == nil &#123;</span><br><span class=\"line\">\t\t\tauth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn err</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tkubeDeps.Auth = auth</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif kubeDeps.CAdvisorInterface == nil &#123;</span><br><span class=\"line\">\t\t\timageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)</span><br><span class=\"line\">\t\t\tkubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn err</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// </span><br><span class=\"line\">\tif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// 启动监听 Healthz 端口的 http server  </span><br><span class=\"line\">\tif s.HealthzPort &gt; 0 &#123;</span><br><span class=\"line\">\t\thealthz.DefaultHealthz()</span><br><span class=\"line\">\t\tgo wait.Until(func() &#123;</span><br><span class=\"line\">\t\t\terr := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Errorf(&quot;Starting health server failed: %v&quot;, err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;, 5*time.Second, wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kubelet 对 pod 资源的获取方式有三种：第一种是通过文件获得，文件一般放在 /etc/kubernetes/manifests 目录下面；第二种也是通过文件过得，只不过文件是通过 URL 获取的；第三种是通过 watch kube-apiserver 获取。其中前两种模式下，我们称 kubelet 运行在 standalone 模式下，运行在 standalone 模式下的 kubelet 一般用于调试某些功能。</p>\n<p>run() 中调用 RunKubelet() 函数进行后续操作。</p>\n<h4 id=\"4、初始化-kubelet-组件内部的模块（cmd-kubelet-app-server-go）\"><a href=\"#4、初始化-kubelet-组件内部的模块（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）\"></a>4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）</h4><p>RunKubelet()  主要功能：</p>\n<ul>\n<li>1、初始化 kubelet 组件中的各个模块，创建出 kubelet 对象。</li>\n<li>2、启动垃圾回收服务。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\"> \t// 初始化 kubelet 内部模块</span><br><span class=\"line\">\tk, err := CreateAndInitKubelet(&amp;kubeServer.KubeletConfiguration,</span><br><span class=\"line\">\t\tkubeDeps,</span><br><span class=\"line\">\t\t&amp;kubeServer.ContainerRuntimeOptions,</span><br><span class=\"line\">\t\tkubeServer.ContainerRuntime,</span><br><span class=\"line\">\t\tkubeServer.RuntimeCgroups,</span><br><span class=\"line\">\t\tkubeServer.HostnameOverride,</span><br><span class=\"line\">\t\tkubeServer.NodeIP,</span><br><span class=\"line\">\t\tkubeServer.ProviderID,</span><br><span class=\"line\">\t\tkubeServer.CloudProvider,</span><br><span class=\"line\">\t\tkubeServer.CertDirectory,</span><br><span class=\"line\">\t\tkubeServer.RootDirectory,</span><br><span class=\"line\">\t\tkubeServer.RegisterNode,</span><br><span class=\"line\">\t\tkubeServer.RegisterWithTaints,</span><br><span class=\"line\">\t\tkubeServer.AllowedUnsafeSysctls,</span><br><span class=\"line\">\t\tkubeServer.RemoteRuntimeEndpoint,</span><br><span class=\"line\">\t\tkubeServer.RemoteImageEndpoint,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalMounterPath,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold,</span><br><span class=\"line\">\t\tkubeServer.MinimumGCAge,</span><br><span class=\"line\">\t\tkubeServer.MaxPerPodContainerCount,</span><br><span class=\"line\">\t\tkubeServer.MaxContainerCount,</span><br><span class=\"line\">\t\tkubeServer.MasterServiceNamespace,</span><br><span class=\"line\">\t\tkubeServer.RegisterSchedulable,</span><br><span class=\"line\">\t\tkubeServer.NonMasqueradeCIDR,</span><br><span class=\"line\">\t\tkubeServer.KeepTerminatedPodVolumes,</span><br><span class=\"line\">\t\tkubeServer.NodeLabels,</span><br><span class=\"line\">\t\tkubeServer.SeccompProfileRoot,</span><br><span class=\"line\">\t\tkubeServer.BootstrapCheckpointPath,</span><br><span class=\"line\">\t\tkubeServer.NodeStatusMaxImages)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn fmt.Errorf(&quot;failed to create kubelet: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t...</span><br><span class=\"line\">\tif runOnce &#123;</span><br><span class=\"line\">\t\tif _, err := k.RunOnce(podCfg.Updates()); err != nil &#123;</span><br><span class=\"line\">\t\t\treturn fmt.Errorf(&quot;runonce failed: %v&quot;, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tglog.Infof(&quot;Started kubelet as runonce&quot;)</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">        // </span><br><span class=\"line\">\t\tstartKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer)</span><br><span class=\"line\">\t\tglog.Infof(&quot;Started kubelet&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func CreateAndInitKubelet(...)&#123;</span><br><span class=\"line\">\t// NewMainKubelet 实例化一个 kubelet 对象，并对 kubelet 内部各个模块进行初始化</span><br><span class=\"line\">\tk, err = kubelet.NewMainKubelet(kubeCfg,</span><br><span class=\"line\">\t\tkubeDeps,</span><br><span class=\"line\">\t\tcrOptions,</span><br><span class=\"line\">\t\tcontainerRuntime,</span><br><span class=\"line\">\t\truntimeCgroups,</span><br><span class=\"line\">\t\thostnameOverride,</span><br><span class=\"line\">\t\tnodeIP,</span><br><span class=\"line\">\t\tproviderID,</span><br><span class=\"line\">\t\tcloudProvider,</span><br><span class=\"line\">\t\tcertDirectory,</span><br><span class=\"line\">\t\trootDirectory,</span><br><span class=\"line\">\t\tregisterNode,</span><br><span class=\"line\">\t\tregisterWithTaints,</span><br><span class=\"line\">\t\tallowedUnsafeSysctls,</span><br><span class=\"line\">\t\tremoteRuntimeEndpoint,</span><br><span class=\"line\">\t\tremoteImageEndpoint,</span><br><span class=\"line\">\t\texperimentalMounterPath,</span><br><span class=\"line\">\t\texperimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\texperimentalCheckNodeCapabilitiesBeforeMount,</span><br><span class=\"line\">\t\texperimentalNodeAllocatableIgnoreEvictionThreshold,</span><br><span class=\"line\">\t\tminimumGCAge,</span><br><span class=\"line\">\t\tmaxPerPodContainerCount,</span><br><span class=\"line\">\t\tmaxContainerCount,</span><br><span class=\"line\">\t\tmasterServiceNamespace,</span><br><span class=\"line\">\t\tregisterSchedulable,</span><br><span class=\"line\">\t\tnonMasqueradeCIDR,</span><br><span class=\"line\">\t\tkeepTerminatedPodVolumes,</span><br><span class=\"line\">\t\tnodeLabels,</span><br><span class=\"line\">\t\tseccompProfileRoot,</span><br><span class=\"line\">\t\tbootstrapCheckpointPath,</span><br><span class=\"line\">\t\tnodeStatusMaxImages)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn nil, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 通知 apiserver kubelet 启动了</span><br><span class=\"line\">\tk.BirthCry()</span><br><span class=\"line\">\t// 启动垃圾回收服务</span><br><span class=\"line\">\tk.StartGarbageCollection()</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn k, nil</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...)&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\tif kubeDeps.PodConfig == nil &#123;</span><br><span class=\"line\">\t\tvar err error</span><br><span class=\"line\">\t\t// 初始化 makePodSourceConfig，监听 pod 元数据的来源(FILE, URL, api-server)，将不同 source 的 pod configuration 合并到一个结构中</span><br><span class=\"line\">\t\tkubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn nil, err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    // kubelet 服务端口，默认 10250</span><br><span class=\"line\">\tdaemonEndpoints := &amp;v1.NodeDaemonEndpoints&#123;</span><br><span class=\"line\">\t\tKubeletEndpoint: v1.DaemonEndpoint&#123;Port: kubeCfg.Port&#125;,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用 reflector 把 ListWatch 得到的服务信息实时同步到 serviceStore 对象中</span><br><span class=\"line\">\tserviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;)</span><br><span class=\"line\">\tif kubeDeps.KubeClient != nil &#123;</span><br><span class=\"line\">\t\tserviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;services&quot;, metav1.NamespaceAll, fields.Everything())</span><br><span class=\"line\">\t\tr := cache.NewReflector(serviceLW, &amp;v1.Service&#123;&#125;, serviceIndexer, 0)</span><br><span class=\"line\">\t\tgo r.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tserviceLister := corelisters.NewServiceLister(serviceIndexer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用 reflector 把 ListWatch 得到的节点信息实时同步到  nodeStore 对象中</span><br><span class=\"line\">\tnodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;&#125;)</span><br><span class=\"line\">\tif kubeDeps.KubeClient != nil &#123;</span><br><span class=\"line\">\t\tfieldSelector := fields.Set&#123;api.ObjectNameField: string(nodeName)&#125;.AsSelector()</span><br><span class=\"line\">\t\tnodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;nodes&quot;, metav1.NamespaceAll, fieldSelector)</span><br><span class=\"line\">\t\tr := cache.NewReflector(nodeLW, &amp;v1.Node&#123;&#125;, nodeIndexer, 0)</span><br><span class=\"line\">\t\tgo r.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tnodeInfo := &amp;predicates.CachedNodeInfo&#123;NodeLister: corelisters.NewNodeLister(nodeIndexer)&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// node 资源不足时的驱逐策略的设定</span><br><span class=\"line\">\tthresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn nil, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tevictionConfig := eviction.Config&#123;</span><br><span class=\"line\">\t\tPressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,</span><br><span class=\"line\">\t\tMaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),</span><br><span class=\"line\">\t\tThresholds:               thresholds,</span><br><span class=\"line\">\t\tKernelMemcgNotification:  experimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\tPodCgroupRoot:            kubeDeps.ContainerManager.GetPodCgroupRoot(),</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    // 容器引用的管理</span><br><span class=\"line\">\tcontainerRefManager := kubecontainer.NewRefManager()</span><br><span class=\"line\">    // oom 监控</span><br><span class=\"line\">\toomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 根据配置信息和各种对象创建 Kubelet 实例</span><br><span class=\"line\">\tklet := &amp;Kubelet&#123;</span><br><span class=\"line\">\t\thostname:                       hostname,</span><br><span class=\"line\">\t\thostnameOverridden:             len(hostnameOverride) &gt; 0,</span><br><span class=\"line\">\t\tnodeName:                       nodeName,</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 从 cAdvisor 获取当前机器的信息</span><br><span class=\"line\">\tmachineInfo, err := klet.cadvisor.MachineInfo()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 对 pod 的管理（如: 增删改等）</span><br><span class=\"line\">\tklet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 容器运行时管理</span><br><span class=\"line\">\truntime, err := kuberuntime.NewKubeGenericRuntimeManager(...)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// pleg</span><br><span class=\"line\">\tklet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 containerGC 对象，进行周期性的容器清理工作</span><br><span class=\"line\">\tcontainerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 imageManager 管理镜像</span><br><span class=\"line\">\timageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// statusManager 实时检测节点上 pod 的状态，并更新到 apiserver 对应的 pod</span><br><span class=\"line\">\tklet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 探针管理</span><br><span class=\"line\">\tklet.probeManager = prober.NewManager(...)</span><br><span class=\"line\"></span><br><span class=\"line\">    // token 管理</span><br><span class=\"line\">\ttokenManager := token.NewManager(kubeDeps.KubeClient)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 磁盘管理</span><br><span class=\"line\">\tklet.volumeManager = volumemanager.NewVolumeManager()</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 将 syncPod() 注入到 podWorkers 中</span><br><span class=\"line\">\tklet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 容器驱逐策略管理</span><br><span class=\"line\">\tevictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RunKubelet 最后会调用 startKubelet() 进行后续的操作。</p>\n<h4 id=\"5、启动-kubelet-内部的模块及服务（cmd-kubelet-app-server-go）\"><a href=\"#5、启动-kubelet-内部的模块及服务（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）\"></a>5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）</h4><p>startKubelet()  的主要功能：</p>\n<ul>\n<li>1、以 goroutine 方式启动 kubelet 中的各个模块。</li>\n<li>2、启动 kubelet http server。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) &#123;</span><br><span class=\"line\">\tgo wait.Until(func() &#123;</span><br><span class=\"line\">\t\t// 以 goroutine 方式启动 kubelet 中的各个模块</span><br><span class=\"line\">\t\tk.Run(podCfg.Updates())</span><br><span class=\"line\">\t&#125;, 0, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 启动 kubelet http server\t</span><br><span class=\"line\">\tif enableServer &#123;</span><br><span class=\"line\">\t\tgo k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif kubeCfg.ReadOnlyPort &gt; 0 &#123;</span><br><span class=\"line\">\t\tgo k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Run starts the kubelet reacting to config updates</span><br><span class=\"line\">func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123;</span><br><span class=\"line\">\tif kl.logServer == nil &#123;</span><br><span class=\"line\">\t\tkl.logServer = http.StripPrefix(&quot;/logs/&quot;, http.FileServer(http.Dir(&quot;/var/log/&quot;)))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif kl.kubeClient == nil &#123;</span><br><span class=\"line\">\t\tglog.Warning(&quot;No api server defined - no node status update will be sent.&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start the cloud provider sync manager</span><br><span class=\"line\">\tif kl.cloudResourceSyncManager != nil &#123;</span><br><span class=\"line\">\t\tgo kl.cloudResourceSyncManager.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tif err := kl.initializeModules(); err != nil &#123;</span><br><span class=\"line\">\t\tkl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())</span><br><span class=\"line\">\t\tglog.Fatal(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start volume manager</span><br><span class=\"line\">\tgo kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\tif kl.kubeClient != nil &#123;</span><br><span class=\"line\">\t\t// Start syncing node status immediately, this may set up things the runtime needs to run.</span><br><span class=\"line\">\t\tgo wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)</span><br><span class=\"line\">\t\tgo kl.fastStatusUpdateOnce()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// start syncing lease</span><br><span class=\"line\">\t\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123;</span><br><span class=\"line\">\t\t\tgo kl.nodeLeaseController.Run(wait.NeverStop)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tgo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start loop to sync iptables util rules</span><br><span class=\"line\">\tif kl.makeIPTablesUtilChains &#123;</span><br><span class=\"line\">\t\tgo wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start a goroutine responsible for killing pods (that are not properly</span><br><span class=\"line\">\t// handled by pod workers).</span><br><span class=\"line\">\tgo wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start component sync loops.</span><br><span class=\"line\">\tkl.statusManager.Start()</span><br><span class=\"line\">\tkl.probeManager.Start()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start syncing RuntimeClasses if enabled.</span><br><span class=\"line\">\tif kl.runtimeClassManager != nil &#123;</span><br><span class=\"line\">\t\tgo kl.runtimeClassManager.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start the pod lifecycle event generator.</span><br><span class=\"line\">\tkl.pleg.Start()</span><br><span class=\"line\"></span><br><span class=\"line\">\tkl.syncLoop(updates, kl)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncLoop 是 kubelet 的主循环方法，它从不同的管道(FILE,URL, API-SERVER)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 Pod 处于期望的状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123;</span><br><span class=\"line\">\tglog.Info(&quot;Starting kubelet main sync loop.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// syncTicker 每秒检测一次是否有需要同步的 pod workers</span><br><span class=\"line\">\tsyncTicker := time.NewTicker(time.Second)</span><br><span class=\"line\">\tdefer syncTicker.Stop()</span><br><span class=\"line\">\thousekeepingTicker := time.NewTicker(housekeepingPeriod)</span><br><span class=\"line\">\tdefer housekeepingTicker.Stop()</span><br><span class=\"line\">\tplegCh := kl.pleg.Watch()</span><br><span class=\"line\">\tconst (</span><br><span class=\"line\">\t\tbase   = 100 * time.Millisecond</span><br><span class=\"line\">\t\tmax    = 5 * time.Second</span><br><span class=\"line\">\t\tfactor = 2</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tduration := base</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123;</span><br><span class=\"line\">\t\t\tglog.Infof(&quot;skipping pod synchronization - %v&quot;, rs)</span><br><span class=\"line\">\t\t\t// exponential backoff</span><br><span class=\"line\">\t\t\ttime.Sleep(duration)</span><br><span class=\"line\">\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// reset backoff if we have a success</span><br><span class=\"line\">\t\tduration = base</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t\t// </span><br><span class=\"line\">\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123;</span><br><span class=\"line\">\t\t\tbreak</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncLoopIteration()  方法对多个管道进行遍历，如果 pod 发生变化，则会调用相应的 Handler，在 Handler 中通过调用 dispatchWork 分发任务。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本篇文章主要讲述了 kubelet 组件从加载配置到初始化内部的各个模块再到启动 kubelet 服务的整个流程，上面的时序图能清楚的看到函数之间的调用关系，但是其中每个组件具体的工作方式以及组件之间的交互方式还不得而知，后面会一探究竟。</p>\n<p>参考：<br><a href=\"http://www.sel.zju.edu.cn/?p=595\" target=\"_blank\" rel=\"noopener\">kubernetes node components – kubelet</a><br><a href=\"https://segmentfault.com/a/1190000008267351\" target=\"_blank\" rel=\"noopener\">Kubelet 源码分析(一):启动流程分析</a><br><a href=\"https://cizixs.com/2017/06/06/kubelet-source-code-analysis-part-1/\" target=\"_blank\" rel=\"noopener\">kubelet 源码分析：启动流程</a><br><a href=\"https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/05/02/Kubernetes-kubelet.html\" target=\"_blank\" rel=\"noopener\">kubernetes 的 kubelet 的工作过程</a><br><a href=\"https://fatsheep9146.github.io/2018/07/08/kubelet%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">kubelet 内部实现解析</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>上篇文章（<a href=\"https://blog.tianfeiyu.com/2018/12/16/kubelet-modules/\" target=\"_blank\" rel=\"noopener\">kubelet 架构浅析</a> ）已经介绍过 kubelet 在整个集群架构中的功能以及自身各模块的用途，本篇文章主要介绍 kubelet 的启动流程。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<h2 id=\"kubelet-启动流程\"><a href=\"#kubelet-启动流程\" class=\"headerlink\" title=\"kubelet 启动流程\"></a>kubelet 启动流程</h2><p>kubelet 代码结构:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  kubernetes git:(release-1.12) ✗ tree cmd/kubelet</span><br><span class=\"line\">cmd/kubelet</span><br><span class=\"line\">├── BUILD</span><br><span class=\"line\">├── OWNERS</span><br><span class=\"line\">├── app</span><br><span class=\"line\">│   ├── BUILD</span><br><span class=\"line\">│   ├── OWNERS</span><br><span class=\"line\">│   ├── auth.go</span><br><span class=\"line\">│   ├── init_others.go</span><br><span class=\"line\">│   ├── init_windows.go</span><br><span class=\"line\">│   ├── options</span><br><span class=\"line\">│   │   ├── BUILD</span><br><span class=\"line\">│   │   ├── container_runtime.go</span><br><span class=\"line\">│   │   ├── globalflags.go</span><br><span class=\"line\">│   │   ├── globalflags_linux.go</span><br><span class=\"line\">│   │   ├── globalflags_other.go</span><br><span class=\"line\">│   │   ├── options.go</span><br><span class=\"line\">│   │   ├── options_test.go</span><br><span class=\"line\">│   │   ├── osflags_others.go</span><br><span class=\"line\">│   │   └── osflags_windows.go</span><br><span class=\"line\">│   ├── plugins.go</span><br><span class=\"line\">│   ├── server.go</span><br><span class=\"line\">│   ├── server_linux.go</span><br><span class=\"line\">│   ├── server_test.go</span><br><span class=\"line\">│   └── server_unsupported.go</span><br><span class=\"line\">└── kubelet.go</span><br><span class=\"line\"></span><br><span class=\"line\">2 directories, 22 files</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-bdeb34a5cdda93d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"kubelet 启动流程时序图\"></p>\n<h4 id=\"1、kubelet-入口函数-main（cmd-kubelet-kubelet-go）\"><a href=\"#1、kubelet-入口函数-main（cmd-kubelet-kubelet-go）\" class=\"headerlink\" title=\"1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）\"></a>1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\trand.Seed(time.Now().UTC().UnixNano())</span><br><span class=\"line\"></span><br><span class=\"line\">\tcommand := app.NewKubeletCommand(server.SetupSignalHandler())</span><br><span class=\"line\">\tlogs.InitLogs()</span><br><span class=\"line\">\tdefer logs.FlushLogs()</span><br><span class=\"line\"></span><br><span class=\"line\">\tif err := command.Execute(); err != nil &#123;</span><br><span class=\"line\">\t\tfmt.Fprintf(os.Stderr, &quot;%v\\n&quot;, err)</span><br><span class=\"line\">\t\tos.Exit(1)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"2、初始化-kubelet-配置（cmd-kubelet-app-server-go）\"><a href=\"#2、初始化-kubelet-配置（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"2、初始化 kubelet 配置（cmd/kubelet/app/server.go）\"></a>2、初始化 kubelet 配置（cmd/kubelet/app/server.go）</h4><p>NewKubeletCommand() 函数主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// NewKubeletCommand creates a *cobra.Command object with default parameters</span><br><span class=\"line\">func NewKubeletCommand(stopCh &lt;-chan struct&#123;&#125;) *cobra.Command &#123;</span><br><span class=\"line\">    cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)</span><br><span class=\"line\">    cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc)</span><br><span class=\"line\">    // Kubelet配置分两部分:</span><br><span class=\"line\">    // KubeletFlag: 指那些不允许在 kubelet 运行时进行修改的配置集，或者不能在集群中各个 Nodes 之间共享的配置集。</span><br><span class=\"line\">    // KubeletConfiguration: 指可以在集群中各个Nodes之间共享的配置集，可以进行动态配置。</span><br><span class=\"line\">    kubeletFlags := options.NewKubeletFlags()</span><br><span class=\"line\">\tkubeletConfig, err := options.NewKubeletConfiguration()</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\tcmd := &amp;cobra.Command&#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tRun: func(cmd *cobra.Command, args []string) &#123;</span><br><span class=\"line\">\t\t\t// 读取 kubelet 配置文件</span><br><span class=\"line\">\t\t\tif configFile := kubeletFlags.KubeletConfigFile; len(configFile) &gt; 0 &#123;</span><br><span class=\"line\">\t\t\t\tkubeletConfig, err = loadConfigFile(configFile)</span><br><span class=\"line\">\t\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t// 校验 kubelet 参数</span><br><span class=\"line\">\t\t\tif err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\t// 此处初始化了 kubeletDeps</span><br><span class=\"line\">\t\t\tkubeletDeps, err := UnsecuredDependencies(kubeletServer)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\t// 启动程序</span><br><span class=\"line\">\t\t\tif err := Run(kubeletServer, kubeletDeps, stopCh); err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\treturn cmd</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为。主要包括监控功能（cadvisor），cgroup 管理功能（containerManager）等。</p>\n<p>NewKubeletCommand() 会调用 Run() 函数，Run() 中主要调用 run() 函数进行一些准备事项。</p>\n<h4 id=\"3、创建和-apiserver-通信的对象（cmd-kubelet-app-server-go）\"><a href=\"#3、创建和-apiserver-通信的对象（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）\"></a>3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）</h4><p>run() 函数的主要功能：</p>\n<ul>\n<li>1、创建 kubeClient，evnetClient 用来和 apiserver 通信。创建 heartbeatClient 向 apiserver 上报心跳状态。</li>\n<li>2、为 kubeDeps 设定一些默认值。</li>\n<li>3、启动监听 Healthz 端口的 http server，默认端口是 10248。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// 判断 kubelet 的启动模式</span><br><span class=\"line\">\tif standaloneMode &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t&#125; else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// 创建对象 kubeClient</span><br><span class=\"line\">\t\tkubeClient, err = clientset.NewForConfig(clientConfig)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">        // 创建对象 evnetClient</span><br><span class=\"line\">\t\teventClient, err = v1core.NewForConfig(&amp;eventClientConfig)</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// heartbeatClient 上报状态</span><br><span class=\"line\">\t\theartbeatClient, err = clientset.NewForConfig(&amp;heartbeatClientConfig)</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 为 kubeDeps 设定一些默认值</span><br><span class=\"line\">\tif kubeDeps.Auth == nil &#123;</span><br><span class=\"line\">\t\t\tauth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn err</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tkubeDeps.Auth = auth</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif kubeDeps.CAdvisorInterface == nil &#123;</span><br><span class=\"line\">\t\t\timageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)</span><br><span class=\"line\">\t\t\tkubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn err</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// </span><br><span class=\"line\">\tif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// 启动监听 Healthz 端口的 http server  </span><br><span class=\"line\">\tif s.HealthzPort &gt; 0 &#123;</span><br><span class=\"line\">\t\thealthz.DefaultHealthz()</span><br><span class=\"line\">\t\tgo wait.Until(func() &#123;</span><br><span class=\"line\">\t\t\terr := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Errorf(&quot;Starting health server failed: %v&quot;, err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;, 5*time.Second, wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kubelet 对 pod 资源的获取方式有三种：第一种是通过文件获得，文件一般放在 /etc/kubernetes/manifests 目录下面；第二种也是通过文件过得，只不过文件是通过 URL 获取的；第三种是通过 watch kube-apiserver 获取。其中前两种模式下，我们称 kubelet 运行在 standalone 模式下，运行在 standalone 模式下的 kubelet 一般用于调试某些功能。</p>\n<p>run() 中调用 RunKubelet() 函数进行后续操作。</p>\n<h4 id=\"4、初始化-kubelet-组件内部的模块（cmd-kubelet-app-server-go）\"><a href=\"#4、初始化-kubelet-组件内部的模块（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）\"></a>4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）</h4><p>RunKubelet()  主要功能：</p>\n<ul>\n<li>1、初始化 kubelet 组件中的各个模块，创建出 kubelet 对象。</li>\n<li>2、启动垃圾回收服务。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\"> \t// 初始化 kubelet 内部模块</span><br><span class=\"line\">\tk, err := CreateAndInitKubelet(&amp;kubeServer.KubeletConfiguration,</span><br><span class=\"line\">\t\tkubeDeps,</span><br><span class=\"line\">\t\t&amp;kubeServer.ContainerRuntimeOptions,</span><br><span class=\"line\">\t\tkubeServer.ContainerRuntime,</span><br><span class=\"line\">\t\tkubeServer.RuntimeCgroups,</span><br><span class=\"line\">\t\tkubeServer.HostnameOverride,</span><br><span class=\"line\">\t\tkubeServer.NodeIP,</span><br><span class=\"line\">\t\tkubeServer.ProviderID,</span><br><span class=\"line\">\t\tkubeServer.CloudProvider,</span><br><span class=\"line\">\t\tkubeServer.CertDirectory,</span><br><span class=\"line\">\t\tkubeServer.RootDirectory,</span><br><span class=\"line\">\t\tkubeServer.RegisterNode,</span><br><span class=\"line\">\t\tkubeServer.RegisterWithTaints,</span><br><span class=\"line\">\t\tkubeServer.AllowedUnsafeSysctls,</span><br><span class=\"line\">\t\tkubeServer.RemoteRuntimeEndpoint,</span><br><span class=\"line\">\t\tkubeServer.RemoteImageEndpoint,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalMounterPath,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold,</span><br><span class=\"line\">\t\tkubeServer.MinimumGCAge,</span><br><span class=\"line\">\t\tkubeServer.MaxPerPodContainerCount,</span><br><span class=\"line\">\t\tkubeServer.MaxContainerCount,</span><br><span class=\"line\">\t\tkubeServer.MasterServiceNamespace,</span><br><span class=\"line\">\t\tkubeServer.RegisterSchedulable,</span><br><span class=\"line\">\t\tkubeServer.NonMasqueradeCIDR,</span><br><span class=\"line\">\t\tkubeServer.KeepTerminatedPodVolumes,</span><br><span class=\"line\">\t\tkubeServer.NodeLabels,</span><br><span class=\"line\">\t\tkubeServer.SeccompProfileRoot,</span><br><span class=\"line\">\t\tkubeServer.BootstrapCheckpointPath,</span><br><span class=\"line\">\t\tkubeServer.NodeStatusMaxImages)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn fmt.Errorf(&quot;failed to create kubelet: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t...</span><br><span class=\"line\">\tif runOnce &#123;</span><br><span class=\"line\">\t\tif _, err := k.RunOnce(podCfg.Updates()); err != nil &#123;</span><br><span class=\"line\">\t\t\treturn fmt.Errorf(&quot;runonce failed: %v&quot;, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tglog.Infof(&quot;Started kubelet as runonce&quot;)</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">        // </span><br><span class=\"line\">\t\tstartKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer)</span><br><span class=\"line\">\t\tglog.Infof(&quot;Started kubelet&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func CreateAndInitKubelet(...)&#123;</span><br><span class=\"line\">\t// NewMainKubelet 实例化一个 kubelet 对象，并对 kubelet 内部各个模块进行初始化</span><br><span class=\"line\">\tk, err = kubelet.NewMainKubelet(kubeCfg,</span><br><span class=\"line\">\t\tkubeDeps,</span><br><span class=\"line\">\t\tcrOptions,</span><br><span class=\"line\">\t\tcontainerRuntime,</span><br><span class=\"line\">\t\truntimeCgroups,</span><br><span class=\"line\">\t\thostnameOverride,</span><br><span class=\"line\">\t\tnodeIP,</span><br><span class=\"line\">\t\tproviderID,</span><br><span class=\"line\">\t\tcloudProvider,</span><br><span class=\"line\">\t\tcertDirectory,</span><br><span class=\"line\">\t\trootDirectory,</span><br><span class=\"line\">\t\tregisterNode,</span><br><span class=\"line\">\t\tregisterWithTaints,</span><br><span class=\"line\">\t\tallowedUnsafeSysctls,</span><br><span class=\"line\">\t\tremoteRuntimeEndpoint,</span><br><span class=\"line\">\t\tremoteImageEndpoint,</span><br><span class=\"line\">\t\texperimentalMounterPath,</span><br><span class=\"line\">\t\texperimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\texperimentalCheckNodeCapabilitiesBeforeMount,</span><br><span class=\"line\">\t\texperimentalNodeAllocatableIgnoreEvictionThreshold,</span><br><span class=\"line\">\t\tminimumGCAge,</span><br><span class=\"line\">\t\tmaxPerPodContainerCount,</span><br><span class=\"line\">\t\tmaxContainerCount,</span><br><span class=\"line\">\t\tmasterServiceNamespace,</span><br><span class=\"line\">\t\tregisterSchedulable,</span><br><span class=\"line\">\t\tnonMasqueradeCIDR,</span><br><span class=\"line\">\t\tkeepTerminatedPodVolumes,</span><br><span class=\"line\">\t\tnodeLabels,</span><br><span class=\"line\">\t\tseccompProfileRoot,</span><br><span class=\"line\">\t\tbootstrapCheckpointPath,</span><br><span class=\"line\">\t\tnodeStatusMaxImages)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn nil, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 通知 apiserver kubelet 启动了</span><br><span class=\"line\">\tk.BirthCry()</span><br><span class=\"line\">\t// 启动垃圾回收服务</span><br><span class=\"line\">\tk.StartGarbageCollection()</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn k, nil</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...)&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\tif kubeDeps.PodConfig == nil &#123;</span><br><span class=\"line\">\t\tvar err error</span><br><span class=\"line\">\t\t// 初始化 makePodSourceConfig，监听 pod 元数据的来源(FILE, URL, api-server)，将不同 source 的 pod configuration 合并到一个结构中</span><br><span class=\"line\">\t\tkubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn nil, err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    // kubelet 服务端口，默认 10250</span><br><span class=\"line\">\tdaemonEndpoints := &amp;v1.NodeDaemonEndpoints&#123;</span><br><span class=\"line\">\t\tKubeletEndpoint: v1.DaemonEndpoint&#123;Port: kubeCfg.Port&#125;,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用 reflector 把 ListWatch 得到的服务信息实时同步到 serviceStore 对象中</span><br><span class=\"line\">\tserviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;)</span><br><span class=\"line\">\tif kubeDeps.KubeClient != nil &#123;</span><br><span class=\"line\">\t\tserviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;services&quot;, metav1.NamespaceAll, fields.Everything())</span><br><span class=\"line\">\t\tr := cache.NewReflector(serviceLW, &amp;v1.Service&#123;&#125;, serviceIndexer, 0)</span><br><span class=\"line\">\t\tgo r.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tserviceLister := corelisters.NewServiceLister(serviceIndexer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用 reflector 把 ListWatch 得到的节点信息实时同步到  nodeStore 对象中</span><br><span class=\"line\">\tnodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;&#125;)</span><br><span class=\"line\">\tif kubeDeps.KubeClient != nil &#123;</span><br><span class=\"line\">\t\tfieldSelector := fields.Set&#123;api.ObjectNameField: string(nodeName)&#125;.AsSelector()</span><br><span class=\"line\">\t\tnodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;nodes&quot;, metav1.NamespaceAll, fieldSelector)</span><br><span class=\"line\">\t\tr := cache.NewReflector(nodeLW, &amp;v1.Node&#123;&#125;, nodeIndexer, 0)</span><br><span class=\"line\">\t\tgo r.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tnodeInfo := &amp;predicates.CachedNodeInfo&#123;NodeLister: corelisters.NewNodeLister(nodeIndexer)&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// node 资源不足时的驱逐策略的设定</span><br><span class=\"line\">\tthresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn nil, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tevictionConfig := eviction.Config&#123;</span><br><span class=\"line\">\t\tPressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,</span><br><span class=\"line\">\t\tMaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),</span><br><span class=\"line\">\t\tThresholds:               thresholds,</span><br><span class=\"line\">\t\tKernelMemcgNotification:  experimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\tPodCgroupRoot:            kubeDeps.ContainerManager.GetPodCgroupRoot(),</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    // 容器引用的管理</span><br><span class=\"line\">\tcontainerRefManager := kubecontainer.NewRefManager()</span><br><span class=\"line\">    // oom 监控</span><br><span class=\"line\">\toomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 根据配置信息和各种对象创建 Kubelet 实例</span><br><span class=\"line\">\tklet := &amp;Kubelet&#123;</span><br><span class=\"line\">\t\thostname:                       hostname,</span><br><span class=\"line\">\t\thostnameOverridden:             len(hostnameOverride) &gt; 0,</span><br><span class=\"line\">\t\tnodeName:                       nodeName,</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 从 cAdvisor 获取当前机器的信息</span><br><span class=\"line\">\tmachineInfo, err := klet.cadvisor.MachineInfo()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 对 pod 的管理（如: 增删改等）</span><br><span class=\"line\">\tklet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 容器运行时管理</span><br><span class=\"line\">\truntime, err := kuberuntime.NewKubeGenericRuntimeManager(...)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// pleg</span><br><span class=\"line\">\tklet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 containerGC 对象，进行周期性的容器清理工作</span><br><span class=\"line\">\tcontainerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 imageManager 管理镜像</span><br><span class=\"line\">\timageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// statusManager 实时检测节点上 pod 的状态，并更新到 apiserver 对应的 pod</span><br><span class=\"line\">\tklet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 探针管理</span><br><span class=\"line\">\tklet.probeManager = prober.NewManager(...)</span><br><span class=\"line\"></span><br><span class=\"line\">    // token 管理</span><br><span class=\"line\">\ttokenManager := token.NewManager(kubeDeps.KubeClient)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 磁盘管理</span><br><span class=\"line\">\tklet.volumeManager = volumemanager.NewVolumeManager()</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 将 syncPod() 注入到 podWorkers 中</span><br><span class=\"line\">\tklet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 容器驱逐策略管理</span><br><span class=\"line\">\tevictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RunKubelet 最后会调用 startKubelet() 进行后续的操作。</p>\n<h4 id=\"5、启动-kubelet-内部的模块及服务（cmd-kubelet-app-server-go）\"><a href=\"#5、启动-kubelet-内部的模块及服务（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）\"></a>5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）</h4><p>startKubelet()  的主要功能：</p>\n<ul>\n<li>1、以 goroutine 方式启动 kubelet 中的各个模块。</li>\n<li>2、启动 kubelet http server。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) &#123;</span><br><span class=\"line\">\tgo wait.Until(func() &#123;</span><br><span class=\"line\">\t\t// 以 goroutine 方式启动 kubelet 中的各个模块</span><br><span class=\"line\">\t\tk.Run(podCfg.Updates())</span><br><span class=\"line\">\t&#125;, 0, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 启动 kubelet http server\t</span><br><span class=\"line\">\tif enableServer &#123;</span><br><span class=\"line\">\t\tgo k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif kubeCfg.ReadOnlyPort &gt; 0 &#123;</span><br><span class=\"line\">\t\tgo k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Run starts the kubelet reacting to config updates</span><br><span class=\"line\">func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123;</span><br><span class=\"line\">\tif kl.logServer == nil &#123;</span><br><span class=\"line\">\t\tkl.logServer = http.StripPrefix(&quot;/logs/&quot;, http.FileServer(http.Dir(&quot;/var/log/&quot;)))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif kl.kubeClient == nil &#123;</span><br><span class=\"line\">\t\tglog.Warning(&quot;No api server defined - no node status update will be sent.&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start the cloud provider sync manager</span><br><span class=\"line\">\tif kl.cloudResourceSyncManager != nil &#123;</span><br><span class=\"line\">\t\tgo kl.cloudResourceSyncManager.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tif err := kl.initializeModules(); err != nil &#123;</span><br><span class=\"line\">\t\tkl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())</span><br><span class=\"line\">\t\tglog.Fatal(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start volume manager</span><br><span class=\"line\">\tgo kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\tif kl.kubeClient != nil &#123;</span><br><span class=\"line\">\t\t// Start syncing node status immediately, this may set up things the runtime needs to run.</span><br><span class=\"line\">\t\tgo wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)</span><br><span class=\"line\">\t\tgo kl.fastStatusUpdateOnce()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// start syncing lease</span><br><span class=\"line\">\t\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123;</span><br><span class=\"line\">\t\t\tgo kl.nodeLeaseController.Run(wait.NeverStop)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tgo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start loop to sync iptables util rules</span><br><span class=\"line\">\tif kl.makeIPTablesUtilChains &#123;</span><br><span class=\"line\">\t\tgo wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start a goroutine responsible for killing pods (that are not properly</span><br><span class=\"line\">\t// handled by pod workers).</span><br><span class=\"line\">\tgo wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start component sync loops.</span><br><span class=\"line\">\tkl.statusManager.Start()</span><br><span class=\"line\">\tkl.probeManager.Start()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start syncing RuntimeClasses if enabled.</span><br><span class=\"line\">\tif kl.runtimeClassManager != nil &#123;</span><br><span class=\"line\">\t\tgo kl.runtimeClassManager.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start the pod lifecycle event generator.</span><br><span class=\"line\">\tkl.pleg.Start()</span><br><span class=\"line\"></span><br><span class=\"line\">\tkl.syncLoop(updates, kl)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncLoop 是 kubelet 的主循环方法，它从不同的管道(FILE,URL, API-SERVER)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 Pod 处于期望的状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123;</span><br><span class=\"line\">\tglog.Info(&quot;Starting kubelet main sync loop.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// syncTicker 每秒检测一次是否有需要同步的 pod workers</span><br><span class=\"line\">\tsyncTicker := time.NewTicker(time.Second)</span><br><span class=\"line\">\tdefer syncTicker.Stop()</span><br><span class=\"line\">\thousekeepingTicker := time.NewTicker(housekeepingPeriod)</span><br><span class=\"line\">\tdefer housekeepingTicker.Stop()</span><br><span class=\"line\">\tplegCh := kl.pleg.Watch()</span><br><span class=\"line\">\tconst (</span><br><span class=\"line\">\t\tbase   = 100 * time.Millisecond</span><br><span class=\"line\">\t\tmax    = 5 * time.Second</span><br><span class=\"line\">\t\tfactor = 2</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tduration := base</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123;</span><br><span class=\"line\">\t\t\tglog.Infof(&quot;skipping pod synchronization - %v&quot;, rs)</span><br><span class=\"line\">\t\t\t// exponential backoff</span><br><span class=\"line\">\t\t\ttime.Sleep(duration)</span><br><span class=\"line\">\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// reset backoff if we have a success</span><br><span class=\"line\">\t\tduration = base</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t\t// </span><br><span class=\"line\">\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123;</span><br><span class=\"line\">\t\t\tbreak</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncLoopIteration()  方法对多个管道进行遍历，如果 pod 发生变化，则会调用相应的 Handler，在 Handler 中通过调用 dispatchWork 分发任务。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本篇文章主要讲述了 kubelet 组件从加载配置到初始化内部的各个模块再到启动 kubelet 服务的整个流程，上面的时序图能清楚的看到函数之间的调用关系，但是其中每个组件具体的工作方式以及组件之间的交互方式还不得而知，后面会一探究竟。</p>\n<p>参考：<br><a href=\"http://www.sel.zju.edu.cn/?p=595\" target=\"_blank\" rel=\"noopener\">kubernetes node components – kubelet</a><br><a href=\"https://segmentfault.com/a/1190000008267351\" target=\"_blank\" rel=\"noopener\">Kubelet 源码分析(一):启动流程分析</a><br><a href=\"https://cizixs.com/2017/06/06/kubelet-source-code-analysis-part-1/\" target=\"_blank\" rel=\"noopener\">kubelet 源码分析：启动流程</a><br><a href=\"https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/05/02/Kubernetes-kubelet.html\" target=\"_blank\" rel=\"noopener\">kubernetes 的 kubelet 的工作过程</a><br><a href=\"https://fatsheep9146.github.io/2018/07/08/kubelet%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">kubelet 内部实现解析</a></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjs8z5ggx000422dvjlurvls6","tag_id":"cjs8z5gh0000622dvhkxm794h","_id":"cjs8z5gh6000b22dv1w283qvg"},{"post_id":"cjs8z5ggx000422dvjlurvls6","tag_id":"cjs8z5gh4000822dvf0vil4xf","_id":"cjs8z5gh6000c22dvop99n1o0"},{"post_id":"cjs8z5ggx000422dvjlurvls6","tag_id":"cjs8z5gh5000922dvlji9t7as","_id":"cjs8z5gh6000d22dvquriva5k"},{"post_id":"cjs8z5ggz000522dveh68gwf4","tag_id":"cjs8z5gh5000a22dv44gckrk6","_id":"cjs8z5gh6000e22dvxl5i9c0b"},{"post_id":"cjs8z5go5000h22dvbdp4izwa","tag_id":"cjs8z5goe000j22dvj7g60qtm","_id":"cjs8z5goq000n22dvzkrey9oy"},{"post_id":"cjs8z5go5000h22dvbdp4izwa","tag_id":"cjs8z5gog000l22dvvhg2r4mm","_id":"cjs8z5got000o22dva8hkrkmf"},{"post_id":"cjs8z5goc000i22dvlp4oh4ej","tag_id":"cjs8z5goi000m22dvpthihpmh","_id":"cjs8z5got000q22dv1vxhwbda"},{"post_id":"cjs8z5goe000k22dvkj677cfa","tag_id":"cjs8z5got000p22dv5hkdg9fs","_id":"cjs8z5gou000r22dvqkf7r6il"},{"post_id":"cjs8z5gpx000s22dvjf8k5z8e","tag_id":"cjs8z5got000p22dv5hkdg9fs","_id":"cjs8z5gpy000u22dvqx7m68wj"},{"post_id":"cjs8z5gpy000v22dv5523nl78","tag_id":"cjs8z5got000p22dv5hkdg9fs","_id":"cjs8z5gpz000w22dvft6dfq92"}],"Tag":[{"name":"crontab","_id":"cjs8z5gh0000622dvhkxm794h"},{"name":"wait","_id":"cjs8z5gh4000822dvf0vil4xf"},{"name":"k8s","_id":"cjs8z5gh5000922dvlji9t7as"},{"name":"kubeconfig","_id":"cjs8z5gh5000a22dv44gckrk6"},{"name":"audit","_id":"cjs8z5goe000j22dvj7g60qtm"},{"name":"log","_id":"cjs8z5gog000l22dvvhg2r4mm"},{"name":"kubeadm","_id":"cjs8z5goi000m22dvpthihpmh"},{"name":"kubelet","_id":"cjs8z5got000p22dv5hkdg9fs"}]}}