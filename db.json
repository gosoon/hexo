{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"8ca07c6d7b4ff1dd8ab4f14fde521ea3ce4eb84a","modified":1559399176306},{"_id":"themes/next/.gitignore","hash":"0b5c2ffd41f66eb1849d6426ba8cf9649eeed329","modified":1559399176329},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1559399176328},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1559399176328},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1559399176328},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1559399176329},{"_id":"themes/next/.javascript_ignore","hash":"8a224b381155f10e6eb132a4d815c5b52962a9d1","modified":1559399176329},{"_id":"themes/next/.travis.yml","hash":"d60d4a5375fea23d53b2156b764a99b2e56fa660","modified":1559399176330},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1559399176329},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1559399176329},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1559399176330},{"_id":"themes/next/bower.json","hash":"0674f11d3d514e087a176da0e1d85c2286aa5fba","modified":1559399176330},{"_id":"themes/next/README.cn.md","hash":"58ffe752bc4b7f0069fcd6304bbc2d5ff7b80f89","modified":1559399176330},{"_id":"themes/next/README.md","hash":"898213e66d34a46c3cf8446bf693bd50db0d3269","modified":1559399176330},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1559399176331},{"_id":"themes/next/_config.yml","hash":"36266688d805445c8eb69b890fa079379a4ff945","modified":1567388325717},{"_id":"themes/next/package.json","hash":"42d4e836442a0f12330a92769429cb530547989a","modified":1567332486221},{"_id":"themes/next/package-lock.json","hash":"4bdb23639fcad335f94d7972ad29bf2d14e4b197","modified":1567332486245},{"_id":"source/_data/recommended_posts.json","hash":"2e3b913302334ea95967a730db84fb5d0994f1a5","modified":1567389754340},{"_id":"source/_posts/code_generator.md","hash":"2b0174b89ca611101109227210f0a56701733315","modified":1565668854787},{"_id":"source/_posts/docker-introduces.md","hash":"e44d06613e82a34189a3151ea92ae49462f7bac3","modified":1563702655384},{"_id":"source/_posts/client-go_informer.md","hash":"61c3998edfdea0390d9085fdc2ca3b6f89eb95b5","modified":1563703677293},{"_id":"source/_posts/etcd-backup.md","hash":"07bb8e8ddcfaac251a5b7fe3613044445de08dff","modified":1559399176307},{"_id":"source/_posts/etcd-enable-https.md","hash":"b6d41d23d296a277615372f297f498e808fbf53f","modified":1559399176307},{"_id":"source/_posts/etcd_improvements.md","hash":"4748a029308c98b3e450fdf0bcdba8638dcefef3","modified":1570522273643},{"_id":"source/_posts/golang_modules.md","hash":"2c618e9dcbcf34e08088f67e4188ddd00bc9014d","modified":1561207774781},{"_id":"source/_posts/k8s-crontab.md","hash":"0d883567cac92795a73d88e54ba87608f5bae5be","modified":1559399176307},{"_id":"source/_posts/k8s-audit-webhook.md","hash":"a5ea8f5a7299c010c2e46cc07d3557bf8f79b48e","modified":1559399176307},{"_id":"source/_posts/k8s_auth_rbac.md","hash":"c490b7448d003b7ab0ded9d9b633b6076414b36d","modified":1566134768396},{"_id":"source/_posts/k8s_crd_verify.md","hash":"26e3afab51068fb91b6ae039efa13ffb68cf346a","modified":1568786024408},{"_id":"source/_posts/k8s_dashboard_prometheus.md","hash":"b07a008a6a6e7185742feac13a99181791d05707","modified":1563703437982},{"_id":"source/_posts/k8s_components_ha.md","hash":"25cde1ab797f28cc67b873fdb9ae15a1e71095f0","modified":1562921781988},{"_id":"source/_posts/k8s_improvements.md","hash":"cf200bb04465be91b2307c4246c96b611c07fdcd","modified":1570870853369},{"_id":"source/_posts/k8s_events.md","hash":"6877d932da7293b70ab85908c3cbae6d8050a08d","modified":1563702388879},{"_id":"source/_posts/k8s_leader_election.md","hash":"10e3debabafa0274b81a6c94e4eb6e8053bb6b1d","modified":1563703262574},{"_id":"source/_posts/k8s_v1.12.md","hash":"8fb615a394e3d2f482539d3a4344866fe4fc06d1","modified":1563703084316},{"_id":"source/_posts/k8s_metrics_server.md","hash":"247f3694017e47ff7b08724fa95c35998360bca2","modified":1559399176308},{"_id":"source/_posts/kind_deploy.md","hash":"5a4f1a41c0cb7cc98b95e2029c870744c97ccdfe","modified":1567737398481},{"_id":"source/_posts/kube_on_kube_operator_3.md","hash":"9a1ff2ab19e27bb1be8b87969ab4c691785cf6a8","modified":1567330329529},{"_id":"source/_posts/kube_on_kube_operator_1.md","hash":"ab5e7177db22914c03b35e338f4c23e41cd43660","modified":1565228167960},{"_id":"source/_posts/k8s_release_version.md","hash":"98343697cc0898aeca4fdf7011a73a15f7a17250","modified":1569498896651},{"_id":"source/_posts/kube_on_kube_operator_2.md","hash":"695cda89bfc6a0614ff4d8053221ab98d926b18e","modified":1565230269250},{"_id":"source/_posts/kubeconfig.md","hash":"8dff92301fa78234d0a28bacb4a5fdac7ae746e6","modified":1559399176309},{"_id":"source/_posts/kube_scheduler_process.md","hash":"e2467d6d5efdf65e577378b907704f0a900a42c0","modified":1571651352782},{"_id":"source/_posts/kubectl_plugin.md","hash":"d0114f6edffa61e76e5a463f0f862b97156e9093","modified":1559399176309},{"_id":"source/_posts/kubeadm.md","hash":"31a7c68204a348f17927bc649a6b2bbefbf0b647","modified":1559399176308},{"_id":"source/_posts/kubelet-modules.md","hash":"a7b91acbf5c141dbe621837398d2e470b8bb79b6","modified":1563702546807},{"_id":"source/_posts/kubelet_create_pod.md","hash":"b02f6c41df2ec0a917b28376d645515a399ada5b","modified":1563702410287},{"_id":"source/_posts/kubelet_init.md","hash":"09f4bfae7fd004e21842f411fbbeb6a238d7b03a","modified":1563702456619},{"_id":"source/_posts/kubernetes-api.md","hash":"d6bcdc046606b452f7f506c394c9f7d983ba8b50","modified":1563702913515},{"_id":"source/_posts/kubernetes-learn.md","hash":"433e40ff9e341caf1b936498b430f33bfb8be670","modified":1563702068891},{"_id":"source/_posts/node_status.md","hash":"481eeb4dbf2d98af31cb374998d2a549f544f39f","modified":1560084558948},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1559399176328},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"352093a1b210c72136687fd2eee649244cee402c","modified":1559399176328},{"_id":"source/categories/index.md","hash":"812daa9e1c97a2c72ee357ad92f4de335db3c177","modified":1559399176310},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"902f627155a65099e0a37842ff396a58d0dc306f","modified":1559399176329},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1559399176329},{"_id":"source/tags/index.md","hash":"5118f5301c54915b86d4e92b56a2fa9a9395abfc","modified":1559399176311},{"_id":"source/about/index.md","hash":"c6077a8dc9537d3620a2a8618e8f001e1905751d","modified":1567912047717},{"_id":"themes/next/languages/de.yml","hash":"057e7df11ddeb1c8c15a5d7c5ff29430d725ec6b","modified":1559399176331},{"_id":"themes/next/languages/en.yml","hash":"7e680d9bb8f3a3a9d1ba1c9d312b3d257183dded","modified":1559399176331},{"_id":"themes/next/languages/default.yml","hash":"44ef3f26917f467459326c2c8be2f73e4d947f35","modified":1559399176331},{"_id":"themes/next/languages/fr-FR.yml","hash":"7e4eb7011b8feee641cfb11c6e73180b0ded1c0f","modified":1559399176331},{"_id":"themes/next/languages/id.yml","hash":"b5de1ea66dd9ef54cac9a1440eaa4e3f5fc011f5","modified":1559399176331},{"_id":"themes/next/languages/it.yml","hash":"aa595f2bda029f73ef7bfa104b4c55c3f4e9fb4c","modified":1559399176331},{"_id":"themes/next/languages/ja.yml","hash":"3c76e16fd19b262864475faa6854b718bc08c4d8","modified":1559399176331},{"_id":"themes/next/languages/ko.yml","hash":"ea5b46056e73ebcee121d5551627af35cbffc900","modified":1559399176332},{"_id":"themes/next/languages/nl-NL.yml","hash":"edca4f3598857dbc3cbf19ed412213329b6edd47","modified":1559399176332},{"_id":"themes/next/languages/pt-BR.yml","hash":"b1694ae766ed90277bcc4daca4b1cfa19cdcb72b","modified":1559399176332},{"_id":"themes/next/languages/pt.yml","hash":"44b61f2d085b827b507909a0b8f8ce31c6ef5d04","modified":1559399176332},{"_id":"themes/next/languages/vi.yml","hash":"fd08d3c8d2c62965a98ac420fdaf95e54c25d97c","modified":1559399176332},{"_id":"themes/next/languages/zh-Hans.yml","hash":"16ef56d0dea94638de7d200984c90ae56f26b4fe","modified":1559399176332},{"_id":"themes/next/languages/ru.yml","hash":"98ec6f0b7183282e11cffc7ff586ceb82400dd75","modified":1559399176332},{"_id":"themes/next/languages/zh-tw.yml","hash":"50b71abb3ecc0686f9739e179e2f829cd074ecd9","modified":1559399176333},{"_id":"themes/next/languages/zh-hk.yml","hash":"9396f41ae76e4fef99b257c93c7354e661f6e0fa","modified":1559399176332},{"_id":"themes/next/layout/_layout.swig","hash":"da0929166674ea637e0ad454f85ad0d7bac4aff2","modified":1559399176333},{"_id":"themes/next/layout/archive.swig","hash":"f0a8225feafd971419837cdb4bcfec98a4a59b2f","modified":1559399176344},{"_id":"themes/next/layout/category.swig","hash":"4472255f4a3e3dd6d79201523a9526dcabdfbf18","modified":1559399176344},{"_id":"themes/next/layout/index.swig","hash":"783611349c941848a0e26ee2f1dc44dd14879bd1","modified":1559399176344},{"_id":"themes/next/layout/schedule.swig","hash":"d86f8de4e118f8c4d778b285c140474084a271db","modified":1559399176345},{"_id":"themes/next/layout/page.swig","hash":"969caaee05bdea725e99016eb63d810893a73e99","modified":1559399176344},{"_id":"themes/next/layout/tag.swig","hash":"7e0a7d7d832883eddb1297483ad22c184e4368de","modified":1559399176345},{"_id":"themes/next/layout/post.swig","hash":"b3589a8e46288a10d20e41c7a5985d2493725aec","modified":1567388325718},{"_id":"themes/next/scripts/merge-configs.js","hash":"81e86717ecfb775986b945d17f0a4ba27532ef07","modified":1559399176345},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1559399176346},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1559399176389},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1559399176390},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1559399176390},{"_id":"themes/next/node_modules/extsprintf/.gitmodules","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1416529720000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1559399176361},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1559399176333},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1559399176333},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1559399176334},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"665a928604f99d2ba7dc4a4a9150178229568cc6","modified":1559399176334},{"_id":"themes/next/layout/_macro/post.swig","hash":"446a35a2cd389f8cfc3aa38973a9b44ad0740134","modified":1559399176334},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"39852700e4084ecccffa6d4669168e5cc0514c9e","modified":1559399176334},{"_id":"themes/next/layout/_macro/reward.swig","hash":"56e8d8556cf474c56ae1bef9cb7bbd26554adb07","modified":1559399176334},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4a6f5b1792b2e5262b7fdab9a716b3108e2f09c7","modified":1559399176335},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c4d6181f5d3db5365e622f78714af8cc58d7a45e","modified":1559399176335},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1559399176335},{"_id":"themes/next/layout/_partials/head.swig","hash":"6b94fe8f3279daea5623c49ef4bb35917ba57510","modified":1559399176335},{"_id":"themes/next/layout/_partials/header.swig","hash":"ed042be6252848058c90109236ec988e392d91d4","modified":1559399176335},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"65fe5c71a6924180c40215962cb0ceb8ef93e79a","modified":1567332405761},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1559399176336},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1559399176336},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1559399176337},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1559399176337},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"a266f96ad06ee87bdeae6e105a4b53cd587bbd04","modified":1559399176338},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1559399176342},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1559399176342},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1559399176342},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"5fe0447cc88a5a63b530cf0426f93c4634811876","modified":1559399176342},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1559399176342},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1559399176342},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1559399176342},{"_id":"themes/next/node_modules/.bin/sshpk-verify","hash":"bed5d9cc90700090b09d785d84f985b7815e55f8","modified":1539302377000},{"_id":"themes/next/node_modules/assert-plus/CHANGES.md","hash":"b27cef2253f8ff8a83584bf1a55cec9d4dfbf517","modified":1453920717000},{"_id":"themes/next/node_modules/assert-plus/AUTHORS","hash":"d1d14de8fc8c21f9f86c2231df2531381f6a9194","modified":1453918488000},{"_id":"themes/next/node_modules/assert-plus/README.md","hash":"d6771de291034391f0ec79175fff3428ed6d82b8","modified":1453920717000},{"_id":"themes/next/node_modules/ajv/.tonic_example.js","hash":"4234fa9e49ee50602f9877e95bf46cdfaf4f618f","modified":1524950955000},{"_id":"themes/next/node_modules/assert-plus/package.json","hash":"57d2d25fe33c20290f78eb378fe08dfed272a087","modified":1567332486128},{"_id":"themes/next/node_modules/ajv/LICENSE","hash":"ea828f38ab0e82ec6d2272e46d794d7c3d55c9c9","modified":1524950955000},{"_id":"themes/next/node_modules/assert-plus/assert.js","hash":"8103ced007b61b3e45a420607de576faf7e1e0e4","modified":1453920717000},{"_id":"themes/next/node_modules/.bin/uuid","hash":"c5b3ec4d8a8f620420aa52a8f3a7cfdff1197667","modified":499162500000},{"_id":"themes/next/node_modules/.bin/sshpk-sign","hash":"75271a4b0392b5549555f50085b991e8d9a3bd0c","modified":1461292363000},{"_id":"themes/next/node_modules/asynckit/LICENSE","hash":"9171131798797e013e6c922921540694b1e3542d","modified":1463552412000},{"_id":"themes/next/node_modules/asynckit/README.md","hash":"030a12b4b2a9151538e9491d3a4f23dfe9848f32","modified":1465928899000},{"_id":"themes/next/node_modules/asynckit/bench.js","hash":"8dacd95640ad3d0ae47aebf5bd1ba4d83f6a1f1d","modified":1463686174000},{"_id":"themes/next/node_modules/ajv/package.json","hash":"68e7e11508f8115e4cf3d5e0229c77afc1dfd9bb","modified":1567332486130},{"_id":"themes/next/node_modules/asynckit/index.js","hash":"9201eacd2650642ff12a8af5fd1dbf7dc7848bee","modified":1463795005000},{"_id":"themes/next/node_modules/asynckit/package.json","hash":"9b53048ce2fe3c6ca3e1a78861b63a4c2d94c0c3","modified":1567332486128},{"_id":"themes/next/node_modules/asynckit/parallel.js","hash":"ecead062825e6790ce70c1879c745808c8ce7528","modified":1465665593000},{"_id":"themes/next/node_modules/asynckit/serialOrdered.js","hash":"26f4cadf2685ee07af06e82803171847f74b9fb3","modified":1465665610000},{"_id":"themes/next/node_modules/asynckit/stream.js","hash":"c8bde32c80d2f3ecb33462a0aed86851c4c7e9e6","modified":1465859636000},{"_id":"themes/next/node_modules/asynckit/serial.js","hash":"9af5e746a0a9f8ba24926bc2fbb9786cf13f081b","modified":1465665583000},{"_id":"themes/next/node_modules/aws-sign2/LICENSE","hash":"05979f0750cf5c2a17bd3aa12450849c151d8b7c","modified":1492024121000},{"_id":"themes/next/node_modules/aws-sign2/package.json","hash":"ed92f07818014774b7743d9b338e46747f03d86d","modified":1567332486128},{"_id":"themes/next/node_modules/aws-sign2/README.md","hash":"5e8e0e7c811b1f319c0e94ff08f38ecf4896e3c9","modified":1492024121000},{"_id":"themes/next/node_modules/aws4/.travis.yml","hash":"73c40717fe695caf839f2bc871320837b3c2abae","modified":499162500000},{"_id":"themes/next/node_modules/aws-sign2/index.js","hash":"fb21f9e87275ef41a4901fa4bf0680d1bf0f0605","modified":1492024121000},{"_id":"themes/next/node_modules/aws4/LICENSE","hash":"9ba903f14c37d0ab0250a8e6920e7269bdc5b294","modified":499162500000},{"_id":"themes/next/node_modules/aws4/lru.js","hash":"015965230b90e69906f66fca1792c4557336336f","modified":499162500000},{"_id":"themes/next/node_modules/aws4/aws4.js","hash":"e7b745e9f188988e700fed38869710b1ea442b9b","modified":499162500000},{"_id":"themes/next/node_modules/aws4/README.md","hash":"8c7e5705921c53e38919481a09ad2d545422648c","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/LICENSE","hash":"1666d1c772864f9a81c15c8aecca0307a658e648","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/changelog.md","hash":"bc6df7caea3b83fa361fa0a96c9b34c09465a93a","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/README.md","hash":"7b04ec21806a36f4dddbb883ee33d84e3ae6b8f3","modified":499162500000},{"_id":"themes/next/node_modules/aws4/package.json","hash":"d8e1b98296f2960500de24ceecc9520847ca328d","modified":1567332486128},{"_id":"themes/next/node_modules/bluebird/package.json","hash":"8191241e0d25b31c6a5d85aad73b9c82a41f8e18","modified":1567332486130},{"_id":"themes/next/node_modules/asn1/LICENSE","hash":"724e48cfc739674999ff82b4e49d76d5376818e9","modified":1518292051000},{"_id":"themes/next/node_modules/asn1/package.json","hash":"7c8adb6b8e65a5a062acdd4261d85f12ab047ca3","modified":1567332486130},{"_id":"themes/next/node_modules/bcrypt-pbkdf/CONTRIBUTING.md","hash":"24ea74be3d4501f6e46a96dd11fed6c1c36d6c56","modified":1530232429000},{"_id":"themes/next/node_modules/asn1/README.md","hash":"ec42f7168854358d5e3611b20c1bad0ac55954b8","modified":1532988025000},{"_id":"themes/next/node_modules/.bin/sshpk-conv","hash":"c870b6d429849b83d74aab97665d0404e1f6f91b","modified":1545357205000},{"_id":"themes/next/node_modules/bcrypt-pbkdf/LICENSE","hash":"fce141a52014eee7adc6585514d55c6330db0307","modified":1530232429000},{"_id":"themes/next/node_modules/bcrypt-pbkdf/README.md","hash":"1c4e671f5ead73535ed536bb37f81a70d43bef58","modified":1530232429000},{"_id":"themes/next/node_modules/caseless/README.md","hash":"5770b9496fb480f7c403b6c38759a27d6170e882","modified":1414608769000},{"_id":"themes/next/node_modules/bcrypt-pbkdf/package.json","hash":"3b268245a261acb65ef2cabde761fb9a5b7b46f4","modified":1567332486130},{"_id":"themes/next/node_modules/caseless/LICENSE","hash":"48f9e0a4c07f36c07d47962212fe022d0417c90f","modified":1435268329000},{"_id":"themes/next/node_modules/bcrypt-pbkdf/index.js","hash":"4be9d75ea7a7a9c91fe8436f52ebd113fdc674f3","modified":1530232429000},{"_id":"themes/next/node_modules/caseless/index.js","hash":"49a6a654b1bdc4e119895edc96a1ee4bb519a38c","modified":1485466642000},{"_id":"themes/next/node_modules/combined-stream/License","hash":"04e7b761eee5270ea8914303516852faf990394b","modified":499162500000},{"_id":"themes/next/node_modules/caseless/package.json","hash":"33da30e61fbd61f6380f45ce4d955a2251562b83","modified":1567332486129},{"_id":"themes/next/node_modules/caseless/test.js","hash":"9cbbc1b43bd3b86518885b2c82f0d8c302e68ed9","modified":1485466642000},{"_id":"themes/next/node_modules/combined-stream/Readme.md","hash":"0e4384a6edea7b3a580ffa5b32236b7f8b251834","modified":499162500000},{"_id":"themes/next/node_modules/combined-stream/package.json","hash":"e12f7f3e2b70658463262b30532a7ac664d899f8","modified":1567332486129},{"_id":"themes/next/node_modules/combined-stream/yarn.lock","hash":"87856e0cab19e1b0c5c4491e1dae88e37d82137d","modified":499162500000},{"_id":"themes/next/node_modules/core-util-is/README.md","hash":"d4987293f1078d937454a14a5ca6f386d227679d","modified":1447978999000},{"_id":"themes/next/node_modules/core-util-is/LICENSE","hash":"a95471326a84657b0e164f84e1285685f63011e0","modified":1447979068000},{"_id":"themes/next/node_modules/core-util-is/package.json","hash":"6d51559c62d2132115c8daf446f9888533c09985","modified":1567332486129},{"_id":"themes/next/node_modules/core-util-is/float.patch","hash":"d7f073ceb05c6f6dd1bb852fc00f5379a3e41301","modified":1447978999000},{"_id":"themes/next/node_modules/core-util-is/test.js","hash":"2837ee1d57f385943ba5ccc7fd0ec4ed1cb8cfdd","modified":1447979358000},{"_id":"themes/next/node_modules/dashdash/CHANGES.md","hash":"d4c1434f92782308105f24a11c0bdb076127be5e","modified":1479854012000},{"_id":"themes/next/node_modules/dashdash/LICENSE.txt","hash":"427497f949741f3f7e72ea1e62749908e9c4f5b9","modified":1451581401000},{"_id":"themes/next/node_modules/dashdash/README.md","hash":"e40739349732594d5d96438bd27aa444915d3f79","modified":1453144686000},{"_id":"themes/next/node_modules/delayed-stream/.npmignore","hash":"4e1243bd22c66e76c2ba9eddc1f91394e57f9f83","modified":1430430799000},{"_id":"themes/next/node_modules/dashdash/package.json","hash":"327a212ba7faee72e21cb689b52871f757dec212","modified":1567332486129},{"_id":"themes/next/node_modules/delayed-stream/License","hash":"04e7b761eee5270ea8914303516852faf990394b","modified":1430160212000},{"_id":"themes/next/node_modules/delayed-stream/Makefile","hash":"1d88cf18c0fef56e91425a086590e31271a7c4d5","modified":1430160212000},{"_id":"themes/next/node_modules/delayed-stream/Readme.md","hash":"21db9b7ec97b7028a031a18867aab00575b09850","modified":1430160212000},{"_id":"themes/next/node_modules/extend/.editorconfig","hash":"b613101963356bfaf6118fc55cf67bd5f5567303","modified":499162500000},{"_id":"themes/next/node_modules/delayed-stream/package.json","hash":"c72efde6836f7f7553b8d75c6417f3a50778838c","modified":1567332486130},{"_id":"themes/next/node_modules/extend/.eslintrc","hash":"495142299502da25082a07dc75ae9d0dfd8d19f6","modified":499162500000},{"_id":"themes/next/node_modules/extend/.travis.yml","hash":"c3d28348f599bc838f7bbbff116ab3ea13d78bbe","modified":499162500000},{"_id":"themes/next/node_modules/extend/.jscs.json","hash":"88712fb751dde48f7326c45a25b87c593b144830","modified":499162500000},{"_id":"themes/next/node_modules/extend/CHANGELOG.md","hash":"c82d9ea712d2098a397643f13b5580f8de49b503","modified":499162500000},{"_id":"themes/next/node_modules/extend/LICENSE","hash":"bed93ae1abcd71e5d1c9c363595dd24bb1b9016c","modified":499162500000},{"_id":"themes/next/node_modules/extend/README.md","hash":"25aa925ed3ef4f533976a45cece2cc8159993783","modified":499162500000},{"_id":"themes/next/node_modules/extend/component.json","hash":"07af5d609ee23647ce3238a0e1322c35b385fc45","modified":499162500000},{"_id":"themes/next/node_modules/extend/index.js","hash":"21ecd848815af34edd426bcdafa52bf13c02be59","modified":499162500000},{"_id":"themes/next/node_modules/extend/package.json","hash":"f26f4ab62edb0bc44e63047504bfb6da806e08cb","modified":1567332486129},{"_id":"themes/next/node_modules/ecc-jsbn/LICENSE","hash":"d9dddd103d636dbaa178cd222de612fd923f62da","modified":1532879415000},{"_id":"themes/next/node_modules/ecc-jsbn/README.md","hash":"97cb1bdec48ebb5e46e64fc5441c79d3d38a3c82","modified":1532879415000},{"_id":"themes/next/node_modules/ecc-jsbn/index.js","hash":"242eff19118d5cd84c53679acc7395dee6bfb1df","modified":1532879415000},{"_id":"themes/next/node_modules/extsprintf/.npmignore","hash":"f6f25c76813204f78147a50d49d3b6a9a4282201","modified":1416529581000},{"_id":"themes/next/node_modules/extsprintf/LICENSE","hash":"772b18147b3bf826978f984f8e01562a9e3cc254","modified":1416529581000},{"_id":"themes/next/node_modules/ecc-jsbn/package.json","hash":"6f7bac698dc4c7fd93012a3e818e07041c311045","modified":1567332486130},{"_id":"themes/next/node_modules/extsprintf/Makefile","hash":"49da84d8f9e1db5727d0a881ef0382e45f26492d","modified":1425686217000},{"_id":"themes/next/node_modules/ecc-jsbn/test.js","hash":"9709f584f556ceb8acaa0fd746029dcb58390eb4","modified":1532879415000},{"_id":"themes/next/node_modules/extsprintf/README.md","hash":"4226a704e2766b5069bb6636555517ed866be6f1","modified":1425686198000},{"_id":"themes/next/node_modules/extsprintf/Makefile.targ","hash":"f66e38899fc918aac830c478ae402c66d8793bc7","modified":1416529581000},{"_id":"themes/next/node_modules/fast-deep-equal/README.md","hash":"09192a92f5ef1b36a62dde12a520bc7529fe437e","modified":1524262875000},{"_id":"themes/next/node_modules/extsprintf/package.json","hash":"f61bbb0b7260e504e09733fe707c066cf4cdaa87","modified":1567332486129},{"_id":"themes/next/node_modules/extsprintf/jsl.node.conf","hash":"7197578b829d6c93d8356bf114c7f3d26722ec0b","modified":1416529581000},{"_id":"themes/next/node_modules/fast-deep-equal/LICENSE","hash":"44bdc0699c385cdf423dbadea7355ff72e5adc36","modified":1519590513000},{"_id":"themes/next/node_modules/fast-deep-equal/index.d.ts","hash":"c2f05139dccaac36615727a10d7b0b31082a0e98","modified":1519591012000},{"_id":"themes/next/node_modules/fast-deep-equal/index.js","hash":"7544a59317225a41d7c3b02605e87459a251ea54","modified":1524909376000},{"_id":"themes/next/node_modules/fast-deep-equal/package.json","hash":"b9ed3d794f064eedce618995336cbc58aec84f71","modified":1567332486129},{"_id":"themes/next/node_modules/form-data/License","hash":"1d11381521bdc7e7df9bb1d7bf85341ffca266d9","modified":1354688992000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/.eslintrc.yml","hash":"e557873fb13e67775c12f025f7254f12547784bc","modified":1508841234000},{"_id":"themes/next/node_modules/form-data/README.md","hash":"d35f6dba2b67cbe947f229485fd7c15277d35225","modified":1539761181000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/.npmignore","hash":"079203ff3bee95cc2b071f1e8b15fec2111f1930","modified":1508866028000},{"_id":"themes/next/node_modules/form-data/package.json","hash":"3ca5350f26fba50dd74b08001dd191a29942a436","modified":1567332486130},{"_id":"themes/next/node_modules/fast-json-stable-stringify/.travis.yml","hash":"4c08200f6602d0fb52c7d3659131401a78f4e34a","modified":1508840229000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/LICENSE","hash":"b2e68ce937c1f851926f7e10280cc93221d4f53c","modified":1508836326000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/README.md","hash":"994000bfa12b63fed32d4c13e91f158097d7f888","modified":1508866787000},{"_id":"themes/next/node_modules/form-data/README.md.bak","hash":"91dbdbd25f0e985599bc346b478eacc1a4f05c51","modified":1518546191000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/index.js","hash":"81aef05bcfffb0f8804dc9684dd6283f0024bd2d","modified":1508865839000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/package.json","hash":"92833de84508cab4a5c096f5ae66fa8e35d1f80f","modified":1567332486129},{"_id":"themes/next/node_modules/forever-agent/README.md","hash":"fd2b1d08b55284aa2abc72d199e3c487744a0c31","modified":1426938522000},{"_id":"themes/next/node_modules/forever-agent/LICENSE","hash":"05979f0750cf5c2a17bd3aa12450849c151d8b7c","modified":1426938522000},{"_id":"themes/next/node_modules/forever-agent/package.json","hash":"2d062a4ca6d2ebdc7de7e271b7f2d0854fa0241d","modified":1567332486129},{"_id":"themes/next/node_modules/getpass/.npmignore","hash":"013de6e5745b61fd07777e2bff18d3d44c11c0a1","modified":1461270348000},{"_id":"themes/next/node_modules/forever-agent/index.js","hash":"a96917d5912eef5a8ed12a4b1f3a41400eac80d1","modified":1428426146000},{"_id":"themes/next/node_modules/getpass/README.md","hash":"72706fc471e2713656e5dbc049a4c1a2c418b2e7","modified":1461275519000},{"_id":"themes/next/node_modules/getpass/LICENSE","hash":"10b8ac49ffb8f7cc8bdca9303209a1b3b2f3587d","modified":1493163595000},{"_id":"themes/next/node_modules/getpass/.travis.yml","hash":"4f7f46c996ede594c41fefbe08df3248206d895d","modified":1461270363000},{"_id":"themes/next/node_modules/getpass/package.json","hash":"bc1ef7f1e6582b15393715db51d095690701b856","modified":1567332486129},{"_id":"themes/next/node_modules/har-validator/README.md","hash":"0e6a44779655556795fb65407114a493b8e27430","modified":499162500000},{"_id":"themes/next/node_modules/har-validator/LICENSE","hash":"181750ad60fd8b4c88ad0f978cc1cd21c9d701e6","modified":499162500000},{"_id":"themes/next/node_modules/har-validator/package.json","hash":"11ad92c0047165bd5222bcdb49fdc51cdf56ff65","modified":1567332486130},{"_id":"themes/next/node_modules/har-schema/README.md","hash":"ade485d13a06d1d75c93216ed9276c86a9b4ace0","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/LICENSE","hash":"57fc502455231f1d10c4d2e67c261f0fc56bb17d","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/package.json","hash":"01a351c2f2648d0f1a13b6cf039e414d80a56041","modified":1567332486129},{"_id":"themes/next/node_modules/is-typedarray/README.md","hash":"8f33b0dd445cfa2ab6712dee34df082f480a5a76","modified":1401649785000},{"_id":"themes/next/node_modules/is-typedarray/LICENSE.md","hash":"b2e68ce937c1f851926f7e10280cc93221d4f53c","modified":1401649683000},{"_id":"themes/next/node_modules/is-typedarray/index.js","hash":"b7ff45ac41fdcb4bb75dd771d6cb9bfbd129e56d","modified":1431824601000},{"_id":"themes/next/node_modules/is-typedarray/package.json","hash":"dc2fdcc53a1fc8d9e56949b3eb43083b1aaf6606","modified":1567332486129},{"_id":"themes/next/node_modules/isstream/.npmignore","hash":"ecb39380a39d86a6861344abf98891016990e2c5","modified":1396757852000},{"_id":"themes/next/node_modules/is-typedarray/test.js","hash":"c3aa563061f8d086aaf066b389b0e811444e3913","modified":1401649839000},{"_id":"themes/next/node_modules/isstream/.jshintrc","hash":"61590c8e7282c327fe7f2987a3c5a7514446561d","modified":1396828238000},{"_id":"themes/next/node_modules/isstream/.travis.yml","hash":"890d1669abc18b7c0541df1483b9f27c60063712","modified":1396829306000},{"_id":"themes/next/node_modules/isstream/LICENSE.md","hash":"3fc5bd6825a9d8fd53c0cd8de0bd16d1c1a7f37a","modified":1425687271000},{"_id":"themes/next/node_modules/isstream/isstream.js","hash":"db3d8708d100bdb66fd33e1eeb6d2b232016934b","modified":1396827239000},{"_id":"themes/next/node_modules/isstream/README.md","hash":"0563c8dd8178c335de8d882a14c0408ba179789b","modified":1425687291000},{"_id":"themes/next/node_modules/isstream/package.json","hash":"944f80058fe9863536fd533cd9c835c466aed0e6","modified":1567332486129},{"_id":"themes/next/node_modules/jsbn/.npmignore","hash":"58caf761f07e5df9cdc21f7f19f0670fde41089b","modified":1457730283000},{"_id":"themes/next/node_modules/isstream/test.js","hash":"3c3c171729a3738bd73b12d311265c7fcfb2a9fa","modified":1396831146000},{"_id":"themes/next/node_modules/jsbn/LICENSE","hash":"d9b1b910dfea689402ffdb75f5467ec263f8480d","modified":1486885358000},{"_id":"themes/next/node_modules/jsbn/example.js","hash":"ba7f33b1196eca41a007177372fe237ac7a85c1b","modified":1486885358000},{"_id":"themes/next/node_modules/jsbn/README.md","hash":"6db673c03e15b4e75f95e25ba87177799511ad0c","modified":1486885358000},{"_id":"themes/next/node_modules/jsbn/example.html","hash":"ec1f94b093e0f1457642f8647294be35d4af033f","modified":1486885358000},{"_id":"themes/next/node_modules/json-schema/README.md","hash":"9065e9ab354ce910706716a42584222674fe1470","modified":1472871200000},{"_id":"themes/next/node_modules/jsbn/package.json","hash":"92ee992c5899836d0dae771fdea20bae0f0d2a9d","modified":1567332486129},{"_id":"themes/next/node_modules/jsbn/index.js","hash":"0a4334c7b70f87e13dbd14b0938106ac0d6beebb","modified":1486886477000},{"_id":"themes/next/node_modules/json-schema/draft-zyp-json-schema-03.xml","hash":"da15453583b019a448fae4b857fe29a583bc1a38","modified":1307480092000},{"_id":"themes/next/node_modules/json-schema/draft-zyp-json-schema-04.xml","hash":"02845f4b0406bffb06ff5cb01eca43f492db6901","modified":1472871200000},{"_id":"themes/next/node_modules/json-schema/package.json","hash":"672302c49acf69801d95c5adfe1d568736608807","modified":1567332486129},{"_id":"themes/next/node_modules/json-schema-traverse/.eslintrc.yml","hash":"b80233178c09213e16f8daace837fb9581ec401b","modified":1525807808000},{"_id":"themes/next/node_modules/json-schema-traverse/index.js","hash":"8302f0123512ec818251c530c24bd493d0ab1744","modified":1528619807000},{"_id":"themes/next/node_modules/json-schema-traverse/README.md","hash":"38e9ed06ec0ecba0058d1df291ff244be22cb7ac","modified":1525808085000},{"_id":"themes/next/node_modules/json-schema-traverse/.travis.yml","hash":"4c08200f6602d0fb52c7d3659131401a78f4e34a","modified":1525807808000},{"_id":"themes/next/node_modules/json-schema-traverse/LICENSE","hash":"44bdc0699c385cdf423dbadea7355ff72e5adc36","modified":1525807808000},{"_id":"themes/next/node_modules/json-stringify-safe/.npmignore","hash":"454bf1af6e2932c514862b67b108ef4ad00d7c1a","modified":1431999690000},{"_id":"themes/next/node_modules/json-schema-traverse/package.json","hash":"e969737838c85bf381e4afaa4e0a1982b3f26e20","modified":1567332486130},{"_id":"themes/next/node_modules/json-stringify-safe/LICENSE","hash":"bb408e929caeb1731945b2ba54bc337edb87cc66","modified":1431999719000},{"_id":"themes/next/node_modules/json-stringify-safe/CHANGELOG.md","hash":"d9b3407ce2eef8d2dadc6b2839a8b16b783eef47","modified":1431999690000},{"_id":"themes/next/node_modules/json-stringify-safe/Makefile","hash":"589819e00a30573f50fa4a11ddb22fb45b372a65","modified":1431999690000},{"_id":"themes/next/node_modules/json-stringify-safe/README.md","hash":"a2972dd72d8ec7c8b1f229f7732f911ed721dbca","modified":1431999690000},{"_id":"themes/next/node_modules/json-stringify-safe/stringify.js","hash":"b36a3a8d0f794fae13dc8e1c93bd4a8cae311bf2","modified":1431999708000},{"_id":"themes/next/node_modules/json-stringify-safe/package.json","hash":"96f9f339e3536ae6918f46b60e88afe1b5a2f2a4","modified":1567332486129},{"_id":"themes/next/node_modules/jsprim/CHANGES.md","hash":"671e4e43c6ee6fc2d711052a7933ebd3cd29a46e","modified":1501690556000},{"_id":"themes/next/node_modules/jsprim/CONTRIBUTING.md","hash":"9147b55a7f13fa4e1db7fe3f83d4415be389ab8a","modified":1501690329000},{"_id":"themes/next/node_modules/jsprim/LICENSE","hash":"772b18147b3bf826978f984f8e01562a9e3cc254","modified":1416249172000},{"_id":"themes/next/node_modules/jsprim/README.md","hash":"9274fef622af8e283f2f1d023a5b17fa2f7623c4","modified":1501690357000},{"_id":"themes/next/node_modules/jsprim/package.json","hash":"2c684886c4cc02c702fad433200ccf2f20d539f6","modified":1567332486131},{"_id":"themes/next/node_modules/mime-db/HISTORY.md","hash":"dce17d5961430e5a8ee88e018d2385861003a5de","modified":499162500000},{"_id":"themes/next/node_modules/mime-db/LICENSE","hash":"b559c45c8d07f2679620d9771e68696ee3d5964b","modified":499162500000},{"_id":"themes/next/node_modules/mime-db/README.md","hash":"2d84dbb2b2f82052e265be6b7302bdf560a28ee4","modified":499162500000},{"_id":"themes/next/node_modules/mime-types/HISTORY.md","hash":"7fe263affc8c7a43fecda5e0ae615036ef1cd1b1","modified":499162500000},{"_id":"themes/next/node_modules/mime-types/LICENSE","hash":"f027af3e61af3880fd7f7b8ba9452a85dd215738","modified":499162500000},{"_id":"themes/next/node_modules/mime-db/index.js","hash":"31180f8d0ae079b1bee7ee03e77ea5323583eb06","modified":499162500000},{"_id":"themes/next/node_modules/mime-db/package.json","hash":"6bd06f98725e7c92d4cd9a2a67829ddbd1c56b9f","modified":1567332486129},{"_id":"themes/next/node_modules/mime-types/index.js","hash":"8d1f07ae6192c4dd6bc08a9247a91af4a0a51eca","modified":499162500000},{"_id":"themes/next/node_modules/mime-types/README.md","hash":"decb3b62e791f36236189df2a0edd5c1ac38d02b","modified":499162500000},{"_id":"themes/next/node_modules/mime-types/package.json","hash":"8d87fe85a14d95b273dfcdfb9f986f214c3a7b5e","modified":1567332486129},{"_id":"themes/next/node_modules/hexo-recommended-posts/.npmignore","hash":"8b894ec0b3bbc33011392ad9bafeb1df2634db45","modified":1522437487000},{"_id":"themes/next/node_modules/hexo-recommended-posts/index.js","hash":"2ba3dec47649391c05606309df794d626179a701","modified":1522437487000},{"_id":"themes/next/node_modules/hexo-recommended-posts/LICENSE","hash":"5835213bd72873e87ee97e546d023ca970bf8c08","modified":1522437487000},{"_id":"themes/next/node_modules/hexo-recommended-posts/README.md","hash":"ab064e043c88c00057365d747dcae9c0cb4538b4","modified":1560285864000},{"_id":"themes/next/node_modules/hexo-recommended-posts/package.json","hash":"bd1c74c05bb2ccc6ff493bfceca3e8605cafab03","modified":1567332486144},{"_id":"themes/next/node_modules/http-signature/.npmignore","hash":"8992dfba5fdf2ba23c1da7e57021d6b1274f2456","modified":1503623992000},{"_id":"themes/next/node_modules/http-signature/.dir-locals.el","hash":"bda1c75ba87348876340908be6799d6ac3a4b807","modified":1503623992000},{"_id":"themes/next/node_modules/http-signature/LICENSE","hash":"10b8ac49ffb8f7cc8bdca9303209a1b3b2f3587d","modified":1503623992000},{"_id":"themes/next/node_modules/http-signature/CHANGES.md","hash":"59a2ab5cc1ac7f7691560038beb2349e628eeacc","modified":1503623992000},{"_id":"themes/next/node_modules/http-signature/README.md","hash":"5847fa2ec86a2ca744776acd5528fdfd6c3377fc","modified":1503623992000},{"_id":"themes/next/node_modules/http-signature/package.json","hash":"8cabb00cde70d5974a4f529febfbf0bf768c4ecb","modified":1567332486144},{"_id":"themes/next/node_modules/http-signature/http_signing.md","hash":"945eb22661d1c43e547371ac8d354a4b761eab09","modified":1503623992000},{"_id":"themes/next/node_modules/oauth-sign/README.md","hash":"63113f7bf7868dc9a27766af9bc5014a47a38c71","modified":499162500000},{"_id":"themes/next/node_modules/oauth-sign/LICENSE","hash":"05979f0750cf5c2a17bd3aa12450849c151d8b7c","modified":499162500000},{"_id":"themes/next/node_modules/performance-now/.tm_properties","hash":"5c5e5a43a6bb68e67a644fc5726f90cfbe1218cc","modified":1483443554000},{"_id":"themes/next/node_modules/oauth-sign/package.json","hash":"f57dd376a72b6c36003930ef04eaa30e0a113975","modified":1567332486129},{"_id":"themes/next/node_modules/performance-now/.npmignore","hash":"eaa3d84cb77d92a21b111fd1e37f53edc1ff9de0","modified":1483443554000},{"_id":"themes/next/node_modules/oauth-sign/index.js","hash":"4f44395b3b6968d1c22b7b9a0db5b5951778fd1d","modified":499162500000},{"_id":"themes/next/node_modules/performance-now/README.md","hash":"1d0d3237cc6e91952e690fdad3beef7ffebb94db","modified":1483850811000},{"_id":"themes/next/node_modules/performance-now/.travis.yml","hash":"73c068ae853eb29403f51762ea2ba273e4f3ae0c","modified":1487513562000},{"_id":"themes/next/node_modules/performance-now/package.json","hash":"d0fa8495218613539628ecd8b2ea6f87988ae6c5","modified":1567332486129},{"_id":"themes/next/node_modules/performance-now/license.txt","hash":"7dabcaa93e54d0a7328e689b637f990f1ec73d8f","modified":1483718998000},{"_id":"themes/next/node_modules/psl/LICENSE","hash":"5d37c256ec93671543bfc8eb89a7a1dcee4f62a6","modified":499162500000},{"_id":"themes/next/node_modules/psl/README.md","hash":"5c81aeee1136210a8ee7777067993023bacbf458","modified":499162500000},{"_id":"themes/next/node_modules/psl/browserstack-logo.svg","hash":"ce56266030efa5233f6f84585232ee6f0284ed32","modified":499162500000},{"_id":"themes/next/node_modules/psl/index.js","hash":"9e7fd00d5e988f4034720c6831e47a0f258261f0","modified":499162500000},{"_id":"themes/next/node_modules/punycode/LICENSE-MIT.txt","hash":"d7384cd3ed0c9614f87dde0f86568017f369814c","modified":1521100211000},{"_id":"themes/next/node_modules/psl/package.json","hash":"47d52852df538b9dcab9feaa35235f2a9b3717bc","modified":1567332486130},{"_id":"themes/next/node_modules/punycode/package.json","hash":"dcb4dd4037f83d32cca26d2110cae5d4b2ebc631","modified":1567332486130},{"_id":"themes/next/node_modules/punycode/README.md","hash":"471282886c3bad44b6359b091e9af61bad2291f6","modified":1526957064000},{"_id":"themes/next/node_modules/punycode/punycode.js","hash":"39ab3964d954c66ab440a590444fd7dd3493be37","modified":1526957064000},{"_id":"themes/next/node_modules/punycode/punycode.es6.js","hash":"4cd0d6df6666175db84f03044e1729d28149bfae","modified":1526957201000},{"_id":"themes/next/node_modules/qs/.eslintrc","hash":"af86f256e00e4a0ebe26c0ef8a926e191d81033f","modified":1525237041000},{"_id":"themes/next/node_modules/qs/.eslintignore","hash":"7b8fdc232fc9b1a8050da3c419dea3d23b5da9c5","modified":1450856430000},{"_id":"themes/next/node_modules/qs/.editorconfig","hash":"482f35688b4e4f8b77ee64b804fca396e2a23022","modified":1501090018000},{"_id":"themes/next/node_modules/qs/LICENSE","hash":"ddf13f1b7345d730677237125bc2e46fbbfbbf32","modified":1450856430000},{"_id":"themes/next/node_modules/qs/CHANGELOG.md","hash":"5374e64b21af4db41ef1b529b7d0d0d8ab8f3bcb","modified":1525377002000},{"_id":"themes/next/node_modules/qs/README.md","hash":"ac4ad81116c9e9e06c46433b58220bc94660ac9e","modified":1497472480000},{"_id":"themes/next/node_modules/qs/package.json","hash":"4f11177a271d21e2abdabd281ee2e57fc1291689","modified":1567332486130},{"_id":"themes/next/node_modules/request/LICENSE","hash":"05979f0750cf5c2a17bd3aa12450849c151d8b7c","modified":499162500000},{"_id":"themes/next/node_modules/request/index.js","hash":"c62c4d3004ac33bfd8e85d9545814592701ed0f8","modified":499162500000},{"_id":"themes/next/node_modules/request/README.md","hash":"094c96f0a70ddd62d7a13a9ad6bfaac0d25f8dab","modified":499162500000},{"_id":"themes/next/node_modules/request/package.json","hash":"e80a8bb5a15e2d67e440e02b86b743aff3417990","modified":1567332486144},{"_id":"themes/next/node_modules/request-promise/LICENSE","hash":"0694609174c951e41307640c65a4a80d84040ff3","modified":1494206972000},{"_id":"themes/next/node_modules/request-promise/.npmignore","hash":"cd57557fb2b3f184de0fa50d99aff6f4b9028b93","modified":1468221013000},{"_id":"themes/next/node_modules/request/request.js","hash":"3343f046367cfa3bba73eb3f40c628342f17c7db","modified":499162500000},{"_id":"themes/next/node_modules/request-promise/errors.js","hash":"6eb5aa243f1f1c3ebca593ab9c8914a3f30de7e9","modified":1470653244000},{"_id":"themes/next/node_modules/request-promise/README.md","hash":"c4983b0dedb2c17ec2986ad5339324fae8861a81","modified":1550208466000},{"_id":"themes/next/node_modules/request-promise/package.json","hash":"2a3885ec4077df8dff49f557acfbf11b3fb4bc1c","modified":1567332486144},{"_id":"themes/next/node_modules/request-promise-core/LICENSE","hash":"51b90ec5dffcc96a67ad05482c10105c3825fed5","modified":499162500000},{"_id":"themes/next/node_modules/request-promise-core/README.md","hash":"ac9b1f25c2a4e0df5cbe225bc6db68a931c97719","modified":499162500000},{"_id":"themes/next/node_modules/request-promise-core/errors.js","hash":"9731a5073ab89b07f5c0ec37b3e7603af4159664","modified":499162500000},{"_id":"themes/next/node_modules/request-promise-core/package.json","hash":"e889e14db6cab834bf44fe5aec5270802ed5835f","modified":1567332486130},{"_id":"themes/next/node_modules/safe-buffer/LICENSE","hash":"07d9563f6153658de124707787ff43f0458ab24a","modified":499162500000},{"_id":"themes/next/node_modules/safe-buffer/README.md","hash":"0e91713afabf9cc3822a6505a93b3f9d6c071fb9","modified":499162500000},{"_id":"themes/next/node_modules/safe-buffer/index.js","hash":"466b55a8a7ec717bceecbe7b44ca93b273c46bb4","modified":499162500000},{"_id":"themes/next/node_modules/safe-buffer/index.d.ts","hash":"0b5844a33b757b9db574541363116917fcbc6d90","modified":499162500000},{"_id":"themes/next/node_modules/safe-buffer/package.json","hash":"d8cadcf58ca9337b42b5b1c9870338733eb7987a","modified":1567332486130},{"_id":"themes/next/node_modules/safer-buffer/LICENSE","hash":"cfcb19ab237382e4ce1253c5f0e28ad153a3c77a","modified":499162500000},{"_id":"themes/next/node_modules/safer-buffer/Readme.md","hash":"133890ecc4218d9c67890f0e5884e754e824c859","modified":499162500000},{"_id":"themes/next/node_modules/safer-buffer/Porting-Buffer.md","hash":"1f23a158dc57c02812baef3334ef96ba1c940e5c","modified":499162500000},{"_id":"themes/next/node_modules/safer-buffer/dangerous.js","hash":"f5d11729706b0de3ed7d6f35060eb73680cce42f","modified":499162500000},{"_id":"themes/next/node_modules/safer-buffer/package.json","hash":"9a28ae96352e641e5ab415acf9240abf9ae3d658","modified":1567332486130},{"_id":"themes/next/node_modules/safer-buffer/safer.js","hash":"ad919cad501061f663026f382334cc6ce5a8b381","modified":499162500000},{"_id":"themes/next/node_modules/sshpk/.npmignore","hash":"695019f4a886543f630c962200d822a468757eb0","modified":1452565517000},{"_id":"themes/next/node_modules/sshpk/.travis.yml","hash":"1cc0c8e1f34a6f75f2cd25a733c765ac6ea793d3","modified":1461292363000},{"_id":"themes/next/node_modules/safer-buffer/tests.js","hash":"dc7fd8ad11f6c0a7de689697afc870f58f7a2844","modified":499162500000},{"_id":"themes/next/node_modules/sshpk/LICENSE","hash":"10b8ac49ffb8f7cc8bdca9303209a1b3b2f3587d","modified":1446856544000},{"_id":"themes/next/node_modules/sshpk/README.md","hash":"a4b37391302777a0f8eadaa61a3e80a9c28360d5","modified":1545357205000},{"_id":"themes/next/node_modules/stealthy-require/.npmignore","hash":"bf345c06675f9c3d7ab0c25777cda673ec6483be","modified":1468865013000},{"_id":"themes/next/node_modules/sshpk/package.json","hash":"34dd7e81973e040bb1467b76f08d9b1cac58e7a2","modified":1567332486131},{"_id":"themes/next/node_modules/stealthy-require/LICENSE","hash":"0759cd9539c8d4a9f631cebd91c31741dc8939f2","modified":1493067079000},{"_id":"themes/next/node_modules/stealthy-require/README.md","hash":"4a52831cbca9bb3cfb329d63ab6d30bd6e62d3d3","modified":1494301514000},{"_id":"themes/next/node_modules/stealthy-require/package.json","hash":"374c3b27b076c0df1afcfcb6e9721e8d6369cc35","modified":1567332486130},{"_id":"themes/next/node_modules/tough-cookie/LICENSE","hash":"27b1c1365696caf2b2be9a9fa391fa93b0152cb6","modified":499162500000},{"_id":"themes/next/node_modules/tough-cookie/README.md","hash":"d64afec360a861d6748989cf4303678159b4e277","modified":499162500000},{"_id":"themes/next/node_modules/tough-cookie/package.json","hash":"ee2f89b526576b8abd56796ed9223f87d740c72d","modified":1567332486130},{"_id":"themes/next/node_modules/tunnel-agent/README.md","hash":"fab5bff0eba532373f36d827d9775105d64ff719","modified":1366607489000},{"_id":"themes/next/node_modules/tunnel-agent/LICENSE","hash":"05979f0750cf5c2a17bd3aa12450849c151d8b7c","modified":1366607489000},{"_id":"themes/next/node_modules/tunnel-agent/index.js","hash":"61c5cadcb5f04336e11d95cb55f86f18cb986f9f","modified":1488673752000},{"_id":"themes/next/node_modules/tweetnacl/.npmignore","hash":"11c8e36af35ec98e1a2d7eddf45646dcb6653169","modified":1460742445000},{"_id":"themes/next/node_modules/tweetnacl/AUTHORS.md","hash":"f075e705a5f8e8ff0e7842de6a26ebf53e08f745","modified":1481623354000},{"_id":"themes/next/node_modules/tunnel-agent/package.json","hash":"774aa35cac486efbc53df4e039d0dff7cb0b5f81","modified":1567332486130},{"_id":"themes/next/node_modules/tweetnacl/CHANGELOG.md","hash":"06dec0b7fb3130df4b0fdd1f8122bf2cd196e900","modified":1481626689000},{"_id":"themes/next/node_modules/tweetnacl/PULL_REQUEST_TEMPLATE.md","hash":"e88e8e670a11b57fc18bb19a15c1c0df04517a09","modified":1481623354000},{"_id":"themes/next/node_modules/tweetnacl/README.md","hash":"cf98261b2d1bc21a11ab1f28ee70cc2597205802","modified":1481627238000},{"_id":"themes/next/node_modules/tweetnacl/LICENSE","hash":"24944bf7920108f5a4790e6071c32e9102760c37","modified":1481623354000},{"_id":"themes/next/node_modules/tweetnacl/nacl.d.ts","hash":"074079a1aca6756d5a4fec3f3fd4a9f6e9c00f66","modified":1481623733000},{"_id":"themes/next/node_modules/tweetnacl/nacl-fast.min.js","hash":"08db9a8b57304c3c117c2a5ba117e25a0d816b34","modified":1481623484000},{"_id":"themes/next/node_modules/tweetnacl/nacl-fast.js","hash":"b8caf62487ca1fcba5ed9e47571bdff0806b8fc1","modified":1460742445000},{"_id":"themes/next/node_modules/tweetnacl/nacl.min.js","hash":"ab05565ba3e2cf29401962ddcd94912e54d916f3","modified":1481623482000},{"_id":"themes/next/node_modules/tweetnacl/nacl.js","hash":"523fe36259524a0f35d056416a18de5ae96027a3","modified":1460742445000},{"_id":"themes/next/node_modules/uri-js/README.md","hash":"f516cd7f6fec8648791a9f05aec4e6ac907dc881","modified":1522550800000},{"_id":"themes/next/node_modules/tweetnacl/package.json","hash":"24bb97507907a16d7ccba20b054417f699ffa3f0","modified":1567332486130},{"_id":"themes/next/node_modules/uri-js/bower.json","hash":"8ab4105201bac6a1463d7794b6a7278e9dd098d5","modified":1490711780000},{"_id":"themes/next/node_modules/uri-js/rollup.config.js","hash":"9dbfc1323ddee512821226775f22ef2f27718185","modified":1523405531000},{"_id":"themes/next/node_modules/uri-js/tsconfig.json","hash":"ae7be1d9765a433dc3e588e759e2127b701432f6","modified":1490368148000},{"_id":"themes/next/node_modules/uri-js/package.json","hash":"8f568fe1446069b93d9ebb56302fa961aab9142e","modified":1567332486130},{"_id":"themes/next/node_modules/uuid/AUTHORS","hash":"55ad68da17b6319a39a82d7c7f7a3ea1e951dee8","modified":499162500000},{"_id":"themes/next/node_modules/uuid/CHANGELOG.md","hash":"904b312647683d8489f630d08c4b5860c9f34839","modified":499162500000},{"_id":"themes/next/node_modules/uuid/LICENSE.md","hash":"65e6555c3308c1d9538808d6c67e75924b8ad912","modified":499162500000},{"_id":"themes/next/node_modules/uuid/index.js","hash":"8e8abb1384d04d1ec5745c9824eee400f77536c0","modified":499162500000},{"_id":"themes/next/node_modules/uuid/README.md","hash":"1127a378766e8cf321ac5ca1e1c40bb8d4a2bab3","modified":499162500000},{"_id":"themes/next/node_modules/uuid/v1.js","hash":"02e4c5e78eb8183c890753bb6ce6f1aa7479bf38","modified":499162500000},{"_id":"themes/next/node_modules/uuid/v3.js","hash":"8815de1e54c316b5fc537712462a07a45e229fec","modified":499162500000},{"_id":"themes/next/node_modules/uuid/v4.js","hash":"af1ab906dd54d3df6319d096b3d5f10686aa7331","modified":499162500000},{"_id":"themes/next/node_modules/uuid/v5.js","hash":"5666846c28ee13c09c22763866145dffe92762bf","modified":499162500000},{"_id":"themes/next/node_modules/verror/CHANGES.md","hash":"557736d3be5363b84f160d480fd2143bbe324c36","modified":1493743176000},{"_id":"themes/next/node_modules/verror/CONTRIBUTING.md","hash":"9147b55a7f13fa4e1db7fe3f83d4415be389ab8a","modified":1475636976000},{"_id":"themes/next/node_modules/uuid/package.json","hash":"4c397cb4217fe5957f3788b26cdd85668c85a981","modified":1567332486131},{"_id":"themes/next/node_modules/verror/.npmignore","hash":"e9062d79bd70146c728778571cbdec1dfa54bcd0","modified":1475636976000},{"_id":"themes/next/scripts/tags/button.js","hash":"d023f10a00077f47082b0517e2ad666e6e994f60","modified":1559399176346},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1559399176346},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1559399176346},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1559399176346},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1559399176346},{"_id":"themes/next/scripts/tags/label.js","hash":"2f8f41a7316372f0d1ed6b51190dc4acd3e16fff","modified":1559399176347},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"eeeabede68cf263de9e6593ecf682f620da16f0a","modified":1559399176347},{"_id":"themes/next/scripts/tags/note.js","hash":"64de4e9d01cf3b491ffc7d53afdf148ee5ad9779","modified":1559399176347},{"_id":"themes/next/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1559399176347},{"_id":"themes/next/node_modules/verror/LICENSE","hash":"ca9d7f4229b385ade88c365257829b4005e85b9e","modified":1475636976000},{"_id":"themes/next/node_modules/verror/package.json","hash":"c4e208d0f759b3ca2dfb4c659bf64cb4d76c06de","modified":1567332486130},{"_id":"themes/next/node_modules/verror/README.md","hash":"4464bdc3da677f07d0f22e1b14f81a81cdee75f0","modified":1493743176000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1559399176361},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1559399176361},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1559399176362},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1559399176362},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1559399176362},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1559399176362},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1559399176362},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1559399176362},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1559399176362},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1559399176362},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1559399176363},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1559399176363},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1559399176363},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1559399176363},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1559399176363},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1559399176363},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1559399176364},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1559399176363},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1559399176364},{"_id":"themes/next/node_modules/lodash/LICENSE","hash":"99f74d4ffae1d1b98bc104c8ff125dd7e7bce729","modified":499162500000},{"_id":"themes/next/node_modules/lodash/README.md","hash":"8d6bbbb26ebb4c46af54058805d59318a8256960","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_Hash.js","hash":"572ffd33bbac9360f2d2591f7b44f72077d52297","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_DataView.js","hash":"f5780de8f4171ced0e5f4c52aba30e58660070e4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_LazyWrapper.js","hash":"9fc0cec80e680d4fe00cf4e33672e7f06d4b9b79","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_ListCache.js","hash":"6df951af4fe1c2e428720e3fea6f7be3e6c64edc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_Map.js","hash":"d58862090ce31a3295cb1dae73e44dcdba9b70bc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_LodashWrapper.js","hash":"d1b595bd38e7764539f2cf99d2374f5c5aa72d70","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_MapCache.js","hash":"afed02775f092be6e7328f19b6991a419baac9c7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_Promise.js","hash":"7c7b81e303018a3045ca269b1be551224b607790","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_Set.js","hash":"d55df157d59851d181aae5086ac2b703f41e422d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_SetCache.js","hash":"20a1a687b681a839dec78ee9b1c07e0fcee4cdda","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_Stack.js","hash":"581507149d88de85ca4bed49ea0c22467d4e65e6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_apply.js","hash":"cf8c13531bb2ebaaa912ed42cd51d35749780b49","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_Symbol.js","hash":"1d38c59f19e1b038981b4f0093d5b504d647c694","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_WeakMap.js","hash":"f384e74a27955f5ab92345281a5fcb7d95111b58","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_Uint8Array.js","hash":"9aca80cbc7c3b25813dbeb9561324d3d6ff7700a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayEach.js","hash":"904f07de69303a57591f6f012ae390d37af3f595","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayAggregator.js","hash":"4c46ec909fd72aca86fbba95f8042b39875a2fc2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayEachRight.js","hash":"d9c9880639a791b9b7a50fa4b9d95987a9adb530","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayEvery.js","hash":"1c52b5548127fec0d8e876e6612bc7579b64c88b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayFilter.js","hash":"de05ba1636ccd6f97f5527aaaa7bcd355ef96a3b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayIncludesWith.js","hash":"ae055079baa36a646be539921afb899598b99ade","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayIncludes.js","hash":"c1211511564c2074c24cd0a45e2fc63b94017d42","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayLikeKeys.js","hash":"3e0338e1cbbfb9c9f1b00adf873e176478b985c2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayMap.js","hash":"50a635afd8bfba130f5ec9cc693c06eb0ed57690","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arraySample.js","hash":"8b9ade97c8c183d58e1bcfbdcdf4b77b44c38245","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayPush.js","hash":"8b9d6d4bcd8f6661ee23634aecda75d5395db2cc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayReduceRight.js","hash":"59ee39e14d1be368c9de8d06b8a62ba7a57f9c18","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayReduce.js","hash":"87b47db2e2ffbf2d671e1a974eec644d16424412","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arraySampleSize.js","hash":"cfe8e3d04b52e678f1475ed3df969bc5eedea859","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arraySome.js","hash":"b3e385ef53a7d44b49f705315ae30efc4b81d5b9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_arrayShuffle.js","hash":"7023c88e8b1fd408755327bf70fe8d58c9496607","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_asciiSize.js","hash":"befe650097f784878651dd1881da7efb3c0d0e73","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_asciiToArray.js","hash":"4331bba9d396c601f9ac9e676283bc3ac25ccf8d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_asciiWords.js","hash":"164db22da4ac8a77d75ff57a1d63d0c4b9e48388","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_assignMergeValue.js","hash":"7fd48396b2393692858a13cd1d9fee4f9f56c034","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_assignValue.js","hash":"8889cd8e010ef1ab8259b9401f1ace23164a676a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_assocIndexOf.js","hash":"b557d0550d466f2e0a20697d953061c8198bc3f2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseAssign.js","hash":"b12b76d29619f0fadf5c75c2b1f4d9c5a140af4d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseAggregator.js","hash":"a0f27fd540785f2f3d54913950fcd555ca789b4f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseAssignIn.js","hash":"1e9e2424746865f1ea7c278592329917a2d2ddd8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseAssignValue.js","hash":"c09bc3ff6aa7082144eef5b7583f795f7d16ac19","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseAt.js","hash":"93dcc3d90a67de6c407d3f6ccda164d3fac140b9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseClamp.js","hash":"ce109abcb1e12c30e97c79917e51520aa544db9a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseClone.js","hash":"69b00b4294919176a661d32d879bd07dab1b1bf3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseConforms.js","hash":"f518eceb5bf9e6dc4c9cf811e07c3ff8469c902b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseConformsTo.js","hash":"f2f1431ffc02fb8ab64d86abecdcfe2685d2eb4b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseCreate.js","hash":"202aced68f2ce65d35a3d9e7ee56e373de463233","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseDelay.js","hash":"2a9133e727e4b0a6a9ec776addec9c791c812a4f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseEachRight.js","hash":"f8f31ce8197c50e01ffe073be5e329bb95c5dd7f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseDifference.js","hash":"b3dc957d88a325a9c4e11645134bc92eb807dec8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseEach.js","hash":"8df288083999c6711cdddc83d3f62ee092e0bd35","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseEvery.js","hash":"6e445241a2ceb58e400c619247c7f1d5a9ad76da","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseExtremum.js","hash":"931adb7b69f0e4683324797c8522f4f6c9ad6321","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseFill.js","hash":"474427d0d470b473ac37456e9ba151a966b2ade7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseFilter.js","hash":"aa90047b0a1231f770807422c67a36a4c3524365","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseFindIndex.js","hash":"a6811f11035fd20ebedd10b014566614f1fee155","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseFindKey.js","hash":"dc99c1f56a9a4559ae3fc9f3dbd7f8cfac677949","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseFor.js","hash":"6d4e2744c99f75df073ca2e005317feea00533f6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseFlatten.js","hash":"045ce95760998450c806995fd49db7ce8fd96982","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseForOwn.js","hash":"8977ba022a8d144093f35c729822e6161d5fd291","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseForOwnRight.js","hash":"29a6ce1c8ff0893a7a5021a5b54d3303dc7e9ef5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseFunctions.js","hash":"17d2e75a62a380223a943a07c0726c2d101c59ff","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseForRight.js","hash":"2d3155596f8e8420da54564a69dadfadbd84cd67","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseGet.js","hash":"2c7bfaa44754b5302e3bd2137d95b4e045bc4c85","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseGetTag.js","hash":"b66f083cb3c9fac408a18fa8e104a20e64355d45","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseGt.js","hash":"7368ca132e9b25b5f2ae74cfc01d619f284e84d3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseGetAllKeys.js","hash":"f7c042fc3a0e1f89ee6f0ab2765615983796b303","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseHas.js","hash":"0643a432e7d64d10615b4cd7092f2483c317cda7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseHasIn.js","hash":"e11103f369bb4fef0cbd193622c7126c70e0fc8c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIndexOf.js","hash":"554c47e0383ef62d126d9d66aeb416e8e75ebf3b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIndexOfWith.js","hash":"bf10b85520bfc5add8a38c341e95a985e1ab3626","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseInRange.js","hash":"5186220a6519de4a9b0882ccbe565b74caea68cf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseInverter.js","hash":"84de1267be8b3b1973a696826a66247ff88e7c25","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIntersection.js","hash":"deed35c6297d406b8f5e0672a4cc99cbc95fd0b1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseInvoke.js","hash":"b287bbe200beaf0e4825bc7bccb7ff702e0eff74","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsArguments.js","hash":"6c428b2e543a752140ee6b1223a659f45025a71c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsArrayBuffer.js","hash":"f154818234664296ec6c8496be8cbe77c6909c8e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsDate.js","hash":"c21fd699a4d3d38d28770746f33992778c3e2e94","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsEqual.js","hash":"7f9ae050c867190bf8379e97f1fd9d01cc2c96f3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsEqualDeep.js","hash":"becc7c47d3ad5aaf2a6e5df7038595f21ff92bb3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsMatch.js","hash":"68d520389a8b2d00f0b649787df02734bc00ad9f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsMap.js","hash":"b28a7248f4a44300ecc8080ad259944d4bb9a356","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsNaN.js","hash":"01512b16931316dc87e670d11c3b3d7c354366a9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsNative.js","hash":"1f36b939f5e5b0dc5f28ba7c47206d717b966c7e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsTypedArray.js","hash":"291f63ba52ab97764a33305efd699a2e394b2517","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsRegExp.js","hash":"663e2101fd6aa3086bddedb08f0d2e7bbc30c0f9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIsSet.js","hash":"52795ae38cc0426dca2468342cf28dc2f78775f2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseIteratee.js","hash":"032e8e56ae36205075f11cef6a7e91890a8425bd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseKeys.js","hash":"29d99b03db5dad4affa41a388470c89595e44991","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseKeysIn.js","hash":"eb88d9e4d4f429c250f3ee2f22312b885ce7a75d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseLodash.js","hash":"dfebf25a399f77aa4b416e1a92ba11ca59c2a8e9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseLt.js","hash":"92e621a7798bec567f79b006e1608a434b18d22c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseMap.js","hash":"e4d4a4031106e999c0f44a81dd62afc0f84f3fe0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseMatchesProperty.js","hash":"0a4a914477d3d5a170f0c5bc7c5b2abd106bbfbf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseMatches.js","hash":"bb2332567191ebdf1e2414efb7279d989fa223ed","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseMean.js","hash":"0e536b0da31875ca4a97989a76d851b5589cc867","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseNth.js","hash":"7afabca34c7963943eab1dc70a433231283cc8c5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseMerge.js","hash":"7351665a72f5448392a3dce645946fd1368c5c87","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseMergeDeep.js","hash":"b20f414c41bf4446a4723ad001247e14568a91b7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseOrderBy.js","hash":"c76b0438a27df64a71220d2e819d9baa79d9641f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_basePick.js","hash":"1eb8ddfe3a55363b22404e9cfe3723a56c9715cf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseProperty.js","hash":"31d8d961f7383f581a360947b0b752e95d35cd14","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_basePickBy.js","hash":"2749eb539f38c35b694e6b8f74d20f5cdc37ca15","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_basePropertyDeep.js","hash":"b9329f5af409f1578f2fb9fd34ccbdf8cbe54070","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_basePropertyOf.js","hash":"33b60f71f3cbf5f26607a2b7356070f8f6953223","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_basePullAll.js","hash":"a140d4775d1ee69b9d6d434d5729f82b7e55bc8e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_basePullAt.js","hash":"7b291db76db38fc2850529f7e519f5a273dd1f5b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseRandom.js","hash":"86fe0bbec2bf3b242a8d861abd894f9281c2408a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseRange.js","hash":"8816c954f5f5c9c100a51ae5be416a4661e418ed","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseReduce.js","hash":"96e87d251b7bbf30707b39b529243e5d3a7f5c2f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseRepeat.js","hash":"e5acd9c33e9026b8848de16db87d9ee1dfe293db","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSample.js","hash":"bf50189b8b304ff4bafec174b108a7fe25a6a003","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseRest.js","hash":"e31a38bffa598aef97317e7b1970a212a4d44d00","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSampleSize.js","hash":"b05eb913d80f60e274e00632d5499319bd0204f2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSet.js","hash":"0d99172a2005d7c0dfa71114b0cb0aca9cafd47a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSetData.js","hash":"fa3ddde4d03be0d225fb7e96d1b5f86f0e321453","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSetToString.js","hash":"4fa840e62be63264b265b19fff3dbad4540a376b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseShuffle.js","hash":"62b58e90df1ca7ee5d44cd923fd531f1964f37a7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSome.js","hash":"4be43bfa949a8292f8f780eeeb83bb5da00f58b7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSlice.js","hash":"959756556fea236343ff752adb6b869bcac82e4b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSortedIndex.js","hash":"6a353c6feb19a86665f9ea37ae0ff7f5ff5a1d9b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSortedIndexBy.js","hash":"23b9368bcfe7afb35249a3a2bf880a170cc72ccb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSortBy.js","hash":"77483347d5381f336bed3174c5b301f40dfc7a24","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSortedUniq.js","hash":"c2ad97df99636c7977b6374e44cb61dedd64aece","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseSum.js","hash":"e96a801668916a3027558a8795a4957e11c791f1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseToNumber.js","hash":"1d6f1a52fdbddf557ef5d870c3ad6fecb7089af9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseTimes.js","hash":"7f3edc2bfd65d843b592ff123759c9d616ce3a64","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseToString.js","hash":"f2a75bd83872d649b56a1c0fe7b785ee13ac3cf5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseUnary.js","hash":"5dbba7edab4fb0d361906358fa2d49dc35c1e603","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseToPairs.js","hash":"d6ad1c204c16fee82d17a5b4febca922a78cda47","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseUniq.js","hash":"ae7942f5facf5430171081eb0d2ee341f69c9fd2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseUnset.js","hash":"d9a16888d3995c743789eb2003d8d5c438f00ca5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseValues.js","hash":"a8cfeda4f95c958cb46ac71f77a631e57617c1f7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseUpdate.js","hash":"12682de8a3a7f8d6e89f4872bbc2bf1c2b66f8b0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseWhile.js","hash":"4fb4c4f09282c0e560a1bb289caaac7d58d64a10","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseWrapperValue.js","hash":"1b1d0a42b03e95693fc88d6a24e4a3473d15392c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseXor.js","hash":"0077b838e654fac3f5ebcc99ab1dafbd158e4e71","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_cacheHas.js","hash":"d676b44402e7e4ff04011e54a086f7efe819ef81","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_baseZipObject.js","hash":"7995093c8f43666bf1f2dc15cc1d752dacc55e35","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_castArrayLikeObject.js","hash":"0a049ad2b5e466bb2173738787645d6aa97b9e03","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_castFunction.js","hash":"af88042305fb1cc58dde3bca997c206dd0359bf7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_castPath.js","hash":"a7a03888171be1aa22a92d12b9a02b8c8798375c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_castSlice.js","hash":"9b963fc73a75bdb6cde5c38b2ab1fac07835a800","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_castRest.js","hash":"82020a1342ba8656765ac9141410d1af2b7730f0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_charsStartIndex.js","hash":"c4db2a95b12b0a442f73d3ad3d27c0048193b216","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_charsEndIndex.js","hash":"4164a27a922efe0dfb1dc15496d199152fd96599","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_cloneArrayBuffer.js","hash":"ffbada38bb191bc5290fc3ae51f6c01c3f8f6d7f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_cloneBuffer.js","hash":"f0b54438e44d83118f342434cabe99dc438d19c3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_cloneDataView.js","hash":"c8e9d1cd47aab00655a7ad7d3103cdddf2fd92a6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_cloneTypedArray.js","hash":"b50a151594d841fdb56e8cf04619529fed4891f7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_cloneRegExp.js","hash":"d0318f8d44ff0e8720cf498982e3183e23579919","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_cloneSymbol.js","hash":"8b27dca2415595e97e38d9fa7fb3f854c4051048","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_compareAscending.js","hash":"456fc00f7549c8ea36a3d2f3b818b50bfcf016e6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_composeArgs.js","hash":"18e0c7dbd15ff30fb10ccaa4ef651751eea88ff7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_composeArgsRight.js","hash":"b826dac099cb5bdc350361b3fb6df6a3c743b49d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_compareMultiple.js","hash":"798b323506b11fa866904e39771daf98df260bca","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_copyArray.js","hash":"cec6a29544aa38261f7cfc1bf123f3289d22c377","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_copyObject.js","hash":"3bfeeec32b43433683d931621a2bf53dbc3db3d0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_copySymbols.js","hash":"871986e31fddf247cd65b93aa92b69392b9bde8d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_coreJsData.js","hash":"15e60e7a6363ae6df943c85e4b9e1078457b06e6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_copySymbolsIn.js","hash":"92a72efc51ad106ed6bc2e4dc03d0f23751523fd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_countHolders.js","hash":"9ff8f78133806ffbcc60d281c0152897aa7640bf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createAggregator.js","hash":"261d83069fe95d76ef0dcf16aecf4d21af132baf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createAssigner.js","hash":"0125f6e53a2b16619993e408f3511862079717e5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createBaseFor.js","hash":"77c64903d26461aaff6bb22d3e2739a5ac9a5e0e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createBaseEach.js","hash":"ade31f23b8e144e8588af940219fc4c8bda2ce53","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createCtor.js","hash":"12be272e2dcd00624119d859d78459c9dae1baa0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createCaseFirst.js","hash":"4eda4cc9710593950268262489e79a507679356f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createCompounder.js","hash":"195c16455366a158b8b855547fafdc602f0be9e9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createBind.js","hash":"56e3d29a4d8774c9709ba42dd15e204a693a8a42","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createCurry.js","hash":"30c237eecdcbbf00b48be6b716264799e0f32c1e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createFlow.js","hash":"c9408be96649f6431781578ada26509a567005a8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createFind.js","hash":"3eb3975cd88173810656e3ab17de9c210773a1f4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createHybrid.js","hash":"15282c26201bfcfdc26829ced58fa9785f1db1fe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createInverter.js","hash":"3b149fffb0fb7fe82cf00dbd794973ddc33f33e4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createMathOperation.js","hash":"d02710cbdbf76e2b2001750b355238b9a0e951c9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createOver.js","hash":"ba8f63fcb4755dc9023793118e5f8180e86c4c64","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createPadding.js","hash":"0597ae4997016438b3d7629560c03d260f2b51a0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createPartial.js","hash":"2dc87c9d5a3fa04f144a85d4d90cfd55a4e4b208","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createRange.js","hash":"1c65b8adbfe1a38fb5827f584a454bc8218050ac","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createRelationalOperation.js","hash":"428d4535498e0c63f8a998bca3403d36ddbf8004","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createRecurry.js","hash":"fa869537d2d2bb656d1a7876435dc4c79f57575e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createToPairs.js","hash":"b16382972bd68fefa26ec9672c3935cfc1af1b7c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createSet.js","hash":"c213de450f86338c23b81de1c99f8136a7538ba3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createRound.js","hash":"f4640823857ab5cc4a35b87df0b475c215e8147a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_createWrap.js","hash":"3cf0222feec115f3d4b71c1adbfe4aca7555ebe6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_customDefaultsAssignIn.js","hash":"3cfaa6d8045c1f0217366218ad4340b50de5ceef","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_customDefaultsMerge.js","hash":"9378ba434e10108ffac43ded9d6d28e32fac527e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_customOmitClone.js","hash":"2bf571311b6785fd53ae57ec710ed05ebad80ab7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_deburrLetter.js","hash":"65de6e749f5e37659ce5798a558a9aa1b50bec5c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_equalByTag.js","hash":"bd57ba1568c522e436bd1af7ec860bcb1e274b85","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_equalArrays.js","hash":"dd3a74350ab4d89beb889ba94510b3e5723b6209","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_defineProperty.js","hash":"308f9b9c0f8822351b71d5801be25a8326bc5584","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_equalObjects.js","hash":"6e408bc1b739bd195adea727a0f9dfdfb778a214","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_escapeHtmlChar.js","hash":"d1da9dd2828cdd0071b82ffea56eab694bafbe37","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_escapeStringChar.js","hash":"d011ca793a93e44d7e6928cced2055ea47f6c39e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_flatRest.js","hash":"1a96086a9a21bff7890036498a1e161fe7955194","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_freeGlobal.js","hash":"df0709e144040efc52114c69b7977ab04f3a799d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getAllKeys.js","hash":"9fcb354c326cd308df841e6846d506165842f9a3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getAllKeysIn.js","hash":"4c08d556179109df462c2ffdecef2d6486315062","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getData.js","hash":"2fea3c244fa4625b978cea63124835c16aefe660","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getFuncName.js","hash":"dd42afbc814c94e5bdd2fbef652efb7801403f6e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getHolder.js","hash":"86951aac23219e1a51267bf5fd7a38da110c877b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getMapData.js","hash":"f8986a4db2b55203a3237e5472a4620816306331","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getMatchData.js","hash":"221908980050132b19c79c53c1b7d78ef5b93d59","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getNative.js","hash":"a3c1a7f525a79c6216c2bd72810ccb1d8afefd9a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getPrototype.js","hash":"150397078aa95ac1ace14e9547b7c28eba508353","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getRawTag.js","hash":"08573f8b464cf110d0873d475890f499a9620d80","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getSymbols.js","hash":"1c87ed98ef573b41c8a015c15857a0991603eaa6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getTag.js","hash":"e74c13874eca982f0fd30b1e35dd05e0664c6af3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getValue.js","hash":"aaeaa4d7600367187650a1ef2d6949e2f788cf76","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getView.js","hash":"3a642a1adfccdac302261e31fadd3918e623d6f6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getSymbolsIn.js","hash":"0bae489598ad76c4f2faaf33bcc1b3bf34e4772b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_getWrapDetails.js","hash":"ab6c90ec5be32187a6cfe2c9e798e7d298263dd4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_hashClear.js","hash":"7a6f43944d3ff13552dcbe9ceb972ae5a83e8b1b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_hasUnicode.js","hash":"2ead86c95243b0dac220736e198e0a14e68c2982","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_hashDelete.js","hash":"5b552f18bab9b54a3192e193d44188e1656556d8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_hasPath.js","hash":"f300ab1b5c17494a458e38d10eb815f5c1c95b76","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_hasUnicodeWord.js","hash":"96f0a2876411b1fba3916ad58533f2dbc6970256","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_hashGet.js","hash":"5645b9091466891d4c892c6b3bd220ebbd3a3d00","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_hashHas.js","hash":"aec7a8cd442a620d0073260ab7dc03960f6c85c5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_initCloneArray.js","hash":"7385f50c6d9fd89db402b725dfcd5b0d6b314c55","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_hashSet.js","hash":"13d89d46c86111ca9941d0c35c29a84b2fb97a87","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_initCloneByTag.js","hash":"a38217b57e875466ed7d07578884c51a4e318560","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_initCloneObject.js","hash":"f2ee8035eaa7751ff081de5681c6603da177f8f6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isFlattenable.js","hash":"f2cfab82055d2ca693f33a874c57d8250467fc04","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_insertWrapDetails.js","hash":"07c0c750900d12cf01ff9d99e429c6be76790f4b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isIndex.js","hash":"f08242d01da4236214f75e0221fe25c270f46dbf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isIterateeCall.js","hash":"8116de0f8e735fb8c0a9d263a369be4b74f5f09c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isKey.js","hash":"e68a3f3cc1d7942c51ce0e8d927c0eb1896bc46e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isKeyable.js","hash":"89389f3a2f157c03608bcce01baf751018f25fa8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isLaziable.js","hash":"857a630edb35c21d76e6e2fd6daed3f3b51ea6a9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isMaskable.js","hash":"54963a24b696b2c57b2e8d2a873ed7db9abe6b11","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isMasked.js","hash":"c001b1b4f9d3f7ece2e7263350f081da707804a5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isPrototype.js","hash":"6ebbd7e2661dbf8b6e35f36d680b7117fcc6b2c0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_isStrictComparable.js","hash":"1db231f38b860e0a94e052c232829f8e8d31eb74","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_iteratorToArray.js","hash":"0512af6634544643fd2dacfb328aa8fca1770d72","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_lazyValue.js","hash":"8e87e755c6f452304a814a29998fe145065aac74","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_lazyClone.js","hash":"528f2c4e0ddaadd246d07757608fafc02842508b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_listCacheClear.js","hash":"cab24e3a5aa064726b71f761bac42e765a21517b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_lazyReverse.js","hash":"a16d79ce7d467ebd00332162dafae250f726fe17","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_listCacheDelete.js","hash":"1421610bad35fbee226a1a6bf8e7edb8958a4d6e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_listCacheGet.js","hash":"266ac036effbb03c4a20397401900068f25ee510","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_listCacheHas.js","hash":"2070fe4515274fd0445b01ccbdad8acbc6b7c9a3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_listCacheSet.js","hash":"cf902d42ba5e57849d08b0c03d2003c56bbc0dde","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_mapCacheClear.js","hash":"0eff5c92ebddbff48603ec60a32720c48fcdfd2a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_mapCacheGet.js","hash":"2f580dedb6aa86e87c9931a6bb2275df8fe8608a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_mapCacheDelete.js","hash":"830ab65244dcd7c5a00fec9be355e9f713e6ea3f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_mapCacheSet.js","hash":"745d2c8aab1a63cd28192eea4ac7bd86d5050b02","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_mapCacheHas.js","hash":"a0104808ea49ff57061f2b4a6ec4ef5f28e20601","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_mapToArray.js","hash":"ba760ad87c596f8a038240cdb8eb21c8f441a288","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_memoizeCapped.js","hash":"6e577b3ba7b897f8a8e6277cee2a9c0d296608d9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_matchesStrictComparable.js","hash":"886455dc09cdf350cadc57a4bb14cf8d700db358","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_mergeData.js","hash":"88ffb153b2cf2792dd67b70ef0f7ba1715c991fa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_nativeCreate.js","hash":"d77d487728a34b77fe91c07ed4a2ca41a7265b43","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_nativeKeys.js","hash":"9cddc8c7f8efeffa319763be371897c861027d65","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_nativeKeysIn.js","hash":"d11eee972281d8eb8c93724d6a9088faf33e9679","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_metaMap.js","hash":"6fe08e4a7c81c515eb34e464db1fe31d20bc8ff6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_nodeUtil.js","hash":"23c321c5b794557e944b47a10ccac8eb6a132f0c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_objectToString.js","hash":"c489ccc8b47254177546b747476ca6b4a339437c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_overArg.js","hash":"1fd1876980219dfd22c7ad86a49994f6ad1ffb0d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_overRest.js","hash":"7020d98e117801d3a38b53367295588fe9574282","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_parent.js","hash":"a509f65490a88db6e1469ba7d8eacf86f11e7578","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_reEscape.js","hash":"1956c226d87ffb9bfb9d0b443e5e7c6665e222a9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_realNames.js","hash":"1d70870e01638a3ba3e742bf871a5618ba866480","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_reInterpolate.js","hash":"1f30c7932ca224ade79f3b50991da604fdc93c0f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_reEvaluate.js","hash":"01062fe9dad608255c7e341c6d3e145c9ae6912b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_reorder.js","hash":"f9ebaaf8bddda0f54270363ed88d8eeb4b0acf95","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_replaceHolders.js","hash":"c24095b10ee93a2b1c4eabe187a37eb54a74b47c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_root.js","hash":"da38a7e1c62b9dec425d3354b1ae5b2f1473ed41","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_safeGet.js","hash":"638d92d4782a94e948a266bcc257a824cb2182ba","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_setCacheAdd.js","hash":"3e8d192f136058af7953acb255f3452e25f9dbd5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_setCacheHas.js","hash":"6b635d8cd68ddaf51b359bd7e89532dbbd5d259d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_setData.js","hash":"27ba08bcf3045df1eb5bc20514f6532d51049445","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_setToArray.js","hash":"8aa29aac79d989a3642638ba90120860d6e7cfd0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_setToPairs.js","hash":"8f144b728d7afc8059364769d27708f6f30f1bc9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_setToString.js","hash":"525d4e7a92d2f5de834b7199c926bf05e5863e02","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_setWrapToString.js","hash":"e1129dc30bd9fe222dc11cf9d319f2d16cf02f82","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_shortOut.js","hash":"2b821fd23499f0583817e801c9ec1aa7fabac0b5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_shuffleSelf.js","hash":"d384018a208bad370c0fa749fe1a7abdcf7d1550","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_stackClear.js","hash":"f10a4d85636bf0a0495cb7222575c139c2963466","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_stackDelete.js","hash":"0035fcf868dd14e85188c010466ead0b44caddc4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_stackGet.js","hash":"edd10aa8b1688c112def671585164abd168f7dc7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_stackHas.js","hash":"2c66efa2be2222610dae4edfe711b8b0d9a55a60","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_stackSet.js","hash":"54d1b0f6e92c9327efb7142d082bea43f49521a4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_strictIndexOf.js","hash":"4d373aae516f6097b486413f2e0815c73aa37bdb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_strictLastIndexOf.js","hash":"541978faa200571da921f5cee0c141cca0436237","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_stringSize.js","hash":"2c19f863e40752a52b8d3347e8b05c8afb61a4d6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_stringToArray.js","hash":"051613c1b18c5156a676b197586fb0e2c1acd0ec","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_stringToPath.js","hash":"90f48337b274bc8475116e992fa8cad995a4029e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_toSource.js","hash":"12282cc7f18c3c7446129f547741c3998b6caf18","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_toKey.js","hash":"e24143e8602a9acd35301e6462b03c6ef0d94bc9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_unescapeHtmlChar.js","hash":"345300e6aa36d8b10133adfc3672efc630f77822","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_unicodeToArray.js","hash":"a9378af7302c381a5f82467c32de3111ca597877","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_unicodeSize.js","hash":"4ef1b8b07c56ba22c273f20ce1f43c99d9e646af","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_unicodeWords.js","hash":"70d7dad3b79c471cce28fb00f4d9491c6d14de30","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_updateWrapDetails.js","hash":"2ef2e983cdda0be8c850d7e59d570862ce6b64d7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/_wrapperClone.js","hash":"c54b050119a2a1ad75146508a03ed4a78ab9b5fb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/add.js","hash":"69ccfa17792261411f27bd7166a6760e3e6e653d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/after.js","hash":"4d65d6ae8537ba01400d74e3540b52b463dc86eb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/ary.js","hash":"6db228f04677c2724e6b8438681c4c0fff0c998c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/array.js","hash":"ac9338e3493fec2bb6d7a1dcef45ef9819b20649","modified":499162500000},{"_id":"themes/next/node_modules/lodash/assign.js","hash":"c80789cd7fd35c7190290526e466eed28caafa80","modified":499162500000},{"_id":"themes/next/node_modules/lodash/assignIn.js","hash":"166da8ae436b3ede129a5842457d68d6fdc9616a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/assignWith.js","hash":"98f683f41fd043bead258cfc6e61bd6765eea779","modified":499162500000},{"_id":"themes/next/node_modules/lodash/assignInWith.js","hash":"7e951bb44bd108e1024ea5c691d9bf9449c112fc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/at.js","hash":"cf037c2d8d960fa96854f59381675ee196cf4069","modified":499162500000},{"_id":"themes/next/node_modules/lodash/attempt.js","hash":"6b981c1f3ff53c572ebd08babf8799c8f118f6f6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/bind.js","hash":"6b960133235c585a9b461e19df07cdd6edff118c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/before.js","hash":"bc0b86f3e03b7056b39193a03c5d39ee4fef4023","modified":499162500000},{"_id":"themes/next/node_modules/lodash/bindAll.js","hash":"cfd017286d7a7497547755272e1a55d39dc4beef","modified":499162500000},{"_id":"themes/next/node_modules/lodash/camelCase.js","hash":"1db1aa1a3c7a3a703a33c660526ef141c0bd3b94","modified":499162500000},{"_id":"themes/next/node_modules/lodash/bindKey.js","hash":"b54c9311a7c9550ffceda021af5c995eee09f693","modified":499162500000},{"_id":"themes/next/node_modules/lodash/capitalize.js","hash":"cf956ec612dfed45fdab26ab30cac6f46d738119","modified":499162500000},{"_id":"themes/next/node_modules/lodash/castArray.js","hash":"8dd901032216d00cf43986adaf961ad7422a64bd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/ceil.js","hash":"13eff7a830076aeea2ffabd36b8d0d3254849953","modified":499162500000},{"_id":"themes/next/node_modules/lodash/chain.js","hash":"e51b12d085b20c8676bf3297d9d0a8acb3871e4f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/chunk.js","hash":"8e0f7e51303031640b539758ed6fa3e48e761783","modified":499162500000},{"_id":"themes/next/node_modules/lodash/clamp.js","hash":"0c356936d91c87fe7938faa3e1bb61b8334e7f0b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/cloneDeep.js","hash":"fcbe65493c8f2c4fb574dbdafaab5ec22ad3ac42","modified":499162500000},{"_id":"themes/next/node_modules/lodash/clone.js","hash":"1268809f4924a5640d425e56eb4cb71df51ccb72","modified":499162500000},{"_id":"themes/next/node_modules/lodash/cloneDeepWith.js","hash":"2be99cb83b8e6ac3f488963b433a8dd1829d3599","modified":499162500000},{"_id":"themes/next/node_modules/lodash/cloneWith.js","hash":"e9108f222791a26a83106488509053910d8460b5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/collection.js","hash":"235db0c51d2fbba77c1c0ab2f25617e3c02d9e5d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/commit.js","hash":"f2962f416c4427b7d5fd2fe8f752c41a8e6ee53f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/compact.js","hash":"f747cf703e5627de69652da7ef01299274cf48e7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/concat.js","hash":"16777de6eeb0cb2828722e48c32e900256c21bd0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/cond.js","hash":"3df6aa9c3966438f382511f0b5ab7647ef5da87c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/conforms.js","hash":"e226b49b5c96a85b5a1796b7a708c3e3fdd9fd9c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/conformsTo.js","hash":"db8ff09750867c73d76ad1275a5cd8c2bc93b873","modified":499162500000},{"_id":"themes/next/node_modules/lodash/constant.js","hash":"0227ab051a5a8ca5e0fc7934bcea54ec25f90292","modified":499162500000},{"_id":"themes/next/node_modules/lodash/curry.js","hash":"cd66258e0cf4fc625293d358d15a7bda051bfa67","modified":499162500000},{"_id":"themes/next/node_modules/lodash/core.min.js","hash":"83f066b03527ae4b73fc3ec77e118c055b5349ae","modified":499162500000},{"_id":"themes/next/node_modules/lodash/countBy.js","hash":"048867f03021430a606b699403133e2e234777a4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/create.js","hash":"41e8f34b4e9429b4b7ab26264bc570214164a5b4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/curryRight.js","hash":"61586b4ac19da0224a3262027f2d280f25bd70e2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/date.js","hash":"6466f5749384af915c2b6a449e74e14e320f4e11","modified":499162500000},{"_id":"themes/next/node_modules/lodash/deburr.js","hash":"460705fd44df7613129dc58981966abcc45f9488","modified":499162500000},{"_id":"themes/next/node_modules/lodash/debounce.js","hash":"8c53b1737148a94705d5ff80e476916d2649d9cd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/defaultTo.js","hash":"07e0f352f98a791be53a0a6eb43dedd756acbfbf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/defaults.js","hash":"1f12f29efd3d103440d5c2cf8895119205ec67eb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/defaultsDeep.js","hash":"bf8d24913d8a34b40f1190a2d4fd01cadde49263","modified":499162500000},{"_id":"themes/next/node_modules/lodash/defer.js","hash":"471017a76ca3f3f71a111ccf7b487f89a50327db","modified":499162500000},{"_id":"themes/next/node_modules/lodash/delay.js","hash":"d7b8c52b90e17f9e42413ac7ea16d39a8e96bd4d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/difference.js","hash":"7a2a5465430dd730ff198a6ffcd5a3de8660c7b3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/differenceBy.js","hash":"ff88a7541ee0b628ea3bbd923077facde3dbab36","modified":499162500000},{"_id":"themes/next/node_modules/lodash/differenceWith.js","hash":"0f2a92e973c183bdcff6395d840f683f318c3424","modified":499162500000},{"_id":"themes/next/node_modules/lodash/drop.js","hash":"6e25554b3f395b8e55d6b291451e7cac2149c7af","modified":499162500000},{"_id":"themes/next/node_modules/lodash/divide.js","hash":"b21dc4cea11b1da27812a545fa75aa0d6b9a834c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/dropRight.js","hash":"e77c7613539baf9a222fcc57142d5a808263688e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/dropRightWhile.js","hash":"f9ccd85e2dc3364b0fdfbc824632e9f792b5a11d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/dropWhile.js","hash":"f710aa52d7e2a9b128e647ec58b24e3c37660790","modified":499162500000},{"_id":"themes/next/node_modules/lodash/each.js","hash":"dbfc772ff0330e9d0bb2ee704242e67b0435929a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/eachRight.js","hash":"c7a90b0a7bec56aea41c7569a652ebf17c20fde1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/endsWith.js","hash":"0d8fd73985ee8c845b0c851704c8700f1055863d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/entries.js","hash":"8788b9edcf643acf20e434e5894d09fb18742112","modified":499162500000},{"_id":"themes/next/node_modules/lodash/entriesIn.js","hash":"995c6287a9c5f6641931c966de331b5521f63d3e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/eq.js","hash":"bc3769b5b5cb8262834ac0a28cc4cbe71257ce94","modified":499162500000},{"_id":"themes/next/node_modules/lodash/escape.js","hash":"60bec78e2fd63cbd52dc8f4f5f5b242bfc9449a5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/escapeRegExp.js","hash":"91892447637ec6b53595e90a9817241b536f5fbe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/every.js","hash":"6da415b41c0bae9c96fd40df253df2de7d965f4c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/extend.js","hash":"79bb5928a674d6122686fc0df5a28f00b22d4d0e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/extendWith.js","hash":"fe394cac415eb0a6518371d1b95e08144a682526","modified":499162500000},{"_id":"themes/next/node_modules/lodash/filter.js","hash":"5e630c86a2bd2a83a887241037b6d714401502d8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fill.js","hash":"bb9b5af63b9b43626fc4db976072b045976ea77a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/find.js","hash":"bb6c19af408438f6a0e19998a230880354afd814","modified":499162500000},{"_id":"themes/next/node_modules/lodash/findIndex.js","hash":"2fc5db1562e58221e0aafbbd317240704eb29985","modified":499162500000},{"_id":"themes/next/node_modules/lodash/findKey.js","hash":"eecde37bba06739228e5f93eac97aeaab4a91d4f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/findLast.js","hash":"1f3be6d3e3eb97733c9a648872b4b3366d590257","modified":499162500000},{"_id":"themes/next/node_modules/lodash/findLastIndex.js","hash":"05743b963c7370d830ce76ddc5da4d85ba687935","modified":499162500000},{"_id":"themes/next/node_modules/lodash/findLastKey.js","hash":"9ba6afda847da380659ae98f0cb575566a0dc85c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/first.js","hash":"a2eac20812e95b557a5bc11dab060163fe1133d0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/flatMapDeep.js","hash":"76d3ae49d148850472922956935d294957f6f04f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/flatMap.js","hash":"78f9ce2566f143366b0998ef02dd8dc89e004133","modified":499162500000},{"_id":"themes/next/node_modules/lodash/flatMapDepth.js","hash":"fb4c4fc58c11b646dfdac83443e7495829da8f45","modified":499162500000},{"_id":"themes/next/node_modules/lodash/flatten.js","hash":"72364918453483de6a71137a9a8003973c141fad","modified":499162500000},{"_id":"themes/next/node_modules/lodash/flattenDeep.js","hash":"422f037a6379f0db6258b9337261c1a9cc719faa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/flattenDepth.js","hash":"537a4e7196561ecae367985622872417cb19eabf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/flip.js","hash":"67207c1ba95e6881968ff8fd5542f29e259aa3d7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/floor.js","hash":"dcc9365721002de964aa2439793b7a10419d3dad","modified":499162500000},{"_id":"themes/next/node_modules/lodash/flowRight.js","hash":"3e028361a9b090f8e4166d89fba374d65d52711f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/flow.js","hash":"7d9e4b712b68551d5b932bd4ed88d15b4aa1722f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/forEach.js","hash":"0f3074268fefe6db115334ca5dd1b89eb56a8a3d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/forEachRight.js","hash":"654c60d4bb9737aa9fa3a7ac55e889f3dc2b50e3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/forIn.js","hash":"61f83e8b63f97c099842046983acd61ac1b31ff2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/forOwn.js","hash":"3e351a24c7fdf0593c116f361f59739cfb447950","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp.js","hash":"15a13940a645e776e00a229a7bec48f6af716505","modified":499162500000},{"_id":"themes/next/node_modules/lodash/forInRight.js","hash":"6a060f1b26a6be2a99c48eb56d104c4684b9bb30","modified":499162500000},{"_id":"themes/next/node_modules/lodash/forOwnRight.js","hash":"91e99b73af52bd6155bef7a259aa2e763aa4b885","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fromPairs.js","hash":"9a5f37cb42d2839c2637ca1792fcc1cd4333c56d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/function.js","hash":"23850ac10d90d3b9eee8723beaa3af727330323e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/functions.js","hash":"0ef50ad30d371a2b7ecf4363fd7862b5417130af","modified":499162500000},{"_id":"themes/next/node_modules/lodash/functionsIn.js","hash":"29e78c6d69d36b48c62e4374c6cd1c02da73da40","modified":499162500000},{"_id":"themes/next/node_modules/lodash/get.js","hash":"fd2860fd39bfc6e8c44bae101e133effe6373734","modified":499162500000},{"_id":"themes/next/node_modules/lodash/groupBy.js","hash":"18d1063f6af8b9cb6901c5ad3257db49027ab77b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/has.js","hash":"c3c35131b701a60eb1f6bf9e530c78d123268cf9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/gt.js","hash":"8446ef3704d57dd44214e6513a6889f4d9db658a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/gte.js","hash":"ec5d91c1d30601111dd6a41af35d55c2c27498ce","modified":499162500000},{"_id":"themes/next/node_modules/lodash/hasIn.js","hash":"8a4bf01f81a8d508b322441659f9c74f38998c3b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/identity.js","hash":"af8a8f8af76663a408cf9f29e5723d05f79eb236","modified":499162500000},{"_id":"themes/next/node_modules/lodash/head.js","hash":"374b879fa821ee60e31efa35f0b847ac5ba770af","modified":499162500000},{"_id":"themes/next/node_modules/lodash/includes.js","hash":"b5d81439dbbb9b6558c9148e37bf5db918a42d1e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/index.js","hash":"14d8b25e9e35c7f1b551d732fb814c982fc5301e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/inRange.js","hash":"38cdcc3285d54fcce6a7b362642c8c93305782f4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/initial.js","hash":"c366df8c3c272bbc5fded392830b438d30d8c4a0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/indexOf.js","hash":"eec5a54dc7285759af46028f7a7c1ec3479993f4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/intersection.js","hash":"b51e7675e09c00ab99a632cdf1fac769f1a874ff","modified":499162500000},{"_id":"themes/next/node_modules/lodash/intersectionBy.js","hash":"969e82efd3befe0749085336f9568eb81f55d7b5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/intersectionWith.js","hash":"1e1c0b6cf9c37aa60ea316a1540245bbff6f7a8a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/invert.js","hash":"c9547b3840dfef6cb81c6507f35a5109798f1a7c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/invertBy.js","hash":"1d85aa7dd69490d38441a0206246460fd4c0f95d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/invokeMap.js","hash":"bfbac7243fe9a57276626d7090d473d9a3b2d796","modified":499162500000},{"_id":"themes/next/node_modules/lodash/invoke.js","hash":"5bdfcbcf073fddbf4f00903e4a8db0f1b868c8e8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isArguments.js","hash":"b9214a907e11a8e2321e6adf907ecde99b9920d8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isArray.js","hash":"c6d5635dc764c9acf7395abde2230c43121641a0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isArrayLike.js","hash":"88d7cd3fc8a6075777cef6c16e1cc4931734159a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isArrayBuffer.js","hash":"b682ff167ce2b34539afe73df93f2abe81b8aac5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isDate.js","hash":"3a7813e7ff48c9ac6ce3bbbc8584b1ce22c4ea1c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isBoolean.js","hash":"ace779f61262f820f5f4ed39695bddaa7ebd08a3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isArrayLikeObject.js","hash":"300effec49e91a13bfaf9b6a9dda4ea2c4d45d92","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isBuffer.js","hash":"246d03a17c274bdcf3acb909f2337c695c9aea45","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isEmpty.js","hash":"935d1e74c2b8a3deb928af20d4440894368ccacb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isElement.js","hash":"652c3967fe6b11419aaa8652a8d8d6e35f15a76e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isEqual.js","hash":"370af44d612ddc96cdc52779fcc844fdf4ae00ae","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isEqualWith.js","hash":"3e9d8f6214fbb2f859254ea9c4b8f9da58ac15a0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isError.js","hash":"b184db78799044db3af29c3f6224d8e2213c9985","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isFinite.js","hash":"388faf4e73347a1c0d77c98a3ac92be8902c5f36","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isFunction.js","hash":"6cb0b30f9e48fc93016a3f34d124224af7901908","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isLength.js","hash":"45606d651b8071d8ef73beb03d31d6073ade0df4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isInteger.js","hash":"ac93647df1dbe92f6358a992b1d5e7039cf85514","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isMap.js","hash":"d19970b50ffa3f5e8345e84c635dc7b553bca992","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isMatchWith.js","hash":"60bf9f292afffa501f83381428457dc7567ee4cf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isMatch.js","hash":"29b9682a150068960adbf9ffa6b1da7f2a45477b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isNative.js","hash":"58dd206b6a97ede6c6c786ae029a36739c5efe7d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isNaN.js","hash":"711cb119497d074148b07dda40eef0fa7519de8d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isNil.js","hash":"5780b6ca6e7dee6c2599a9ac77eb302812a373ca","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isNumber.js","hash":"75152f3218ff6f88b1ad22a258b16c85ebbf9408","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isNull.js","hash":"bd68451baaba662307893e3216b99ed20cf8a0d2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isObject.js","hash":"38f00e3e4772d23d56550ef75959d10f92448e3f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isPlainObject.js","hash":"104adb7c6dc9854cb91ac5995b17969c44614262","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isRegExp.js","hash":"af11b03682761c1292525e290f0312da974c02c9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isObjectLike.js","hash":"7213913d764526ff2189c115d354bb940470bfde","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isSet.js","hash":"8753a0c0f5a73feb22942035f87351120db0f402","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isSafeInteger.js","hash":"18473b31325225abe68ccaa69aa84635ab02b651","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isTypedArray.js","hash":"d92eaa92fa13419d2df184ff11f270c266590f66","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isString.js","hash":"7eb7706298dea12e813be37db6cdc8ea92177630","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isSymbol.js","hash":"aaa0f4e5003e5d33ed7d67b8a274a9a14c3cd64f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isUndefined.js","hash":"4e087abc1b0997f71bbf33e2065b40d438511e2e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isWeakSet.js","hash":"2582214ba7a25b23bebf2ef4ad8a799248a5e5f8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/isWeakMap.js","hash":"d88fd7f6dab0bc1a14c8b7b9f5adabf3d5430ef6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/iteratee.js","hash":"5e22aa3f442a49c5061a7328b39c7ef197b9a249","modified":499162500000},{"_id":"themes/next/node_modules/lodash/join.js","hash":"236f8abe4dfa446351d07525089fabee2e2d3f0b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/kebabCase.js","hash":"ed770da5a2e3ada160f9dcd625a94d16ffa8fef0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/keys.js","hash":"84fe8dcc57c7b368d5fbd3ccf0c194cde0c78515","modified":499162500000},{"_id":"themes/next/node_modules/lodash/keyBy.js","hash":"96e32f75404ee87c0fe8bf2d8b7a2c69cee308a7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/last.js","hash":"dcdc5c04379818603815dfc64e96bd5e2626fd29","modified":499162500000},{"_id":"themes/next/node_modules/lodash/lastIndexOf.js","hash":"64b8f7981a889c14b785738da32da89ea7b06519","modified":499162500000},{"_id":"themes/next/node_modules/lodash/keysIn.js","hash":"e444c4b265f9f39968467a82c09935bfa1be501a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/lang.js","hash":"2e1998c6fa6bd4a5c691edd1eb1fffaea2a35b85","modified":499162500000},{"_id":"themes/next/node_modules/lodash/lowerCase.js","hash":"6280f901a37fb21a9acc5df8678661c1dd47d425","modified":499162500000},{"_id":"themes/next/node_modules/lodash/lt.js","hash":"244011c8209227a5d714f5843106b13de0f5fc15","modified":499162500000},{"_id":"themes/next/node_modules/lodash/lowerFirst.js","hash":"6b7a190cd8d1f01254d0556b7725ed8ad3ee9cb3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/lte.js","hash":"02b0d1b5d68ff6e8cabda05c1e5a4c9675b10479","modified":499162500000},{"_id":"themes/next/node_modules/lodash/map.js","hash":"484de2e64aa6e9cadbac92703bf79be5dbd53942","modified":499162500000},{"_id":"themes/next/node_modules/lodash/mapKeys.js","hash":"6c2885c6249ff8dbd5c18910338240ddcd441e48","modified":499162500000},{"_id":"themes/next/node_modules/lodash/mapValues.js","hash":"7d273ea92cb971d298e84137f49b89c5d7249336","modified":499162500000},{"_id":"themes/next/node_modules/lodash/matchesProperty.js","hash":"993988bbf9037d03aa3ec66e3d6ba3db185a58e9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/matches.js","hash":"b760524ca6b6ba617641aa4353e7598c0cebe50a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/max.js","hash":"2c5910535b9bc06c066c57f71b5b02965b894cfe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/math.js","hash":"09b7cdb94b0829a362572cd9eb490c6b86471f35","modified":499162500000},{"_id":"themes/next/node_modules/lodash/meanBy.js","hash":"5a9d6be568a698c6ef430a07f3431c11bbba3dcb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/memoize.js","hash":"507cd3e0ae52967b105d0452fd0f93fbbc69bfc9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/mean.js","hash":"17efb674e29e2456f3b1e595210a9929480e14a3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/maxBy.js","hash":"85f841238f0c60a3496eecb25a3bf7aec5fc36fc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/merge.js","hash":"20f06dc73414778002395627c8d4316699ff947b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/mergeWith.js","hash":"18a5592394d4cf1a0772e6ff38bacb9b547105f6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/method.js","hash":"6dedac7c45efb248890558a8d42a2656623ce0a5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/methodOf.js","hash":"6af752f45f4152f036fb378075d72f6ab82573ea","modified":499162500000},{"_id":"themes/next/node_modules/lodash/min.js","hash":"0ca11cbe1894931dd09246e001badc22eb734887","modified":499162500000},{"_id":"themes/next/node_modules/lodash/minBy.js","hash":"8fa3de3c1efcc246ee906a281757f905cb13507e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/mixin.js","hash":"813b628b7fe94024fa308fbc6d06d1883dee9325","modified":499162500000},{"_id":"themes/next/node_modules/lodash/next.js","hash":"8504ac753cf6e3e19d811038503ee000b7829e8a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/noop.js","hash":"35300b7ebb741e7a54fe528a8a718b26c35bf698","modified":499162500000},{"_id":"themes/next/node_modules/lodash/multiply.js","hash":"47e4d5d6014da46305a0be1082d66d2f41928e82","modified":499162500000},{"_id":"themes/next/node_modules/lodash/negate.js","hash":"a01d8c228ce6992ec79ee4146677e52dcc487254","modified":499162500000},{"_id":"themes/next/node_modules/lodash/now.js","hash":"ddba13c834824d9467e379769e67f414a6179d15","modified":499162500000},{"_id":"themes/next/node_modules/lodash/nth.js","hash":"4c0c437715ed4fb0942b8dee7273b9d0dc5f62e0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/number.js","hash":"8c2564ca466fcb7f4f0decd5eb79f803f39983f9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/nthArg.js","hash":"47191997e9ebf3b2b8dd667ee362e48d381217fb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/omit.js","hash":"874d0d4204aa0cd72c79d7b3a4c80a577f46b564","modified":499162500000},{"_id":"themes/next/node_modules/lodash/object.js","hash":"4b9c99e1cb054bfcb6777077f59a2a53a4759490","modified":499162500000},{"_id":"themes/next/node_modules/lodash/once.js","hash":"5bb536b8c7f08f94ccc9019a51601cc97d9b24d3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/omitBy.js","hash":"4216fc454388cdba0292cc5199b2d28d7484ac7b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/orderBy.js","hash":"7665eee9891d204d48cdd799992dbd6a9678125a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/overArgs.js","hash":"e60acc78f35d412319457c41a3bcecc898f9653e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/over.js","hash":"e28a6b283217955bd749ce30edeba3e6b977273f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/overEvery.js","hash":"03c5153a4c43d913a9162032f5041cc1f982250d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/package.json","hash":"5ac203ca9a6c85da3b5874cb33308bbcc0a67a31","modified":1567332486129},{"_id":"themes/next/node_modules/lodash/overSome.js","hash":"153452d7a4349ee32447e45f59e975f929de5c19","modified":499162500000},{"_id":"themes/next/node_modules/lodash/pad.js","hash":"b6a0479a2ce0289d29214e8f7ff83a0074406d25","modified":499162500000},{"_id":"themes/next/node_modules/lodash/padStart.js","hash":"d6ea6887328ca596d505af7b181edcad862865f4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/padEnd.js","hash":"5973017726462db292d41c2f28b53b8a4a24c507","modified":499162500000},{"_id":"themes/next/node_modules/lodash/parseInt.js","hash":"786b6be24ff93b68962925663708aca1236f2cc8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/partial.js","hash":"5c9f9b1c6719f70ceaa7fa314193f7ed4ed5770e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/partition.js","hash":"36957b0323cbd50f5ec8e4be3b5cf571241407da","modified":499162500000},{"_id":"themes/next/node_modules/lodash/pickBy.js","hash":"99eb4eed1acb33f67be3b41f359601f0f6d75ae3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/pick.js","hash":"9c8c0c82b1db06c624523239aea143410c5258be","modified":499162500000},{"_id":"themes/next/node_modules/lodash/partialRight.js","hash":"7c551ed65cc1b160ab8a7abb78b9e37c3b426e5f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/plant.js","hash":"2b370614388e8fc4a0e12d2eaa644c25622de026","modified":499162500000},{"_id":"themes/next/node_modules/lodash/pull.js","hash":"e40207e049af5517cc5cfd59ffd23b1c3a1406dc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/propertyOf.js","hash":"d23f88041fae20fcd9b92740fffd39016a6715ff","modified":499162500000},{"_id":"themes/next/node_modules/lodash/property.js","hash":"276a7afddaacbac4fa184f0096a92f32ee517ced","modified":499162500000},{"_id":"themes/next/node_modules/lodash/pullAllBy.js","hash":"5dc628f7fe0c8e71edf931bebecdaeb0e30d6f2e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/pullAll.js","hash":"1bb4ac35a97475e16cfa8a10abfc5d911c58d743","modified":499162500000},{"_id":"themes/next/node_modules/lodash/pullAt.js","hash":"176790538e4f38124826d06462d089f8acfe1b0e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/range.js","hash":"6bc797b60a5f0b1fe8e93c0c4c41361c7f8f1189","modified":499162500000},{"_id":"themes/next/node_modules/lodash/pullAllWith.js","hash":"f205f46a35a1b3d40f86c92dd9488e25c59cf5b9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/random.js","hash":"d64df688329131b297c269ce8e10b44cabe52e66","modified":499162500000},{"_id":"themes/next/node_modules/lodash/rearg.js","hash":"ef1d75bb13e2f07e1737b028878775f67ed2b50a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/rangeRight.js","hash":"6640704aba8af652f6f8d13758946125dc324e08","modified":499162500000},{"_id":"themes/next/node_modules/lodash/reduceRight.js","hash":"8c8897c831800592470dc2fec9f1f575f844b2fe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/reduce.js","hash":"a6da684222d6a2179aa409e3728758572961152f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/remove.js","hash":"a91465f281fb69fc2cf3a9fa751acfe89c5d95fb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/reject.js","hash":"6b18138c39a26986320b80d9748bf5385c2b0c71","modified":499162500000},{"_id":"themes/next/node_modules/lodash/repeat.js","hash":"8a5c222ac1514d5732374f165d8df9dc0274bd71","modified":499162500000},{"_id":"themes/next/node_modules/lodash/replace.js","hash":"a35231e4112316c03e6c65162affe9effee35922","modified":499162500000},{"_id":"themes/next/node_modules/lodash/rest.js","hash":"61b930cc021a8f03bb466ac833596d6d37416714","modified":499162500000},{"_id":"themes/next/node_modules/lodash/result.js","hash":"7180962c1b72ea113dd6516d5c28d006080e97f5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/reverse.js","hash":"09f86bfe19d080bb52606967a9df5f017333e963","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sample.js","hash":"b861fdd759aa14bb6747b0e333df66b8995300b2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sampleSize.js","hash":"7a14f0d38676fcf8ed64a12ac379c8203eda1e4a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/round.js","hash":"629b46fafec63f591a3c6b02ee03529ef43a8acf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/seq.js","hash":"e8dae0be3b8ece53ea5d6594d0b45d221f643576","modified":499162500000},{"_id":"themes/next/node_modules/lodash/set.js","hash":"bd5aefade6c00bbf37fd21dcd79e0ffd085a6084","modified":499162500000},{"_id":"themes/next/node_modules/lodash/setWith.js","hash":"012090d3598aeb4e36cadb5fb3ee7ecd9d184211","modified":499162500000},{"_id":"themes/next/node_modules/lodash/shuffle.js","hash":"1d035fe3d9776decf38cab2e19e2b2da99aa5dfe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/size.js","hash":"baeb54345201e06c8c096f9a4f62192c8584f982","modified":499162500000},{"_id":"themes/next/node_modules/lodash/slice.js","hash":"fffe0c44921ec28dd2fc1512493b912411d06628","modified":499162500000},{"_id":"themes/next/node_modules/lodash/snakeCase.js","hash":"b95290beac2d4eb4436d72bde0b191a8db147970","modified":499162500000},{"_id":"themes/next/node_modules/lodash/some.js","hash":"b6d44fdefb9894407b8d48ff379c6288fec935ee","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sortedIndex.js","hash":"e31955eb8a9dcf9868e2df8f112a8ef873709970","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sortBy.js","hash":"5f928288640eec63d9ce82e3c16750b4952f5aa1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sortedIndexBy.js","hash":"785066b03fae1e7fa508b7380f95727d02284d7f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sortedIndexOf.js","hash":"21e71189ed17c2769fcd2c91625af1a65b84591b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sortedLastIndex.js","hash":"b886560112d50f2aef5f1acdcc5cd9296ff6f8b7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sortedLastIndexBy.js","hash":"4bf9117b8f1d0161a8369f127220b897819cd6a9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sortedUniq.js","hash":"612a221cb2088447b6510930f35f7140ecf41d1f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sortedUniqBy.js","hash":"e9ee6345a66fbef43ac161edd568feb2f197a2ce","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sortedLastIndexOf.js","hash":"f4a56782a4a913c330d832d80000e0340093e020","modified":499162500000},{"_id":"themes/next/node_modules/lodash/split.js","hash":"f33b9e68923d6bd511a37658a794104452f1bacd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/startCase.js","hash":"5cae4afa2846579e8173a62ae27253a75fb57119","modified":499162500000},{"_id":"themes/next/node_modules/lodash/spread.js","hash":"b5e45484e55917c836dfc464d1c1dfb5e016a53b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/startsWith.js","hash":"10e9857ce37a805453b6e1c5e0caac454d08ba89","modified":499162500000},{"_id":"themes/next/node_modules/lodash/string.js","hash":"9da506e6420a685f56f77ccdcdd4dfe0ffe552b0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/stubArray.js","hash":"bdc0e5ed494f2924ffd64e91620fbcd8161f7223","modified":499162500000},{"_id":"themes/next/node_modules/lodash/stubFalse.js","hash":"e5b35ac8e1872e9c87f126c972891005c94e7d19","modified":499162500000},{"_id":"themes/next/node_modules/lodash/stubString.js","hash":"053b1bf13c37041494b3afed57eea26d24b46c59","modified":499162500000},{"_id":"themes/next/node_modules/lodash/stubObject.js","hash":"dc710610031cb2255019c0bd8465df0d6db0111e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/stubTrue.js","hash":"35d765e8ae55e010a1b5eda7aa6596a14a052ef6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/subtract.js","hash":"6927a0ddb3037e644d7d49f7aadd99f45ac2226d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sum.js","hash":"b468caad43dbd33443e584c510697155ec12a71c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/sumBy.js","hash":"9a28d421380fb4347c651dd4935461c69a465251","modified":499162500000},{"_id":"themes/next/node_modules/lodash/tail.js","hash":"9f994bae44d6e96a75142cfdbfb362f1f8190d8b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/take.js","hash":"07615e176130761c61262b882598a6aab8bbbd48","modified":499162500000},{"_id":"themes/next/node_modules/lodash/takeRight.js","hash":"aecb30e9e3d49c9194b57e4c7988a11b4e911170","modified":499162500000},{"_id":"themes/next/node_modules/lodash/takeRightWhile.js","hash":"b4562aaf5ce999f9c4b68b1bb5296da5799e4e37","modified":499162500000},{"_id":"themes/next/node_modules/lodash/takeWhile.js","hash":"7ab5baee95d7a1aa64183a266176725180d74c8d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/tap.js","hash":"a1f5e1966c3620b69def36154a27a8d9893d87f1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/throttle.js","hash":"dc6c20077c14d5b184f3471924032e242c9ec3b9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/templateSettings.js","hash":"fa92b30409e71f025035d7a47921a061b46f70df","modified":499162500000},{"_id":"themes/next/node_modules/lodash/thru.js","hash":"d006fad2ab19f7bc90e0ec69eb5037f081d99bee","modified":499162500000},{"_id":"themes/next/node_modules/lodash/template.js","hash":"aac2f4654f470c3c50c945dadf70062223ea7dbb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toArray.js","hash":"5cde4e05caa391716f89e179755c861677498c0c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/times.js","hash":"ee9fd905cf9b189471102ac6b4a75b745085ce48","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toFinite.js","hash":"44b15d3279c2f94ac2098503c427e1e0eb34c3f4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toInteger.js","hash":"06e857ac41a5b7056f7b522db601b4679f04f5ee","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toIterator.js","hash":"f8fc807ff29125bb2dab968633ad964ec94194b0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toJSON.js","hash":"dcaa02fb24d8915128f62a50e2782e30d7d4fe8e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toLower.js","hash":"3cf1e67f3e193b9f982b8814fec29cdf99b6b038","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toNumber.js","hash":"2b6f75e579b5ec0e84426be3ce1fd0d3a1abec18","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toPairsIn.js","hash":"ddc2a974ba685da424c9de9e2f7949b662844273","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toPairs.js","hash":"48dbccab7f479c713471fa2378d5ffa402745f77","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toLength.js","hash":"9a0a62cbf4f795ff3efba1fd3b1a84cd5ec94ee1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toPath.js","hash":"973bea7fc9852f05f88b1e11a768b7b7ab786990","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toPlainObject.js","hash":"4e96ce3252e389b1633a2c73fc9d68031a12df29","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toSafeInteger.js","hash":"c49cb9237787895a00956b20b569bd384c684269","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toUpper.js","hash":"5e6fa81c32735a5c142884a1da0b1d88b780d05d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/toString.js","hash":"6c4365360e107172a439c756d9be5db5470b1142","modified":499162500000},{"_id":"themes/next/node_modules/lodash/transform.js","hash":"2cb75bbd3d23239daa2681abefea716e5f7e0bfe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/trim.js","hash":"de870b0426ce1ef6506652312b8298aa7caa16fc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/trimEnd.js","hash":"c18ce828f8e810a17d423d75705261ff77b34462","modified":499162500000},{"_id":"themes/next/node_modules/lodash/truncate.js","hash":"24f4a9506442f2cafc20a024b9d79fb6e5df3326","modified":499162500000},{"_id":"themes/next/node_modules/lodash/unary.js","hash":"079326455dbdd90b20704f1a3abf73cfab59d088","modified":499162500000},{"_id":"themes/next/node_modules/lodash/union.js","hash":"2cd89b9ad0a574cfddd5daa4c5c95ad71db6b4ee","modified":499162500000},{"_id":"themes/next/node_modules/lodash/unescape.js","hash":"89bc8a104147bab3b23f5e6f1f1c3819c6168293","modified":499162500000},{"_id":"themes/next/node_modules/lodash/trimStart.js","hash":"fdf32db72208d65d988d3a7672d2d22a2bce89b3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/unionBy.js","hash":"0d110e48caa5b130f45e00bb7d65dcebe66e275c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/unionWith.js","hash":"16881474335022100f41832178f2109f7525061d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/uniqBy.js","hash":"5591128502a52624cfadfc2d4c7b20db4011267b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/uniqWith.js","hash":"0a37f6a99454364d2683df713e678bc9669d6684","modified":499162500000},{"_id":"themes/next/node_modules/lodash/uniq.js","hash":"2b735c46c626dba6b45907c8c70f17475366016f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/unset.js","hash":"5f2a9664870f21d2b7dcf0255f50ad9b37a6c850","modified":499162500000},{"_id":"themes/next/node_modules/lodash/uniqueId.js","hash":"0809eace58255a033eb37e73ce581e2cc6fe26d1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/unzip.js","hash":"9adc3e8903f6134c27951f3a558286a7667656d9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/unzipWith.js","hash":"13255720f48e23ba96205aa49fe5bb538b3c640c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/upperCase.js","hash":"4ce59b5839e848688ed76fc99d62f35168f9f46c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/upperFirst.js","hash":"091005e45cf8b9ce4f51e1b2ff2da30558b40714","modified":499162500000},{"_id":"themes/next/node_modules/lodash/update.js","hash":"abae1fa2fd4b937bb1cf1f4a0fb4a73eb9d4aaab","modified":499162500000},{"_id":"themes/next/node_modules/lodash/updateWith.js","hash":"dd63e7b716f897c2bc9f7d39884e0282ba81bccf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/util.js","hash":"9e3bd39fa03689fa7d9897be9c2387e19646662f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/valueOf.js","hash":"dcaa02fb24d8915128f62a50e2782e30d7d4fe8e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/value.js","hash":"dcaa02fb24d8915128f62a50e2782e30d7d4fe8e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/values.js","hash":"6304ca5a2c8a30cb1eef8cfb80aed716e29d3495","modified":499162500000},{"_id":"themes/next/node_modules/lodash/valuesIn.js","hash":"5d6cd8fa3358eafef39b236579720c3d6c4450c0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/without.js","hash":"979bbb5dc3e1e17ea311c4d85a47b5b8b5c9c58a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/wrapperChain.js","hash":"0b6afa60d6fedd5ae935c2127562ba2a590b5b14","modified":499162500000},{"_id":"themes/next/node_modules/lodash/words.js","hash":"74ebd3c81508567ca14f57f717488d4cf337101f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/wrap.js","hash":"956d1896e8a55677cef08f879674d4de21d45184","modified":499162500000},{"_id":"themes/next/node_modules/lodash/wrapperAt.js","hash":"4974d8fad13c6913d78fe90db2de8cd83a811f47","modified":499162500000},{"_id":"themes/next/node_modules/lodash/wrapperLodash.js","hash":"b20f7aa45935b9d38b954bc7fc07b0a15298df77","modified":499162500000},{"_id":"themes/next/node_modules/lodash/wrapperReverse.js","hash":"2318334d83fc88acd2b7fa81c1a709b957719f4a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/xor.js","hash":"77e7c70d299ec7430efd31d411e46bac4dd0450d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/wrapperValue.js","hash":"dcf31c73fbacb8ab409179125d9ffe51e1ec8696","modified":499162500000},{"_id":"themes/next/node_modules/lodash/xorBy.js","hash":"5e7d74900860098de29d83658deb9eccc1f7b840","modified":499162500000},{"_id":"themes/next/node_modules/lodash/xorWith.js","hash":"ceb3b359ce93812b82f622e0c117585ba83b2996","modified":499162500000},{"_id":"themes/next/node_modules/lodash/zip.js","hash":"2df2c5d391c0c7fb710357e33d6a9f16425d66fa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/zipObjectDeep.js","hash":"a4cd8a063e87152a2229d1493af4fdd0419d3ff0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/zipWith.js","hash":"8cb8637b0b2b1174922ef35138408a777911267a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/zipObject.js","hash":"fa1c2ef419f663fe0b552effed4b64428cb52f15","modified":499162500000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1559399176338},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1559399176338},{"_id":"themes/next/node_modules/ajv/README.md","hash":"3e809a7073cbed5737735a4368e3963d83451946","modified":1562793001000},{"_id":"themes/next/node_modules/form-data/yarn.lock","hash":"dc0bb50636fa8f779471708f86657a37eb41422f","modified":1518539554000},{"_id":"themes/next/node_modules/hexo-recommended-posts/wechat.png","hash":"cab76bdce6717d1b81902ba247c9c72881e6b812","modified":1522437487000},{"_id":"themes/next/node_modules/request/CHANGELOG.md","hash":"c19e990e25828536705fd6d8c6db4410380201b4","modified":499162500000},{"_id":"themes/next/node_modules/uri-js/yarn.lock","hash":"ed46014fa76c4a547f30fdc35561bd4962a0842c","modified":1525380349000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1559399176357},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1559399176357},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1559399176357},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1559399176361},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1559399176361},{"_id":"themes/next/node_modules/lodash/core.js","hash":"5f7dc5bccf42598a5a38a0d761e340f3f708342f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/lodash.min.js","hash":"32f09ec3ec0950f47a35fc0d656559d5b164dacd","modified":499162500000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1559399176335},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1559399176335},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1559399176336},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1559399176336},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1559399176336},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1559399176336},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1559399176337},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1559399176336},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1559399176337},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1559399176337},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1559399176337},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1559399176338},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1559399176338},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1559399176339},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1559399176339},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1559399176339},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"1cd01c6e92ab1913d48e556a92bb4f28b6dc4996","modified":1559399176339},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1559399176339},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1559399176339},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1559399176340},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"fc65b9c98a0a8ab43a5e7aabff6c5f03838e09c8","modified":1559399176340},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1559399176340},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1559399176340},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1559399176340},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1559399176340},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1559399176341},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1559399176340},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"10160daceaa6f1ecf632323d422ebe2caae49ddf","modified":1559399176341},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1559399176341},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1559399176341},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"aa0629277d751c55c6d973e7691bf84af9b17a60","modified":1559399176341},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1559399176341},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"cd47989f957b06ed778b2137478bfb344187abc0","modified":1565171004804},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1559399176343},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"8b6650f77fe0a824c8075b2659e0403e0c78a705","modified":1559399176341},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"385c066af96bee30be2459dbec8aae1f15d382f5","modified":1559399176344},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1559399176344},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1559399176344},{"_id":"themes/next/node_modules/ajv/lib/cache.js","hash":"ca2c2b8d327699c615a231ce5b3d2ea2207ca558","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/keyword.js","hash":"38bec594109b68406c2b34e0165adf14108805f9","modified":1556211191000},{"_id":"themes/next/node_modules/ajv/lib/ajv.d.ts","hash":"87c57bf4f15257083582abce67e774c2488a0cf4","modified":1562431780000},{"_id":"themes/next/node_modules/ajv/lib/ajv.js","hash":"486009383b996e8229ebc49d5b12f4af39712d20","modified":1551604577000},{"_id":"themes/next/node_modules/ajv/lib/definition_schema.js","hash":"39376c06203d32739f8a124624b4b4836a65e1c5","modified":1556211263000},{"_id":"themes/next/node_modules/ajv/lib/data.js","hash":"37b38a682fde67b977191568b2ec8c1629df504a","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/scripts/.eslintrc.yml","hash":"a9741a141e6a7184474a2cce07fa3b6651edd8d8","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/scripts/bundle.js","hash":"11eba606302f969b6d27975408a41b622a81f1e2","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/scripts/publish-built-version","hash":"c68147e86149df70c332c65f41de257452eb45f7","modified":1550826789000},{"_id":"themes/next/node_modules/ajv/scripts/info","hash":"a95450a7ad65a96a1c5e6c35cf4b58b765d24753","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/scripts/compile-dots.js","hash":"4d065f52ccb7866d81ebb7fb707348505a9c93f2","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/scripts/prepare-tests","hash":"6e39ebab96c5eaa9f384728e203435551158459e","modified":1550828388000},{"_id":"themes/next/node_modules/ajv/scripts/travis-gh-pages","hash":"d4319e13aaa6fab37f07912157029797da9ce24e","modified":1550826789000},{"_id":"themes/next/node_modules/asynckit/lib/abort.js","hash":"440629b13ff27be58720005cb549f38903d80737","modified":1465585654000},{"_id":"themes/next/node_modules/asynckit/lib/async.js","hash":"c5d0ae608872e8fcc47a465d51564b6576c49401","modified":1463755306000},{"_id":"themes/next/node_modules/asynckit/lib/readable_parallel.js","hash":"55538262a65702eb61a29e2ea3a20637490f96d9","modified":1465716911000},{"_id":"themes/next/node_modules/asynckit/lib/readable_asynckit.js","hash":"6a883852b3639a6c4a7e7e4fdefa2bd08c6ecedf","modified":1465752133000},{"_id":"themes/next/node_modules/asynckit/lib/iterate.js","hash":"219e4942b78515ab728825d33473715c246fd081","modified":1465585679000},{"_id":"themes/next/node_modules/asynckit/lib/defer.js","hash":"751e8dd7ef7b877d05e66eb8782f95ae78351c4b","modified":1463755276000},{"_id":"themes/next/node_modules/asynckit/lib/readable_serial.js","hash":"05bbc69be79b76f58faeb2805697ea9469c60d89","modified":1465715271000},{"_id":"themes/next/node_modules/asynckit/lib/terminator.js","hash":"1f8a78f42a386e65c9fb5cf621a90a2792dbb63c","modified":1465665521000},{"_id":"themes/next/node_modules/asynckit/lib/readable_serial_ordered.js","hash":"bf2240383cb36d9a191d5ed87abb5d8674ddd8d4","modified":1465859663000},{"_id":"themes/next/node_modules/asynckit/lib/streamify.js","hash":"cad24d3ac61e7d9ad281c5190c5cc4e4ba8e6f3b","modified":1465693502000},{"_id":"themes/next/node_modules/asynckit/lib/state.js","hash":"fa825d1971532d33a039d634de8aed494c09c08a","modified":1465664364000},{"_id":"themes/next/node_modules/asn1/lib/index.js","hash":"6034c7cc87cd727d2979a95eeb432203a3690b79","modified":1532629842000},{"_id":"themes/next/node_modules/combined-stream/lib/combined_stream.js","hash":"952620a43b9cbcb4134a3b8282dfdee87bdc5812","modified":499162500000},{"_id":"themes/next/node_modules/delayed-stream/lib/delayed_stream.js","hash":"097ab12e3837896d11d3f1d2c7a891d177b493c4","modified":1430160544000},{"_id":"themes/next/node_modules/core-util-is/lib/util.js","hash":"7d34435928ee9228995c04eaa7bdcef875c41e65","modified":1447979840000},{"_id":"themes/next/node_modules/dashdash/lib/dashdash.js","hash":"bcac2fce732aa9b1084e924d9ea49189d20cf60c","modified":1464847153000},{"_id":"themes/next/node_modules/dashdash/etc/dashdash.bash_completion.in","hash":"749ea2069b183cd2ef4e83f496ca754eef30c3fd","modified":1479854012000},{"_id":"themes/next/node_modules/ecc-jsbn/lib/LICENSE-jsbn","hash":"4f94910918b5e57f3e55c7387f43b0d6293a4319","modified":1532879415000},{"_id":"themes/next/node_modules/ecc-jsbn/lib/ec.js","hash":"d9a60c831fe91bd2c62ff3e3c9675b0379ef504c","modified":1532879415000},{"_id":"themes/next/node_modules/ecc-jsbn/lib/sec.js","hash":"b1632526290dfddc063f05ac6ed29cc40bd9e221","modified":1532879415000},{"_id":"themes/next/node_modules/form-data/lib/populate.js","hash":"1808bbf4f1478a79ea4f00b38bf46983e48c0ff3","modified":1472196493000},{"_id":"themes/next/node_modules/extsprintf/lib/extsprintf.js","hash":"2331ac9de23b7cc92510c3ead98efa1a8a2de8f8","modified":1425687068000},{"_id":"themes/next/node_modules/form-data/lib/form_data.js","hash":"0597bc0beff7f785b19e8be3e7565d183fb642e6","modified":1539761003000},{"_id":"themes/next/node_modules/form-data/lib/browser.js","hash":"0ae55bb7ee9e39bd3dda8d9b6e6d6bff3855d121","modified":1478555747000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/benchmark/index.js","hash":"17619f7e28ced52a18ff262b4e16d4dad2105deb","modified":1508860414000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/benchmark/test.json","hash":"9fe75dfdacdf2fa66188890ed112d4da355815f7","modified":1508861235000},{"_id":"themes/next/node_modules/getpass/lib/index.js","hash":"3e1d47c5742292c6964267fe3b1ef54a5918f419","modified":1493163595000},{"_id":"themes/next/node_modules/har-validator/lib/error.js","hash":"80e4d7c599adb31f6245d6b79bcc190f6ca061f5","modified":499162500000},{"_id":"themes/next/node_modules/har-validator/lib/async.js","hash":"a5f651ea4e3c4f4427ad69842c469bbca6f457a8","modified":499162500000},{"_id":"themes/next/node_modules/har-validator/lib/promise.js","hash":"a27b54732dd98f8d9e9900815e93032f75ad625b","modified":499162500000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/example/value_cmp.js","hash":"71a64ddc23c9ded23dc156a9a2a7baf55846c1ec","modified":1508836326000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/example/key_cmp.js","hash":"9e5797f7e3c351f6bfe5c4c72304b3048d1515c8","modified":1508836326000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/example/nested.js","hash":"1259337a975367c26a59b9d8dfaa91e0f0a612b5","modified":1508836326000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/example/str.js","hash":"9145496a1e5d513eda1a10298b412b3becf68d71","modified":1508836326000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/test/cmp.js","hash":"deff3391b199444278e9cc781fdf41b9ccf5bc98","modified":1508841742000},{"_id":"themes/next/node_modules/har-schema/lib/afterRequest.json","hash":"f7f70adc3b0512cc2e04133192ea4f87e0146ab1","modified":1492544568000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/test/to-json.js","hash":"2928abc4469f3b1b8af084795138a4eb82d83375","modified":1508841670000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/test/nested.js","hash":"1d7b098c0ce2bdc5549c083ab09a2d7470734a90","modified":1508841718000},{"_id":"themes/next/node_modules/har-schema/lib/beforeRequest.json","hash":"0722001e46c933b79fec640cae9cd1ffa39dcca7","modified":1492544568000},{"_id":"themes/next/node_modules/fast-json-stable-stringify/test/str.js","hash":"1e4807d200cc96dcddbb34ad2777a0b86060661c","modified":1508866010000},{"_id":"themes/next/node_modules/har-schema/lib/content.json","hash":"81d4f4003ddd3350eff241826c14c0b9cc6d7444","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/cookie.json","hash":"752404cc8cbd04ce0ac8c1fa32eb2dd8e248826d","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/cache.json","hash":"50bb69d7fdbbb6155f26a04ff72d00551eb36202","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/browser.json","hash":"4db37d46769f3ecf86aa5224c185d04d4c83f56a","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/header.json","hash":"751bf4dbbefb43a6f8752ae6c82b1c18be523cee","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/page.json","hash":"684677171808e6ca501390734f451eebfc981213","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/creator.json","hash":"1b471a27337a94c37d4c5ddc6e15b6ba14b7f422","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/entry.json","hash":"ec3c862d75ce851061e3c9c41dab28e3f7290795","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/har.json","hash":"e91c94dc990dce4a0f4f20f3972fd34d4a40cb76","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/log.json","hash":"39e98d99850119a095f065ab923ac951ef2e55ef","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/index.js","hash":"1e6ac13e2a43e93530c96896efbf395028e621a1","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/postData.json","hash":"753ec1ce064487fab23fbb0dc8cd3c14fdafc904","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/pageTimings.json","hash":"2646f23aaf7706b16932e2d0e0f407a685483b90","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/request.json","hash":"799df36973dfea0675ee63e187a05e831ee74361","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/response.json","hash":"7a5c6856928f38525919c20e71c8cb10664f240f","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/timings.json","hash":"2f02b1a5200eea10b0171913e032cc4d74f015b1","modified":1492544568000},{"_id":"themes/next/node_modules/har-schema/lib/query.json","hash":"da4c7add90f000ded1e872114ba3630460ccea61","modified":1492544568000},{"_id":"themes/next/node_modules/json-schema/draft-02/json-ref","hash":"31b76b211e24e56b0ff5ae3ba739835039cd96b8","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-02/schema","hash":"ba84a3e7e9332328db986a1123084f2a6dba57f7","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-03/hyper-schema","hash":"552aaa9921cbeb19c2c9b485e64965f7a98e22fc","modified":1472871200000},{"_id":"themes/next/node_modules/json-schema/draft-02/hyper-schema","hash":"117eb70efaa120edeb97caa6ca7e53eae8f761e2","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-02/links","hash":"f8a835ea08959ceca9682cf1c30df6d5cf9ff033","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-03/json-ref","hash":"9f121cedcda14cbdd55e2fd80a96e289d9a1f2f9","modified":1472871200000},{"_id":"themes/next/node_modules/json-schema/draft-03/links","hash":"5f1601142843a6028beb35cef8b851413a4dfa91","modified":1472871200000},{"_id":"themes/next/node_modules/json-schema/draft-01/hyper-schema","hash":"51802f7884c813fe2a002776aa97a440c7442d9a","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-01/links","hash":"09b2131c15d754a61a1baf1158a8e6c73644287f","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-03/schema","hash":"4b500e66b7c4a36bc4d3ff6da089d6228c54f94c","modified":1472871200000},{"_id":"themes/next/node_modules/json-schema/draft-01/json-ref","hash":"2132a5a9319f8bf190040b49150e005e0916ed22","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-00/hyper-schema","hash":"a5972bcc99abc4936ff350f116a69390f8ac99e8","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-01/schema","hash":"10942d74811e1e16425e6219b0ba808b493321ab","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-00/json-ref","hash":"b5f7e93c3edc2eb501e913441ddfdeef0c04386a","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-00/links","hash":"490c91fb104a16a76290377797a5411d21379614","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-00/schema","hash":"253c4fcfccaf49023c10a1a92d81cc3d21344e8a","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-04/hyper-schema","hash":"362194bf4c6690db17b848c1a796ef747bf7a6c1","modified":1472871200000},{"_id":"themes/next/node_modules/json-schema/lib/links.js","hash":"bc70a0d21a6639c78017ad1ab1254d8e1516bc75","modified":1472878014000},{"_id":"themes/next/node_modules/json-schema/lib/validate.js","hash":"c336cd87629b485336a56636cd36144220281161","modified":1472877970000},{"_id":"themes/next/node_modules/json-schema/draft-04/schema","hash":"7a6839ef21cf49fd6bdc687be7dbfbb92c97ac7c","modified":1472871200000},{"_id":"themes/next/node_modules/json-schema/draft-04/links","hash":"92cd269ccdfe3f3bc673205133cb9940afa465c4","modified":1472871200000},{"_id":"themes/next/node_modules/json-schema/test/tests.js","hash":"f3f322d2cec777f57df41b071818a05e97d35f2a","modified":1472871200000},{"_id":"themes/next/node_modules/json-schema-traverse/spec/index.spec.js","hash":"06e5616d7577f23f7c7b5989662e1f8f1d93fb0e","modified":1525808085000},{"_id":"themes/next/node_modules/json-schema-traverse/spec/.eslintrc.yml","hash":"d105e036ac5efe87b86bbe2a32afd751f03d9e14","modified":1525807808000},{"_id":"themes/next/node_modules/json-stringify-safe/test/mocha.opts","hash":"98216b4fbc2e0c1c17ee946b02272a26fcc1f4c1","modified":1431999690000},{"_id":"themes/next/node_modules/json-stringify-safe/test/stringify_test.js","hash":"4760401b04513ae7eb79fff0e1239a9947a8af78","modified":1431999690000},{"_id":"themes/next/node_modules/hexo-recommended-posts/lib/recommend.js","hash":"0b0b9c3f2d55783ffd0285a60e952d9dd2b143d5","modified":1526021055000},{"_id":"themes/next/node_modules/hexo-recommended-posts/lib/recommend_console.js","hash":"c33307ea40491b43b930a0d2ce0092da3b71fd2c","modified":1560285864000},{"_id":"themes/next/node_modules/hexo-recommended-posts/lib/recommend_filter.js","hash":"eb49375904e4d75b3daaa6e6800f8b647c6bfb8d","modified":1526021055000},{"_id":"themes/next/node_modules/hexo-recommended-posts/lib/recommend_helper.js","hash":"5a0f58312964c3d65fc1572f01c14334d076e63a","modified":1526021055000},{"_id":"themes/next/node_modules/hexo-recommended-posts/lib/recommend_config.js","hash":"4c1800976ae2815483cee26e812bfa6ec5ffd56f","modified":1567388547420},{"_id":"themes/next/node_modules/jsprim/lib/jsprim.js","hash":"794c578fd13608d65a0469ed74ac30fd3d15ebe3","modified":1494025745000},{"_id":"themes/next/node_modules/http-signature/lib/index.js","hash":"996c8efa1d7896034c980e498bec866824662d52","modified":1503623992000},{"_id":"themes/next/node_modules/http-signature/lib/utils.js","hash":"dce96d033869c424854178ae71991fe39b7c58b5","modified":1503623992000},{"_id":"themes/next/node_modules/http-signature/lib/verify.js","hash":"f0603457d003b72d484d608b266566e771644438","modified":1503623992000},{"_id":"themes/next/node_modules/mime-db/db.json","hash":"0a6ca4f701b66a06e8165e2dc372a72bbc39c727","modified":499162500000},{"_id":"themes/next/node_modules/http-signature/lib/parser.js","hash":"993a592d1b1c86a21b77a66f84e8cdeb7b6b780b","modified":1503689277000},{"_id":"themes/next/node_modules/http-signature/lib/signer.js","hash":"65d73fbe8f2c699e36d290e0784ec926d68b06ae","modified":1503624550000},{"_id":"themes/next/node_modules/performance-now/lib/performance-now.js.map","hash":"50bdd3412d57f86521de00afc24177415ee70f3b","modified":1487514525000},{"_id":"themes/next/node_modules/performance-now/lib/performance-now.js","hash":"441fe91cf077ba520a22bb82487439a292ed7e2d","modified":1487514525000},{"_id":"themes/next/node_modules/performance-now/src/index.d.ts","hash":"cab5735ccbd2666329124c3284b12c8aaa4a5881","modified":1487513961000},{"_id":"themes/next/node_modules/performance-now/test/performance-now.coffee","hash":"9a86c23591031408a9e2e5b17a9600bc82556f28","modified":1484002078000},{"_id":"themes/next/node_modules/performance-now/src/performance-now.coffee","hash":"1afaa86418105a30940d34c3f84dcbabced93229","modified":1483849977000},{"_id":"themes/next/node_modules/performance-now/test/mocha.opts","hash":"bb896e2b4872a971113be6df6cf74bb1ad34cdbf","modified":1483443554000},{"_id":"themes/next/node_modules/performance-now/test/scripts.coffee","hash":"ac50e35ea322260600fe12cfb49af3d37678d77a","modified":1484000743000},{"_id":"themes/next/node_modules/qs/test/.eslintrc","hash":"19532afcfb2ca609b9687da0d7dd20cdb4ccf7e4","modified":1504939656000},{"_id":"themes/next/node_modules/qs/test/parse.js","hash":"c484d2bf4182a9e2271ef85cbb5f9d25b9ca8231","modified":1525237041000},{"_id":"themes/next/node_modules/qs/test/index.js","hash":"597801d7cbacf7a83f59b0615ec53af28dd5ab2c","modified":1496129637000},{"_id":"themes/next/node_modules/qs/lib/formats.js","hash":"268fd11b0d3295eb9e87943e70ef9f1834d573ac","modified":1496129637000},{"_id":"themes/next/node_modules/qs/dist/qs.js","hash":"778cc175ebf3c3c9cfda14994726c9c711b2ad71","modified":1525413963000},{"_id":"themes/next/node_modules/qs/lib/index.js","hash":"9039aef43bebb9c999e4711d367211cc97b710d3","modified":1497471592000},{"_id":"themes/next/node_modules/qs/lib/stringify.js","hash":"f3e93e4a7577328ddcdaed6a4c3fb74d066eea4d","modified":1504939656000},{"_id":"themes/next/node_modules/qs/test/stringify.js","hash":"a2f01e48a855accf752c08ea59b5ae5a4d9e522e","modified":1525237041000},{"_id":"themes/next/node_modules/qs/lib/utils.js","hash":"236fcc5250a635993bf6dac33b6024e737a5137d","modified":1525237041000},{"_id":"themes/next/node_modules/qs/lib/parse.js","hash":"bcbbc2323305b1ce595610a585b916c9662821c6","modified":1504939441000},{"_id":"themes/next/node_modules/qs/test/utils.js","hash":"cd07caa5d78b48f035da3d761e19894911561df5","modified":1497471592000},{"_id":"themes/next/node_modules/request/lib/cookies.js","hash":"f43e796e4f8388308df04ec93dcb9b5f8b1a36c4","modified":499162500000},{"_id":"themes/next/node_modules/request/lib/getProxyFromURI.js","hash":"9ba23efd9897fa29c71684461903bc935bbf43c4","modified":499162500000},{"_id":"themes/next/node_modules/request/lib/auth.js","hash":"959043ff806795ed9801f509c4bff077231e0fd7","modified":499162500000},{"_id":"themes/next/node_modules/request/lib/har.js","hash":"62a20877d776f39a5055e0579ffeb04c791d1c53","modified":499162500000},{"_id":"themes/next/node_modules/request/lib/hawk.js","hash":"36f7cd4dbeadd8252506b629e6c7499353d80d52","modified":499162500000},{"_id":"themes/next/node_modules/request/lib/helpers.js","hash":"dde269662f846192111a95dfac3f5c96c7168615","modified":499162500000},{"_id":"themes/next/node_modules/request/lib/multipart.js","hash":"83077c64750f6362c0958d0699b62798c944ec1f","modified":499162500000},{"_id":"themes/next/node_modules/request/lib/oauth.js","hash":"5a9889f3f2b88169257d4a1866779fcddd5d2d3b","modified":499162500000},{"_id":"themes/next/node_modules/request/lib/querystring.js","hash":"47b77094b91220564ab53b30f56cc1f96ac7bcaf","modified":499162500000},{"_id":"themes/next/node_modules/request-promise/lib/rp.js","hash":"cb0927b7bf3eea6bfeeeeb11686c9c39e0b54294","modified":1550170129000},{"_id":"themes/next/node_modules/request/lib/redirect.js","hash":"86ac7634a518331071c240093530acd896a61dd8","modified":499162500000},{"_id":"themes/next/node_modules/request-promise-core/lib/errors.js","hash":"5fa6ced92ac49641b3cfe06c911bb1e24948fe69","modified":499162500000},{"_id":"themes/next/node_modules/request/lib/tunnel.js","hash":"0bd291728498557a1a489179fcee22006ecc934d","modified":499162500000},{"_id":"themes/next/node_modules/request-promise-core/configure/request-next.js","hash":"1defbaafa5ad9a1550f4eb3cd9b5fe6058466a74","modified":499162500000},{"_id":"themes/next/node_modules/request-promise-core/lib/plumbing.js","hash":"2c1509ea88ef16c45a07c8ff8886ef33a08da67e","modified":499162500000},{"_id":"themes/next/node_modules/request-promise-core/configure/request2.js","hash":"27fc633c95633c3e452b5e63481b5bdefe669329","modified":499162500000},{"_id":"themes/next/node_modules/sshpk/bin/sshpk-conv","hash":"c870b6d429849b83d74aab97665d0404e1f6f91b","modified":1545357205000},{"_id":"themes/next/node_modules/sshpk/bin/sshpk-sign","hash":"75271a4b0392b5549555f50085b991e8d9a3bd0c","modified":1461292363000},{"_id":"themes/next/node_modules/sshpk/bin/sshpk-verify","hash":"bed5d9cc90700090b09d785d84f985b7815e55f8","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/algs.js","hash":"456cb47c5dbad4853a16580bf5a119fdcd3961ab","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/certificate.js","hash":"509a53f51324443625e3d3ab1f498f0225f191c0","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/dhe.js","hash":"de90c735bc5770277c01bc139b1864df4390b09d","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/ed-compat.js","hash":"cb351f4afbc015683a402c837482dfd407b0aa75","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/errors.js","hash":"b1492a5f7867af3b22be7135c44764a6e9176d3c","modified":1488421315000},{"_id":"themes/next/node_modules/sshpk/lib/fingerprint.js","hash":"fcb020da5f1afb0b272a7c44aad85523fba919f6","modified":1545357205000},{"_id":"themes/next/node_modules/sshpk/lib/index.js","hash":"4c77cad52af4a117559e8d040e7e5fcd54e0cd04","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/identity.js","hash":"862587437d1261cac39bc43633e2010e7181c175","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/signature.js","hash":"549617f1491c572a55d31a0f1e57a8ac79af795d","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/key.js","hash":"3b017fb74988c51f2f84fec9f8d3d42d51a34029","modified":1545357205000},{"_id":"themes/next/node_modules/sshpk/lib/private-key.js","hash":"c6f41d2fec4eae6b9cb878471d944ae2a7541db4","modified":1545357205000},{"_id":"themes/next/node_modules/sshpk/lib/ssh-buffer.js","hash":"d4f6c446907021e26f507ce40f9f54adf7031d78","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/utils.js","hash":"8e77a9209a981e54ba236dfff3322bb8d265539d","modified":1545357205000},{"_id":"themes/next/node_modules/stealthy-require/lib/index.js","hash":"740589a4924b1e9a2c9d694d306a2bf8c456d9ab","modified":1494301028000},{"_id":"themes/next/node_modules/tough-cookie/lib/memstore.js","hash":"36e9a243ca6edea4975a77cb4637da999d37dd18","modified":499162500000},{"_id":"themes/next/node_modules/tough-cookie/lib/cookie.js","hash":"5a589e00d94e808c426464e57c3982468eacd717","modified":499162500000},{"_id":"themes/next/node_modules/tough-cookie/lib/pathMatch.js","hash":"3d403b8aad536db9e97eef4915eb01ef53405323","modified":499162500000},{"_id":"themes/next/node_modules/tough-cookie/lib/pubsuffix-psl.js","hash":"e6f44577dc69725e632107a5f710de480e178eac","modified":499162500000},{"_id":"themes/next/node_modules/tough-cookie/lib/permuteDomain.js","hash":"d028d9ac2af6241ebc40810eaa1c1d886d922451","modified":499162500000},{"_id":"themes/next/node_modules/tough-cookie/lib/store.js","hash":"bcce2205e56bb588b4e6cfcd48a34610a5eccf82","modified":499162500000},{"_id":"themes/next/node_modules/uri-js/tests/qunit.css","hash":"1a2a1d5fa27a1c2c27f619313ebb75b7d27cb7e7","modified":1287256133000},{"_id":"themes/next/node_modules/uri-js/tests/test-es5-min.html","hash":"12cc04e102a4b50926c5000b8c6d1268c5464ef9","modified":1490368148000},{"_id":"themes/next/node_modules/uri-js/tests/test-es5.html","hash":"9292bfa92ba9cc500cb860497cb87dec8873bece","modified":1490368148000},{"_id":"themes/next/node_modules/uri-js/tests/tests.js","hash":"a73aa104c22f2ad20d294b61079bda4170df675e","modified":1523369966000},{"_id":"themes/next/node_modules/uri-js/tests/qunit.js","hash":"f66f00e5d6e690001827c737a6dc666ef6c6a4d0","modified":1456807931000},{"_id":"themes/next/node_modules/uri-js/src/index.ts","hash":"04b3098c3bac270f145ac11ec9d23a5abbf61571","modified":1522461171000},{"_id":"themes/next/node_modules/uri-js/src/punycode.d.ts","hash":"958a000735610b62e4495cf34db3d3eeb9b7dad4","modified":1525380349000},{"_id":"themes/next/node_modules/uuid/bin/uuid","hash":"c5b3ec4d8a8f620420aa52a8f3a7cfdff1197667","modified":499162500000},{"_id":"themes/next/node_modules/uri-js/src/regexps-iri.ts","hash":"f6c7430cf3daaa4e8911dcd715839dcf7b0635d7","modified":1490368148000},{"_id":"themes/next/node_modules/uri-js/src/regexps-uri.ts","hash":"5f788850613b451c4d0400b0d9d67b22320ec63b","modified":1522550340000},{"_id":"themes/next/node_modules/uri-js/src/util.ts","hash":"8231602d00fb0702205e6b80492c26b63d2ac31d","modified":1523374525000},{"_id":"themes/next/node_modules/uuid/lib/bytesToUuid.js","hash":"21f3fe1411c266f0c539291ec70f90fdc50a0151","modified":499162500000},{"_id":"themes/next/node_modules/uuid/lib/rng-browser.js","hash":"7886d07e07d82af17c52b5b5debddac5f38484e7","modified":499162500000},{"_id":"themes/next/node_modules/uuid/lib/md5.js","hash":"ec146c8bbb3628e50110ce5fb2e8c5c44eeb175c","modified":499162500000},{"_id":"themes/next/node_modules/uuid/lib/rng.js","hash":"be0888747039b095cfc50d0fb6c82105d78ab40e","modified":499162500000},{"_id":"themes/next/node_modules/uuid/lib/md5-browser.js","hash":"6e2bef8a5892014d27b026e93c00abcba28a173f","modified":499162500000},{"_id":"themes/next/node_modules/uuid/lib/sha1.js","hash":"4f039ea8cae56dc7ed50a2e4595cbe22d7259438","modified":499162500000},{"_id":"themes/next/node_modules/uuid/lib/sha1-browser.js","hash":"b5419bcb23bc32a79ac898b5e6b11b27d486ccb0","modified":499162500000},{"_id":"themes/next/node_modules/uuid/lib/v35.js","hash":"db09248b7cef34de5673a2ea24f8780e9bda27ca","modified":499162500000},{"_id":"themes/next/node_modules/uri-js/src/uri.ts","hash":"bfe94370e136360032c9519a30ef84c4cb4eb11b","modified":1525380349000},{"_id":"themes/next/node_modules/verror/lib/verror.js","hash":"ac944c003b5f5bb7bda3bf12b6f2496c573291ab","modified":1493743176000},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1559399176357},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1559399176357},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"9ab65361ba0a12a986edd103e56492644c2db0b8","modified":1559399176357},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"82f9055955920ed88a2ab6a20ab02169abb2c634","modified":1559399176357},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"99fbb4686ea9a3e03a4726ed7cf4d8f529034452","modified":1559399176360},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1559399176361},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"f29165e36489a87ba32d17dddfd2720d84e3f3ec","modified":1559399176361},{"_id":"themes/next/source/css/_variables/base.styl","hash":"29c261fa6b4046322559074d75239c6b272fb8a3","modified":1559399176361},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1559399176365},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1559399176364},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1559399176365},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1559399176365},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1559399176366},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1559399176366},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1559399176367},{"_id":"themes/next/source/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1559399176366},{"_id":"themes/next/source/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1559399176366},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1559399176368},{"_id":"themes/next/source/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1559399176368},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1559399176372},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1559399176372},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1559399176375},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1559399176370},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1559399176374},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1559399176376},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1559399176375},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1559399176375},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1559399176376},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1559399176376},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1559399176376},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1559399176376},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1559399176380},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1559399176381},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1559399176381},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1559399176381},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1559399176381},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1559399176381},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1559399176382},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1559399176382},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1559399176383},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1559399176382},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1559399176382},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1559399176383},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1559399176383},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1559399176383},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1559399176383},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1559399176383},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1559399176383},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1559399176383},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1559399176383},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1559399176384},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1559399176384},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1559399176384},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1559399176384},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1559399176384},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1559399176384},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1559399176385},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1559399176387},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1559399176388},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1559399176389},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1559399176389},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1559399176389},{"_id":"themes/next/node_modules/lodash/fp/F.js","hash":"18f568c6af0e139bc63d9943f5e798fd09bf01b6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/__.js","hash":"9c805f4d417e7a952726f3cd1d0bfa7c30cfa7b8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/_baseConvert.js","hash":"74ba4f62d0a348474acc189c83fceb6a760ce27c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/_convertBrowser.js","hash":"b4cceca914f9af59a7c47c384127602dce2abf6a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/_falseOptions.js","hash":"ad9c0db1f5df98f88d63bd70d729417a7eab2adf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/_util.js","hash":"5fbff66b69ff1dbdde5f1f64e9965ba5f63ecc94","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/add.js","hash":"9a7af31a7389d2b2b2ada006440c4cc5c2e42e14","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/after.js","hash":"b9b375ce8fc1a7d08495feac490844967e5c6ac0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/allPass.js","hash":"d6accb07bf9f23b3d0f72ce21b787618d575da04","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/_mapping.js","hash":"0fa88f5921a98c85fb6d3d2310e8869d5daf9e1a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/all.js","hash":"ca7a311804ca0107de2a6f00602710f36160c765","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/always.js","hash":"d3c13a79ddd4488702c96b285bbfe2556ee67e17","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/any.js","hash":"64f468269587c745e361d31e0d8b26cb8f67fb06","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/anyPass.js","hash":"4a3f89614864715dcc36c7ee5dbb9850f6ae2f95","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/T.js","hash":"f10191c08571d40fee301d26e97240125511bb31","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/apply.js","hash":"eb2c8c5f1efdd93f4d55605df4874bea9f831b54","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/ary.js","hash":"b811ed8b5b34ff86e65589b38cf22f1d7699ed4d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assign.js","hash":"52e569f6c7d83714506850d8174be2c3a2992852","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/array.js","hash":"6c3a1a438a1f341a9bfbf059da2d1efd4a582178","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assignAll.js","hash":"57a1a620f001ae55ba98b560fa7a858cebc13023","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assignAllWith.js","hash":"404f26b37a62ce5a1e9b6fb33672e69913d4efed","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assignInAll.js","hash":"3d99977ee0df2bc32edca5ceae451a08cb43bf76","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assignInAllWith.js","hash":"47ae8b9efcba88af49e9a58fa2a5b7c35813cb7f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assignIn.js","hash":"71da9458e2fdabef9d716d9b48060a12c0185afe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assignInWith.js","hash":"b3b0adbd4b47e963b9f3ec93335b375deca2a320","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assocPath.js","hash":"3cd42a8269c6339b7a9eb3bfde6a404ac4a01dde","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/at.js","hash":"615c43358bc89d4eaed242300b9fd77631f72443","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assignWith.js","hash":"d4a0c3092be5c92abca026002a322407a1e12f2d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/assoc.js","hash":"3cd42a8269c6339b7a9eb3bfde6a404ac4a01dde","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/attempt.js","hash":"eb059364d0e518867b8fe2d4d9773ca481f80b15","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/before.js","hash":"c9fbf0e49c14385711d6335cb1dc0d6e09f3aea4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/bind.js","hash":"5620199ffaf7354deb17332bd96d023ef93a7563","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/bindAll.js","hash":"2e3944394c315756627aab74b8ce3cebab98662a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/camelCase.js","hash":"8eb598ce4ff5d309c2ab8fb299f41df23c4cd308","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/capitalize.js","hash":"101f5c2148a579105036a5cdd25b4193812f64d3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/bindKey.js","hash":"8884ec3af184de8a5f20829f6eb5fb6ea459582b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/castArray.js","hash":"ab5ea04ba48ee2920164fadb8b78d8a03f1cee6d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/ceil.js","hash":"0539c156c827c85dd682ece5986b9f7bac19a097","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/chunk.js","hash":"63359d1585edc86fb70c74388c36e0a96e16b96c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/clamp.js","hash":"d10d4fb46ea8518851b6578ef67dde23b2a9b013","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/clone.js","hash":"6cd53703b2949cbee86ce6d708942e248afb8b4d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/chain.js","hash":"c85611a5e7da7d0aaf29a45bd34130b56315199c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/collection.js","hash":"75a23149e229dc1435bbddc870542ac1ff298f91","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/cloneDeep.js","hash":"179860c9bdc495da2f5d8281fec35077b2f2b216","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/cloneDeepWith.js","hash":"365d2b00fa2e722ce8c60b0605bf3af06c6ff454","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/cloneWith.js","hash":"a303da7355a2bcf3314b47558b57c8e3b235094f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/commit.js","hash":"2b2f58504601fed318dd5f6ef943a873018195cd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/compact.js","hash":"b6603e874a29627153368292851fe987f5462bcf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/complement.js","hash":"db8673391fb52d0d4682d80065e5f82809a3292a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/compose.js","hash":"ea122818ed87e1559af41edff0f2d2fa54319b9a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/cond.js","hash":"c5bcad894a249463aeabed85ac3d3e7474455533","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/conforms.js","hash":"03fc81a616faf63ab5d5097dc6ec2580e8a32fe2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/conformsTo.js","hash":"fa5e2fa22ac3442d8ad5b09e3a6c0959316431ed","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/concat.js","hash":"64240c44714fa5369fc4092f00ef58caced12344","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/contains.js","hash":"cf59973c08cea72d4b25f223aa3c30e99fde8e55","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/constant.js","hash":"109c49b151baa999c023e7934b4223e0fe3d84c5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/convert.js","hash":"5e125cf746680131339a8ab7f41d917bdccdf356","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/countBy.js","hash":"52e5e9e4b6064c30131e7f678551f23f8120cf0d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/create.js","hash":"7fdb4d9dfc8e2b4876ec6b47b9b32fa37eb1aa2c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/curryN.js","hash":"13f2d44be73b0df20161c72c0b77b69f8bfdba39","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/curryRight.js","hash":"333b1a096341c2552aae7af172dfc6b1aa2d68d2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/curry.js","hash":"ded1e6452bc77478e51a4b9a7f8f2eb9c07d1b03","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/curryRightN.js","hash":"919ca15f73c98eb40bfddd2213edff6a8992e2e4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/date.js","hash":"bbf85ffcdfc83df6d5c24c84b03ca937f9796257","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/debounce.js","hash":"c702651f3d6844c12c8e7fb2d2d07ee898f3c78e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/defaultTo.js","hash":"39bad3a9a0314e20e1fcc52f2c7c1711a796754d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/defaultsAll.js","hash":"2006e77adc3f6e20fc16af80af1a26d166d14c07","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/defaults.js","hash":"0afcaa5650946e068d13f298bdcf3e185f5a1f07","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/defaultsDeep.js","hash":"6eea71a30ff427217403c02e6721e6bdaa0de604","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/deburr.js","hash":"b4aba68c497e84459d6d9739d71b0f1a3bb7182e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/delay.js","hash":"1f6a34ed80e76bcfa748c686fbc67488d0164d78","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/defaultsDeepAll.js","hash":"ee6d0af3f9942f2314c2b6fc2f283debd885fe36","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/defer.js","hash":"49b70a2e28eca9a02a89437582cf42c165dd2d05","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/difference.js","hash":"13ba75b75ad25ad44bcfcc552dd0fe00114c13da","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/differenceBy.js","hash":"55c72db5c42636b2f5824b45979834483161d817","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/dissoc.js","hash":"4daf52a97c16d6624e3e1abfa4d0b8d8d2362add","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/drop.js","hash":"2cf18a3172602551b9950bf4b21ef23b07f5c8e4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/differenceWith.js","hash":"290648f706217284bb229fc0957d21578b89b0e2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/divide.js","hash":"86636acadd06d91f809acd2c1d31cf780caa8649","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/dissocPath.js","hash":"4daf52a97c16d6624e3e1abfa4d0b8d8d2362add","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/dropLastWhile.js","hash":"3901051acf9fea300dfe2961fcc31890adabe4da","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/dropLast.js","hash":"297bd9ff4c5ed9fd6323d3ce10286936be549bfd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/dropWhile.js","hash":"3d82fced32a443d92d80485508cd79c34b0a0ac2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/dropRight.js","hash":"e8beb9bca520499fc14e793b6e8f7a428d197bb9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/each.js","hash":"dbfc772ff0330e9d0bb2ee704242e67b0435929a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/dropRightWhile.js","hash":"779915c003cbb1a9643965f284858729bc032b6a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/eachRight.js","hash":"c7a90b0a7bec56aea41c7569a652ebf17c20fde1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/entriesIn.js","hash":"995c6287a9c5f6641931c966de331b5521f63d3e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/endsWith.js","hash":"1e2c542c82f7aefc5fad986136445a7a4425e145","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/eq.js","hash":"e22365568b08047e550d002e351cef842cddabbd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/escapeRegExp.js","hash":"2a88b89297d34797244d963f2323e59a7c3d4d31","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/entries.js","hash":"8788b9edcf643acf20e434e5894d09fb18742112","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/equals.js","hash":"735240597ea9393397bbd638934ebfaea4f6b2c8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/escape.js","hash":"68a5b97faa5723827ef59f065101bbbe2556e6bd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/every.js","hash":"0499e846512778b49de2da8574e81f21dbd3ca40","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/extend.js","hash":"79bb5928a674d6122686fc0df5a28f00b22d4d0e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/extendAllWith.js","hash":"1d2dab39cc17f3beec2cbdf117837509ca72309d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/extendAll.js","hash":"33d62f7192821133af2eb6ed064b6dcd173a13aa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/fill.js","hash":"783c4109619e522701ef2956d070ec6f94ebb258","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/extendWith.js","hash":"fe394cac415eb0a6518371d1b95e08144a682526","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/filter.js","hash":"7f77256838b2435ebf7539e186cce009aa54c1a5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/find.js","hash":"86e525b8931055cfd1dbed74e13e649b39e8fa6e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/findFrom.js","hash":"7a92981afdd815aa0d39348b752306abd7697559","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/findIndexFrom.js","hash":"61e3f312582261bc44d36fac85ddbe6f11f26a30","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/findIndex.js","hash":"6dae88bc0ea9a1c2ceb6bed27e6fcc1a993795cd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/findKey.js","hash":"7a3a8649e4119245549d42a71a20752a7a3258bc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/findLast.js","hash":"a19b3954a423f675b6e556a4838d45a28eca25b1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/findLastIndex.js","hash":"25b3d6da9b75a73e37d908e0bd1c2c071eb81def","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/findLastFrom.js","hash":"72d9c3ee81778f2f3be72594451a11c7ece2e35b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/findLastIndexFrom.js","hash":"077ebe8808fe2fe62d216b354af30236aa471903","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/findLastKey.js","hash":"d71353c58b59d98cc7d909a543f8f49ace63bf10","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/flatMap.js","hash":"1b0399b113f8f036d69c78626ffb7fdbd531a452","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/flatMapDeep.js","hash":"68eb25628604462da50f41131335e5bdd79dd931","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/flatMapDepth.js","hash":"04d4e92f43b87d2f5504ebfd7d095c0421322e6e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/flattenDeep.js","hash":"3d4a37408b913e82d947aa842b984fca60b9dd0d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/first.js","hash":"a2eac20812e95b557a5bc11dab060163fe1133d0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/flattenDepth.js","hash":"c606dc88f1ff4ae80ccb61d0e293d46f53e687ba","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/flatten.js","hash":"93940d34c59306c75e8680b2f54a9535f1eb56fe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/flowRight.js","hash":"ba997b9952e847801199c2767ee07ffd64c4810a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/flip.js","hash":"e13e97806d2757e3661b52657b0a81077bddddcf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/forEach.js","hash":"fe364b04d37562751d1e68482ddf6a7808156916","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/forIn.js","hash":"83e5d965ee8ff794867c6c8a2e01ca56fe76a15b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/floor.js","hash":"a7a4526f7ffcc6fd12b5805cc687f5e31fb41152","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/flow.js","hash":"b1fa8a6738958fb2b0f61f55670e80e72b37f3f6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/forEachRight.js","hash":"fd98cd06ffb50506eacfab8999a3cc06d2c646b6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/forInRight.js","hash":"3f48c8bb263b1eb71cbaf4ad9d2606b3656f8a83","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/function.js","hash":"c7d47f84639e315320f598f32c3a8f8dc8b0a724","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/forOwnRight.js","hash":"5e5befea43de625a990a571cabcf5545f7ce1ad2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/forOwn.js","hash":"60ef3e46e036d39572320f2c6f7601881ca41955","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/fromPairs.js","hash":"1e7734eac86bf368e7d1bf63ca1657952b20f13f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/functions.js","hash":"82f6797b95c815f8082611ca84cffad654ed7ee0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/get.js","hash":"27edd6c7c585dcd0222c1cea97765e5eb1c52925","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/getOr.js","hash":"423682307e027ac900a0cc83948ad83b19c2fca1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/groupBy.js","hash":"28032f75b908231d97cfacc99fc04ff0a3f22e65","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/functionsIn.js","hash":"8b79a911c832b5683868fa56bc7c6cdaeb4d5e88","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/gte.js","hash":"d6e1cf7bd2d03d198791afb3eedae8323b06c262","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/has.js","hash":"20fd58ff42e0550eb9f94e183225458d52436836","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/gt.js","hash":"36dfa14d07d98b751b8d37ba1ee1e64c64971161","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/hasIn.js","hash":"c9e75e41b1638417c4d1f9101c43469032522afd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/identical.js","hash":"40efa57b258af781819e8bd050c32583837184b5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/includes.js","hash":"c5b5262c60578989e7569a4ec24c14c22af9d4be","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/head.js","hash":"886dc89b060616ea92340f7d68bc946f058e8c2c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/identity.js","hash":"59c993b05e7ec2f4653e4bc55b3025dcd5ca2f13","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/indexBy.js","hash":"c4137629ebf66269744e84038e8a5c02084d931c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/includesFrom.js","hash":"13cb72c249c72145915615e75617f529d36ad4d6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/indexOf.js","hash":"f296c1ad76022ce01e1f733136933820a2744cfa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/inRange.js","hash":"57145747d9c73137b76796e00ed79263c9049c14","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/indexOfFrom.js","hash":"a9af522fbc2eb5a6727c604d91e8ffc21198f5d4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/init.js","hash":"22ffae77010a1ac713f0326fa468621c3ab83a2c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/initial.js","hash":"345cc5bbcd2fcb19a0d5e3ba7c44dc8d4bb85be9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/intersectionWith.js","hash":"f264dacabb5c65b41cdb9301fd281252ceefcd45","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/intersectionBy.js","hash":"c5f67ae844b3c57014cf4ca6878fd805e7a0c7d8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/invert.js","hash":"fd7502e8c46743d84540d1f6e83e3e70e4190812","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/invertBy.js","hash":"f23b5d7017a48c8757e9e02f8e9198de8af6f181","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/invoke.js","hash":"af082b901fb05e5ca0b8c366f2d91c1a80a02fd1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/invertObj.js","hash":"f2acc5a6d3985a3d717e180810fc8da4b70eda6c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/invokeArgs.js","hash":"fae0d54f2a10c1da45de1894b3a854b2f42f88a7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/intersection.js","hash":"1391dbb03a7ea4de5e8c8a937667c5b2f58d3b28","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/invokeArgsMap.js","hash":"9cb53649858eb1e33827cc8513c16d9e7363d9da","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isArguments.js","hash":"675c9c3ee2f01b2ecbbcaa9637bbccd65f89d943","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/invokeMap.js","hash":"e53c69c9c4a6be115fea0a3574c4d1255bfe5428","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isArray.js","hash":"45376d39c70fb04bc0cea0bdda3b1998d0c76b3a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isArrayLike.js","hash":"f6ef38c98c8344533e90a2212650cf936ce808c1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isBoolean.js","hash":"b91a773b54bd4822f9c23bb88b139b67f4f1eafb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isArrayBuffer.js","hash":"6d6c14aae84e1662f0fbac87ff49d1dc0c568444","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isArrayLikeObject.js","hash":"4d053b5fbd0477fe770249346cbc0bcfb37a6219","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isBuffer.js","hash":"192efd2ec17cbeeadc1e68b893e504b8b14afcaa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isElement.js","hash":"41b8187c0a4dfb59cdc5c939f1b3a68d674e7048","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isDate.js","hash":"ccd1b64eeefbb7d146ab6294c52aeec2ad1a93a5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isEmpty.js","hash":"517aca0c6b7b51087db094f4ff1ac06bc50c765c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isEqual.js","hash":"e143e5a19f1ceadc7a6d93d5bc95a4e62b6be530","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isEqualWith.js","hash":"685a3ecf2190a5656547f3a62fd8fc0c4fefcad4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isError.js","hash":"03dabdbd0e30796a8bdc2a341394d329a461a9d2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isFinite.js","hash":"79c3ad8c8702d1aab54ae80ea400d5f73ae49dc1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isFunction.js","hash":"a5644120eabb397e67c38922ffb9b87d22aab5b7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isInteger.js","hash":"b30a8a902259cad6f2300f7b06d3f2bc3975f34e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isLength.js","hash":"6aabade08bfdd6e8bb07e28225629c419b5e8c97","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isNaN.js","hash":"c607c72d2d7f67fd2da46ca370acf5e12a024e44","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isMap.js","hash":"c2421f5389375dd9697de826b777b3d99c8e833b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isMatchWith.js","hash":"0afec1beb71c2424691fd4b4dcb9100820374c26","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isNil.js","hash":"00960af435b0e8ad04bb1fea65d6a30ae8a61d06","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isMatch.js","hash":"ea5bb01a981706a79b547149e5aadb39369967ea","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isNumber.js","hash":"6caecaae0ff0b438dbe62b2a814b14db80c6afd1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isNative.js","hash":"4dc770dd04ad9f97aad0b764d876375aad3c8a9e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isNull.js","hash":"dc555ce657002d169ecd3f2af155b6ddd64fc6f7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isObject.js","hash":"ffe7fe0c07a623545dedbd0c45ec219e3745cc99","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isPlainObject.js","hash":"14b52f86eae7bcbe9d5f0400377d9315a588bfef","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isObjectLike.js","hash":"49d650f710d56de004f9adb20ad8cf1b61f26554","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isRegExp.js","hash":"fc73a7faaf1e7ed7821d0600676678cd495326f0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isSafeInteger.js","hash":"e570e129d68bd49f8ba775f6b85ed596c4a96fbd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isTypedArray.js","hash":"370900f84ff88e22af1153a209583c2bf6fc9415","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isSet.js","hash":"7c2c1be0d45c657157fe1ba88986a6b105c0318f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isString.js","hash":"512ad2c4610306d11bb3bb3491f53abd029e5ea2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isSymbol.js","hash":"d374ee1761cd7c88e18c0d89797249ad5848ddc1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isUndefined.js","hash":"0022a9050a33a2d65905711d005042506bc9abad","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isWeakSet.js","hash":"00837472f9eff86d2ffecbc9b54ed3605b3c72da","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/join.js","hash":"3ef2d8c47d359b75da09f77c3fe2266508d4f021","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/isWeakMap.js","hash":"2d9fb05f212d14e0ace898c6bab8563582d707cb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/kebabCase.js","hash":"8c4c6c334759ea0ecf4b1529287aae9fb187e47d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/juxt.js","hash":"19b5b68440238c0945975ba74e5a43d633a4cb25","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/iteratee.js","hash":"5a57ada3203e3a1b57b61923cda5737bc5bdf7ed","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/keysIn.js","hash":"8e6ccf89f46ff56b75f2b23f820112e0982845ed","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/keyBy.js","hash":"623811ac634d2ea4518def1661ebcf95b0201530","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/lang.js","hash":"62dc2db0a784f758cb9347da261f8089d1df95b7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/keys.js","hash":"5e7f14bc64715f1731977c278b871574eb7796dd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/last.js","hash":"482343e9c2122718717131d20ff454537bc3452f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/lastIndexOf.js","hash":"34238beb4a520fcdcd9c46854d6dc7bfbcc1b5f9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/lowerFirst.js","hash":"e496119d716f19967b0070c7ec9c03d28b919122","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/lastIndexOfFrom.js","hash":"2206dc50977fb1f15fb7ddbce38cd8adcefdc0fa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/lowerCase.js","hash":"10a1cbec9a77d960a0c2b470450877fe4596bd02","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/lt.js","hash":"ebbdda9e5136aca682159edb9a951257370bc6f4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/mapKeys.js","hash":"ecdd3cd7edca30157bbb0bc2a3cd91058dc75199","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/map.js","hash":"b353305d3741738e30ccc46b96421df43380d99a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/lte.js","hash":"4926168ae6c4fe1a9493b20ddceb81f5ffef82a9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/mapValues.js","hash":"15803c364129828e85167dca814187bd2ac879c1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/matches.js","hash":"95d70e584abab7e886e137823be0c719bb67ca2e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/math.js","hash":"dd031d6043c326be2e5c3a8b3a4bd728bc74650f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/matchesProperty.js","hash":"73c7321e05c3a79d869afe876a64ae009d8d3da0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/maxBy.js","hash":"9a5c1f5cbbb6375b8f314d5c3508854a76541613","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/mean.js","hash":"73e71b6ebd607cc99741a6bd8a504f4b2d753cb3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/memoize.js","hash":"d6bd34e0088a0f017e8c36785c6cb79450b2e0f3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/max.js","hash":"72e93ed6cc51a6bb7ba5131cffbc4ed64b0d5148","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/merge.js","hash":"087b5cb62cf464d2154b378d4a79fceb5987966f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/mergeAll.js","hash":"e9d2d85cbcc2a728e6baf932d05f053901f0a7e7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/mergeAllWith.js","hash":"7ed92253b45294795a4bf5e6fe8d5f716ad27b99","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/meanBy.js","hash":"062613f280d0bed68bb11a9999721c0c90730c6a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/method.js","hash":"9c12a91f85dc26106523d5662166b6ee0056dd14","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/mergeWith.js","hash":"061bbc4f62c653eb99021e78accc7ae915c3d8a0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/methodOf.js","hash":"d7ca6c11064364211c6b7f7fa8465377f2a07342","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/minBy.js","hash":"7c81c5d5a852cb0ece8e3026820c1000d6ad10f5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/min.js","hash":"e0a8670ec5ee7da49ea872f9eff2d5ec5bb5c1e6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/mixin.js","hash":"bff01db6635beb3588b48b53aa0b83d92430f62d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/nAry.js","hash":"bc98ebd544e4def682ac61ff204c16aba3b5a227","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/next.js","hash":"ff0900b4e5fdd889ec980c50e5e52d05d469b9f1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/negate.js","hash":"1503b32cfd9bc78f575f865ac3984bbb3dec2695","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/noop.js","hash":"dd608cb4eb01d202042edb374958ea37494d0ffc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/now.js","hash":"e455b0b859454a7852eeef13a2a7e2a26ba00502","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/multiply.js","hash":"373b10cb68541026463c91d41901e052f3587044","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/nth.js","hash":"3cea03c8cc79b44fe085d53b73c2160ccd302029","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/object.js","hash":"1f4e289be4a66881114caa5198b881b1f16fd4de","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/nthArg.js","hash":"14cf05b970cf3ce3f5878e4396db31f286dc1653","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/number.js","hash":"56db9afc452bfa9338b206bf092497b8652f61f0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/omit.js","hash":"b9a90a8700d55718cb1a3d68b15223d0b9d7de61","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/omitAll.js","hash":"5c3a9227c6f9ddc785688becab205f8e912a9668","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/orderBy.js","hash":"7a17f923611248c0ea7086507bf568a043b98129","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/omitBy.js","hash":"1ae512e4ff2af4984900b15292137fcc10161589","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/once.js","hash":"b6d33d5a9c4379ef7e1c46b09f9c865ecb8603bf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/overArgs.js","hash":"adcf40dad62a1c856764978731537f30889b1c74","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/over.js","hash":"51db6346f35cb56c4ceff448ce83bb4b0529dd47","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/overEvery.js","hash":"3442bc9336ab31d30983323ab9e88f628085a1b1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/overSome.js","hash":"7de036f2f58dc0faecaf54be9adea63226aad560","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/padCharsEnd.js","hash":"07914fc0fb8ecc33d9611986519b7c6607fe455a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/padCharsStart.js","hash":"5c7255195a70d48bba88ed00ac177d59c02e88e6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/padChars.js","hash":"4c63a99952765c19e7858c17f18da7c6612c35a7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/padEnd.js","hash":"4aebe4522188368f563e7e72b2549083b542331b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/padStart.js","hash":"a312c6f78be12857bd315385eebcc8eabc67c1e2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/parseInt.js","hash":"9f69cbcb1ac43f1e3e7d6cc58efd7685fe4a00aa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/partial.js","hash":"f2acb8a2cc54f7e90ae1cbca437be0f9c8bc4d68","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/partialRight.js","hash":"bcf63208bf8072ae6e7ff226467ec45fe20b7ace","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pad.js","hash":"383ad74a3c1d0de2b6ca0304eb1d1faa4bc5bd0a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/partition.js","hash":"205607a8169b9faeb2b51b0974805fb3cef84b89","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pathEq.js","hash":"2449e34c17552d94cd719c23e1ffaee752b19a49","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pathOr.js","hash":"8453669dae1bef8fb6e22746d8c5b09e21705dab","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/path.js","hash":"563c5eb1769785a3350bfd1cb2b4e090a650c994","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/paths.js","hash":"6dc80a19f4b7e95104d76ad38cab58672e823c41","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pickAll.js","hash":"a24ed77399bda55b6c76f52a7b2d0318e15e9d53","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pickBy.js","hash":"7e90e85df68c3e5ed8e18b36ef2f1033f9c192eb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pick.js","hash":"7818ee7a56a4493a25f0262b42226a99b5ea4e74","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pipe.js","hash":"9a9cf735bad407cdf098744f528e907de4b8f8bd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/placeholder.js","hash":"d211b85c131bc2d7c0230fc611c2f88d1aa1ae62","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pluck.js","hash":"56c45930b2236d7778657a85d48fb7c709e3010d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/plant.js","hash":"909ca1575840a93993b0cac28834173e1474eb9f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/prop.js","hash":"563c5eb1769785a3350bfd1cb2b4e090a650c994","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/property.js","hash":"563c5eb1769785a3350bfd1cb2b4e090a650c994","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/propEq.js","hash":"2449e34c17552d94cd719c23e1ffaee752b19a49","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/propOr.js","hash":"8453669dae1bef8fb6e22746d8c5b09e21705dab","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/propertyOf.js","hash":"a0309506e91e32300edd07989a6765ba76469785","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pullAll.js","hash":"9d43ec3e85737776d1cba4dd43dfe33dcb691213","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/props.js","hash":"6dc80a19f4b7e95104d76ad38cab58672e823c41","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pullAllBy.js","hash":"82df6feed00d7586139b8d19babff6a119d12abc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pull.js","hash":"e212c433d15f9e4aae0c7b7472df7478dce59826","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pullAllWith.js","hash":"a4c03d7ae4b54c96e5050bbc7421bce119aed17e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/pullAt.js","hash":"e64a22662e4110f3a7598bcbb20c3444ace05733","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/range.js","hash":"c30a51ad9b00e95df9e8d84359393789adff6f70","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/rangeRight.js","hash":"87d5bc191b63a837147e1a06ff573017932f9956","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/random.js","hash":"c9cacb8c48422049484d1b43f3e10da43f54fcf0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/rangeStepRight.js","hash":"2706912addb4a0f3cde29fdbc07e8e6efbca5022","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/rangeStep.js","hash":"8bb13a69dce53d8cb160b155579da0e6167551f0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/rearg.js","hash":"d5b08395ea2c22d24207c8e80e19a62b8b68d49b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/reduce.js","hash":"7607f41b09579aa65dca0224fae135f2a7bd90fe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/reject.js","hash":"f24c316dcf99cac19664e5f3cba90bcb7928ef1c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/reduceRight.js","hash":"db8224e00025bde9179b073a6aa8aaef52cbaf6e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/remove.js","hash":"f6c76702ae01ec215d972d87850624d302a45b9f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/repeat.js","hash":"7af31ea1d388664c134aeff68f0d5cb72297004a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/replace.js","hash":"f4cf1769a167cbafbca362598669cee3b2048b03","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/restFrom.js","hash":"7658cbf9d7d17aa68e24ae89e087b1f1f82858ac","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/rest.js","hash":"a860492108f67bef2af79852208f2dfd4f541d7f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/result.js","hash":"e805c5972b63deb60f49d1513e570ed116a864b5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/reverse.js","hash":"723596c407da9faeb8a6029ef74e42e97bdc6be9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/round.js","hash":"39a12f5a5a3ec3058dec8aa5118425b16d070281","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sampleSize.js","hash":"fcfe74bb18b06d6da53495935d23a7600f9b24f7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sample.js","hash":"2d93d3727f92362eeadcff288c3a8a733d023740","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/seq.js","hash":"4fd21585867c1b120e02b78852d98320ad8a38b2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/set.js","hash":"35767c3a9b3e46a963c7fcec39391fa87136861a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/setWith.js","hash":"c5fab5be851b87976674b1459d61b445d0d5be14","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/shuffle.js","hash":"2d110e1a05b0780b5ad4159ea18a807f7ea1e505","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/size.js","hash":"3fb55f18bf4ff0daea2c053a54a53c2674499e45","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/slice.js","hash":"ad400dcd46e81370daf0ad7c745ea1ab58df49a1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/snakeCase.js","hash":"d233254a26e81ab2a81fac31c48215d4b0b44adc","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/some.js","hash":"0ab071ab57a97cb20b860f9125f2d3f1f131f833","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sortedIndex.js","hash":"04be86485281acfc53118491293043d70e3a7f2e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sortBy.js","hash":"bbfdf129dc3ce45a017989b1b825542383812b73","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sortedLastIndex.js","hash":"d6598c35bd7fb8887fd6fdbdcb8b18befa6632ad","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sortedLastIndexBy.js","hash":"e53794520cd5b3ca6df2ffb977e0e6b7a5f9502c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sortedIndexBy.js","hash":"47138204798bfa0b43928340b12bbdcff317f60d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sortedUniqBy.js","hash":"dc651f780172d66beef630e5c50c006d9fba50d3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sortedUniq.js","hash":"8e361b43dd485f7aa5971e947939324cda143dd3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sortedLastIndexOf.js","hash":"94fc33bdee1fdebb80f0b00d195eb4a13bb926f0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/split.js","hash":"e785df2f7e7764f1bf518b872bc302679b505589","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/spread.js","hash":"347036d4ea05dea39b09f20275d06e8594b55ccf","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/spreadFrom.js","hash":"30b843077c519cbe18ec47e2ba9685f3a37e19e2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/startCase.js","hash":"aee3f36ca72d7fa30de64c0541e80b8ce2d729bd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sortedIndexOf.js","hash":"70b13a8537667e2fcfeba4993992875bdf8ff864","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/startsWith.js","hash":"73df7824ddaf5d8dbcbfb2894234f8e5f4214497","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/string.js","hash":"3a30865f86edf4dc3baff7567a894bc6f280a654","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/stubArray.js","hash":"dd9b5ab21a9e90c5c2d33af270b15bb3a0d5a160","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/stubObject.js","hash":"3c5521da1935e4b266a480fb8c2240fc2e572ea9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/stubFalse.js","hash":"74f54c09c21a95c6e2168df204016afb1da85c08","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/stubString.js","hash":"493b8cce32dd2d2f0915c335941afd8a2030c59f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/stubTrue.js","hash":"7d69609e3844f8e596fbdd77e68be6352240ef7d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sumBy.js","hash":"73e4807968a99437d8168517ac9fd13ceeac5886","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/sum.js","hash":"430189ab2c6a23195cb8ea5111921794b9175ebb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/subtract.js","hash":"f76a659ba3d7724c8e4ff6049dfb6cb17627ae41","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/symmetricDifference.js","hash":"c1db644f2796082ce8c1445f0788558fbd4a6ce4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/take.js","hash":"f9dd7f687a04982ac7aad57c525d98b1c3ffd1fe","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/symmetricDifferenceBy.js","hash":"8cee02add02dca3ecabeeb07632dc8a2a1905e23","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/tail.js","hash":"4515c52305fd72e45da2ac90042e05c2215178a2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/symmetricDifferenceWith.js","hash":"129d57b15724b6e3dbe5a83af781cf7e9a74bffa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/takeLast.js","hash":"ac465095bcff9ff29ae8337a6d86a7658bb67196","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/takeRight.js","hash":"eed0e281c0ecf99c20f5668ce9e0dd154e45ca71","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/takeLastWhile.js","hash":"88248a7d663078cb48e2ecfc912a33ac3b097afa","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/takeRightWhile.js","hash":"b090924faefdfd4a31e25c54931f3b46f61c8203","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/takeWhile.js","hash":"072405b57aebe0b9a274eb3709ecee82226092b9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/tap.js","hash":"26ebee1454a84078cc66c9acb6c4844a5dbb6223","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/throttle.js","hash":"93c605d12d488e2ea9c6e6c63dbf42a538c66146","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/template.js","hash":"ee7becfdb63a3d9ff3dda057f1a9caef3a814554","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/thru.js","hash":"e93cff48a14c89ca44d3cf7b14dd7ec275dd53ff","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/times.js","hash":"216300e9d1f02445f1af5f8c8427f949133b75a1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/templateSettings.js","hash":"b9a7dd2e08852490ea86ed35b1e7469482c2bb23","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toArray.js","hash":"f47d0cfbc5a869b78087164b18237c5448f21851","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toFinite.js","hash":"3a63c6c9616bda4574b31fe27ef369c01cafd6c8","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toIterator.js","hash":"961722cad23f72206b6f7d83e054c271b6a2add4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toInteger.js","hash":"1a1797465c07599f7abc0f8430e46cff38d50986","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toNumber.js","hash":"3f6ed987b363575f5f35f6189b144ba3d907eb43","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toLower.js","hash":"e0fb8417621f54e54d3df261d1e873438502ed70","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toPairs.js","hash":"c28dd8a85dfaf032e5db9c1ae75da809d8527223","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toPairsIn.js","hash":"407a3784056e15bba952ddd7d835aff27eecc673","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toLength.js","hash":"7d567f44a37d6a32bbd5a3856d011d71a9665f5e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toJSON.js","hash":"7ac9ef07def4a64b062a05e942d3d110e3492ec7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toPath.js","hash":"d5cb075719634a9c336bea31d017ac20389c044d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toPlainObject.js","hash":"fa912ce1e006d37f4483a53391271acb71692997","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toSafeInteger.js","hash":"fa9edb660be230da19a83e120f48052d9ee5400b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toUpper.js","hash":"4312523e48af3aadf918218214ab329523305334","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/toString.js","hash":"70b8676332f4ca060ba84f05a9bf101749fbf3f3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/transform.js","hash":"932c1b9f13d1e4892db0ea9eefd4902f843f74b4","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/trim.js","hash":"f57135b04ed9e9db794f75b6b47b328440f06922","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/trimChars.js","hash":"28eb05ea233dc6596ed3b17a3684c7225efa82be","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/trimCharsEnd.js","hash":"c2d9c19c69c5e695022119baabadaa64ef02414c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/trimEnd.js","hash":"a9f669bd68b6fb04ca71e18d5f2c88805ec7d61a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/trimCharsStart.js","hash":"8415cb22cbb73b5fd101e360400bf6182f4d488e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/truncate.js","hash":"af7ab1a8bcac66a7504359d2b2625b0b4bb6445c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/trimStart.js","hash":"50c328eddeec91b92f4eb516edcc78faaae87333","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/unapply.js","hash":"936c484aca78852157be00e7b5cb71da6f747bfd","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/unescape.js","hash":"e91a70341966dbafb94a8d6fc723f23f3c3a31a7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/unary.js","hash":"22e12bb090f8f305fb360a2372d87fcd9467399d","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/unionBy.js","hash":"139742a9be22d7eb2195e7e7182d87a912936e2f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/union.js","hash":"8e1193fe25429e87a9cd2e4a29dbd279c5decd5f","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/unionWith.js","hash":"c48bcdd3435f2cf4a650d30608cc929712943578","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/uniqWith.js","hash":"af2818a584a4317e5148e52222afffa0c5fee59b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/unnest.js","hash":"3d20beb0655413d13c2c55fdc3ac06013125edc6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/uniq.js","hash":"54b58010d1988f7a6ec28dc4a2df2205e83029ae","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/uniqBy.js","hash":"4111e9001db74d691176ba3e02af5527a4a77ed6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/uniqueId.js","hash":"414c720c8d11aeeca6b05cccb6deb16a0f27c943","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/unset.js","hash":"401e919462b8e66630dfc74e18796fdfa62ba6a3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/unzip.js","hash":"29cd397f4baa016f6e6d1c8fc4326219106896e9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/unzipWith.js","hash":"afc880fef61b32bc409f3a9a7359d755cdddf83b","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/update.js","hash":"a7d197f6473f8a7a38c00acc709f53481c545474","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/updateWith.js","hash":"9dd189e80bb45da43b4076a1643a45c69f8cf1a7","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/upperCase.js","hash":"612a73d5f39d6d5a6f3c3b80ec89a6377e2e3c4a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/upperFirst.js","hash":"084545106fd93e9acff00c4401918ed9054d55e3","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/useWith.js","hash":"992d0f455bb3b824323580e322a9c79bd378d0c6","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/util.js","hash":"0724cd943bdab0b6bac86e7ac14e815e5aa1d9ed","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/value.js","hash":"e2d89ba1db613274b31d1d0fb1acb503644ad2e2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/valuesIn.js","hash":"b7b2dd2b81111cce5adfeeb45254a5372e6c28f5","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/where.js","hash":"03fc81a616faf63ab5d5097dc6ec2580e8a32fe2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/valueOf.js","hash":"a2ea35c715e7b3b68051a38cbf6802b29370d975","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/values.js","hash":"97fd629e19d9740fbfc87a26f86a259bbe7ca7f9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/whereEq.js","hash":"95d70e584abab7e886e137823be0c719bb67ca2e","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/without.js","hash":"2599adcc37dbd771a16ea4cc2a0c1d4513f38a9a","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/words.js","hash":"88c9e852c37f155c0dc429a401d44df09d9ca51c","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/wrapperLodash.js","hash":"ee531effe7e5e4472083f98b35497f858be33bb0","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/wrap.js","hash":"3631e867dfc78a67db38ffff790ee94022015a52","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/wrapperAt.js","hash":"7d374b517c61846dba2ec2898d75181025fde1be","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/wrapperReverse.js","hash":"0e41311c145f305b07da8c06db6e8e7ebe5433ec","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/wrapperChain.js","hash":"1e4f82ce87696ad134450678d2fba27bb46cd123","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/wrapperValue.js","hash":"51e6e912753e7d425a490e90c8205fe83dc435f1","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/xor.js","hash":"bb7e8f6d42f3c73d3358ad081b4e00796d1cc771","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/xorWith.js","hash":"8d0c86364c19f9c2b17feac93b112781ff81b3c2","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/xorBy.js","hash":"476caccbc9387505d0e110f814102335c77bb511","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/zip.js","hash":"3017d4235f2ff6d2658a88eb540246821e9dcf04","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/zipObj.js","hash":"353dfff83f25574ff58209ae8656869ab5516e81","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/zipAll.js","hash":"e5e2a68f3c74299cb44da02653f7773d7f6ae554","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/zipObject.js","hash":"8449412e15d18c596810ebfa34c5a0766f22b7e9","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/zipObjectDeep.js","hash":"e27ad4a21f3e47d1a59b796566d3a4c5ffbe5ebb","modified":499162500000},{"_id":"themes/next/node_modules/lodash/fp/zipWith.js","hash":"641ede21898d379ba0b8b9038913fa0400a271b9","modified":499162500000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1559399176381},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1559399176382},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1559399176343},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1559399176343},{"_id":"themes/next/node_modules/ajv/lib/compile/equal.js","hash":"c7ede80246da381027e076806d365f59d707eeed","modified":1546470630000},{"_id":"themes/next/node_modules/ajv/lib/compile/error_classes.js","hash":"e5d1be01d1186282d7084b0b1635b25c9de0edd3","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/compile/async.js","hash":"b92fd01f77b6173d30b61f50a1558e4b7a5e8901","modified":1549795975000},{"_id":"themes/next/node_modules/ajv/lib/compile/rules.js","hash":"c323bc0238e4847bf585da9646c78ac905fabfdf","modified":1551608391000},{"_id":"themes/next/node_modules/ajv/lib/compile/resolve.js","hash":"2ec459f3b4e593fe59f7059e153d27c6373ce3ce","modified":1556359206000},{"_id":"themes/next/node_modules/ajv/lib/compile/formats.js","hash":"cbc34eb98002090696a7fff0b69967156c41813b","modified":1541366994000},{"_id":"themes/next/node_modules/ajv/lib/compile/index.js","hash":"bc8336fc0bea81579271dc06e804a17bd98799db","modified":1550828083000},{"_id":"themes/next/node_modules/ajv/lib/compile/schema_obj.js","hash":"2ab2f6d0a3cf8f771cfda8884b2e674902a91205","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/compile/ucs2length.js","hash":"d9c7a635a1729fd3c961c2cc40303b88fdca0f31","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/compile/util.js","hash":"6d897b9983bb008efd68be8f0ccd4dd0c60bcac0","modified":1562435400000},{"_id":"themes/next/node_modules/ajv/lib/refs/data.json","hash":"2e29c1cb400f7f4e02165ae4d898bab103f23ded","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/refs/json-schema-draft-04.json","hash":"495a9cb5d013ccfd70753dce5c4aff98a96b8c2c","modified":1544991699000},{"_id":"themes/next/node_modules/ajv/lib/refs/json-schema-draft-07.json","hash":"177b34219475baf7616d881ecb5742f0c8b91435","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/refs/json-schema-draft-06.json","hash":"1096d2305c0f7173cc25027a20e48085b19f3cae","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/refs/json-schema-secure.json","hash":"e1decba35a40a39c40de1183f8415cf6a3fc782a","modified":1549136722000},{"_id":"themes/next/node_modules/ajv/lib/dot/_limitItems.jst","hash":"a5f08855d0b2a882c4b3341122bc5aa07cc0d38b","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/_limit.jst","hash":"6d458a410022561395ed03e3fe6e685b6160f5da","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/_limitProperties.jst","hash":"9dd3a6a07f75c6bc7cdd46feda6a096e0bbf430d","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/_limitLength.jst","hash":"15a0514666e349800af264c9f6e9ee8c45b92a62","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/allOf.jst","hash":"49ff68e885014484926de4e250c62086a0902abb","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/coerce.def","hash":"f0a517b2dd8e31da0c6b808bcc8d7904cf1648fa","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/comment.jst","hash":"930de86311e5800b0d21ee60311e4c75a74d6574","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/anyOf.jst","hash":"29416f6f53c67cc8c8161e6cf822572fa6de929c","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/contains.jst","hash":"36e37f3bd5a76e2f0c73a58885aaa2148adfd0db","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/custom.jst","hash":"f517c326ef04765265fa6d5acf4f173e8b830c6b","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/const.jst","hash":"65cac846e3a2e69328d396c9dc4ee67d527fbaa2","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/defaults.def","hash":"67975ec84373ef9bc3053c8f3f42b5a00c7f7ee2","modified":1551610349000},{"_id":"themes/next/node_modules/ajv/lib/dot/definitions.def","hash":"30cec040eafab284c83bf6a7fbe8d9b0377b9256","modified":1563050038000},{"_id":"themes/next/node_modules/ajv/lib/dot/dependencies.jst","hash":"22925e478bd2a4dac309963f2723193676d2c91e","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/format.jst","hash":"5092f81395ac19ad8dc31b4f26e3ab01b7cbf110","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/if.jst","hash":"5a8ba0af02d9cc2a8bcd454b392b267842376f62","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/enum.jst","hash":"37e8b49379577599a005a7894ed0c18c5db174eb","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/errors.def","hash":"0875027f34a53ade9094d4a40f89e05d473f8df2","modified":1537698056000},{"_id":"themes/next/node_modules/ajv/lib/dot/multipleOf.jst","hash":"f459196cc2232c4b5213281398d98a3045b37584","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/missing.def","hash":"2b26bb3614903b676ae185b6c21e50b755efacb8","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/oneOf.jst","hash":"f723d7e025fbb1a77b8269b60063b57c74a71898","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/items.jst","hash":"d92f27b253b011a601e2e3e905d3a1102b3cce23","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/not.jst","hash":"d56a4d3eb89d13805d9d034f9f06f738c70db267","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/pattern.jst","hash":"e58b96f12768261e83c1358ca9b32b0fb7c28a31","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/propertyNames.jst","hash":"36498e8c7d9c46dd6a82edcec1d0d564107bee1c","modified":1537699721000},{"_id":"themes/next/node_modules/ajv/lib/dot/ref.jst","hash":"9e3023769566658e18608eed8fac87103a723a96","modified":1537699674000},{"_id":"themes/next/node_modules/ajv/lib/dot/properties.jst","hash":"1905d51b0b54df7c847efcce4a3ec5be4d5bb04c","modified":1546547057000},{"_id":"themes/next/node_modules/ajv/lib/dot/validate.jst","hash":"3f49e5bd73c686ed87706aca4905b9738c7506d5","modified":1563050101000},{"_id":"themes/next/node_modules/ajv/lib/dot/uniqueItems.jst","hash":"4fda787abf34112f08ccbb7a6840dd0c169ab671","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dot/required.jst","hash":"03476ddfca57724405f9a0add4078c5b373a1193","modified":1524950955000},{"_id":"themes/next/node_modules/bluebird/js/browser/bluebird.core.min.js","hash":"f5f3d9eec5dc6a59a5aaaf8ebdc6de722fed1b96","modified":499162500000},{"_id":"themes/next/node_modules/ajv/dist/ajv.min.js.map","hash":"c664a48c128433d17fd1909e42248614bd1ba2b3","modified":1563113730000},{"_id":"themes/next/node_modules/asn1/lib/ber/index.js","hash":"e1a2a7e3cc467b39253564334361ca647b922e08","modified":1532629842000},{"_id":"themes/next/node_modules/asn1/lib/ber/types.js","hash":"8a41886747dc1f7cde7e4e5cb60e1a3b8a4c813c","modified":1518292051000},{"_id":"themes/next/node_modules/ajv/dist/ajv.min.js","hash":"aadccd03579437344af466f73d91f51aa5c22adb","modified":1563113730000},{"_id":"themes/next/node_modules/asn1/lib/ber/errors.js","hash":"d12f2ffed9bce548fe961a440ba0bf3af1003bd1","modified":1532987578000},{"_id":"themes/next/node_modules/asn1/lib/ber/reader.js","hash":"4dec016f17c4bd3ec055f0b8154524d2349f93f2","modified":1532629842000},{"_id":"themes/next/node_modules/bluebird/js/release/any.js","hash":"424dfe2a1afeaad729ca2be5ccfd443311716c41","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/assert.js","hash":"da909f13035601fbcdc3a28937dd44e3008327ee","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/async.js","hash":"f37a2ef4a85f402afa91d9c765bd48fcf00c4f74","modified":499162500000},{"_id":"themes/next/node_modules/asn1/lib/ber/writer.js","hash":"d3b17b8bb9e7404ed0fa11465a565883583a2e4e","modified":1532652160000},{"_id":"themes/next/node_modules/bluebird/js/release/bluebird.js","hash":"f68f4e491f49be5a5a3c5d04fafcd8abaf02fc2a","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/bind.js","hash":"6ba64f83e985a1cb7dcf945490e4c280460ccb25","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/call_get.js","hash":"b53dcc39da361dbc72a49ced90d5290d94c70b80","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/catch_filter.js","hash":"cbdedec0db19fe2af76d5594ed8bf819000d7c79","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/cancel.js","hash":"1a6c901863d671048f58095fe9b568b2a8407729","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/context.js","hash":"ea7eb1ab2c8231fa6bfa8446cf730736735396e0","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/each.js","hash":"44949d491571dd38f6ffdd777cb44f96aebb5fc8","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/debuggability.js","hash":"95d144d42063262c33f2b8c6c0ab2c836032faf4","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/direct_resolve.js","hash":"470ca4d9a7e387ceceb383bc2640202f5fa6bda2","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/es5.js","hash":"019c4e8b62031ea49aedc86dedd20318c6122698","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/errors.js","hash":"98deaaee17f36851937108257e028a7e237b3f5e","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/finally.js","hash":"cc388e93ecc2e12145c080cc67b0d8517e481c13","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/generators.js","hash":"6b334d17275b48548e44f74f477b2e8bd49da304","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/filter.js","hash":"a5f3aee4afbc67d372e5b4fbaeac047d0d9c779b","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/map.js","hash":"4d96c67c999d7f8a155934d25bfe8cc4912ad469","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/join.js","hash":"5ab50e4fd76d09506700b47e7672c9d528816d4b","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/nodeify.js","hash":"ebd75c010fcf1c4f709f4b444e62b80cfcde2a0f","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/nodeback.js","hash":"46fc87ede6ca68434439a5216fe8e6f89d8d8a1f","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/props.js","hash":"d26e05c3860fd45b7ae9008995493c6d01c2f2e7","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/method.js","hash":"02a5a493c2cf2045fbb20b2751381e4b4e29dbe8","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/promisify.js","hash":"3831be99120a57a3c07406f7012467f02346e92c","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/promise.js","hash":"d788b4e6432e3e1ccbb060d7582da58c188e3745","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/race.js","hash":"5f5b5403be19793dec5658af27fe996173eb4990","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/reduce.js","hash":"8f0d6b75ed5682d3a7e45254d76618258b45bb2a","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/promise_array.js","hash":"2b05f82b39c408715260b264dbeb886d080efc59","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/queue.js","hash":"e6fddd439658a46c57132ec8d07dd04bf1d743e6","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/settle.js","hash":"36f97c3732ae907af622fd4c859d29da1255fdfa","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/timers.js","hash":"ad830b808b63bd4b291a3b23210bdaf850f1f818","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/synchronous_inspection.js","hash":"21be4c93fd5b93b07315c5edc930800e686e4dea","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/schedule.js","hash":"8a64f94ea735e05a06ace65d39eb00f769e4c646","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/some.js","hash":"b548ddd7eb6b35ae6e97d931e0a8ef6dd512b3b5","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/thenables.js","hash":"b06e617f4b1e5c9c33c9c4e0baabe709550154b1","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/using.js","hash":"5b287a7d967647d38091aec63f880abc46caeb4f","modified":499162500000},{"_id":"themes/next/node_modules/bluebird/js/release/util.js","hash":"04336ba302f321dd2fb4a6be7d6fa3cab5408d1b","modified":499162500000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/_limitLength.js","hash":"edd42a334c6d62277032ba3c4ec1258125af9f38","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/_limit.js","hash":"29a7e7b7ac1bd3fdf668a89099c1a6d85a8c6057","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/README.md","hash":"0c215e65e288a0fff457173582f2fc5909c4b30b","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/_limitItems.js","hash":"84065362fde2facc8ec61445ca5f8a33de4271c7","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/allOf.js","hash":"3070a50bd6296b0acd6ac069a15de75dee1e9d0c","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/anyOf.js","hash":"43d573153727106909874c1983c4444dcb288e3e","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/comment.js","hash":"f66436b86d39f7957ae1620dc6622a035c326652","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/_limitProperties.js","hash":"fe68f0ca4c0c262288b8c8c4f9938ccde0b24d21","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/contains.js","hash":"777f4c2c96f3a54612716339e75e982ac55a5252","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/format.js","hash":"277c829939db5d6b085c087477d3a322e8c64b41","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/dependencies.js","hash":"54279f1cb1f15c01f00dab1c08d3284ffe0d6d79","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/const.js","hash":"bd1497e489a5705f94f54c163290ca0ad9f3efa9","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/custom.js","hash":"3caee52d9b3a90a66868be0f94caa11f90fcc9b1","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/enum.js","hash":"0d78a4241b424975a4ebf46ebe0c6cf5670843b1","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/index.js","hash":"ed26a5156bd9f684f3617c3700ea5ba3cc8ce6aa","modified":1524950955000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/if.js","hash":"9efcce7a4f6f98125b3bd55df3f1b50d01dabf3a","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/not.js","hash":"0a245b0ac8ee8849df526b7f5ebe0a554efe2b8e","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/oneOf.js","hash":"6ba59ce92d14af607a3cd964ccf767885e209a35","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/multipleOf.js","hash":"35a8669773a17aea549749b4b1a2068c0b8f367b","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/items.js","hash":"f6b5ec08c6648f89695de8c73427b20e6a455a77","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/properties.js","hash":"652807f57c4438e1a603a0dad2ebf53748dd0b63","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/propertyNames.js","hash":"c8b534ca5e8087184eb0ddcb4346b1d2cf04d35c","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/ref.js","hash":"92878f3ac42b3d9e4c0a331ae4837c6f357d53fd","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/pattern.js","hash":"7147321be498be8d610a675c896c3f5b6c5480ad","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/uniqueItems.js","hash":"bc8d2ffa2224dde169405c7b27f0349981bf80c4","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/required.js","hash":"43b2fd789e4b4535df4b033a12b7c1511dda696c","modified":1563113728000},{"_id":"themes/next/node_modules/ajv/lib/dotjs/validate.js","hash":"51c7f7f3f2951bc14214802a4fab0991b2a99366","modified":1563113728000},{"_id":"themes/next/node_modules/json-schema/draft-03/examples/address","hash":"b6e70440f5faf725b978da4e92f6f39100377674","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-03/examples/calendar","hash":"3e53cb296ba1799f39c5a00bc4627fa10e26e165","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-03/examples/card","hash":"c1405e471f81bbb4eea94c2ef85fa07195a8ad80","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-03/examples/geo","hash":"74d8c66f1ba74c6cd330b8acd5948efb5f2879e4","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema/draft-03/examples/interfaces","hash":"9666b12a3d05e526886aa2650f0439d32b25f0dd","modified":1289526444000},{"_id":"themes/next/node_modules/json-schema-traverse/spec/fixtures/schema.js","hash":"a900ee83c77812d766a338c77661982a3e232eac","modified":1525807808000},{"_id":"themes/next/node_modules/performance-now/test/scripts/delayed-call.coffee","hash":"e4f912e6ef8cd309316302052de85110e44059f3","modified":1484000812000},{"_id":"themes/next/node_modules/performance-now/test/scripts/delayed-require.coffee","hash":"d7dede6d953440f6ff3c71e3a5676d5d00f523cf","modified":1484000809000},{"_id":"themes/next/node_modules/performance-now/test/scripts/difference.coffee","hash":"4a22bfede1f6f6ab3224c20db5132f957c2a12bc","modified":1483841832000},{"_id":"themes/next/node_modules/performance-now/test/scripts/initial-value.coffee","hash":"c71cfd6cb07e088440755a867c2082a40276cef4","modified":1484000674000},{"_id":"themes/next/node_modules/psl/data/rules.json","hash":"5c0dd25b6c1e6c5c936fbf4be4f7980ecd80f874","modified":499162500000},{"_id":"themes/next/node_modules/psl/dist/psl.min.js","hash":"7517a9603b2b7e229fe66b83606284eca7d02dcb","modified":499162500000},{"_id":"themes/next/node_modules/psl/dist/psl.js","hash":"57c5f5a6dca7009fbf5428de4d141678df49f7dd","modified":499162500000},{"_id":"themes/next/node_modules/sshpk/lib/formats/auto.js","hash":"8abe6e50315b25fb98f72db675d5b85d9a4be5ff","modified":1545357205000},{"_id":"themes/next/node_modules/sshpk/lib/formats/dnssec.js","hash":"a1fe32a1c83ca6a7ff22f3e33dd3dc5db25efcbe","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/formats/pem.js","hash":"d48582f351bf4e3c8d998b0ed530c335a9d6c53a","modified":1545357205000},{"_id":"themes/next/node_modules/sshpk/lib/formats/pkcs1.js","hash":"2c3c0d6eaec70316d822066319bf5b304e28c84b","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/formats/rfc4253.js","hash":"f9cf4a4a600db418c97b91a788253936373e1996","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/formats/openssh-cert.js","hash":"1c64265f089e7324b7bd21aee5ae150e60e5363c","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/formats/putty.js","hash":"409bea5049f08db667a1ec6279d8cd745d88cc7a","modified":1545357205000},{"_id":"themes/next/node_modules/sshpk/lib/formats/ssh-private.js","hash":"81059e3cc027b55923d1742ff195f6a81ca5b808","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/formats/pkcs8.js","hash":"e371c7335004e81013eeb161ccbee0c09d948496","modified":1548279249000},{"_id":"themes/next/node_modules/sshpk/lib/formats/x509-pem.js","hash":"649d1fed517cac91a479b027afc5012fb5d2d89b","modified":1545357205000},{"_id":"themes/next/node_modules/sshpk/lib/formats/ssh.js","hash":"c8966077b15dd7244605704070c2807d74abc180","modified":1539302377000},{"_id":"themes/next/node_modules/sshpk/lib/formats/x509.js","hash":"5f049035a7f9b8b799dc817e769ef6d01cf23423","modified":1548279249000},{"_id":"themes/next/node_modules/sshpk/man/man1/sshpk-conv.1","hash":"a64a4769ca648e2047d255bf5237ce051f6af15a","modified":1452565517000},{"_id":"themes/next/node_modules/sshpk/man/man1/sshpk-verify.1","hash":"a5c70f82de5313ee103ba9bf3cfb9774465de370","modified":1452565517000},{"_id":"themes/next/node_modules/sshpk/man/man1/sshpk-sign.1","hash":"742f5bb2986ad635eb081bad695573cd6826c96b","modified":1452565517000},{"_id":"themes/next/node_modules/tough-cookie/node_modules/punycode/package.json","hash":"1deb40d9b0da10cef20da977ed7028fa2530a93d","modified":1567332486132},{"_id":"themes/next/node_modules/tough-cookie/node_modules/punycode/LICENSE-MIT.txt","hash":"d7384cd3ed0c9614f87dde0f86568017f369814c","modified":1408532732000},{"_id":"themes/next/node_modules/tough-cookie/node_modules/punycode/README.md","hash":"c332a96ebde88c232159d794e4d417a6d834e1e1","modified":1424815497000},{"_id":"themes/next/node_modules/uri-js/dist/es5/uri.all.d.ts","hash":"d4324455b07945e1532bcfa34bae846af6c4415c","modified":1525378838000},{"_id":"themes/next/node_modules/tough-cookie/node_modules/punycode/punycode.js","hash":"cc7535a3f0235f4b3ad0030b16f418eb2abf454c","modified":1458437195000},{"_id":"themes/next/node_modules/uri-js/dist/es5/uri.all.min.js","hash":"5d1c8363b0b1eafb671506adfab85e92daa4b404","modified":1525380350000},{"_id":"themes/next/node_modules/uri-js/dist/es5/uri.all.min.d.ts","hash":"d4324455b07945e1532bcfa34bae846af6c4415c","modified":1525378842000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/index.d.ts","hash":"b1ba4e83c7b85c425348e42afbdd6623d58ec116","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/index.js","hash":"a0d37347c70b264ed70e333f6970c39f9621d2ef","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/regexps-iri.d.ts","hash":"6655ab97cac00bd3d0c52793653bd39a96939093","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/regexps-iri.js","hash":"02cfa793d18c0a40d35f39b9e1897b9797121d0e","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/index.js.map","hash":"4a43fd520e4c81e0ef4a7840289d99b097e0596a","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/regexps-iri.js.map","hash":"523cf639bad4c664cfd93f9f084b7458128f341c","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/regexps-uri.js","hash":"5b1d3ca1d46a19b0bf3b64d2f3c4376d20557e53","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/regexps-uri.d.ts","hash":"eba9707106c4287ad5b00db8477f23b9930c1ab5","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/regexps-uri.js.map","hash":"c5d6f4a6fde82cf5c364f718626020b8e82ee971","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/uri.d.ts","hash":"d4324455b07945e1532bcfa34bae846af6c4415c","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/util.d.ts","hash":"6e78858714bb05d182a73cf5eb5e6f109a197c48","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/util.js","hash":"7a005e716e192efae74e9fb59e6e5e95c61ffb02","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/util.js.map","hash":"3ff2f3e25c2ee01972ea660cdb920f666bdbae51","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/src/schemes/http.ts","hash":"0677c6506506fd7cd4787006a271a72bef32bc15","modified":1522462102000},{"_id":"themes/next/node_modules/uri-js/src/schemes/https.ts","hash":"7d2cce6fbbb3f4cdf198a7f16ead6d74c11f5ce3","modified":1522462102000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/uri.js","hash":"8ef3aa2bf904853b749f93453f448bd8a5d42ae4","modified":1525380350000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/uri.js.map","hash":"cf3a3e6bee6836b6b0a8246a150597718fc067ec","modified":1525380350000},{"_id":"themes/next/node_modules/uri-js/src/schemes/mailto.ts","hash":"c2e5f71d53c48abf356e7eae2142342dd7c9fa44","modified":1525380349000},{"_id":"themes/next/node_modules/uri-js/src/schemes/urn-uuid.ts","hash":"da56bdbb6354e8661ebbe44513bf5875c70b6589","modified":1522462037000},{"_id":"themes/next/node_modules/uri-js/src/schemes/urn.ts","hash":"84542997f2b0f5df1ef20b0e1659b44c1a703471","modified":1522461415000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1559399176347},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1559399176347},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1559399176348},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1559399176347},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1559399176348},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1559399176350},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1559399176353},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1559399176356},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1559399176356},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"f7c44b0ee46cf2cf82a4c9455ba8d8b55299976f","modified":1559399176356},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1559399176356},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1559399176356},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1559399176356},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1559399176357},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1559399176358},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"18c3336ee3d09bd2da6a876e1336539f03d5a973","modified":1559399176358},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1559399176358},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1559399176358},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1559399176358},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"3b25edfa187d1bbbd0d38b50dd013cef54758abf","modified":1559399176358},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1559399176358},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1559399176358},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1559399176359},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1559399176359},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"02fb8fa6b6c252b6bed469539cd057716606a787","modified":1559399176359},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1559399176359},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1559399176359},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1559399176360},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1559399176360},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1559399176360},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1559399176360},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"9d16fa3c14ed76b71229f022b63a02fd0f580958","modified":1559399176360},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1559399176360},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1559399176367},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1559399176370},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1559399176370},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1559399176372},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1559399176372},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1559399176373},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1559399176373},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1559399176373},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1559399176373},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1559399176374},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1559399176374},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1559399176374},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1559399176375},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1559399176375},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1559399176377},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1559399176377},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1559399176377},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1559399176387},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1559399176387},{"_id":"themes/next/node_modules/bluebird/js/browser/bluebird.min.js","hash":"a1c0973967503119917d03e0e75794b2d2c76864","modified":499162500000},{"_id":"themes/next/node_modules/uri-js/dist/es5/uri.all.min.js.map","hash":"41917e25135dffb916236131ee738f6b23c2c43b","modified":1525380350000},{"_id":"themes/next/node_modules/uri-js/dist/es5/uri.all.js","hash":"faef07b7550055eb3db406ebb523941308156663","modified":1525380350000},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1559399176369},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1559399176369},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1559399176380},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1559399176380},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1559399176389},{"_id":"themes/next/node_modules/bluebird/js/browser/bluebird.core.js","hash":"1e220fda6bd0a97f18e2bcc468688cceb2819776","modified":499162500000},{"_id":"themes/next/node_modules/ajv/dist/ajv.bundle.js","hash":"0d5e9abd6c541a28e20083764e2cdc8a548af519","modified":1563113730000},{"_id":"themes/next/node_modules/bluebird/js/browser/bluebird.js","hash":"a1e4f214fafd3f19bc05a9541590c03f5908583c","modified":499162500000},{"_id":"themes/next/node_modules/uri-js/dist/es5/uri.all.js.map","hash":"f265088db905ae5cc183e982c5a2fdeb400ac5d1","modified":1525380350000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/http.d.ts","hash":"f06607d56875f60a1881072276649478b370937d","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/http.js.map","hash":"d390843750c21e54a387d0c8983ba957fda77acf","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/https.d.ts","hash":"f06607d56875f60a1881072276649478b370937d","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/http.js","hash":"0ad5a305a7fcfcd629a256e86cd300538ffacec7","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/https.js","hash":"497cb25d5d58f625f5f3744bfa61324e53c29ebc","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/https.js.map","hash":"8117bf180143107503255bf7c8f42bb4a62e393b","modified":1525378824000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/mailto.d.ts","hash":"a69bf0e11fa925926186999e7316f86a6d2d7e29","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/mailto.js","hash":"1b770da12297ddd4dd6382c7960513f189f1b599","modified":1525380350000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/mailto.js.map","hash":"fbd2785adbd9b3fcbfbf8232d855964b391594dd","modified":1525380350000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/urn-uuid.d.ts","hash":"43c5d0581b1088bafe9da4070bcdec4befda911c","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/urn-uuid.js","hash":"54e26134ab217bf898b18e87a247eaf87f7f880f","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/urn-uuid.js.map","hash":"0dbc4c2eb8eec044d25ff6c97e7b5908d940fcb4","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/urn.d.ts","hash":"24a9ea3b1cca29b3423ae7103c66f8cb183135c5","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/urn.js.map","hash":"28cd13de1b465d39e7ba73110a03cc86b6c8302d","modified":1525378825000},{"_id":"themes/next/node_modules/uri-js/dist/esnext/schemes/urn.js","hash":"47cd5fdba7dd474dc8eea658cdf0598f411696ad","modified":1525378825000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1559399176348},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1559399176348},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1559399176348},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1559399176348},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1559399176348},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"7905a7f625702b45645d8be1268cb8af3f698c70","modified":1559399176348},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1559399176349},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1559399176349},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1559399176349},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1559399176350},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1559399176349},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1559399176349},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1559399176349},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"25dc25f61a232f03ca72472b7852f882448ec185","modified":1559399176349},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1559399176350},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1559399176350},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1559399176350},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1559399176350},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1559399176351},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"535b3b4f8cb1eec2558e094320e7dfb01f94c0e7","modified":1559399176350},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"aea21141015ca8c409d8b33e3e34ec505f464e93","modified":1559399176351},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1559399176351},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1559399176351},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1559399176351},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1559399176351},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1559399176351},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"d5a4e4fc17f1f7e7c3a61b52d8e2e9677e139de7","modified":1559399176351},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1559399176352},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"262debfd4442fa03d9919ceb88b948339df03fb0","modified":1559399176352},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1559399176354},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1559399176354},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"37e951e734a252fe8a81f452b963df2ba90bfe90","modified":1559399176354},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1559399176354},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1559399176354},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1559399176354},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1559399176354},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1559399176355},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1559399176355},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1559399176352},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"0a6c0efffdf18bddbc1d1238feaed282b09cd0fe","modified":1559399176352},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1559399176352},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1559399176352},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1559399176352},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"12937cae17c96c74d5c58db6cb29de3b2dfa14a2","modified":1559399176353},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1559399176353},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"50305b6ad7d09d2ffa4854e39f41ec1f4fe984fd","modified":1559399176353},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1559399176355},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1559399176353},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1559399176353},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1559399176355},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1559399176355},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1559399176355},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1559399176355},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1559399176355},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1559399176355},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1559399176356},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1559399176356},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1ccfbd4d0f5754b2dc2719a91245c95f547a7652","modified":1559399176356},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1559399176359},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1559399176360},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1559399176368},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1559399176359},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1559399176368},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1559399176369},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1559399176369},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1559399176373},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1559399176373},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1559399176373},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1559399176374},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1559399176374},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1559399176374},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1559399176377},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1559399176368},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1559399176378},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1559399176380},{"_id":"themes/next/node_modules/lodash/lodash.js","hash":"9b8862d749672156a62da0e842c66967ff6cd307","modified":499162500000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1559399176372},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1559399176387},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1559399176379}],"Category":[],"Data":[{"_id":"recommended_posts","data":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}],"Page":[{"title":"categories","date":"2018-12-08T08:52:49.000Z","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-12-08 16:52:49\n---\n","updated":"2019-06-01T14:26:16.310Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ck21ro5dw003iapwn8obscpan","content":"","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":""},{"title":"About","date":"2018-12-08T08:46:18.000Z","_content":"\n###  专注 k8s 云原生实践\n<br/>\n\ngithub: [https://github.com/gosoon](https://github.com/gosoon)\n\n简书：[https://www.jianshu.com/u/a004b422adae](https://www.jianshu.com/u/a004b422adae)\n\n<br/>\n最新发表的文章会及时推送到公众号，欢迎关注：\n\n![](http://cdn.tianfeiyu.com/qrcode_for_gh_82681b81d20b_258.jpg)\n","source":"about/index.md","raw":"---\ntitle: About\ndate: 2018-12-08 16:46:18\n---\n\n###  专注 k8s 云原生实践\n<br/>\n\ngithub: [https://github.com/gosoon](https://github.com/gosoon)\n\n简书：[https://www.jianshu.com/u/a004b422adae](https://www.jianshu.com/u/a004b422adae)\n\n<br/>\n最新发表的文章会及时推送到公众号，欢迎关注：\n\n![](http://cdn.tianfeiyu.com/qrcode_for_gh_82681b81d20b_258.jpg)\n","updated":"2019-09-08T03:07:27.717Z","path":"about/index.html","comments":1,"layout":"page","_id":"ck21ro5dx003japwn8e6233kt","content":"<h3 id=\"专注-k8s-云原生实践\"><a href=\"#专注-k8s-云原生实践\" class=\"headerlink\" title=\"专注 k8s 云原生实践\"></a>专注 k8s 云原生实践</h3><p><br></p>\n<p>github: <a href=\"https://github.com/gosoon\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon</a></p>\n<p>简书：<a href=\"https://www.jianshu.com/u/a004b422adae\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/u/a004b422adae</a></p>\n<p><br><br>最新发表的文章会及时推送到公众号，欢迎关注：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/qrcode_for_gh_82681b81d20b_258.jpg\" alt=\"\"></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<h3 id=\"专注-k8s-云原生实践\"><a href=\"#专注-k8s-云原生实践\" class=\"headerlink\" title=\"专注 k8s 云原生实践\"></a>专注 k8s 云原生实践</h3><p><br></p>\n<p>github: <a href=\"https://github.com/gosoon\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon</a></p>\n<p>简书：<a href=\"https://www.jianshu.com/u/a004b422adae\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/u/a004b422adae</a></p>\n<p><br><br>最新发表的文章会及时推送到公众号，欢迎关注：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/qrcode_for_gh_82681b81d20b_258.jpg\" alt=\"\"></p>\n"},{"title":"tags","date":"2018-12-08T08:46:27.000Z","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-12-08 16:46:27\n---\n","updated":"2019-06-01T14:26:16.311Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ck21ro5dy003kapwn6372p01e","content":"","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":""}],"Post":[{"title":"使用 code-generator 为 CustomResources 生成代码","date":"2019-08-06T12:50:30.000Z","type":"code-generator","_content":"\nkubernetes 项目中有相当一部分代码是自动生成的，主要是 API 的定义和调用方法，kubernetes 项目下 `k8s.io/kubernetes/hack/` 目录中以 update 开头的大部分脚本都是用来生成代码的。[code-generator](https://github.com/kubernetes/code-generator/) 是官方提供的代码生成工具，在实现自定义 controller 的时候需要用到 CRD，也需要使用该工具生成对 CRD 操作的代码。\n\n\n### 要生成哪些代码\n\n在自定义 controller 时需要用到 typed clientsets，informers，listers 和 deep-copy 等函数，这些函数都可以使用 [code-generator](https://github.com/kubernetes/code-generator/) 来生成，具体的作用可以参考：[kubernetes 中 informer 的使用]([http://blog.tianfeiyu.com/2019/05/17/client-go_informer/](http://blog.tianfeiyu.com/2019/05/17/client-go_informer/)。\n\ncode-generator 里面包含多个生成代码的工具，下面是需要用到的几个：\n\n- deepcopy-gen：为每种类型T生成方法： `func (t* T) DeepCopy() *T`，CustomResources 必须实现runtime.Object 接口且要有 DeepCopy 方法\n\n- client-gen：为 CustomResource APIGroups 生成 typed clientsets\n\n- informer-gen：为 CustomResources 创建 informers，用来 watch 对应 CRD 所触发的事件，以便对 CustomResources 的变化进行对应的处理\n\n- lister-gen：为 CustomResources 创建 listers，用来对 GET/List 请求提供只读的缓存层\n\n\n\n除了上面几个工具外，code-generator 中还提供了 conversion-gen、defaulter-gen、register-gen、set-gen，这些生成器可以应用在其他场景，比如构建聚合 API 服务时会用到一些内部的类型，conversion-gen 会为这些内部和外部类型之间创建转换函数，defaulter-gen 会处理某些字段的默认值。\n\n\n\n### 代码生成步骤\n\n使用 code-generator 生成代码还需要以下几步：\n\n- 创建指定的目录格式\n- 在代码中使用 tag 标注要生成哪些代码\n\n\n\n首先要创建指定的目录格式，目录的格式可以参考官方提供的示例项目：[sample-controller](https://github.com/kubernetes/sample-controller)，下文也会讲到，目录中需要包含对应 CustomResources 的定义以及  group 和 version 信息。\n\n其次要在在代码中使用 tag 标注要生成哪些代码，tag 有两钟类型，全局的和局部的，所有类型的 deepcopy tag 会默认启用，更多关于 tag 的使用方法可以参考：[Kubernetes Deep Dive: Code Generation for CustomResources](https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/)，也可以参考官方的示例 [code-generator/_example](https://github.com/kubernetes/code-generator/blob/master/_examples/crd/apis/example) 。\n\n\n\n### 开始生成代码\n\n本文以该 CRD 为例子进行演示，group 为`ecs.yun.com` ，version 为 `v1`：\n\n```\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kubernetesclusters.ecs.yun.com\nspec:\n  group: ecs.yun.com\n  names:\n    kind: KubernetesCluster\n    listKind: KubernetesClusterList\n    plural: kubernetesclusters\n    singular: kubernetescluster\n    shortNames:\n    - ecs\n  scope: Namespaced\n  subresources:\n    status: {}\n  version: v1\n  versions:\n  - name: v1\n    served: true\n    storage: true\n```\n\n创建指定目录结构 pkg/apis/${group}/${version}，group 可以定义一个 shortNames，也就是 CRD 中的 shortNames\n\n```\n$ mkdir -pv pkg/apis/ecs/v1\n```\n\n创建 doc.go：\n```\n$ cat << EOF > pkg/apis/ecs/v1/doc.go\n// Package v1 contains API Schema definitions for the ecs v1 API group\n// +k8s:deepcopy-gen=package,register\n// +groupName=ecs.yun.com\npackage v1\nEOF\n```\n\n创建 register.go：\n\n```\n$ cat << EOF > pkg/apis/ecs/v1/register.go\npackage v1\n\nimport (\n\t\"github.com/gosoon/kubernetes-operator/pkg/apis/ecs\"\n\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/runtime/schema\"\n)\n\n// SchemeGroupVersion is group version used to register these objects\nvar SchemeGroupVersion = schema.GroupVersion{Group: ecs.GroupName, Version: \"v1\"}\n\n// Kind takes an unqualified kind and returns back a Group qualified GroupKind\nfunc Kind(kind string) schema.GroupKind {\n\treturn SchemeGroupVersion.WithKind(kind).GroupKind()\n}\n\n// Resource takes an unqualified resource and returns a Group qualified GroupResource\nfunc Resource(resource string) schema.GroupResource {\n\treturn SchemeGroupVersion.WithResource(resource).GroupResource()\n}\n\nvar (\n\tSchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)\n\tAddToScheme   = SchemeBuilder.AddToScheme\n)\n\n// Adds the list of known types to Scheme.\nfunc addKnownTypes(scheme *runtime.Scheme) error {\n\tscheme.AddKnownTypes(SchemeGroupVersion,\n\t\t&KubernetesCluster{},\n\t\t&KubernetesClusterList{},\n\t)\n\tmetav1.AddToGroupVersion(scheme, SchemeGroupVersion)\n\treturn nil\n}\nEOF\n```\n\n创建 types.go，该文件中会定义多个 tag\n\n```\n$ cat << EOF > pkg/apis/ecs/v1/types.go\npackage v1\n\nimport (\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\n// +genclient\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// KubernetesCluster is the Schema for the kubernetesclusters API\ntype KubernetesCluster struct {\n\tmetav1.TypeMeta   `json:\",inline\"`\n\tmetav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n\tSpec   KubernetesClusterSpec   `json:\"spec,omitempty\"`\n\tStatus KubernetesClusterStatus `json:\"status,omitempty\"`\n}\n\n// KubernetesClusterSpec defines the desired state of KubernetesCluster\ntype KubernetesClusterSpec struct {\n\t// Add custom validation using kubebuilder tags:\n    // https://book.kubebuilder.io/beyond_basics/generating_crd.html\n\tTimeoutMins   string     `json:\"timeout_mins,omitempty\"`\n\tClusterType   string     `json:\"clusterType,omitempty\"`\n\tContainerCIDR string     `json:\"containerCIDR,omitempty\"`\n\tServiceCIDR   string     `json:\"serviceCIDR,omitempty\"`\n\tMasterList    []Node     `json:\"masterList\" tag:\"required\"`\n\tMasterVIP     string     `json:\"masterVIP,omitempty\"`\n\tNodeList      []Node     `json:\"nodeList\" tag:\"required\"`\n\tEtcdList      []Node     `json:\"etcdList,omitempty\"`\n\tRegion        string     `json:\"region,omitempty\"`\n\tAuthConfig    AuthConfig `json:\"authConfig,omitempty\"`\n}\n\n// AuthConfig defines the nodes peer authentication\ntype AuthConfig struct {\n\tUsername      string `json:\"username,omitempty\"`\n\tPassword      string `json:\"password,omitempty\"`\n\tPrivateSSHKey string `json:\"privateSSHKey,omitempty\"`\n}\n\n// KubernetesClusterStatus defines the observed state of KubernetesCluster\ntype KubernetesClusterStatus struct {\n\t// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html\n\tPhase KubernetesOperatorPhase `json:\"phase,omitempty\"`\n\n\t// when job failed callback or job timeout used\n\tReason string `json:\"reason,omitempty\"`\n\n\t// JobName is store each job name\n\tJobName string `json:\"jobName,omitempty\"`\n\n\t// Last time the condition transitioned from one status to another.\n\tLastTransitionTime metav1.Time `json:\"lastTransitionTime,omitempty\"`\n}\n\n// +genclient:nonNamespaced\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// KubernetesClusterList contains a list of KubernetesCluster\ntype KubernetesClusterList struct {\n\tmetav1.TypeMeta `json:\",inline\"`\n\tmetav1.ListMeta `json:\"metadata,omitempty\"`\n\tItems           []KubernetesCluster `json:\"items\"`\n}\n\n// users\n// \"None,Creating,Running,Failed,Scaling\"\ntype KubernetesOperatorPhase string\n\ntype Node struct {\n\tIP string `json:\"ip,omitempty\"`\n}\nEOF\n```\n\n\n\n执行命令生成代码：\n\n```\n$ $GOPATH/src/k8s.io/code-generator/generate-groups.sh all github.com/gosoon/kubernetes-operator/pkg/client github.com/gosoon/kubernetes-operator/pkg/apis ecs:v1\n```\n\n\n\ngenerate-groups.sh 需要四个参数：\n\n- 第一个 参数：all，也就是要生成所有的模块，clientset，informers，listers\n- 第二个参数：github.com/gosoon/test/pkg/client  这个是你要生成代码的目录，目录的名称一般定义为 client\n- 第三个参数：github.com/gosoon/test/pkg/apis  这个目录是已经创建好的源目录\n- 第四个参数：\"ecs:v1\" 是 group 和 version 信息，ecs 是 apis 下的目录，v1 是 ecs 下面的目录\n\n\n\n生成的代码如下所示：\n\n```\n.\n└── pkg\n    ├── apis\n    │   └── ecs\n    │       └── v1\n    │           ├── doc.go\n    │           ├── register.go\n    │           ├── types.go\n    │           └── zz_generated.deepcopy.go\n    └── client\n        ├── clientset\n        │   └── versioned\n        │       ├── clientset.go\n        │       ├── doc.go\n        │       ├── fake\n        │       │   ├── clientset_generated.go\n        │       │   ├── doc.go\n        │       │   └── register.go\n        │       ├── scheme\n        │       │   ├── doc.go\n        │       │   └── register.go\n        │       └── typed\n        │           └── ecs\n        │               └── v1\n        │                   ├── doc.go\n        │                   ├── ecs_client.go\n        │                   ├── fake\n        │                   │   ├── doc.go\n        │                   │   ├── fake_ecs_client.go\n        │                   │   └── fake_kubernetescluster.go\n        │                   ├── generated_expansion.go\n        │                   └── kubernetescluster.go\n        ├── informers\n        │   └── externalversions\n        │       ├── ecs\n        │       │   ├── interface.go\n        │       │   └── v1\n        │       │       ├── interface.go\n        │       │       └── kubernetescluster.go\n        │       ├── factory.go\n        │       ├── generic.go\n        │       └── internalinterfaces\n        │           └── factory_interfaces.go\n        └── listers\n            └── ecs\n                └── v1\n                    ├── expansion_generated.go\n                    └── kubernetescluster.go\n\n21 directories, 26 files\n```\n\n\n\nCRD 以及生成的代码见：[kubernetes-operator](https://github.com/gosoon/kubernetes-operator)。\n\n\n\n### 总结\n\n本问讲述了如何使用 code-generator 生成代码，要使用自定义 controller 代码生成是最开始的一步，下文会继续讲述自定义 controller 的详细步骤，感兴趣的可以关注笔者 github 的项目 [kubernetes-operator](https://github.com/gosoon/kubernetes-operator)。\n\n\n\n参考：\n\nhttps://github.com/kubernetes/sample-controller\n\nhttps://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/\n\nhttps://hexo.do1618.com/2018/04/04/Kubernetes-Deep-Dive-Code-Generation-for-CustomResources/\n","source":"_posts/code_generator.md","raw":"---\ntitle: 使用 code-generator 为 CustomResources 生成代码\ndate: 2019-08-06 20:50:30\ntags: [\"code-generator\",\"crd\"]\ntype: \"code-generator\"\n\n---\n\nkubernetes 项目中有相当一部分代码是自动生成的，主要是 API 的定义和调用方法，kubernetes 项目下 `k8s.io/kubernetes/hack/` 目录中以 update 开头的大部分脚本都是用来生成代码的。[code-generator](https://github.com/kubernetes/code-generator/) 是官方提供的代码生成工具，在实现自定义 controller 的时候需要用到 CRD，也需要使用该工具生成对 CRD 操作的代码。\n\n\n### 要生成哪些代码\n\n在自定义 controller 时需要用到 typed clientsets，informers，listers 和 deep-copy 等函数，这些函数都可以使用 [code-generator](https://github.com/kubernetes/code-generator/) 来生成，具体的作用可以参考：[kubernetes 中 informer 的使用]([http://blog.tianfeiyu.com/2019/05/17/client-go_informer/](http://blog.tianfeiyu.com/2019/05/17/client-go_informer/)。\n\ncode-generator 里面包含多个生成代码的工具，下面是需要用到的几个：\n\n- deepcopy-gen：为每种类型T生成方法： `func (t* T) DeepCopy() *T`，CustomResources 必须实现runtime.Object 接口且要有 DeepCopy 方法\n\n- client-gen：为 CustomResource APIGroups 生成 typed clientsets\n\n- informer-gen：为 CustomResources 创建 informers，用来 watch 对应 CRD 所触发的事件，以便对 CustomResources 的变化进行对应的处理\n\n- lister-gen：为 CustomResources 创建 listers，用来对 GET/List 请求提供只读的缓存层\n\n\n\n除了上面几个工具外，code-generator 中还提供了 conversion-gen、defaulter-gen、register-gen、set-gen，这些生成器可以应用在其他场景，比如构建聚合 API 服务时会用到一些内部的类型，conversion-gen 会为这些内部和外部类型之间创建转换函数，defaulter-gen 会处理某些字段的默认值。\n\n\n\n### 代码生成步骤\n\n使用 code-generator 生成代码还需要以下几步：\n\n- 创建指定的目录格式\n- 在代码中使用 tag 标注要生成哪些代码\n\n\n\n首先要创建指定的目录格式，目录的格式可以参考官方提供的示例项目：[sample-controller](https://github.com/kubernetes/sample-controller)，下文也会讲到，目录中需要包含对应 CustomResources 的定义以及  group 和 version 信息。\n\n其次要在在代码中使用 tag 标注要生成哪些代码，tag 有两钟类型，全局的和局部的，所有类型的 deepcopy tag 会默认启用，更多关于 tag 的使用方法可以参考：[Kubernetes Deep Dive: Code Generation for CustomResources](https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/)，也可以参考官方的示例 [code-generator/_example](https://github.com/kubernetes/code-generator/blob/master/_examples/crd/apis/example) 。\n\n\n\n### 开始生成代码\n\n本文以该 CRD 为例子进行演示，group 为`ecs.yun.com` ，version 为 `v1`：\n\n```\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kubernetesclusters.ecs.yun.com\nspec:\n  group: ecs.yun.com\n  names:\n    kind: KubernetesCluster\n    listKind: KubernetesClusterList\n    plural: kubernetesclusters\n    singular: kubernetescluster\n    shortNames:\n    - ecs\n  scope: Namespaced\n  subresources:\n    status: {}\n  version: v1\n  versions:\n  - name: v1\n    served: true\n    storage: true\n```\n\n创建指定目录结构 pkg/apis/${group}/${version}，group 可以定义一个 shortNames，也就是 CRD 中的 shortNames\n\n```\n$ mkdir -pv pkg/apis/ecs/v1\n```\n\n创建 doc.go：\n```\n$ cat << EOF > pkg/apis/ecs/v1/doc.go\n// Package v1 contains API Schema definitions for the ecs v1 API group\n// +k8s:deepcopy-gen=package,register\n// +groupName=ecs.yun.com\npackage v1\nEOF\n```\n\n创建 register.go：\n\n```\n$ cat << EOF > pkg/apis/ecs/v1/register.go\npackage v1\n\nimport (\n\t\"github.com/gosoon/kubernetes-operator/pkg/apis/ecs\"\n\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/runtime/schema\"\n)\n\n// SchemeGroupVersion is group version used to register these objects\nvar SchemeGroupVersion = schema.GroupVersion{Group: ecs.GroupName, Version: \"v1\"}\n\n// Kind takes an unqualified kind and returns back a Group qualified GroupKind\nfunc Kind(kind string) schema.GroupKind {\n\treturn SchemeGroupVersion.WithKind(kind).GroupKind()\n}\n\n// Resource takes an unqualified resource and returns a Group qualified GroupResource\nfunc Resource(resource string) schema.GroupResource {\n\treturn SchemeGroupVersion.WithResource(resource).GroupResource()\n}\n\nvar (\n\tSchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)\n\tAddToScheme   = SchemeBuilder.AddToScheme\n)\n\n// Adds the list of known types to Scheme.\nfunc addKnownTypes(scheme *runtime.Scheme) error {\n\tscheme.AddKnownTypes(SchemeGroupVersion,\n\t\t&KubernetesCluster{},\n\t\t&KubernetesClusterList{},\n\t)\n\tmetav1.AddToGroupVersion(scheme, SchemeGroupVersion)\n\treturn nil\n}\nEOF\n```\n\n创建 types.go，该文件中会定义多个 tag\n\n```\n$ cat << EOF > pkg/apis/ecs/v1/types.go\npackage v1\n\nimport (\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\n// +genclient\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// KubernetesCluster is the Schema for the kubernetesclusters API\ntype KubernetesCluster struct {\n\tmetav1.TypeMeta   `json:\",inline\"`\n\tmetav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n\tSpec   KubernetesClusterSpec   `json:\"spec,omitempty\"`\n\tStatus KubernetesClusterStatus `json:\"status,omitempty\"`\n}\n\n// KubernetesClusterSpec defines the desired state of KubernetesCluster\ntype KubernetesClusterSpec struct {\n\t// Add custom validation using kubebuilder tags:\n    // https://book.kubebuilder.io/beyond_basics/generating_crd.html\n\tTimeoutMins   string     `json:\"timeout_mins,omitempty\"`\n\tClusterType   string     `json:\"clusterType,omitempty\"`\n\tContainerCIDR string     `json:\"containerCIDR,omitempty\"`\n\tServiceCIDR   string     `json:\"serviceCIDR,omitempty\"`\n\tMasterList    []Node     `json:\"masterList\" tag:\"required\"`\n\tMasterVIP     string     `json:\"masterVIP,omitempty\"`\n\tNodeList      []Node     `json:\"nodeList\" tag:\"required\"`\n\tEtcdList      []Node     `json:\"etcdList,omitempty\"`\n\tRegion        string     `json:\"region,omitempty\"`\n\tAuthConfig    AuthConfig `json:\"authConfig,omitempty\"`\n}\n\n// AuthConfig defines the nodes peer authentication\ntype AuthConfig struct {\n\tUsername      string `json:\"username,omitempty\"`\n\tPassword      string `json:\"password,omitempty\"`\n\tPrivateSSHKey string `json:\"privateSSHKey,omitempty\"`\n}\n\n// KubernetesClusterStatus defines the observed state of KubernetesCluster\ntype KubernetesClusterStatus struct {\n\t// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html\n\tPhase KubernetesOperatorPhase `json:\"phase,omitempty\"`\n\n\t// when job failed callback or job timeout used\n\tReason string `json:\"reason,omitempty\"`\n\n\t// JobName is store each job name\n\tJobName string `json:\"jobName,omitempty\"`\n\n\t// Last time the condition transitioned from one status to another.\n\tLastTransitionTime metav1.Time `json:\"lastTransitionTime,omitempty\"`\n}\n\n// +genclient:nonNamespaced\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// KubernetesClusterList contains a list of KubernetesCluster\ntype KubernetesClusterList struct {\n\tmetav1.TypeMeta `json:\",inline\"`\n\tmetav1.ListMeta `json:\"metadata,omitempty\"`\n\tItems           []KubernetesCluster `json:\"items\"`\n}\n\n// users\n// \"None,Creating,Running,Failed,Scaling\"\ntype KubernetesOperatorPhase string\n\ntype Node struct {\n\tIP string `json:\"ip,omitempty\"`\n}\nEOF\n```\n\n\n\n执行命令生成代码：\n\n```\n$ $GOPATH/src/k8s.io/code-generator/generate-groups.sh all github.com/gosoon/kubernetes-operator/pkg/client github.com/gosoon/kubernetes-operator/pkg/apis ecs:v1\n```\n\n\n\ngenerate-groups.sh 需要四个参数：\n\n- 第一个 参数：all，也就是要生成所有的模块，clientset，informers，listers\n- 第二个参数：github.com/gosoon/test/pkg/client  这个是你要生成代码的目录，目录的名称一般定义为 client\n- 第三个参数：github.com/gosoon/test/pkg/apis  这个目录是已经创建好的源目录\n- 第四个参数：\"ecs:v1\" 是 group 和 version 信息，ecs 是 apis 下的目录，v1 是 ecs 下面的目录\n\n\n\n生成的代码如下所示：\n\n```\n.\n└── pkg\n    ├── apis\n    │   └── ecs\n    │       └── v1\n    │           ├── doc.go\n    │           ├── register.go\n    │           ├── types.go\n    │           └── zz_generated.deepcopy.go\n    └── client\n        ├── clientset\n        │   └── versioned\n        │       ├── clientset.go\n        │       ├── doc.go\n        │       ├── fake\n        │       │   ├── clientset_generated.go\n        │       │   ├── doc.go\n        │       │   └── register.go\n        │       ├── scheme\n        │       │   ├── doc.go\n        │       │   └── register.go\n        │       └── typed\n        │           └── ecs\n        │               └── v1\n        │                   ├── doc.go\n        │                   ├── ecs_client.go\n        │                   ├── fake\n        │                   │   ├── doc.go\n        │                   │   ├── fake_ecs_client.go\n        │                   │   └── fake_kubernetescluster.go\n        │                   ├── generated_expansion.go\n        │                   └── kubernetescluster.go\n        ├── informers\n        │   └── externalversions\n        │       ├── ecs\n        │       │   ├── interface.go\n        │       │   └── v1\n        │       │       ├── interface.go\n        │       │       └── kubernetescluster.go\n        │       ├── factory.go\n        │       ├── generic.go\n        │       └── internalinterfaces\n        │           └── factory_interfaces.go\n        └── listers\n            └── ecs\n                └── v1\n                    ├── expansion_generated.go\n                    └── kubernetescluster.go\n\n21 directories, 26 files\n```\n\n\n\nCRD 以及生成的代码见：[kubernetes-operator](https://github.com/gosoon/kubernetes-operator)。\n\n\n\n### 总结\n\n本问讲述了如何使用 code-generator 生成代码，要使用自定义 controller 代码生成是最开始的一步，下文会继续讲述自定义 controller 的详细步骤，感兴趣的可以关注笔者 github 的项目 [kubernetes-operator](https://github.com/gosoon/kubernetes-operator)。\n\n\n\n参考：\n\nhttps://github.com/kubernetes/sample-controller\n\nhttps://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/\n\nhttps://hexo.do1618.com/2018/04/04/Kubernetes-Deep-Dive-Code-Generation-for-CustomResources/\n","slug":"code_generator","published":1,"updated":"2019-08-13T04:00:54.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro58y0000apwncnpi87h9","content":"<p>kubernetes 项目中有相当一部分代码是自动生成的，主要是 API 的定义和调用方法，kubernetes 项目下 <code>k8s.io/kubernetes/hack/</code> 目录中以 update 开头的大部分脚本都是用来生成代码的。<a href=\"https://github.com/kubernetes/code-generator/\" target=\"_blank\" rel=\"noopener\">code-generator</a> 是官方提供的代码生成工具，在实现自定义 controller 的时候需要用到 CRD，也需要使用该工具生成对 CRD 操作的代码。</p>\n<h3 id=\"要生成哪些代码\"><a href=\"#要生成哪些代码\" class=\"headerlink\" title=\"要生成哪些代码\"></a>要生成哪些代码</h3><p>在自定义 controller 时需要用到 typed clientsets，informers，listers 和 deep-copy 等函数，这些函数都可以使用 <a href=\"https://github.com/kubernetes/code-generator/\" target=\"_blank\" rel=\"noopener\">code-generator</a> 来生成，具体的作用可以参考：<a href=\"[http://blog.tianfeiyu.com/2019/05/17/client-go_informer/](http://blog.tianfeiyu.com/2019/05/17/client-go_informer/\">kubernetes 中 informer 的使用</a>。</p>\n<p>code-generator 里面包含多个生成代码的工具，下面是需要用到的几个：</p>\n<ul>\n<li><p>deepcopy-gen：为每种类型T生成方法： <code>func (t* T) DeepCopy() *T</code>，CustomResources 必须实现runtime.Object 接口且要有 DeepCopy 方法</p>\n</li>\n<li><p>client-gen：为 CustomResource APIGroups 生成 typed clientsets</p>\n</li>\n<li><p>informer-gen：为 CustomResources 创建 informers，用来 watch 对应 CRD 所触发的事件，以便对 CustomResources 的变化进行对应的处理</p>\n</li>\n<li><p>lister-gen：为 CustomResources 创建 listers，用来对 GET/List 请求提供只读的缓存层</p>\n</li>\n</ul>\n<p>除了上面几个工具外，code-generator 中还提供了 conversion-gen、defaulter-gen、register-gen、set-gen，这些生成器可以应用在其他场景，比如构建聚合 API 服务时会用到一些内部的类型，conversion-gen 会为这些内部和外部类型之间创建转换函数，defaulter-gen 会处理某些字段的默认值。</p>\n<h3 id=\"代码生成步骤\"><a href=\"#代码生成步骤\" class=\"headerlink\" title=\"代码生成步骤\"></a>代码生成步骤</h3><p>使用 code-generator 生成代码还需要以下几步：</p>\n<ul>\n<li>创建指定的目录格式</li>\n<li>在代码中使用 tag 标注要生成哪些代码</li>\n</ul>\n<p>首先要创建指定的目录格式，目录的格式可以参考官方提供的示例项目：<a href=\"https://github.com/kubernetes/sample-controller\" target=\"_blank\" rel=\"noopener\">sample-controller</a>，下文也会讲到，目录中需要包含对应 CustomResources 的定义以及  group 和 version 信息。</p>\n<p>其次要在在代码中使用 tag 标注要生成哪些代码，tag 有两钟类型，全局的和局部的，所有类型的 deepcopy tag 会默认启用，更多关于 tag 的使用方法可以参考：<a href=\"https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/\" target=\"_blank\" rel=\"noopener\">Kubernetes Deep Dive: Code Generation for CustomResources</a>，也可以参考官方的示例 <a href=\"https://github.com/kubernetes/code-generator/blob/master/_examples/crd/apis/example\" target=\"_blank\" rel=\"noopener\">code-generator/_example</a> 。</p>\n<h3 id=\"开始生成代码\"><a href=\"#开始生成代码\" class=\"headerlink\" title=\"开始生成代码\"></a>开始生成代码</h3><p>本文以该 CRD 为例子进行演示，group 为<code>ecs.yun.com</code> ，version 为 <code>v1</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class=\"line\">kind: CustomResourceDefinition</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetesclusters.ecs.yun.com</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  group: ecs.yun.com</span><br><span class=\"line\">  names:</span><br><span class=\"line\">    kind: KubernetesCluster</span><br><span class=\"line\">    listKind: KubernetesClusterList</span><br><span class=\"line\">    plural: kubernetesclusters</span><br><span class=\"line\">    singular: kubernetescluster</span><br><span class=\"line\">    shortNames:</span><br><span class=\"line\">    - ecs</span><br><span class=\"line\">  scope: Namespaced</span><br><span class=\"line\">  subresources:</span><br><span class=\"line\">    status: &#123;&#125;</span><br><span class=\"line\">  version: v1</span><br><span class=\"line\">  versions:</span><br><span class=\"line\">  - name: v1</span><br><span class=\"line\">    served: true</span><br><span class=\"line\">    storage: true</span><br></pre></td></tr></table></figure>\n<p>创建指定目录结构 pkg/apis/${group}/${version}，group 可以定义一个 shortNames，也就是 CRD 中的 shortNames</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -pv pkg/apis/ecs/v1</span><br></pre></td></tr></table></figure>\n<p>创建 doc.go：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/doc.go</span><br><span class=\"line\">// Package v1 contains API Schema definitions for the ecs v1 API group</span><br><span class=\"line\">// +k8s:deepcopy-gen=package,register</span><br><span class=\"line\">// +groupName=ecs.yun.com</span><br><span class=\"line\">package v1</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>创建 register.go：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/register.go</span><br><span class=\"line\">package v1</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;github.com/gosoon/kubernetes-operator/pkg/apis/ecs&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\tmetav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apimachinery/pkg/runtime&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apimachinery/pkg/runtime/schema&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// SchemeGroupVersion is group version used to register these objects</span><br><span class=\"line\">var SchemeGroupVersion = schema.GroupVersion&#123;Group: ecs.GroupName, Version: &quot;v1&quot;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Kind takes an unqualified kind and returns back a Group qualified GroupKind</span><br><span class=\"line\">func Kind(kind string) schema.GroupKind &#123;</span><br><span class=\"line\">\treturn SchemeGroupVersion.WithKind(kind).GroupKind()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Resource takes an unqualified resource and returns a Group qualified GroupResource</span><br><span class=\"line\">func Resource(resource string) schema.GroupResource &#123;</span><br><span class=\"line\">\treturn SchemeGroupVersion.WithResource(resource).GroupResource()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">var (</span><br><span class=\"line\">\tSchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)</span><br><span class=\"line\">\tAddToScheme   = SchemeBuilder.AddToScheme</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// Adds the list of known types to Scheme.</span><br><span class=\"line\">func addKnownTypes(scheme *runtime.Scheme) error &#123;</span><br><span class=\"line\">\tscheme.AddKnownTypes(SchemeGroupVersion,</span><br><span class=\"line\">\t\t&amp;KubernetesCluster&#123;&#125;,</span><br><span class=\"line\">\t\t&amp;KubernetesClusterList&#123;&#125;,</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tmetav1.AddToGroupVersion(scheme, SchemeGroupVersion)</span><br><span class=\"line\">\treturn nil</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<p>创建 types.go，该文件中会定义多个 tag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/types.go</span><br><span class=\"line\">package v1</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\tmetav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// +genclient</span><br><span class=\"line\">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span><br><span class=\"line\"></span><br><span class=\"line\">// KubernetesCluster is the Schema for the kubernetesclusters API</span><br><span class=\"line\">type KubernetesCluster struct &#123;</span><br><span class=\"line\">\tmetav1.TypeMeta   `json:&quot;,inline&quot;`</span><br><span class=\"line\">\tmetav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">\tSpec   KubernetesClusterSpec   `json:&quot;spec,omitempty&quot;`</span><br><span class=\"line\">\tStatus KubernetesClusterStatus `json:&quot;status,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// KubernetesClusterSpec defines the desired state of KubernetesCluster</span><br><span class=\"line\">type KubernetesClusterSpec struct &#123;</span><br><span class=\"line\">\t// Add custom validation using kubebuilder tags:</span><br><span class=\"line\">    // https://book.kubebuilder.io/beyond_basics/generating_crd.html</span><br><span class=\"line\">\tTimeoutMins   string     `json:&quot;timeout_mins,omitempty&quot;`</span><br><span class=\"line\">\tClusterType   string     `json:&quot;clusterType,omitempty&quot;`</span><br><span class=\"line\">\tContainerCIDR string     `json:&quot;containerCIDR,omitempty&quot;`</span><br><span class=\"line\">\tServiceCIDR   string     `json:&quot;serviceCIDR,omitempty&quot;`</span><br><span class=\"line\">\tMasterList    []Node     `json:&quot;masterList&quot; tag:&quot;required&quot;`</span><br><span class=\"line\">\tMasterVIP     string     `json:&quot;masterVIP,omitempty&quot;`</span><br><span class=\"line\">\tNodeList      []Node     `json:&quot;nodeList&quot; tag:&quot;required&quot;`</span><br><span class=\"line\">\tEtcdList      []Node     `json:&quot;etcdList,omitempty&quot;`</span><br><span class=\"line\">\tRegion        string     `json:&quot;region,omitempty&quot;`</span><br><span class=\"line\">\tAuthConfig    AuthConfig `json:&quot;authConfig,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// AuthConfig defines the nodes peer authentication</span><br><span class=\"line\">type AuthConfig struct &#123;</span><br><span class=\"line\">\tUsername      string `json:&quot;username,omitempty&quot;`</span><br><span class=\"line\">\tPassword      string `json:&quot;password,omitempty&quot;`</span><br><span class=\"line\">\tPrivateSSHKey string `json:&quot;privateSSHKey,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// KubernetesClusterStatus defines the observed state of KubernetesCluster</span><br><span class=\"line\">type KubernetesClusterStatus struct &#123;</span><br><span class=\"line\">\t// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html</span><br><span class=\"line\">\tPhase KubernetesOperatorPhase `json:&quot;phase,omitempty&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">\t// when job failed callback or job timeout used</span><br><span class=\"line\">\tReason string `json:&quot;reason,omitempty&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">\t// JobName is store each job name</span><br><span class=\"line\">\tJobName string `json:&quot;jobName,omitempty&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Last time the condition transitioned from one status to another.</span><br><span class=\"line\">\tLastTransitionTime metav1.Time `json:&quot;lastTransitionTime,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// +genclient:nonNamespaced</span><br><span class=\"line\">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span><br><span class=\"line\"></span><br><span class=\"line\">// KubernetesClusterList contains a list of KubernetesCluster</span><br><span class=\"line\">type KubernetesClusterList struct &#123;</span><br><span class=\"line\">\tmetav1.TypeMeta `json:&quot;,inline&quot;`</span><br><span class=\"line\">\tmetav1.ListMeta `json:&quot;metadata,omitempty&quot;`</span><br><span class=\"line\">\tItems           []KubernetesCluster `json:&quot;items&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// users</span><br><span class=\"line\">// &quot;None,Creating,Running,Failed,Scaling&quot;</span><br><span class=\"line\">type KubernetesOperatorPhase string</span><br><span class=\"line\"></span><br><span class=\"line\">type Node struct &#123;</span><br><span class=\"line\">\tIP string `json:&quot;ip,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<p>执行命令生成代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ $GOPATH/src/k8s.io/code-generator/generate-groups.sh all github.com/gosoon/kubernetes-operator/pkg/client github.com/gosoon/kubernetes-operator/pkg/apis ecs:v1</span><br></pre></td></tr></table></figure>\n<p>generate-groups.sh 需要四个参数：</p>\n<ul>\n<li>第一个 参数：all，也就是要生成所有的模块，clientset，informers，listers</li>\n<li>第二个参数：github.com/gosoon/test/pkg/client  这个是你要生成代码的目录，目录的名称一般定义为 client</li>\n<li>第三个参数：github.com/gosoon/test/pkg/apis  这个目录是已经创建好的源目录</li>\n<li>第四个参数：”ecs:v1” 是 group 和 version 信息，ecs 是 apis 下的目录，v1 是 ecs 下面的目录</li>\n</ul>\n<p>生成的代码如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.</span><br><span class=\"line\">└── pkg</span><br><span class=\"line\">    ├── apis</span><br><span class=\"line\">    │   └── ecs</span><br><span class=\"line\">    │       └── v1</span><br><span class=\"line\">    │           ├── doc.go</span><br><span class=\"line\">    │           ├── register.go</span><br><span class=\"line\">    │           ├── types.go</span><br><span class=\"line\">    │           └── zz_generated.deepcopy.go</span><br><span class=\"line\">    └── client</span><br><span class=\"line\">        ├── clientset</span><br><span class=\"line\">        │   └── versioned</span><br><span class=\"line\">        │       ├── clientset.go</span><br><span class=\"line\">        │       ├── doc.go</span><br><span class=\"line\">        │       ├── fake</span><br><span class=\"line\">        │       │   ├── clientset_generated.go</span><br><span class=\"line\">        │       │   ├── doc.go</span><br><span class=\"line\">        │       │   └── register.go</span><br><span class=\"line\">        │       ├── scheme</span><br><span class=\"line\">        │       │   ├── doc.go</span><br><span class=\"line\">        │       │   └── register.go</span><br><span class=\"line\">        │       └── typed</span><br><span class=\"line\">        │           └── ecs</span><br><span class=\"line\">        │               └── v1</span><br><span class=\"line\">        │                   ├── doc.go</span><br><span class=\"line\">        │                   ├── ecs_client.go</span><br><span class=\"line\">        │                   ├── fake</span><br><span class=\"line\">        │                   │   ├── doc.go</span><br><span class=\"line\">        │                   │   ├── fake_ecs_client.go</span><br><span class=\"line\">        │                   │   └── fake_kubernetescluster.go</span><br><span class=\"line\">        │                   ├── generated_expansion.go</span><br><span class=\"line\">        │                   └── kubernetescluster.go</span><br><span class=\"line\">        ├── informers</span><br><span class=\"line\">        │   └── externalversions</span><br><span class=\"line\">        │       ├── ecs</span><br><span class=\"line\">        │       │   ├── interface.go</span><br><span class=\"line\">        │       │   └── v1</span><br><span class=\"line\">        │       │       ├── interface.go</span><br><span class=\"line\">        │       │       └── kubernetescluster.go</span><br><span class=\"line\">        │       ├── factory.go</span><br><span class=\"line\">        │       ├── generic.go</span><br><span class=\"line\">        │       └── internalinterfaces</span><br><span class=\"line\">        │           └── factory_interfaces.go</span><br><span class=\"line\">        └── listers</span><br><span class=\"line\">            └── ecs</span><br><span class=\"line\">                └── v1</span><br><span class=\"line\">                    ├── expansion_generated.go</span><br><span class=\"line\">                    └── kubernetescluster.go</span><br><span class=\"line\"></span><br><span class=\"line\">21 directories, 26 files</span><br></pre></td></tr></table></figure>\n<p>CRD 以及生成的代码见：<a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">kubernetes-operator</a>。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本问讲述了如何使用 code-generator 生成代码，要使用自定义 controller 代码生成是最开始的一步，下文会继续讲述自定义 controller 的详细步骤，感兴趣的可以关注笔者 github 的项目 <a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">kubernetes-operator</a>。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes/sample-controller\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/sample-controller</a></p>\n<p><a href=\"https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/\" target=\"_blank\" rel=\"noopener\">https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/</a></p>\n<p><a href=\"https://hexo.do1618.com/2018/04/04/Kubernetes-Deep-Dive-Code-Generation-for-CustomResources/\" target=\"_blank\" rel=\"noopener\">https://hexo.do1618.com/2018/04/04/Kubernetes-Deep-Dive-Code-Generation-for-CustomResources/</a></p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/07/02/k8s_crd_verify/\">kubernetes 自定义资源（CRD）的校验</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>kubernetes 项目中有相当一部分代码是自动生成的，主要是 API 的定义和调用方法，kubernetes 项目下 <code>k8s.io/kubernetes/hack/</code> 目录中以 update 开头的大部分脚本都是用来生成代码的。<a href=\"https://github.com/kubernetes/code-generator/\" target=\"_blank\" rel=\"noopener\">code-generator</a> 是官方提供的代码生成工具，在实现自定义 controller 的时候需要用到 CRD，也需要使用该工具生成对 CRD 操作的代码。</p>\n<h3 id=\"要生成哪些代码\"><a href=\"#要生成哪些代码\" class=\"headerlink\" title=\"要生成哪些代码\"></a>要生成哪些代码</h3><p>在自定义 controller 时需要用到 typed clientsets，informers，listers 和 deep-copy 等函数，这些函数都可以使用 <a href=\"https://github.com/kubernetes/code-generator/\" target=\"_blank\" rel=\"noopener\">code-generator</a> 来生成，具体的作用可以参考：<a href=\"[http://blog.tianfeiyu.com/2019/05/17/client-go_informer/](http://blog.tianfeiyu.com/2019/05/17/client-go_informer/\">kubernetes 中 informer 的使用</a>。</p>\n<p>code-generator 里面包含多个生成代码的工具，下面是需要用到的几个：</p>\n<ul>\n<li><p>deepcopy-gen：为每种类型T生成方法： <code>func (t* T) DeepCopy() *T</code>，CustomResources 必须实现runtime.Object 接口且要有 DeepCopy 方法</p>\n</li>\n<li><p>client-gen：为 CustomResource APIGroups 生成 typed clientsets</p>\n</li>\n<li><p>informer-gen：为 CustomResources 创建 informers，用来 watch 对应 CRD 所触发的事件，以便对 CustomResources 的变化进行对应的处理</p>\n</li>\n<li><p>lister-gen：为 CustomResources 创建 listers，用来对 GET/List 请求提供只读的缓存层</p>\n</li>\n</ul>\n<p>除了上面几个工具外，code-generator 中还提供了 conversion-gen、defaulter-gen、register-gen、set-gen，这些生成器可以应用在其他场景，比如构建聚合 API 服务时会用到一些内部的类型，conversion-gen 会为这些内部和外部类型之间创建转换函数，defaulter-gen 会处理某些字段的默认值。</p>\n<h3 id=\"代码生成步骤\"><a href=\"#代码生成步骤\" class=\"headerlink\" title=\"代码生成步骤\"></a>代码生成步骤</h3><p>使用 code-generator 生成代码还需要以下几步：</p>\n<ul>\n<li>创建指定的目录格式</li>\n<li>在代码中使用 tag 标注要生成哪些代码</li>\n</ul>\n<p>首先要创建指定的目录格式，目录的格式可以参考官方提供的示例项目：<a href=\"https://github.com/kubernetes/sample-controller\" target=\"_blank\" rel=\"noopener\">sample-controller</a>，下文也会讲到，目录中需要包含对应 CustomResources 的定义以及  group 和 version 信息。</p>\n<p>其次要在在代码中使用 tag 标注要生成哪些代码，tag 有两钟类型，全局的和局部的，所有类型的 deepcopy tag 会默认启用，更多关于 tag 的使用方法可以参考：<a href=\"https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/\" target=\"_blank\" rel=\"noopener\">Kubernetes Deep Dive: Code Generation for CustomResources</a>，也可以参考官方的示例 <a href=\"https://github.com/kubernetes/code-generator/blob/master/_examples/crd/apis/example\" target=\"_blank\" rel=\"noopener\">code-generator/_example</a> 。</p>\n<h3 id=\"开始生成代码\"><a href=\"#开始生成代码\" class=\"headerlink\" title=\"开始生成代码\"></a>开始生成代码</h3><p>本文以该 CRD 为例子进行演示，group 为<code>ecs.yun.com</code> ，version 为 <code>v1</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class=\"line\">kind: CustomResourceDefinition</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetesclusters.ecs.yun.com</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  group: ecs.yun.com</span><br><span class=\"line\">  names:</span><br><span class=\"line\">    kind: KubernetesCluster</span><br><span class=\"line\">    listKind: KubernetesClusterList</span><br><span class=\"line\">    plural: kubernetesclusters</span><br><span class=\"line\">    singular: kubernetescluster</span><br><span class=\"line\">    shortNames:</span><br><span class=\"line\">    - ecs</span><br><span class=\"line\">  scope: Namespaced</span><br><span class=\"line\">  subresources:</span><br><span class=\"line\">    status: &#123;&#125;</span><br><span class=\"line\">  version: v1</span><br><span class=\"line\">  versions:</span><br><span class=\"line\">  - name: v1</span><br><span class=\"line\">    served: true</span><br><span class=\"line\">    storage: true</span><br></pre></td></tr></table></figure>\n<p>创建指定目录结构 pkg/apis/${group}/${version}，group 可以定义一个 shortNames，也就是 CRD 中的 shortNames</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -pv pkg/apis/ecs/v1</span><br></pre></td></tr></table></figure>\n<p>创建 doc.go：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/doc.go</span><br><span class=\"line\">// Package v1 contains API Schema definitions for the ecs v1 API group</span><br><span class=\"line\">// +k8s:deepcopy-gen=package,register</span><br><span class=\"line\">// +groupName=ecs.yun.com</span><br><span class=\"line\">package v1</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure></p>\n<p>创建 register.go：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/register.go</span><br><span class=\"line\">package v1</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;github.com/gosoon/kubernetes-operator/pkg/apis/ecs&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\tmetav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apimachinery/pkg/runtime&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apimachinery/pkg/runtime/schema&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// SchemeGroupVersion is group version used to register these objects</span><br><span class=\"line\">var SchemeGroupVersion = schema.GroupVersion&#123;Group: ecs.GroupName, Version: &quot;v1&quot;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Kind takes an unqualified kind and returns back a Group qualified GroupKind</span><br><span class=\"line\">func Kind(kind string) schema.GroupKind &#123;</span><br><span class=\"line\">\treturn SchemeGroupVersion.WithKind(kind).GroupKind()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Resource takes an unqualified resource and returns a Group qualified GroupResource</span><br><span class=\"line\">func Resource(resource string) schema.GroupResource &#123;</span><br><span class=\"line\">\treturn SchemeGroupVersion.WithResource(resource).GroupResource()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">var (</span><br><span class=\"line\">\tSchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)</span><br><span class=\"line\">\tAddToScheme   = SchemeBuilder.AddToScheme</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// Adds the list of known types to Scheme.</span><br><span class=\"line\">func addKnownTypes(scheme *runtime.Scheme) error &#123;</span><br><span class=\"line\">\tscheme.AddKnownTypes(SchemeGroupVersion,</span><br><span class=\"line\">\t\t&amp;KubernetesCluster&#123;&#125;,</span><br><span class=\"line\">\t\t&amp;KubernetesClusterList&#123;&#125;,</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tmetav1.AddToGroupVersion(scheme, SchemeGroupVersion)</span><br><span class=\"line\">\treturn nil</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<p>创建 types.go，该文件中会定义多个 tag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/types.go</span><br><span class=\"line\">package v1</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\tmetav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// +genclient</span><br><span class=\"line\">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span><br><span class=\"line\"></span><br><span class=\"line\">// KubernetesCluster is the Schema for the kubernetesclusters API</span><br><span class=\"line\">type KubernetesCluster struct &#123;</span><br><span class=\"line\">\tmetav1.TypeMeta   `json:&quot;,inline&quot;`</span><br><span class=\"line\">\tmetav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">\tSpec   KubernetesClusterSpec   `json:&quot;spec,omitempty&quot;`</span><br><span class=\"line\">\tStatus KubernetesClusterStatus `json:&quot;status,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// KubernetesClusterSpec defines the desired state of KubernetesCluster</span><br><span class=\"line\">type KubernetesClusterSpec struct &#123;</span><br><span class=\"line\">\t// Add custom validation using kubebuilder tags:</span><br><span class=\"line\">    // https://book.kubebuilder.io/beyond_basics/generating_crd.html</span><br><span class=\"line\">\tTimeoutMins   string     `json:&quot;timeout_mins,omitempty&quot;`</span><br><span class=\"line\">\tClusterType   string     `json:&quot;clusterType,omitempty&quot;`</span><br><span class=\"line\">\tContainerCIDR string     `json:&quot;containerCIDR,omitempty&quot;`</span><br><span class=\"line\">\tServiceCIDR   string     `json:&quot;serviceCIDR,omitempty&quot;`</span><br><span class=\"line\">\tMasterList    []Node     `json:&quot;masterList&quot; tag:&quot;required&quot;`</span><br><span class=\"line\">\tMasterVIP     string     `json:&quot;masterVIP,omitempty&quot;`</span><br><span class=\"line\">\tNodeList      []Node     `json:&quot;nodeList&quot; tag:&quot;required&quot;`</span><br><span class=\"line\">\tEtcdList      []Node     `json:&quot;etcdList,omitempty&quot;`</span><br><span class=\"line\">\tRegion        string     `json:&quot;region,omitempty&quot;`</span><br><span class=\"line\">\tAuthConfig    AuthConfig `json:&quot;authConfig,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// AuthConfig defines the nodes peer authentication</span><br><span class=\"line\">type AuthConfig struct &#123;</span><br><span class=\"line\">\tUsername      string `json:&quot;username,omitempty&quot;`</span><br><span class=\"line\">\tPassword      string `json:&quot;password,omitempty&quot;`</span><br><span class=\"line\">\tPrivateSSHKey string `json:&quot;privateSSHKey,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// KubernetesClusterStatus defines the observed state of KubernetesCluster</span><br><span class=\"line\">type KubernetesClusterStatus struct &#123;</span><br><span class=\"line\">\t// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html</span><br><span class=\"line\">\tPhase KubernetesOperatorPhase `json:&quot;phase,omitempty&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">\t// when job failed callback or job timeout used</span><br><span class=\"line\">\tReason string `json:&quot;reason,omitempty&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">\t// JobName is store each job name</span><br><span class=\"line\">\tJobName string `json:&quot;jobName,omitempty&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Last time the condition transitioned from one status to another.</span><br><span class=\"line\">\tLastTransitionTime metav1.Time `json:&quot;lastTransitionTime,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// +genclient:nonNamespaced</span><br><span class=\"line\">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span><br><span class=\"line\"></span><br><span class=\"line\">// KubernetesClusterList contains a list of KubernetesCluster</span><br><span class=\"line\">type KubernetesClusterList struct &#123;</span><br><span class=\"line\">\tmetav1.TypeMeta `json:&quot;,inline&quot;`</span><br><span class=\"line\">\tmetav1.ListMeta `json:&quot;metadata,omitempty&quot;`</span><br><span class=\"line\">\tItems           []KubernetesCluster `json:&quot;items&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// users</span><br><span class=\"line\">// &quot;None,Creating,Running,Failed,Scaling&quot;</span><br><span class=\"line\">type KubernetesOperatorPhase string</span><br><span class=\"line\"></span><br><span class=\"line\">type Node struct &#123;</span><br><span class=\"line\">\tIP string `json:&quot;ip,omitempty&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<p>执行命令生成代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ $GOPATH/src/k8s.io/code-generator/generate-groups.sh all github.com/gosoon/kubernetes-operator/pkg/client github.com/gosoon/kubernetes-operator/pkg/apis ecs:v1</span><br></pre></td></tr></table></figure>\n<p>generate-groups.sh 需要四个参数：</p>\n<ul>\n<li>第一个 参数：all，也就是要生成所有的模块，clientset，informers，listers</li>\n<li>第二个参数：github.com/gosoon/test/pkg/client  这个是你要生成代码的目录，目录的名称一般定义为 client</li>\n<li>第三个参数：github.com/gosoon/test/pkg/apis  这个目录是已经创建好的源目录</li>\n<li>第四个参数：”ecs:v1” 是 group 和 version 信息，ecs 是 apis 下的目录，v1 是 ecs 下面的目录</li>\n</ul>\n<p>生成的代码如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.</span><br><span class=\"line\">└── pkg</span><br><span class=\"line\">    ├── apis</span><br><span class=\"line\">    │   └── ecs</span><br><span class=\"line\">    │       └── v1</span><br><span class=\"line\">    │           ├── doc.go</span><br><span class=\"line\">    │           ├── register.go</span><br><span class=\"line\">    │           ├── types.go</span><br><span class=\"line\">    │           └── zz_generated.deepcopy.go</span><br><span class=\"line\">    └── client</span><br><span class=\"line\">        ├── clientset</span><br><span class=\"line\">        │   └── versioned</span><br><span class=\"line\">        │       ├── clientset.go</span><br><span class=\"line\">        │       ├── doc.go</span><br><span class=\"line\">        │       ├── fake</span><br><span class=\"line\">        │       │   ├── clientset_generated.go</span><br><span class=\"line\">        │       │   ├── doc.go</span><br><span class=\"line\">        │       │   └── register.go</span><br><span class=\"line\">        │       ├── scheme</span><br><span class=\"line\">        │       │   ├── doc.go</span><br><span class=\"line\">        │       │   └── register.go</span><br><span class=\"line\">        │       └── typed</span><br><span class=\"line\">        │           └── ecs</span><br><span class=\"line\">        │               └── v1</span><br><span class=\"line\">        │                   ├── doc.go</span><br><span class=\"line\">        │                   ├── ecs_client.go</span><br><span class=\"line\">        │                   ├── fake</span><br><span class=\"line\">        │                   │   ├── doc.go</span><br><span class=\"line\">        │                   │   ├── fake_ecs_client.go</span><br><span class=\"line\">        │                   │   └── fake_kubernetescluster.go</span><br><span class=\"line\">        │                   ├── generated_expansion.go</span><br><span class=\"line\">        │                   └── kubernetescluster.go</span><br><span class=\"line\">        ├── informers</span><br><span class=\"line\">        │   └── externalversions</span><br><span class=\"line\">        │       ├── ecs</span><br><span class=\"line\">        │       │   ├── interface.go</span><br><span class=\"line\">        │       │   └── v1</span><br><span class=\"line\">        │       │       ├── interface.go</span><br><span class=\"line\">        │       │       └── kubernetescluster.go</span><br><span class=\"line\">        │       ├── factory.go</span><br><span class=\"line\">        │       ├── generic.go</span><br><span class=\"line\">        │       └── internalinterfaces</span><br><span class=\"line\">        │           └── factory_interfaces.go</span><br><span class=\"line\">        └── listers</span><br><span class=\"line\">            └── ecs</span><br><span class=\"line\">                └── v1</span><br><span class=\"line\">                    ├── expansion_generated.go</span><br><span class=\"line\">                    └── kubernetescluster.go</span><br><span class=\"line\"></span><br><span class=\"line\">21 directories, 26 files</span><br></pre></td></tr></table></figure>\n<p>CRD 以及生成的代码见：<a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">kubernetes-operator</a>。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本问讲述了如何使用 code-generator 生成代码，要使用自定义 controller 代码生成是最开始的一步，下文会继续讲述自定义 controller 的详细步骤，感兴趣的可以关注笔者 github 的项目 <a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">kubernetes-operator</a>。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes/sample-controller\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/sample-controller</a></p>\n<p><a href=\"https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/\" target=\"_blank\" rel=\"noopener\">https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/</a></p>\n<p><a href=\"https://hexo.do1618.com/2018/04/04/Kubernetes-Deep-Dive-Code-Generation-for-CustomResources/\" target=\"_blank\" rel=\"noopener\">https://hexo.do1618.com/2018/04/04/Kubernetes-Deep-Dive-Code-Generation-for-CustomResources/</a></p>\n"},{"title":"Docker 架构中的几个核心概念","date":"2018-12-05T12:57:00.000Z","type":"docker","_content":"\n\n## 一、Docker 开源之路\n\n2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。\n\n2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。\n\n从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。\n\n\n## 二、Docker 架构\n\n![docker 架构](http://cdn.tianfeiyu.com/docker-1.png)\n\n\n![docker 进程关系](http://cdn.tianfeiyu.com/docker-2.png)\n\n\n### 三、核心概念\n\ndocker 1.13 版本中包含以下几个二进制文件。\n```\n$ docker --version\nDocker version 1.13.1, build 092cba3\n\n$ docker\ndocker             docker-containerd-ctr   dockerd      docker-proxy\ndocker-containerd  docker-containerd-shim  docker-init  docker-runc\n```\n\n#### 1、docker \ndocker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。\n\n#### 2、dockerd \ndockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面：\n\n- 接收并处理 docker client 发送的请求\n- 管理所有的 docker 容器\n\n有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。\n\n#### 3、containerd\n\ncontainerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有：\n- 容器生命周期管理\n- 日志管理\n- 镜像管理\n- 存储管理\n- 容器网络接口及网络管理\n\n#### 4、containerd-shim\n\ncontainerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是：\n\n- 它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC)\n- 即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的\n- 向 containerd 报告容器的退出状态\n\n有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。\n\n#### 5、runC\nrunC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。\n\n一个例子，使用 runC 运行 busybox 容器:\n```\n# mkdir /container\n# cd /container/\n# mkdir rootfs\n\n准备容器镜像的文件系统,从 busybox 镜像中提取\n# docker export $(docker create busybox) | tar -C rootfs -xvf -    \n# ls rootfs/\nbin  dev  etc  home  proc  root  sys  tmp  usr  var\n\n有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，\n包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成\n# docker-runc spec\n# ls\nconfig.json  rootfs\n# docker-runc run simplebusybox    #启动容器\n/ # ls\nbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/ # hostname\nrunc\n```\n---\n参考：\n[Use of containerd-shim in docker-architecture](https://groups.google.com/forum/#!topic/docker-dev/zaZFlvIx1_k)\n[从 docker 到 runC](https://www.cnblogs.com/sparkdev/p/9129334.html)\n[OCI 和 runc：容器标准化和 docker](http://cizixs.com/2017/11/05/oci-and-runc/)\n[Open Container Initiative](https://github.com/opencontainers)\n","source":"_posts/docker-introduces.md","raw":"---\ntitle: Docker 架构中的几个核心概念\ndate: 2018-12-05 20:57:00\ntype: \"docker\"\n\n---\n\n\n## 一、Docker 开源之路\n\n2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。\n\n2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。\n\n从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。\n\n\n## 二、Docker 架构\n\n![docker 架构](http://cdn.tianfeiyu.com/docker-1.png)\n\n\n![docker 进程关系](http://cdn.tianfeiyu.com/docker-2.png)\n\n\n### 三、核心概念\n\ndocker 1.13 版本中包含以下几个二进制文件。\n```\n$ docker --version\nDocker version 1.13.1, build 092cba3\n\n$ docker\ndocker             docker-containerd-ctr   dockerd      docker-proxy\ndocker-containerd  docker-containerd-shim  docker-init  docker-runc\n```\n\n#### 1、docker \ndocker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。\n\n#### 2、dockerd \ndockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面：\n\n- 接收并处理 docker client 发送的请求\n- 管理所有的 docker 容器\n\n有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。\n\n#### 3、containerd\n\ncontainerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有：\n- 容器生命周期管理\n- 日志管理\n- 镜像管理\n- 存储管理\n- 容器网络接口及网络管理\n\n#### 4、containerd-shim\n\ncontainerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是：\n\n- 它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC)\n- 即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的\n- 向 containerd 报告容器的退出状态\n\n有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。\n\n#### 5、runC\nrunC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。\n\n一个例子，使用 runC 运行 busybox 容器:\n```\n# mkdir /container\n# cd /container/\n# mkdir rootfs\n\n准备容器镜像的文件系统,从 busybox 镜像中提取\n# docker export $(docker create busybox) | tar -C rootfs -xvf -    \n# ls rootfs/\nbin  dev  etc  home  proc  root  sys  tmp  usr  var\n\n有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，\n包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成\n# docker-runc spec\n# ls\nconfig.json  rootfs\n# docker-runc run simplebusybox    #启动容器\n/ # ls\nbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/ # hostname\nrunc\n```\n---\n参考：\n[Use of containerd-shim in docker-architecture](https://groups.google.com/forum/#!topic/docker-dev/zaZFlvIx1_k)\n[从 docker 到 runC](https://www.cnblogs.com/sparkdev/p/9129334.html)\n[OCI 和 runc：容器标准化和 docker](http://cizixs.com/2017/11/05/oci-and-runc/)\n[Open Container Initiative](https://github.com/opencontainers)\n","slug":"docker-introduces","published":1,"updated":"2019-07-21T09:50:55.384Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro5920001apwnos0bizxa","content":"<h2 id=\"一、Docker-开源之路\"><a href=\"#一、Docker-开源之路\" class=\"headerlink\" title=\"一、Docker 开源之路\"></a>一、Docker 开源之路</h2><p>2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。</p>\n<p>2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。</p>\n<p>从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。</p>\n<h2 id=\"二、Docker-架构\"><a href=\"#二、Docker-架构\" class=\"headerlink\" title=\"二、Docker 架构\"></a>二、Docker 架构</h2><p><img src=\"http://cdn.tianfeiyu.com/docker-1.png\" alt=\"docker 架构\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/docker-2.png\" alt=\"docker 进程关系\"></p>\n<h3 id=\"三、核心概念\"><a href=\"#三、核心概念\" class=\"headerlink\" title=\"三、核心概念\"></a>三、核心概念</h3><p>docker 1.13 版本中包含以下几个二进制文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker --version</span><br><span class=\"line\">Docker version 1.13.1, build 092cba3</span><br><span class=\"line\"></span><br><span class=\"line\">$ docker</span><br><span class=\"line\">docker             docker-containerd-ctr   dockerd      docker-proxy</span><br><span class=\"line\">docker-containerd  docker-containerd-shim  docker-init  docker-runc</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1、docker\"><a href=\"#1、docker\" class=\"headerlink\" title=\"1、docker\"></a>1、docker</h4><p>docker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。</p>\n<h4 id=\"2、dockerd\"><a href=\"#2、dockerd\" class=\"headerlink\" title=\"2、dockerd\"></a>2、dockerd</h4><p>dockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面：</p>\n<ul>\n<li>接收并处理 docker client 发送的请求</li>\n<li>管理所有的 docker 容器</li>\n</ul>\n<p>有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。</p>\n<h4 id=\"3、containerd\"><a href=\"#3、containerd\" class=\"headerlink\" title=\"3、containerd\"></a>3、containerd</h4><p>containerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有：</p>\n<ul>\n<li>容器生命周期管理</li>\n<li>日志管理</li>\n<li>镜像管理</li>\n<li>存储管理</li>\n<li>容器网络接口及网络管理</li>\n</ul>\n<h4 id=\"4、containerd-shim\"><a href=\"#4、containerd-shim\" class=\"headerlink\" title=\"4、containerd-shim\"></a>4、containerd-shim</h4><p>containerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是：</p>\n<ul>\n<li>它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC)</li>\n<li>即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的</li>\n<li>向 containerd 报告容器的退出状态</li>\n</ul>\n<p>有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。</p>\n<h4 id=\"5、runC\"><a href=\"#5、runC\" class=\"headerlink\" title=\"5、runC\"></a>5、runC</h4><p>runC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。</p>\n<p>一个例子，使用 runC 运行 busybox 容器:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># mkdir /container</span><br><span class=\"line\"># cd /container/</span><br><span class=\"line\"># mkdir rootfs</span><br><span class=\"line\"></span><br><span class=\"line\">准备容器镜像的文件系统,从 busybox 镜像中提取</span><br><span class=\"line\"># docker export $(docker create busybox) | tar -C rootfs -xvf -    </span><br><span class=\"line\"># ls rootfs/</span><br><span class=\"line\">bin  dev  etc  home  proc  root  sys  tmp  usr  var</span><br><span class=\"line\"></span><br><span class=\"line\">有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，</span><br><span class=\"line\">包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成</span><br><span class=\"line\"># docker-runc spec</span><br><span class=\"line\"># ls</span><br><span class=\"line\">config.json  rootfs</span><br><span class=\"line\"># docker-runc run simplebusybox    #启动容器</span><br><span class=\"line\">/ # ls</span><br><span class=\"line\">bin   dev   etc   home  proc  root  sys   tmp   usr   var</span><br><span class=\"line\">/ # hostname</span><br><span class=\"line\">runc</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>参考：<br><a href=\"https://groups.google.com/forum/#!topic/docker-dev/zaZFlvIx1_k\" target=\"_blank\" rel=\"noopener\">Use of containerd-shim in docker-architecture</a><br><a href=\"https://www.cnblogs.com/sparkdev/p/9129334.html\" target=\"_blank\" rel=\"noopener\">从 docker 到 runC</a><br><a href=\"http://cizixs.com/2017/11/05/oci-and-runc/\" target=\"_blank\" rel=\"noopener\">OCI 和 runc：容器标准化和 docker</a><br><a href=\"https://github.com/opencontainers\" target=\"_blank\" rel=\"noopener\">Open Container Initiative</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<h2 id=\"一、Docker-开源之路\"><a href=\"#一、Docker-开源之路\" class=\"headerlink\" title=\"一、Docker 开源之路\"></a>一、Docker 开源之路</h2><p>2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。</p>\n<p>2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。</p>\n<p>从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。</p>\n<h2 id=\"二、Docker-架构\"><a href=\"#二、Docker-架构\" class=\"headerlink\" title=\"二、Docker 架构\"></a>二、Docker 架构</h2><p><img src=\"http://cdn.tianfeiyu.com/docker-1.png\" alt=\"docker 架构\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/docker-2.png\" alt=\"docker 进程关系\"></p>\n<h3 id=\"三、核心概念\"><a href=\"#三、核心概念\" class=\"headerlink\" title=\"三、核心概念\"></a>三、核心概念</h3><p>docker 1.13 版本中包含以下几个二进制文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ docker --version</span><br><span class=\"line\">Docker version 1.13.1, build 092cba3</span><br><span class=\"line\"></span><br><span class=\"line\">$ docker</span><br><span class=\"line\">docker             docker-containerd-ctr   dockerd      docker-proxy</span><br><span class=\"line\">docker-containerd  docker-containerd-shim  docker-init  docker-runc</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"1、docker\"><a href=\"#1、docker\" class=\"headerlink\" title=\"1、docker\"></a>1、docker</h4><p>docker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。</p>\n<h4 id=\"2、dockerd\"><a href=\"#2、dockerd\" class=\"headerlink\" title=\"2、dockerd\"></a>2、dockerd</h4><p>dockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面：</p>\n<ul>\n<li>接收并处理 docker client 发送的请求</li>\n<li>管理所有的 docker 容器</li>\n</ul>\n<p>有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。</p>\n<h4 id=\"3、containerd\"><a href=\"#3、containerd\" class=\"headerlink\" title=\"3、containerd\"></a>3、containerd</h4><p>containerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有：</p>\n<ul>\n<li>容器生命周期管理</li>\n<li>日志管理</li>\n<li>镜像管理</li>\n<li>存储管理</li>\n<li>容器网络接口及网络管理</li>\n</ul>\n<h4 id=\"4、containerd-shim\"><a href=\"#4、containerd-shim\" class=\"headerlink\" title=\"4、containerd-shim\"></a>4、containerd-shim</h4><p>containerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是：</p>\n<ul>\n<li>它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC)</li>\n<li>即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的</li>\n<li>向 containerd 报告容器的退出状态</li>\n</ul>\n<p>有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。</p>\n<h4 id=\"5、runC\"><a href=\"#5、runC\" class=\"headerlink\" title=\"5、runC\"></a>5、runC</h4><p>runC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。</p>\n<p>一个例子，使用 runC 运行 busybox 容器:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># mkdir /container</span><br><span class=\"line\"># cd /container/</span><br><span class=\"line\"># mkdir rootfs</span><br><span class=\"line\"></span><br><span class=\"line\">准备容器镜像的文件系统,从 busybox 镜像中提取</span><br><span class=\"line\"># docker export $(docker create busybox) | tar -C rootfs -xvf -    </span><br><span class=\"line\"># ls rootfs/</span><br><span class=\"line\">bin  dev  etc  home  proc  root  sys  tmp  usr  var</span><br><span class=\"line\"></span><br><span class=\"line\">有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，</span><br><span class=\"line\">包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成</span><br><span class=\"line\"># docker-runc spec</span><br><span class=\"line\"># ls</span><br><span class=\"line\">config.json  rootfs</span><br><span class=\"line\"># docker-runc run simplebusybox    #启动容器</span><br><span class=\"line\">/ # ls</span><br><span class=\"line\">bin   dev   etc   home  proc  root  sys   tmp   usr   var</span><br><span class=\"line\">/ # hostname</span><br><span class=\"line\">runc</span><br></pre></td></tr></table></figure></p>\n<hr>\n<p>参考：<br><a href=\"https://groups.google.com/forum/#!topic/docker-dev/zaZFlvIx1_k\" target=\"_blank\" rel=\"noopener\">Use of containerd-shim in docker-architecture</a><br><a href=\"https://www.cnblogs.com/sparkdev/p/9129334.html\" target=\"_blank\" rel=\"noopener\">从 docker 到 runC</a><br><a href=\"http://cizixs.com/2017/11/05/oci-and-runc/\" target=\"_blank\" rel=\"noopener\">OCI 和 runc：容器标准化和 docker</a><br><a href=\"https://github.com/opencontainers\" target=\"_blank\" rel=\"noopener\">Open Container Initiative</a></p>\n"},{"title":"kubernetes 中 informer 的使用","date":"2019-05-17T03:00:30.000Z","type":"client-go","_content":"\n#### 一、kubernetes 集群的几种访问方式\n\n在实际开发过程中，若想要获取 kubernetes 中某个资源（比如 pod）的所有对象，可以使用 kubectl、k8s REST API、client-go(ClientSet、Dynamic Client、RESTClient 三种方式) 等多种方式访问 k8s 集群获取资源。在笔者的开发过程中，最初都是直接调用 k8s 的 REST API 来获取的，使用 `kubectl get pod -v=9` 可以直接看到调用 k8s 的接口，然后在程序中直接访问还是比较方便的。但是随着集群规模的增长或者从国内获取海外 k8s 集群的数据，直接调用 k8s 接口获取所有 pod 还是比较耗时，这个问题有多种解决方法，最初是直接使用 k8s 原生的 watch 接口来获取的，下面是一个伪代码：\n\n```\nconst (\n\tADDED    string = \"ADDED\"\n\tMODIFIED string = \"MODIFIED\"\n\tDELETED  string = \"DELETED\"\n\tERROR    string = \"ERROR\"\n)\n\ntype Event struct {\n\tType   string          `json:\"type\"`\n\tObject json.RawMessage `json:\"object\"`\n}\n\nfunc main() {\n\tresp, err := http.Get(\"http://apiserver:8080/api/v1/watch/pods?watch=yes\")\n\tif err != nil {\n\t\t// ...\n\t}\n\tdecoder := json.NewDecoder(resp.Body)\n\tfor {\n\t\tvar event Event\n\t\terr = decoder.Decode(&event)\n\t\tif err != nil {\n\t\t\t// ...\n\t\t}\n\t\tswitch event.Type {\n\t\tcase ADDED, MODIFIED:\n\t\t\t// ...\n\t\tcase DELETED:\n\t\t\t// ...\n\t\tcase ERROR:\n\t\t\t// ...\n\t\t}\n\t}\n}\n```\n\n\n调用 watch 接口后会先将所有的对象 list 一次，然后 apiserver 会将变化的数据推送到 client 端，可以看到每次对于 watch 到的事件都需要判断后进行处理，然后将处理后的结果写入到本地的缓存中，原生的 watch 操作还是非常麻烦的。后来了解到官方推出一个客户端工具 client-go ，client-go 中的 Informer 对 watch 操作做了封装，使用起来非常方便，下面会主要介绍一下 client-go 的使用。\n\n#### 二、Informer 的机制\n\ncient-go 是从 k8s 代码中抽出来的一个客户端工具，Informer 是 client-go 中的核心工具包，已经被 kubernetes 中众多组件所使用。所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client，本地缓存被称为 Store，索引被称为 Index。使用 informer 的目的是为了减轻 apiserver 数据交互的压力而抽象出来的一个 cache 层, 客户端对 apiserver 数据的 \"读取\" 和 \"监听\" 操作都通过本地 informer 进行。Informer 实例的`Lister()`方法可以直接查找缓存在本地内存中的数据。\n\nInformer 的主要功能：\n\n- 同步数据到本地缓存\n- 根据对应的事件类型，触发事先注册好的 ResourceEventHandler\n\n##### 1、Informer 中几个组件的作用\nInformer 中主要有 Reflector、Delta FIFO Queue、Local Store、WorkQueue 几个组件。以下是 Informer 的工作流程图。\n\n![Informer 组件](http://cdn.tianfeiyu.com/informer-1.png)\n\n\n\n根据流程图来解释一下 Informer 中几个组件的作用：\n\n- Reflector：称之为反射器，实现对 apiserver 指定类型对象的监控(ListAndWatch)，其中反射实现的就是把监控的结果实例化成具体的对象，最终也是调用 Kubernetes 的 List/Watch API；\n\n- DeltaIFIFO Queue：一个增量队列，将 Reflector 监控变化的对象形成一个 FIFO 队列，此处的 Delta 就是变化；\n\n- LocalStore：就是 informer 的 cache，这里面缓存的是 apiserver 中的对象(其中有一部分可能还在DeltaFIFO 中)，此时使用者再查询对象的时候就直接从 cache 中查找，减少了 apiserver 的压力，LocalStore 只会被 Lister 的 List/Get 方法访问。\n\n- WorkQueue：DeltaIFIFO 收到时间后会先将时间存储在自己的数据结构中，然后直接操作 Store 中存储的数据，更新完 store 后 DeltaIFIFO 会将该事件 pop 到 WorkQueue 中，Controller 收到 WorkQueue  中的事件会根据对应的类型触发对应的回调函数。\n\n\n##### 2、Informer 的工作流程\n\n- Informer 首先会 list/watch apiserver，Informer 所使用的 Reflector 包负责与 apiserver 建立连接，Reflector 使用 ListAndWatch 的方法，会先从 apiserver 中 list 该资源的所有实例，list 会拿到该对象最新的 resourceVersion，然后使用 watch 方法监听该 resourceVersion 之后的所有变化，若中途出现异常，reflector 则会从断开的 resourceVersion 处重现尝试监听所有变化，一旦该对象的实例有创建、删除、更新动作，Reflector 都会收到\"事件通知\"，这时，该事件及它对应的 API 对象这个组合，被称为增量（Delta），它会被放进 DeltaFIFO 中。\n- Informer 会不断地从这个 DeltaFIFO 中读取增量，每拿出一个对象，Informer 就会判断这个增量的时间类型，然后创建或更新本地的缓存，也就是 store。\n- 如果事件类型是 Added（添加对象），那么 Informer 会通过 Indexer 的库把这个增量里的 API 对象保存到本地的缓存中，并为它创建索引，若为删除操作，则在本地缓存中删除该对象。\n- DeltaFIFO 再 pop 这个事件到 controller 中，controller 会调用事先注册的 ResourceEventHandler 回调函数进行处理。\n- 在 ResourceEventHandler 回调函数中，其实只是做了一些很简单的过滤，然后将关心变更的 Object 放到 workqueue 里面。\n- Controller 从 workqueue 里面取出 Object，启动一个 worker 来执行自己的业务逻辑，业务逻辑通常是计算目前集群的状态和用户希望达到的状态有多大的区别，然后孜孜不倦地让 apiserver 将状态演化到用户希望达到的状态，比如为 deployment 创建新的 pods，或者是扩容/缩容 deployment。\n- 在worker中就可以使用 lister 来获取 resource，而不用频繁的访问 apiserver，因为 apiserver 中 resource 的变更都会反映到本地的 cache 中。\n\n  \nInformer 在使用时需要先初始化一个 InformerFactory，目前主要推荐使用的是 SharedInformerFactory，Shared 指的是在多个 Informer 中共享一个本地 cache。\n\nInformer 中的 ResourceEventHandler  函数有三种：\n\n```\n// ResourceEventHandlerFuncs is an adaptor to let you easily specify as many or\n// as few of the notification functions as you want while still implementing\n// ResourceEventHandler.\ntype ResourceEventHandlerFuncs struct {\n    AddFunc    func(obj interface{})\n    UpdateFunc func(oldObj, newObj interface{})\n    DeleteFunc func(obj interface{})\n}\n```\n\n这三种函数的处理逻辑是用户自定义的，在初始化 controller 时注册完 ResourceEventHandler 后，一旦该对象的实例有创建、删除、更新三中操作后就会触发对应的 ResourceEventHandler。\n\n\n#### 三、Informer 使用示例\n\n在实际的开发工作中，Informer 主要用在两处：\n- 在访问 k8s apiserver 的客户端作为一个 client 缓存对象使用；\n- 在一些自定义 controller 中使用，比如 operator 的开发；\n\n\n#### 1、下面是一个作为 client 的使用示例：\n\n\n```\npackage main\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"log\"\n\t\"path/filepath\"\n\n\tcorev1 \"k8s.io/api/core/v1\"\n\t\"k8s.io/apimachinery/pkg/labels\"\n\t\"k8s.io/apimachinery/pkg/util/runtime\"\n\n\t\"k8s.io/client-go/informers\"\n\t\"k8s.io/client-go/kubernetes\"\n\t\"k8s.io/client-go/tools/cache\"\n\t\"k8s.io/client-go/tools/clientcmd\"\n\t\"k8s.io/client-go/util/homedir\"\n)\n\nfunc main() {\n\tvar kubeconfig *string\n\tif home := homedir.HomeDir(); home != \"\" {\n\t\tkubeconfig = flag.String(\"kubeconfig\", filepath.Join(home, \".kube\", \"config\"), \"(optional) absolute path to the kubeconfig file\")\n\t} else {\n\t\tkubeconfig = flag.String(\"kubeconfig\", \"\", \"absolute path to the kubeconfig file\")\n\t}\n\tflag.Parse()\n\n\tconfig, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// 初始化 client\n\tclientset, err := kubernetes.NewForConfig(config)\n\tif err != nil {\n\t\tlog.Panic(err.Error())\n\t}\n\n\tstopper := make(chan struct{})\n\tdefer close(stopper)\n\t\n\t// 初始化 informer\n\tfactory := informers.NewSharedInformerFactory(clientset, 0)\n\tnodeInformer := factory.Core().V1().Nodes()\n\tinformer := nodeInformer.Informer()\n\tdefer runtime.HandleCrash()\n\t\n\t// 启动 informer，list & watch\n\tgo factory.Start(stopper)\n\t\n\t// 从 apiserver 同步资源，即 list \n\tif !cache.WaitForCacheSync(stopper, informer.HasSynced) {\n\t\truntime.HandleError(fmt.Errorf(\"Timed out waiting for caches to sync\"))\n\t\treturn\n\t}\n\n\t// 使用自定义 handler\n\tinformer.AddEventHandler(cache.ResourceEventHandlerFuncs{\n\t\tAddFunc:    onAdd,\n\t\tUpdateFunc: func(interface{}, interface{}) { fmt.Println(\"update not implemented\") }, // 此处省略 workqueue 的使用\n\t\tDeleteFunc: func(interface{}) { fmt.Println(\"delete not implemented\") },\n\t})\n\t\n\t// 创建 lister\n\tnodeLister := nodeInformer.Lister()\n\t// 从 lister 中获取所有 items\n\tnodeList, err := nodeLister.List(labels.Everything())\n\tif err != nil {\n\t\tfmt.Println(err)\n\t}\n\tfmt.Println(\"nodelist:\", nodeList)\n\t<-stopper\n}\n\nfunc onAdd(obj interface{}) {\n\tnode := obj.(*corev1.Node)\n\tfmt.Println(\"add a node:\", node.Name)\n}\n```\n\nShared指的是多个 lister 共享同一个cache，而且资源的变化会同时通知到cache和 listers。这个解释和上面图所展示的内容的是一致的，cache我们在Indexer的介绍中已经分析过了，lister 指的就是OnAdd、OnUpdate、OnDelete 这些回调函数背后的对象。\n\n\n\n#### 2、以下是作为 controller 使用的一个整体工作流程\n\n(1) 创建一个控制器\n- 为控制器创建 workqueue\n- 创建 informer, 为 informer 添加 callback 函数，创建 lister\n\n(2) 启动控制器\n- 启动 informer\n- 等待本地 cache sync 完成后， 启动 workers\n\n(3) 当收到变更事件后，执行 callback \n- 等待事件触发\n- 从事件中获取变更的 Object\n- 做一些必要的检查\n- 生成 object key，一般是 namespace/name 的形式\n- 将 key 放入 workqueue 中\n\n(4) worker loop\n- 等待从 workqueue 中获取到 item，一般为 object key\n- 用 object key 通过 lister 从本地 cache 中获取到真正的 object 对象\n- 做一些检查\n- 执行真正的业务逻辑\n- 处理下一个 item\n\n\n下面是自定义 controller 使用的一个参考：\n\n```\nvar (\n    masterURL  string\n    kubeconfig string\n)\n\nfunc init() {\n    flag.StringVar(&kubeconfig, \"kubeconfig\", \"\", \"Path to a kubeconfig. Only required if out-of-cluster.\")\n    flag.StringVar(&masterURL, \"master\", \"\", \"The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.\")\n}\n\nfunc main() {\n    flag.Parse()\n\n    stopCh := signals.SetupSignalHandler()\n\n    cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)\n    if err != nil {\n        glog.Fatalf(\"Error building kubeconfig: %s\", err.Error())\n    }\n\n    kubeClient, err := kubernetes.NewForConfig(cfg)\n    if err != nil {\n        glog.Fatalf(\"Error building kubernetes clientset: %s\", err.Error())\n    }\n\n    // 所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client\n    // informer watch apiserver,每隔 30 秒 resync 一次(list)\n    kubeInformerFactory := informers.NewSharedInformerFactory(kubeClient, time.Second*30)\n\n    controller := controller.NewController(kubeClient, kubeInformerFactory.Core().V1().Nodes())\n\n    //  启动 informer\n    go kubeInformerFactory.Start(stopCh)\n\n\t // start controller \n    if err = controller.Run(2, stopCh); err != nil {\n        glog.Fatalf(\"Error running controller: %s\", err.Error())\n    }\n}\n\n\n// NewController returns a new network controller\nfunc NewController(\n    kubeclientset kubernetes.Interface,\n    networkclientset clientset.Interface,\n    networkInformer informers.NetworkInformer) *Controller {\n\n    // Create event broadcaster\n    // Add sample-controller types to the default Kubernetes Scheme so Events can be\n    // logged for sample-controller types.\n    utilruntime.Must(networkscheme.AddToScheme(scheme.Scheme))\n    glog.V(4).Info(\"Creating event broadcaster\")\n    eventBroadcaster := record.NewBroadcaster()\n    eventBroadcaster.StartLogging(glog.Infof)\n    eventBroadcaster.StartRecordingToSink(&typedcorev1.EventSinkImpl{Interface: kubeclientset.CoreV1().Events(\"\")})\n    recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: controllerAgentName})\n\n    controller := &Controller{\n        kubeclientset:    kubeclientset,\n        networkclientset: networkclientset,\n        networksLister:   networkInformer.Lister(),\n        networksSynced:   networkInformer.Informer().HasSynced,\n        workqueue:        workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"Networks\"),\n        recorder:         recorder,\n    }\n\n    glog.Info(\"Setting up event handlers\")\n    // Set up an event handler for when Network resources change\n    networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\n        AddFunc: controller.enqueueNetwork,\n        UpdateFunc: func(old, new interface{}) {\n            oldNetwork := old.(*samplecrdv1.Network)\n            newNetwork := new.(*samplecrdv1.Network)\n            if oldNetwork.ResourceVersion == newNetwork.ResourceVersion {\n                // Periodic resync will send update events for all known Networks.\n                // Two different versions of the same Network will always have different RVs.\n                return\n            }\n            controller.enqueueNetwork(new)\n        },\n        DeleteFunc: controller.enqueueNetworkForDelete,\n    })\n\n    return controller\n}\n\n```\n自定义 controller 的详细使用方法可以参考：[k8s-controller-custom-resource](https://github.com/resouer/k8s-controller-custom-resource)\n\n\n\n#### 四、使用中的一些问题\n\n##### 1、Informer 二级缓存中的同步问题\n\n虽然 Informer 和 Kubernetes 之间没有 resync 机制，但 Informer 内部的这两级缓存 DeltaIFIFO 和 LocalStore 之间会存在 resync 机制，k8s 中 kube-controller-manager 的 StatefulSetController 中使用了两级缓存的 resync 机制（如下图所示），我们在生产环境中发现 sts 创建后过了很久 pod 才会创建，主要是由于 StatefulSetController 的两级缓存之间 30s 会同步一次，由于  StatefulSetController watch 到变化后就会把对应的 sts 放入 DeltaIFIFO 中，且每隔30s会把 LocalStore 中全部的 sts 重新入一遍 DeltaIFIFO，入队时会做一些处理，过滤掉一些不需要重复入队列的 sts，若间隔的 30s 内没有处理完队列中所有的 sts，则待处理队列中始终存在未处理完的 sts，并且在同步过程中产生的 sts 会加的队列的尾部，新加入队尾的 sts 只能等到前面的 sts 处理完成（也就是 resync 完成）才会被处理，所以导致的现象就是 sts 创建后过了很久 pod 才会创建。\n\n优化的方法就是去掉二级缓存的同步策略（将 setInformer.Informer().AddEventHandlerWithResyncPeriod() 改为 informer.AddEventHandler()）或者调大同步周期，但是在研究 kube-controller-manager 其他 controller 时发现并不是所有的 controller 都有同步策略，社区也有相关的 issue 反馈了这一问题，[Remove resync period for sset controller](https://github.com/kubernetes/kubernetes/pull/75622)，社区也会在以后的版本中去掉两级缓存之间的 resync 策略。\n\n`k8s.io/kubernetes/pkg/controller/statefulset/stateful_set.go`\n\n![kube-controller-manager sts controller](http://cdn.tianfeiyu.com/informer-2.png)\n\n\n\n##### 2、使用 Informer 如何监听所有资源对象？\n\n一个 Informer 实例只能监听一种 resource，每个 resource 需要创建对应的 Informer 实例。\n\n\n\n##### 3、为什么不是使用 workqueue？\n\n建议使用 RateLimitingQueue，它相比普通的 workqueue 多了以下的功能: \n\n- 限流：可以限制一个 item 被 reenqueued 的次数。\n- 防止 hot loop：它保证了一个 item 被 reenqueued 后，不会马上被处理。\n\n\n\n#### 五、总结\n\n本文介绍了 client-go 包中核心组件 Informer 的原理以及使用方法，Informer 主要功能是缓存对象到本地以及根据对应的事件类型触发已注册好的 ResourceEventHandler，其主要用在访问 k8s apiserver 的客户端和 operator 中。\n\n\n\n\n参考：\n\n[如何用 client-go 拓展 Kubernetes 的 API](https://mp.weixin.qq.com/s?__biz=MzU1OTAzNzc5MQ==&mid=2247484052&idx=1&sn=cec9f4a1ee0d21c5b2c51bd147b8af59&chksm=fc1c2ea4cb6ba7b283eef5ac4a45985437c648361831bc3e6dd5f38053be1968b3389386e415&scene=21#wechat_redirect)\n\nhttps://www.kubernetes.org.cn/2693.html\n\n[Kubernetes 大咖秀徐超《使用 client-go 控制原生及拓展的 Kubernetes API》](https://studygolang.com/articles/9270)\n\n[Use prometheus conventions for workqueue metrics](https://github.com/kubernetes/kubernetes/issues/71165)\n\n[深入浅出kubernetes之client-go的workqueue](https://blog.csdn.net/weixin_42663840/article/details/81482553#%E9%99%90%E9%80%9F%E9%98%9F%E5%88%97)\n\n<https://gianarb.it/blog/kubernetes-shared-informer>\n\n[理解 K8S 的设计精髓之 List-Watch机制和Informer模块](https://zhuanlan.zhihu.com/p/59660536)\n\n<https://ranler.org/notes/file/528>\n\n[Kubernetes Client-go Informer 源码分析](https://yq.aliyun.com/articles/688485)\n","source":"_posts/client-go_informer.md","raw":"---\ntitle: kubernetes 中 informer 的使用\ndate: 2019-05-17 11:00:30\ntags: [\"client-go\",\"informer\"]\ntype: \"client-go\"\n---\n\n#### 一、kubernetes 集群的几种访问方式\n\n在实际开发过程中，若想要获取 kubernetes 中某个资源（比如 pod）的所有对象，可以使用 kubectl、k8s REST API、client-go(ClientSet、Dynamic Client、RESTClient 三种方式) 等多种方式访问 k8s 集群获取资源。在笔者的开发过程中，最初都是直接调用 k8s 的 REST API 来获取的，使用 `kubectl get pod -v=9` 可以直接看到调用 k8s 的接口，然后在程序中直接访问还是比较方便的。但是随着集群规模的增长或者从国内获取海外 k8s 集群的数据，直接调用 k8s 接口获取所有 pod 还是比较耗时，这个问题有多种解决方法，最初是直接使用 k8s 原生的 watch 接口来获取的，下面是一个伪代码：\n\n```\nconst (\n\tADDED    string = \"ADDED\"\n\tMODIFIED string = \"MODIFIED\"\n\tDELETED  string = \"DELETED\"\n\tERROR    string = \"ERROR\"\n)\n\ntype Event struct {\n\tType   string          `json:\"type\"`\n\tObject json.RawMessage `json:\"object\"`\n}\n\nfunc main() {\n\tresp, err := http.Get(\"http://apiserver:8080/api/v1/watch/pods?watch=yes\")\n\tif err != nil {\n\t\t// ...\n\t}\n\tdecoder := json.NewDecoder(resp.Body)\n\tfor {\n\t\tvar event Event\n\t\terr = decoder.Decode(&event)\n\t\tif err != nil {\n\t\t\t// ...\n\t\t}\n\t\tswitch event.Type {\n\t\tcase ADDED, MODIFIED:\n\t\t\t// ...\n\t\tcase DELETED:\n\t\t\t// ...\n\t\tcase ERROR:\n\t\t\t// ...\n\t\t}\n\t}\n}\n```\n\n\n调用 watch 接口后会先将所有的对象 list 一次，然后 apiserver 会将变化的数据推送到 client 端，可以看到每次对于 watch 到的事件都需要判断后进行处理，然后将处理后的结果写入到本地的缓存中，原生的 watch 操作还是非常麻烦的。后来了解到官方推出一个客户端工具 client-go ，client-go 中的 Informer 对 watch 操作做了封装，使用起来非常方便，下面会主要介绍一下 client-go 的使用。\n\n#### 二、Informer 的机制\n\ncient-go 是从 k8s 代码中抽出来的一个客户端工具，Informer 是 client-go 中的核心工具包，已经被 kubernetes 中众多组件所使用。所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client，本地缓存被称为 Store，索引被称为 Index。使用 informer 的目的是为了减轻 apiserver 数据交互的压力而抽象出来的一个 cache 层, 客户端对 apiserver 数据的 \"读取\" 和 \"监听\" 操作都通过本地 informer 进行。Informer 实例的`Lister()`方法可以直接查找缓存在本地内存中的数据。\n\nInformer 的主要功能：\n\n- 同步数据到本地缓存\n- 根据对应的事件类型，触发事先注册好的 ResourceEventHandler\n\n##### 1、Informer 中几个组件的作用\nInformer 中主要有 Reflector、Delta FIFO Queue、Local Store、WorkQueue 几个组件。以下是 Informer 的工作流程图。\n\n![Informer 组件](http://cdn.tianfeiyu.com/informer-1.png)\n\n\n\n根据流程图来解释一下 Informer 中几个组件的作用：\n\n- Reflector：称之为反射器，实现对 apiserver 指定类型对象的监控(ListAndWatch)，其中反射实现的就是把监控的结果实例化成具体的对象，最终也是调用 Kubernetes 的 List/Watch API；\n\n- DeltaIFIFO Queue：一个增量队列，将 Reflector 监控变化的对象形成一个 FIFO 队列，此处的 Delta 就是变化；\n\n- LocalStore：就是 informer 的 cache，这里面缓存的是 apiserver 中的对象(其中有一部分可能还在DeltaFIFO 中)，此时使用者再查询对象的时候就直接从 cache 中查找，减少了 apiserver 的压力，LocalStore 只会被 Lister 的 List/Get 方法访问。\n\n- WorkQueue：DeltaIFIFO 收到时间后会先将时间存储在自己的数据结构中，然后直接操作 Store 中存储的数据，更新完 store 后 DeltaIFIFO 会将该事件 pop 到 WorkQueue 中，Controller 收到 WorkQueue  中的事件会根据对应的类型触发对应的回调函数。\n\n\n##### 2、Informer 的工作流程\n\n- Informer 首先会 list/watch apiserver，Informer 所使用的 Reflector 包负责与 apiserver 建立连接，Reflector 使用 ListAndWatch 的方法，会先从 apiserver 中 list 该资源的所有实例，list 会拿到该对象最新的 resourceVersion，然后使用 watch 方法监听该 resourceVersion 之后的所有变化，若中途出现异常，reflector 则会从断开的 resourceVersion 处重现尝试监听所有变化，一旦该对象的实例有创建、删除、更新动作，Reflector 都会收到\"事件通知\"，这时，该事件及它对应的 API 对象这个组合，被称为增量（Delta），它会被放进 DeltaFIFO 中。\n- Informer 会不断地从这个 DeltaFIFO 中读取增量，每拿出一个对象，Informer 就会判断这个增量的时间类型，然后创建或更新本地的缓存，也就是 store。\n- 如果事件类型是 Added（添加对象），那么 Informer 会通过 Indexer 的库把这个增量里的 API 对象保存到本地的缓存中，并为它创建索引，若为删除操作，则在本地缓存中删除该对象。\n- DeltaFIFO 再 pop 这个事件到 controller 中，controller 会调用事先注册的 ResourceEventHandler 回调函数进行处理。\n- 在 ResourceEventHandler 回调函数中，其实只是做了一些很简单的过滤，然后将关心变更的 Object 放到 workqueue 里面。\n- Controller 从 workqueue 里面取出 Object，启动一个 worker 来执行自己的业务逻辑，业务逻辑通常是计算目前集群的状态和用户希望达到的状态有多大的区别，然后孜孜不倦地让 apiserver 将状态演化到用户希望达到的状态，比如为 deployment 创建新的 pods，或者是扩容/缩容 deployment。\n- 在worker中就可以使用 lister 来获取 resource，而不用频繁的访问 apiserver，因为 apiserver 中 resource 的变更都会反映到本地的 cache 中。\n\n  \nInformer 在使用时需要先初始化一个 InformerFactory，目前主要推荐使用的是 SharedInformerFactory，Shared 指的是在多个 Informer 中共享一个本地 cache。\n\nInformer 中的 ResourceEventHandler  函数有三种：\n\n```\n// ResourceEventHandlerFuncs is an adaptor to let you easily specify as many or\n// as few of the notification functions as you want while still implementing\n// ResourceEventHandler.\ntype ResourceEventHandlerFuncs struct {\n    AddFunc    func(obj interface{})\n    UpdateFunc func(oldObj, newObj interface{})\n    DeleteFunc func(obj interface{})\n}\n```\n\n这三种函数的处理逻辑是用户自定义的，在初始化 controller 时注册完 ResourceEventHandler 后，一旦该对象的实例有创建、删除、更新三中操作后就会触发对应的 ResourceEventHandler。\n\n\n#### 三、Informer 使用示例\n\n在实际的开发工作中，Informer 主要用在两处：\n- 在访问 k8s apiserver 的客户端作为一个 client 缓存对象使用；\n- 在一些自定义 controller 中使用，比如 operator 的开发；\n\n\n#### 1、下面是一个作为 client 的使用示例：\n\n\n```\npackage main\n\nimport (\n\t\"flag\"\n\t\"fmt\"\n\t\"log\"\n\t\"path/filepath\"\n\n\tcorev1 \"k8s.io/api/core/v1\"\n\t\"k8s.io/apimachinery/pkg/labels\"\n\t\"k8s.io/apimachinery/pkg/util/runtime\"\n\n\t\"k8s.io/client-go/informers\"\n\t\"k8s.io/client-go/kubernetes\"\n\t\"k8s.io/client-go/tools/cache\"\n\t\"k8s.io/client-go/tools/clientcmd\"\n\t\"k8s.io/client-go/util/homedir\"\n)\n\nfunc main() {\n\tvar kubeconfig *string\n\tif home := homedir.HomeDir(); home != \"\" {\n\t\tkubeconfig = flag.String(\"kubeconfig\", filepath.Join(home, \".kube\", \"config\"), \"(optional) absolute path to the kubeconfig file\")\n\t} else {\n\t\tkubeconfig = flag.String(\"kubeconfig\", \"\", \"absolute path to the kubeconfig file\")\n\t}\n\tflag.Parse()\n\n\tconfig, err := clientcmd.BuildConfigFromFlags(\"\", *kubeconfig)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\t// 初始化 client\n\tclientset, err := kubernetes.NewForConfig(config)\n\tif err != nil {\n\t\tlog.Panic(err.Error())\n\t}\n\n\tstopper := make(chan struct{})\n\tdefer close(stopper)\n\t\n\t// 初始化 informer\n\tfactory := informers.NewSharedInformerFactory(clientset, 0)\n\tnodeInformer := factory.Core().V1().Nodes()\n\tinformer := nodeInformer.Informer()\n\tdefer runtime.HandleCrash()\n\t\n\t// 启动 informer，list & watch\n\tgo factory.Start(stopper)\n\t\n\t// 从 apiserver 同步资源，即 list \n\tif !cache.WaitForCacheSync(stopper, informer.HasSynced) {\n\t\truntime.HandleError(fmt.Errorf(\"Timed out waiting for caches to sync\"))\n\t\treturn\n\t}\n\n\t// 使用自定义 handler\n\tinformer.AddEventHandler(cache.ResourceEventHandlerFuncs{\n\t\tAddFunc:    onAdd,\n\t\tUpdateFunc: func(interface{}, interface{}) { fmt.Println(\"update not implemented\") }, // 此处省略 workqueue 的使用\n\t\tDeleteFunc: func(interface{}) { fmt.Println(\"delete not implemented\") },\n\t})\n\t\n\t// 创建 lister\n\tnodeLister := nodeInformer.Lister()\n\t// 从 lister 中获取所有 items\n\tnodeList, err := nodeLister.List(labels.Everything())\n\tif err != nil {\n\t\tfmt.Println(err)\n\t}\n\tfmt.Println(\"nodelist:\", nodeList)\n\t<-stopper\n}\n\nfunc onAdd(obj interface{}) {\n\tnode := obj.(*corev1.Node)\n\tfmt.Println(\"add a node:\", node.Name)\n}\n```\n\nShared指的是多个 lister 共享同一个cache，而且资源的变化会同时通知到cache和 listers。这个解释和上面图所展示的内容的是一致的，cache我们在Indexer的介绍中已经分析过了，lister 指的就是OnAdd、OnUpdate、OnDelete 这些回调函数背后的对象。\n\n\n\n#### 2、以下是作为 controller 使用的一个整体工作流程\n\n(1) 创建一个控制器\n- 为控制器创建 workqueue\n- 创建 informer, 为 informer 添加 callback 函数，创建 lister\n\n(2) 启动控制器\n- 启动 informer\n- 等待本地 cache sync 完成后， 启动 workers\n\n(3) 当收到变更事件后，执行 callback \n- 等待事件触发\n- 从事件中获取变更的 Object\n- 做一些必要的检查\n- 生成 object key，一般是 namespace/name 的形式\n- 将 key 放入 workqueue 中\n\n(4) worker loop\n- 等待从 workqueue 中获取到 item，一般为 object key\n- 用 object key 通过 lister 从本地 cache 中获取到真正的 object 对象\n- 做一些检查\n- 执行真正的业务逻辑\n- 处理下一个 item\n\n\n下面是自定义 controller 使用的一个参考：\n\n```\nvar (\n    masterURL  string\n    kubeconfig string\n)\n\nfunc init() {\n    flag.StringVar(&kubeconfig, \"kubeconfig\", \"\", \"Path to a kubeconfig. Only required if out-of-cluster.\")\n    flag.StringVar(&masterURL, \"master\", \"\", \"The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.\")\n}\n\nfunc main() {\n    flag.Parse()\n\n    stopCh := signals.SetupSignalHandler()\n\n    cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)\n    if err != nil {\n        glog.Fatalf(\"Error building kubeconfig: %s\", err.Error())\n    }\n\n    kubeClient, err := kubernetes.NewForConfig(cfg)\n    if err != nil {\n        glog.Fatalf(\"Error building kubernetes clientset: %s\", err.Error())\n    }\n\n    // 所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client\n    // informer watch apiserver,每隔 30 秒 resync 一次(list)\n    kubeInformerFactory := informers.NewSharedInformerFactory(kubeClient, time.Second*30)\n\n    controller := controller.NewController(kubeClient, kubeInformerFactory.Core().V1().Nodes())\n\n    //  启动 informer\n    go kubeInformerFactory.Start(stopCh)\n\n\t // start controller \n    if err = controller.Run(2, stopCh); err != nil {\n        glog.Fatalf(\"Error running controller: %s\", err.Error())\n    }\n}\n\n\n// NewController returns a new network controller\nfunc NewController(\n    kubeclientset kubernetes.Interface,\n    networkclientset clientset.Interface,\n    networkInformer informers.NetworkInformer) *Controller {\n\n    // Create event broadcaster\n    // Add sample-controller types to the default Kubernetes Scheme so Events can be\n    // logged for sample-controller types.\n    utilruntime.Must(networkscheme.AddToScheme(scheme.Scheme))\n    glog.V(4).Info(\"Creating event broadcaster\")\n    eventBroadcaster := record.NewBroadcaster()\n    eventBroadcaster.StartLogging(glog.Infof)\n    eventBroadcaster.StartRecordingToSink(&typedcorev1.EventSinkImpl{Interface: kubeclientset.CoreV1().Events(\"\")})\n    recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource{Component: controllerAgentName})\n\n    controller := &Controller{\n        kubeclientset:    kubeclientset,\n        networkclientset: networkclientset,\n        networksLister:   networkInformer.Lister(),\n        networksSynced:   networkInformer.Informer().HasSynced,\n        workqueue:        workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"Networks\"),\n        recorder:         recorder,\n    }\n\n    glog.Info(\"Setting up event handlers\")\n    // Set up an event handler for when Network resources change\n    networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\n        AddFunc: controller.enqueueNetwork,\n        UpdateFunc: func(old, new interface{}) {\n            oldNetwork := old.(*samplecrdv1.Network)\n            newNetwork := new.(*samplecrdv1.Network)\n            if oldNetwork.ResourceVersion == newNetwork.ResourceVersion {\n                // Periodic resync will send update events for all known Networks.\n                // Two different versions of the same Network will always have different RVs.\n                return\n            }\n            controller.enqueueNetwork(new)\n        },\n        DeleteFunc: controller.enqueueNetworkForDelete,\n    })\n\n    return controller\n}\n\n```\n自定义 controller 的详细使用方法可以参考：[k8s-controller-custom-resource](https://github.com/resouer/k8s-controller-custom-resource)\n\n\n\n#### 四、使用中的一些问题\n\n##### 1、Informer 二级缓存中的同步问题\n\n虽然 Informer 和 Kubernetes 之间没有 resync 机制，但 Informer 内部的这两级缓存 DeltaIFIFO 和 LocalStore 之间会存在 resync 机制，k8s 中 kube-controller-manager 的 StatefulSetController 中使用了两级缓存的 resync 机制（如下图所示），我们在生产环境中发现 sts 创建后过了很久 pod 才会创建，主要是由于 StatefulSetController 的两级缓存之间 30s 会同步一次，由于  StatefulSetController watch 到变化后就会把对应的 sts 放入 DeltaIFIFO 中，且每隔30s会把 LocalStore 中全部的 sts 重新入一遍 DeltaIFIFO，入队时会做一些处理，过滤掉一些不需要重复入队列的 sts，若间隔的 30s 内没有处理完队列中所有的 sts，则待处理队列中始终存在未处理完的 sts，并且在同步过程中产生的 sts 会加的队列的尾部，新加入队尾的 sts 只能等到前面的 sts 处理完成（也就是 resync 完成）才会被处理，所以导致的现象就是 sts 创建后过了很久 pod 才会创建。\n\n优化的方法就是去掉二级缓存的同步策略（将 setInformer.Informer().AddEventHandlerWithResyncPeriod() 改为 informer.AddEventHandler()）或者调大同步周期，但是在研究 kube-controller-manager 其他 controller 时发现并不是所有的 controller 都有同步策略，社区也有相关的 issue 反馈了这一问题，[Remove resync period for sset controller](https://github.com/kubernetes/kubernetes/pull/75622)，社区也会在以后的版本中去掉两级缓存之间的 resync 策略。\n\n`k8s.io/kubernetes/pkg/controller/statefulset/stateful_set.go`\n\n![kube-controller-manager sts controller](http://cdn.tianfeiyu.com/informer-2.png)\n\n\n\n##### 2、使用 Informer 如何监听所有资源对象？\n\n一个 Informer 实例只能监听一种 resource，每个 resource 需要创建对应的 Informer 实例。\n\n\n\n##### 3、为什么不是使用 workqueue？\n\n建议使用 RateLimitingQueue，它相比普通的 workqueue 多了以下的功能: \n\n- 限流：可以限制一个 item 被 reenqueued 的次数。\n- 防止 hot loop：它保证了一个 item 被 reenqueued 后，不会马上被处理。\n\n\n\n#### 五、总结\n\n本文介绍了 client-go 包中核心组件 Informer 的原理以及使用方法，Informer 主要功能是缓存对象到本地以及根据对应的事件类型触发已注册好的 ResourceEventHandler，其主要用在访问 k8s apiserver 的客户端和 operator 中。\n\n\n\n\n参考：\n\n[如何用 client-go 拓展 Kubernetes 的 API](https://mp.weixin.qq.com/s?__biz=MzU1OTAzNzc5MQ==&mid=2247484052&idx=1&sn=cec9f4a1ee0d21c5b2c51bd147b8af59&chksm=fc1c2ea4cb6ba7b283eef5ac4a45985437c648361831bc3e6dd5f38053be1968b3389386e415&scene=21#wechat_redirect)\n\nhttps://www.kubernetes.org.cn/2693.html\n\n[Kubernetes 大咖秀徐超《使用 client-go 控制原生及拓展的 Kubernetes API》](https://studygolang.com/articles/9270)\n\n[Use prometheus conventions for workqueue metrics](https://github.com/kubernetes/kubernetes/issues/71165)\n\n[深入浅出kubernetes之client-go的workqueue](https://blog.csdn.net/weixin_42663840/article/details/81482553#%E9%99%90%E9%80%9F%E9%98%9F%E5%88%97)\n\n<https://gianarb.it/blog/kubernetes-shared-informer>\n\n[理解 K8S 的设计精髓之 List-Watch机制和Informer模块](https://zhuanlan.zhihu.com/p/59660536)\n\n<https://ranler.org/notes/file/528>\n\n[Kubernetes Client-go Informer 源码分析](https://yq.aliyun.com/articles/688485)\n","slug":"client-go_informer","published":1,"updated":"2019-07-21T10:07:57.293Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro5950003apwndnp522i3","content":"<h4 id=\"一、kubernetes-集群的几种访问方式\"><a href=\"#一、kubernetes-集群的几种访问方式\" class=\"headerlink\" title=\"一、kubernetes 集群的几种访问方式\"></a>一、kubernetes 集群的几种访问方式</h4><p>在实际开发过程中，若想要获取 kubernetes 中某个资源（比如 pod）的所有对象，可以使用 kubectl、k8s REST API、client-go(ClientSet、Dynamic Client、RESTClient 三种方式) 等多种方式访问 k8s 集群获取资源。在笔者的开发过程中，最初都是直接调用 k8s 的 REST API 来获取的，使用 <code>kubectl get pod -v=9</code> 可以直接看到调用 k8s 的接口，然后在程序中直接访问还是比较方便的。但是随着集群规模的增长或者从国内获取海外 k8s 集群的数据，直接调用 k8s 接口获取所有 pod 还是比较耗时，这个问题有多种解决方法，最初是直接使用 k8s 原生的 watch 接口来获取的，下面是一个伪代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">const (</span><br><span class=\"line\">\tADDED    string = &quot;ADDED&quot;</span><br><span class=\"line\">\tMODIFIED string = &quot;MODIFIED&quot;</span><br><span class=\"line\">\tDELETED  string = &quot;DELETED&quot;</span><br><span class=\"line\">\tERROR    string = &quot;ERROR&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">type Event struct &#123;</span><br><span class=\"line\">\tType   string          `json:&quot;type&quot;`</span><br><span class=\"line\">\tObject json.RawMessage `json:&quot;object&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tresp, err := http.Get(&quot;http://apiserver:8080/api/v1/watch/pods?watch=yes&quot;)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t// ...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tdecoder := json.NewDecoder(resp.Body)</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tvar event Event</span><br><span class=\"line\">\t\terr = decoder.Decode(&amp;event)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t// ...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tswitch event.Type &#123;</span><br><span class=\"line\">\t\tcase ADDED, MODIFIED:</span><br><span class=\"line\">\t\t\t// ...</span><br><span class=\"line\">\t\tcase DELETED:</span><br><span class=\"line\">\t\t\t// ...</span><br><span class=\"line\">\t\tcase ERROR:</span><br><span class=\"line\">\t\t\t// ...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>调用 watch 接口后会先将所有的对象 list 一次，然后 apiserver 会将变化的数据推送到 client 端，可以看到每次对于 watch 到的事件都需要判断后进行处理，然后将处理后的结果写入到本地的缓存中，原生的 watch 操作还是非常麻烦的。后来了解到官方推出一个客户端工具 client-go ，client-go 中的 Informer 对 watch 操作做了封装，使用起来非常方便，下面会主要介绍一下 client-go 的使用。</p>\n<h4 id=\"二、Informer-的机制\"><a href=\"#二、Informer-的机制\" class=\"headerlink\" title=\"二、Informer 的机制\"></a>二、Informer 的机制</h4><p>cient-go 是从 k8s 代码中抽出来的一个客户端工具，Informer 是 client-go 中的核心工具包，已经被 kubernetes 中众多组件所使用。所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client，本地缓存被称为 Store，索引被称为 Index。使用 informer 的目的是为了减轻 apiserver 数据交互的压力而抽象出来的一个 cache 层, 客户端对 apiserver 数据的 “读取” 和 “监听” 操作都通过本地 informer 进行。Informer 实例的<code>Lister()</code>方法可以直接查找缓存在本地内存中的数据。</p>\n<p>Informer 的主要功能：</p>\n<ul>\n<li>同步数据到本地缓存</li>\n<li>根据对应的事件类型，触发事先注册好的 ResourceEventHandler</li>\n</ul>\n<h5 id=\"1、Informer-中几个组件的作用\"><a href=\"#1、Informer-中几个组件的作用\" class=\"headerlink\" title=\"1、Informer 中几个组件的作用\"></a>1、Informer 中几个组件的作用</h5><p>Informer 中主要有 Reflector、Delta FIFO Queue、Local Store、WorkQueue 几个组件。以下是 Informer 的工作流程图。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/informer-1.png\" alt=\"Informer 组件\"></p>\n<p>根据流程图来解释一下 Informer 中几个组件的作用：</p>\n<ul>\n<li><p>Reflector：称之为反射器，实现对 apiserver 指定类型对象的监控(ListAndWatch)，其中反射实现的就是把监控的结果实例化成具体的对象，最终也是调用 Kubernetes 的 List/Watch API；</p>\n</li>\n<li><p>DeltaIFIFO Queue：一个增量队列，将 Reflector 监控变化的对象形成一个 FIFO 队列，此处的 Delta 就是变化；</p>\n</li>\n<li><p>LocalStore：就是 informer 的 cache，这里面缓存的是 apiserver 中的对象(其中有一部分可能还在DeltaFIFO 中)，此时使用者再查询对象的时候就直接从 cache 中查找，减少了 apiserver 的压力，LocalStore 只会被 Lister 的 List/Get 方法访问。</p>\n</li>\n<li><p>WorkQueue：DeltaIFIFO 收到时间后会先将时间存储在自己的数据结构中，然后直接操作 Store 中存储的数据，更新完 store 后 DeltaIFIFO 会将该事件 pop 到 WorkQueue 中，Controller 收到 WorkQueue  中的事件会根据对应的类型触发对应的回调函数。</p>\n</li>\n</ul>\n<h5 id=\"2、Informer-的工作流程\"><a href=\"#2、Informer-的工作流程\" class=\"headerlink\" title=\"2、Informer 的工作流程\"></a>2、Informer 的工作流程</h5><ul>\n<li>Informer 首先会 list/watch apiserver，Informer 所使用的 Reflector 包负责与 apiserver 建立连接，Reflector 使用 ListAndWatch 的方法，会先从 apiserver 中 list 该资源的所有实例，list 会拿到该对象最新的 resourceVersion，然后使用 watch 方法监听该 resourceVersion 之后的所有变化，若中途出现异常，reflector 则会从断开的 resourceVersion 处重现尝试监听所有变化，一旦该对象的实例有创建、删除、更新动作，Reflector 都会收到”事件通知”，这时，该事件及它对应的 API 对象这个组合，被称为增量（Delta），它会被放进 DeltaFIFO 中。</li>\n<li>Informer 会不断地从这个 DeltaFIFO 中读取增量，每拿出一个对象，Informer 就会判断这个增量的时间类型，然后创建或更新本地的缓存，也就是 store。</li>\n<li>如果事件类型是 Added（添加对象），那么 Informer 会通过 Indexer 的库把这个增量里的 API 对象保存到本地的缓存中，并为它创建索引，若为删除操作，则在本地缓存中删除该对象。</li>\n<li>DeltaFIFO 再 pop 这个事件到 controller 中，controller 会调用事先注册的 ResourceEventHandler 回调函数进行处理。</li>\n<li>在 ResourceEventHandler 回调函数中，其实只是做了一些很简单的过滤，然后将关心变更的 Object 放到 workqueue 里面。</li>\n<li>Controller 从 workqueue 里面取出 Object，启动一个 worker 来执行自己的业务逻辑，业务逻辑通常是计算目前集群的状态和用户希望达到的状态有多大的区别，然后孜孜不倦地让 apiserver 将状态演化到用户希望达到的状态，比如为 deployment 创建新的 pods，或者是扩容/缩容 deployment。</li>\n<li>在worker中就可以使用 lister 来获取 resource，而不用频繁的访问 apiserver，因为 apiserver 中 resource 的变更都会反映到本地的 cache 中。</li>\n</ul>\n<p>Informer 在使用时需要先初始化一个 InformerFactory，目前主要推荐使用的是 SharedInformerFactory，Shared 指的是在多个 Informer 中共享一个本地 cache。</p>\n<p>Informer 中的 ResourceEventHandler  函数有三种：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// ResourceEventHandlerFuncs is an adaptor to let you easily specify as many or</span><br><span class=\"line\">// as few of the notification functions as you want while still implementing</span><br><span class=\"line\">// ResourceEventHandler.</span><br><span class=\"line\">type ResourceEventHandlerFuncs struct &#123;</span><br><span class=\"line\">    AddFunc    func(obj interface&#123;&#125;)</span><br><span class=\"line\">    UpdateFunc func(oldObj, newObj interface&#123;&#125;)</span><br><span class=\"line\">    DeleteFunc func(obj interface&#123;&#125;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这三种函数的处理逻辑是用户自定义的，在初始化 controller 时注册完 ResourceEventHandler 后，一旦该对象的实例有创建、删除、更新三中操作后就会触发对应的 ResourceEventHandler。</p>\n<h4 id=\"三、Informer-使用示例\"><a href=\"#三、Informer-使用示例\" class=\"headerlink\" title=\"三、Informer 使用示例\"></a>三、Informer 使用示例</h4><p>在实际的开发工作中，Informer 主要用在两处：</p>\n<ul>\n<li>在访问 k8s apiserver 的客户端作为一个 client 缓存对象使用；</li>\n<li>在一些自定义 controller 中使用，比如 operator 的开发；</li>\n</ul>\n<h4 id=\"1、下面是一个作为-client-的使用示例：\"><a href=\"#1、下面是一个作为-client-的使用示例：\" class=\"headerlink\" title=\"1、下面是一个作为 client 的使用示例：\"></a>1、下面是一个作为 client 的使用示例：</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;flag&quot;</span><br><span class=\"line\">\t&quot;fmt&quot;</span><br><span class=\"line\">\t&quot;log&quot;</span><br><span class=\"line\">\t&quot;path/filepath&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\tcorev1 &quot;k8s.io/api/core/v1&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apimachinery/pkg/labels&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apimachinery/pkg/util/runtime&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&quot;k8s.io/client-go/informers&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/kubernetes&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/cache&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/clientcmd&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/util/homedir&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tvar kubeconfig *string</span><br><span class=\"line\">\tif home := homedir.HomeDir(); home != &quot;&quot; &#123;</span><br><span class=\"line\">\t\tkubeconfig = flag.String(&quot;kubeconfig&quot;, filepath.Join(home, &quot;.kube&quot;, &quot;config&quot;), &quot;(optional) absolute path to the kubeconfig file&quot;)</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\tkubeconfig = flag.String(&quot;kubeconfig&quot;, &quot;&quot;, &quot;absolute path to the kubeconfig file&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tflag.Parse()</span><br><span class=\"line\"></span><br><span class=\"line\">\tconfig, err := clientcmd.BuildConfigFromFlags(&quot;&quot;, *kubeconfig)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tpanic(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 初始化 client</span><br><span class=\"line\">\tclientset, err := kubernetes.NewForConfig(config)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tlog.Panic(err.Error())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tstopper := make(chan struct&#123;&#125;)</span><br><span class=\"line\">\tdefer close(stopper)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 初始化 informer</span><br><span class=\"line\">\tfactory := informers.NewSharedInformerFactory(clientset, 0)</span><br><span class=\"line\">\tnodeInformer := factory.Core().V1().Nodes()</span><br><span class=\"line\">\tinformer := nodeInformer.Informer()</span><br><span class=\"line\">\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 启动 informer，list &amp; watch</span><br><span class=\"line\">\tgo factory.Start(stopper)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 从 apiserver 同步资源，即 list </span><br><span class=\"line\">\tif !cache.WaitForCacheSync(stopper, informer.HasSynced) &#123;</span><br><span class=\"line\">\t\truntime.HandleError(fmt.Errorf(&quot;Timed out waiting for caches to sync&quot;))</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用自定义 handler</span><br><span class=\"line\">\tinformer.AddEventHandler(cache.ResourceEventHandlerFuncs&#123;</span><br><span class=\"line\">\t\tAddFunc:    onAdd,</span><br><span class=\"line\">\t\tUpdateFunc: func(interface&#123;&#125;, interface&#123;&#125;) &#123; fmt.Println(&quot;update not implemented&quot;) &#125;, // 此处省略 workqueue 的使用</span><br><span class=\"line\">\t\tDeleteFunc: func(interface&#123;&#125;) &#123; fmt.Println(&quot;delete not implemented&quot;) &#125;,</span><br><span class=\"line\">\t&#125;)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 创建 lister</span><br><span class=\"line\">\tnodeLister := nodeInformer.Lister()</span><br><span class=\"line\">\t// 从 lister 中获取所有 items</span><br><span class=\"line\">\tnodeList, err := nodeLister.List(labels.Everything())</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tfmt.Println(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tfmt.Println(&quot;nodelist:&quot;, nodeList)</span><br><span class=\"line\">\t&lt;-stopper</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func onAdd(obj interface&#123;&#125;) &#123;</span><br><span class=\"line\">\tnode := obj.(*corev1.Node)</span><br><span class=\"line\">\tfmt.Println(&quot;add a node:&quot;, node.Name)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Shared指的是多个 lister 共享同一个cache，而且资源的变化会同时通知到cache和 listers。这个解释和上面图所展示的内容的是一致的，cache我们在Indexer的介绍中已经分析过了，lister 指的就是OnAdd、OnUpdate、OnDelete 这些回调函数背后的对象。</p>\n<h4 id=\"2、以下是作为-controller-使用的一个整体工作流程\"><a href=\"#2、以下是作为-controller-使用的一个整体工作流程\" class=\"headerlink\" title=\"2、以下是作为 controller 使用的一个整体工作流程\"></a>2、以下是作为 controller 使用的一个整体工作流程</h4><p>(1) 创建一个控制器</p>\n<ul>\n<li>为控制器创建 workqueue</li>\n<li>创建 informer, 为 informer 添加 callback 函数，创建 lister</li>\n</ul>\n<p>(2) 启动控制器</p>\n<ul>\n<li>启动 informer</li>\n<li>等待本地 cache sync 完成后， 启动 workers</li>\n</ul>\n<p>(3) 当收到变更事件后，执行 callback </p>\n<ul>\n<li>等待事件触发</li>\n<li>从事件中获取变更的 Object</li>\n<li>做一些必要的检查</li>\n<li>生成 object key，一般是 namespace/name 的形式</li>\n<li>将 key 放入 workqueue 中</li>\n</ul>\n<p>(4) worker loop</p>\n<ul>\n<li>等待从 workqueue 中获取到 item，一般为 object key</li>\n<li>用 object key 通过 lister 从本地 cache 中获取到真正的 object 对象</li>\n<li>做一些检查</li>\n<li>执行真正的业务逻辑</li>\n<li>处理下一个 item</li>\n</ul>\n<p>下面是自定义 controller 使用的一个参考：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var (</span><br><span class=\"line\">    masterURL  string</span><br><span class=\"line\">    kubeconfig string</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func init() &#123;</span><br><span class=\"line\">    flag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;Path to a kubeconfig. Only required if out-of-cluster.&quot;)</span><br><span class=\"line\">    flag.StringVar(&amp;masterURL, &quot;master&quot;, &quot;&quot;, &quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.&quot;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">    flag.Parse()</span><br><span class=\"line\"></span><br><span class=\"line\">    stopCh := signals.SetupSignalHandler()</span><br><span class=\"line\"></span><br><span class=\"line\">    cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        glog.Fatalf(&quot;Error building kubeconfig: %s&quot;, err.Error())</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    kubeClient, err := kubernetes.NewForConfig(cfg)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        glog.Fatalf(&quot;Error building kubernetes clientset: %s&quot;, err.Error())</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client</span><br><span class=\"line\">    // informer watch apiserver,每隔 30 秒 resync 一次(list)</span><br><span class=\"line\">    kubeInformerFactory := informers.NewSharedInformerFactory(kubeClient, time.Second*30)</span><br><span class=\"line\"></span><br><span class=\"line\">    controller := controller.NewController(kubeClient, kubeInformerFactory.Core().V1().Nodes())</span><br><span class=\"line\"></span><br><span class=\"line\">    //  启动 informer</span><br><span class=\"line\">    go kubeInformerFactory.Start(stopCh)</span><br><span class=\"line\"></span><br><span class=\"line\">\t // start controller </span><br><span class=\"line\">    if err = controller.Run(2, stopCh); err != nil &#123;</span><br><span class=\"line\">        glog.Fatalf(&quot;Error running controller: %s&quot;, err.Error())</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// NewController returns a new network controller</span><br><span class=\"line\">func NewController(</span><br><span class=\"line\">    kubeclientset kubernetes.Interface,</span><br><span class=\"line\">    networkclientset clientset.Interface,</span><br><span class=\"line\">    networkInformer informers.NetworkInformer) *Controller &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    // Create event broadcaster</span><br><span class=\"line\">    // Add sample-controller types to the default Kubernetes Scheme so Events can be</span><br><span class=\"line\">    // logged for sample-controller types.</span><br><span class=\"line\">    utilruntime.Must(networkscheme.AddToScheme(scheme.Scheme))</span><br><span class=\"line\">    glog.V(4).Info(&quot;Creating event broadcaster&quot;)</span><br><span class=\"line\">    eventBroadcaster := record.NewBroadcaster()</span><br><span class=\"line\">    eventBroadcaster.StartLogging(glog.Infof)</span><br><span class=\"line\">    eventBroadcaster.StartRecordingToSink(&amp;typedcorev1.EventSinkImpl&#123;Interface: kubeclientset.CoreV1().Events(&quot;&quot;)&#125;)</span><br><span class=\"line\">    recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource&#123;Component: controllerAgentName&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    controller := &amp;Controller&#123;</span><br><span class=\"line\">        kubeclientset:    kubeclientset,</span><br><span class=\"line\">        networkclientset: networkclientset,</span><br><span class=\"line\">        networksLister:   networkInformer.Lister(),</span><br><span class=\"line\">        networksSynced:   networkInformer.Informer().HasSynced,</span><br><span class=\"line\">        workqueue:        workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;Networks&quot;),</span><br><span class=\"line\">        recorder:         recorder,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    glog.Info(&quot;Setting up event handlers&quot;)</span><br><span class=\"line\">    // Set up an event handler for when Network resources change</span><br><span class=\"line\">    networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123;</span><br><span class=\"line\">        AddFunc: controller.enqueueNetwork,</span><br><span class=\"line\">        UpdateFunc: func(old, new interface&#123;&#125;) &#123;</span><br><span class=\"line\">            oldNetwork := old.(*samplecrdv1.Network)</span><br><span class=\"line\">            newNetwork := new.(*samplecrdv1.Network)</span><br><span class=\"line\">            if oldNetwork.ResourceVersion == newNetwork.ResourceVersion &#123;</span><br><span class=\"line\">                // Periodic resync will send update events for all known Networks.</span><br><span class=\"line\">                // Two different versions of the same Network will always have different RVs.</span><br><span class=\"line\">                return</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            controller.enqueueNetwork(new)</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        DeleteFunc: controller.enqueueNetworkForDelete,</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    return controller</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>自定义 controller 的详细使用方法可以参考：<a href=\"https://github.com/resouer/k8s-controller-custom-resource\" target=\"_blank\" rel=\"noopener\">k8s-controller-custom-resource</a></p>\n<h4 id=\"四、使用中的一些问题\"><a href=\"#四、使用中的一些问题\" class=\"headerlink\" title=\"四、使用中的一些问题\"></a>四、使用中的一些问题</h4><h5 id=\"1、Informer-二级缓存中的同步问题\"><a href=\"#1、Informer-二级缓存中的同步问题\" class=\"headerlink\" title=\"1、Informer 二级缓存中的同步问题\"></a>1、Informer 二级缓存中的同步问题</h5><p>虽然 Informer 和 Kubernetes 之间没有 resync 机制，但 Informer 内部的这两级缓存 DeltaIFIFO 和 LocalStore 之间会存在 resync 机制，k8s 中 kube-controller-manager 的 StatefulSetController 中使用了两级缓存的 resync 机制（如下图所示），我们在生产环境中发现 sts 创建后过了很久 pod 才会创建，主要是由于 StatefulSetController 的两级缓存之间 30s 会同步一次，由于  StatefulSetController watch 到变化后就会把对应的 sts 放入 DeltaIFIFO 中，且每隔30s会把 LocalStore 中全部的 sts 重新入一遍 DeltaIFIFO，入队时会做一些处理，过滤掉一些不需要重复入队列的 sts，若间隔的 30s 内没有处理完队列中所有的 sts，则待处理队列中始终存在未处理完的 sts，并且在同步过程中产生的 sts 会加的队列的尾部，新加入队尾的 sts 只能等到前面的 sts 处理完成（也就是 resync 完成）才会被处理，所以导致的现象就是 sts 创建后过了很久 pod 才会创建。</p>\n<p>优化的方法就是去掉二级缓存的同步策略（将 setInformer.Informer().AddEventHandlerWithResyncPeriod() 改为 informer.AddEventHandler()）或者调大同步周期，但是在研究 kube-controller-manager 其他 controller 时发现并不是所有的 controller 都有同步策略，社区也有相关的 issue 反馈了这一问题，<a href=\"https://github.com/kubernetes/kubernetes/pull/75622\" target=\"_blank\" rel=\"noopener\">Remove resync period for sset controller</a>，社区也会在以后的版本中去掉两级缓存之间的 resync 策略。</p>\n<p><code>k8s.io/kubernetes/pkg/controller/statefulset/stateful_set.go</code></p>\n<p><img src=\"http://cdn.tianfeiyu.com/informer-2.png\" alt=\"kube-controller-manager sts controller\"></p>\n<h5 id=\"2、使用-Informer-如何监听所有资源对象？\"><a href=\"#2、使用-Informer-如何监听所有资源对象？\" class=\"headerlink\" title=\"2、使用 Informer 如何监听所有资源对象？\"></a>2、使用 Informer 如何监听所有资源对象？</h5><p>一个 Informer 实例只能监听一种 resource，每个 resource 需要创建对应的 Informer 实例。</p>\n<h5 id=\"3、为什么不是使用-workqueue？\"><a href=\"#3、为什么不是使用-workqueue？\" class=\"headerlink\" title=\"3、为什么不是使用 workqueue？\"></a>3、为什么不是使用 workqueue？</h5><p>建议使用 RateLimitingQueue，它相比普通的 workqueue 多了以下的功能: </p>\n<ul>\n<li>限流：可以限制一个 item 被 reenqueued 的次数。</li>\n<li>防止 hot loop：它保证了一个 item 被 reenqueued 后，不会马上被处理。</li>\n</ul>\n<h4 id=\"五、总结\"><a href=\"#五、总结\" class=\"headerlink\" title=\"五、总结\"></a>五、总结</h4><p>本文介绍了 client-go 包中核心组件 Informer 的原理以及使用方法，Informer 主要功能是缓存对象到本地以及根据对应的事件类型触发已注册好的 ResourceEventHandler，其主要用在访问 k8s apiserver 的客户端和 operator 中。</p>\n<p>参考：</p>\n<p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU1OTAzNzc5MQ==&amp;mid=2247484052&amp;idx=1&amp;sn=cec9f4a1ee0d21c5b2c51bd147b8af59&amp;chksm=fc1c2ea4cb6ba7b283eef5ac4a45985437c648361831bc3e6dd5f38053be1968b3389386e415&amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"noopener\">如何用 client-go 拓展 Kubernetes 的 API</a></p>\n<p><a href=\"https://www.kubernetes.org.cn/2693.html\" target=\"_blank\" rel=\"noopener\">https://www.kubernetes.org.cn/2693.html</a></p>\n<p><a href=\"https://studygolang.com/articles/9270\" target=\"_blank\" rel=\"noopener\">Kubernetes 大咖秀徐超《使用 client-go 控制原生及拓展的 Kubernetes API》</a></p>\n<p><a href=\"https://github.com/kubernetes/kubernetes/issues/71165\" target=\"_blank\" rel=\"noopener\">Use prometheus conventions for workqueue metrics</a></p>\n<p><a href=\"https://blog.csdn.net/weixin_42663840/article/details/81482553#%E9%99%90%E9%80%9F%E9%98%9F%E5%88%97\" target=\"_blank\" rel=\"noopener\">深入浅出kubernetes之client-go的workqueue</a></p>\n<p><a href=\"https://gianarb.it/blog/kubernetes-shared-informer\" target=\"_blank\" rel=\"noopener\">https://gianarb.it/blog/kubernetes-shared-informer</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/59660536\" target=\"_blank\" rel=\"noopener\">理解 K8S 的设计精髓之 List-Watch机制和Informer模块</a></p>\n<p><a href=\"https://ranler.org/notes/file/528\" target=\"_blank\" rel=\"noopener\">https://ranler.org/notes/file/528</a></p>\n<p><a href=\"https://yq.aliyun.com/articles/688485\" target=\"_blank\" rel=\"noopener\">Kubernetes Client-go Informer 源码分析</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<h4 id=\"一、kubernetes-集群的几种访问方式\"><a href=\"#一、kubernetes-集群的几种访问方式\" class=\"headerlink\" title=\"一、kubernetes 集群的几种访问方式\"></a>一、kubernetes 集群的几种访问方式</h4><p>在实际开发过程中，若想要获取 kubernetes 中某个资源（比如 pod）的所有对象，可以使用 kubectl、k8s REST API、client-go(ClientSet、Dynamic Client、RESTClient 三种方式) 等多种方式访问 k8s 集群获取资源。在笔者的开发过程中，最初都是直接调用 k8s 的 REST API 来获取的，使用 <code>kubectl get pod -v=9</code> 可以直接看到调用 k8s 的接口，然后在程序中直接访问还是比较方便的。但是随着集群规模的增长或者从国内获取海外 k8s 集群的数据，直接调用 k8s 接口获取所有 pod 还是比较耗时，这个问题有多种解决方法，最初是直接使用 k8s 原生的 watch 接口来获取的，下面是一个伪代码：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">const (</span><br><span class=\"line\">\tADDED    string = &quot;ADDED&quot;</span><br><span class=\"line\">\tMODIFIED string = &quot;MODIFIED&quot;</span><br><span class=\"line\">\tDELETED  string = &quot;DELETED&quot;</span><br><span class=\"line\">\tERROR    string = &quot;ERROR&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">type Event struct &#123;</span><br><span class=\"line\">\tType   string          `json:&quot;type&quot;`</span><br><span class=\"line\">\tObject json.RawMessage `json:&quot;object&quot;`</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tresp, err := http.Get(&quot;http://apiserver:8080/api/v1/watch/pods?watch=yes&quot;)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t// ...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tdecoder := json.NewDecoder(resp.Body)</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tvar event Event</span><br><span class=\"line\">\t\terr = decoder.Decode(&amp;event)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t// ...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tswitch event.Type &#123;</span><br><span class=\"line\">\t\tcase ADDED, MODIFIED:</span><br><span class=\"line\">\t\t\t// ...</span><br><span class=\"line\">\t\tcase DELETED:</span><br><span class=\"line\">\t\t\t// ...</span><br><span class=\"line\">\t\tcase ERROR:</span><br><span class=\"line\">\t\t\t// ...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>调用 watch 接口后会先将所有的对象 list 一次，然后 apiserver 会将变化的数据推送到 client 端，可以看到每次对于 watch 到的事件都需要判断后进行处理，然后将处理后的结果写入到本地的缓存中，原生的 watch 操作还是非常麻烦的。后来了解到官方推出一个客户端工具 client-go ，client-go 中的 Informer 对 watch 操作做了封装，使用起来非常方便，下面会主要介绍一下 client-go 的使用。</p>\n<h4 id=\"二、Informer-的机制\"><a href=\"#二、Informer-的机制\" class=\"headerlink\" title=\"二、Informer 的机制\"></a>二、Informer 的机制</h4><p>cient-go 是从 k8s 代码中抽出来的一个客户端工具，Informer 是 client-go 中的核心工具包，已经被 kubernetes 中众多组件所使用。所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client，本地缓存被称为 Store，索引被称为 Index。使用 informer 的目的是为了减轻 apiserver 数据交互的压力而抽象出来的一个 cache 层, 客户端对 apiserver 数据的 “读取” 和 “监听” 操作都通过本地 informer 进行。Informer 实例的<code>Lister()</code>方法可以直接查找缓存在本地内存中的数据。</p>\n<p>Informer 的主要功能：</p>\n<ul>\n<li>同步数据到本地缓存</li>\n<li>根据对应的事件类型，触发事先注册好的 ResourceEventHandler</li>\n</ul>\n<h5 id=\"1、Informer-中几个组件的作用\"><a href=\"#1、Informer-中几个组件的作用\" class=\"headerlink\" title=\"1、Informer 中几个组件的作用\"></a>1、Informer 中几个组件的作用</h5><p>Informer 中主要有 Reflector、Delta FIFO Queue、Local Store、WorkQueue 几个组件。以下是 Informer 的工作流程图。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/informer-1.png\" alt=\"Informer 组件\"></p>\n<p>根据流程图来解释一下 Informer 中几个组件的作用：</p>\n<ul>\n<li><p>Reflector：称之为反射器，实现对 apiserver 指定类型对象的监控(ListAndWatch)，其中反射实现的就是把监控的结果实例化成具体的对象，最终也是调用 Kubernetes 的 List/Watch API；</p>\n</li>\n<li><p>DeltaIFIFO Queue：一个增量队列，将 Reflector 监控变化的对象形成一个 FIFO 队列，此处的 Delta 就是变化；</p>\n</li>\n<li><p>LocalStore：就是 informer 的 cache，这里面缓存的是 apiserver 中的对象(其中有一部分可能还在DeltaFIFO 中)，此时使用者再查询对象的时候就直接从 cache 中查找，减少了 apiserver 的压力，LocalStore 只会被 Lister 的 List/Get 方法访问。</p>\n</li>\n<li><p>WorkQueue：DeltaIFIFO 收到时间后会先将时间存储在自己的数据结构中，然后直接操作 Store 中存储的数据，更新完 store 后 DeltaIFIFO 会将该事件 pop 到 WorkQueue 中，Controller 收到 WorkQueue  中的事件会根据对应的类型触发对应的回调函数。</p>\n</li>\n</ul>\n<h5 id=\"2、Informer-的工作流程\"><a href=\"#2、Informer-的工作流程\" class=\"headerlink\" title=\"2、Informer 的工作流程\"></a>2、Informer 的工作流程</h5><ul>\n<li>Informer 首先会 list/watch apiserver，Informer 所使用的 Reflector 包负责与 apiserver 建立连接，Reflector 使用 ListAndWatch 的方法，会先从 apiserver 中 list 该资源的所有实例，list 会拿到该对象最新的 resourceVersion，然后使用 watch 方法监听该 resourceVersion 之后的所有变化，若中途出现异常，reflector 则会从断开的 resourceVersion 处重现尝试监听所有变化，一旦该对象的实例有创建、删除、更新动作，Reflector 都会收到”事件通知”，这时，该事件及它对应的 API 对象这个组合，被称为增量（Delta），它会被放进 DeltaFIFO 中。</li>\n<li>Informer 会不断地从这个 DeltaFIFO 中读取增量，每拿出一个对象，Informer 就会判断这个增量的时间类型，然后创建或更新本地的缓存，也就是 store。</li>\n<li>如果事件类型是 Added（添加对象），那么 Informer 会通过 Indexer 的库把这个增量里的 API 对象保存到本地的缓存中，并为它创建索引，若为删除操作，则在本地缓存中删除该对象。</li>\n<li>DeltaFIFO 再 pop 这个事件到 controller 中，controller 会调用事先注册的 ResourceEventHandler 回调函数进行处理。</li>\n<li>在 ResourceEventHandler 回调函数中，其实只是做了一些很简单的过滤，然后将关心变更的 Object 放到 workqueue 里面。</li>\n<li>Controller 从 workqueue 里面取出 Object，启动一个 worker 来执行自己的业务逻辑，业务逻辑通常是计算目前集群的状态和用户希望达到的状态有多大的区别，然后孜孜不倦地让 apiserver 将状态演化到用户希望达到的状态，比如为 deployment 创建新的 pods，或者是扩容/缩容 deployment。</li>\n<li>在worker中就可以使用 lister 来获取 resource，而不用频繁的访问 apiserver，因为 apiserver 中 resource 的变更都会反映到本地的 cache 中。</li>\n</ul>\n<p>Informer 在使用时需要先初始化一个 InformerFactory，目前主要推荐使用的是 SharedInformerFactory，Shared 指的是在多个 Informer 中共享一个本地 cache。</p>\n<p>Informer 中的 ResourceEventHandler  函数有三种：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// ResourceEventHandlerFuncs is an adaptor to let you easily specify as many or</span><br><span class=\"line\">// as few of the notification functions as you want while still implementing</span><br><span class=\"line\">// ResourceEventHandler.</span><br><span class=\"line\">type ResourceEventHandlerFuncs struct &#123;</span><br><span class=\"line\">    AddFunc    func(obj interface&#123;&#125;)</span><br><span class=\"line\">    UpdateFunc func(oldObj, newObj interface&#123;&#125;)</span><br><span class=\"line\">    DeleteFunc func(obj interface&#123;&#125;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这三种函数的处理逻辑是用户自定义的，在初始化 controller 时注册完 ResourceEventHandler 后，一旦该对象的实例有创建、删除、更新三中操作后就会触发对应的 ResourceEventHandler。</p>\n<h4 id=\"三、Informer-使用示例\"><a href=\"#三、Informer-使用示例\" class=\"headerlink\" title=\"三、Informer 使用示例\"></a>三、Informer 使用示例</h4><p>在实际的开发工作中，Informer 主要用在两处：</p>\n<ul>\n<li>在访问 k8s apiserver 的客户端作为一个 client 缓存对象使用；</li>\n<li>在一些自定义 controller 中使用，比如 operator 的开发；</li>\n</ul>\n<h4 id=\"1、下面是一个作为-client-的使用示例：\"><a href=\"#1、下面是一个作为-client-的使用示例：\" class=\"headerlink\" title=\"1、下面是一个作为 client 的使用示例：\"></a>1、下面是一个作为 client 的使用示例：</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;flag&quot;</span><br><span class=\"line\">\t&quot;fmt&quot;</span><br><span class=\"line\">\t&quot;log&quot;</span><br><span class=\"line\">\t&quot;path/filepath&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\tcorev1 &quot;k8s.io/api/core/v1&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apimachinery/pkg/labels&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apimachinery/pkg/util/runtime&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&quot;k8s.io/client-go/informers&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/kubernetes&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/cache&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/clientcmd&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/util/homedir&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tvar kubeconfig *string</span><br><span class=\"line\">\tif home := homedir.HomeDir(); home != &quot;&quot; &#123;</span><br><span class=\"line\">\t\tkubeconfig = flag.String(&quot;kubeconfig&quot;, filepath.Join(home, &quot;.kube&quot;, &quot;config&quot;), &quot;(optional) absolute path to the kubeconfig file&quot;)</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\tkubeconfig = flag.String(&quot;kubeconfig&quot;, &quot;&quot;, &quot;absolute path to the kubeconfig file&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tflag.Parse()</span><br><span class=\"line\"></span><br><span class=\"line\">\tconfig, err := clientcmd.BuildConfigFromFlags(&quot;&quot;, *kubeconfig)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tpanic(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 初始化 client</span><br><span class=\"line\">\tclientset, err := kubernetes.NewForConfig(config)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tlog.Panic(err.Error())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tstopper := make(chan struct&#123;&#125;)</span><br><span class=\"line\">\tdefer close(stopper)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 初始化 informer</span><br><span class=\"line\">\tfactory := informers.NewSharedInformerFactory(clientset, 0)</span><br><span class=\"line\">\tnodeInformer := factory.Core().V1().Nodes()</span><br><span class=\"line\">\tinformer := nodeInformer.Informer()</span><br><span class=\"line\">\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 启动 informer，list &amp; watch</span><br><span class=\"line\">\tgo factory.Start(stopper)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 从 apiserver 同步资源，即 list </span><br><span class=\"line\">\tif !cache.WaitForCacheSync(stopper, informer.HasSynced) &#123;</span><br><span class=\"line\">\t\truntime.HandleError(fmt.Errorf(&quot;Timed out waiting for caches to sync&quot;))</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用自定义 handler</span><br><span class=\"line\">\tinformer.AddEventHandler(cache.ResourceEventHandlerFuncs&#123;</span><br><span class=\"line\">\t\tAddFunc:    onAdd,</span><br><span class=\"line\">\t\tUpdateFunc: func(interface&#123;&#125;, interface&#123;&#125;) &#123; fmt.Println(&quot;update not implemented&quot;) &#125;, // 此处省略 workqueue 的使用</span><br><span class=\"line\">\t\tDeleteFunc: func(interface&#123;&#125;) &#123; fmt.Println(&quot;delete not implemented&quot;) &#125;,</span><br><span class=\"line\">\t&#125;)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 创建 lister</span><br><span class=\"line\">\tnodeLister := nodeInformer.Lister()</span><br><span class=\"line\">\t// 从 lister 中获取所有 items</span><br><span class=\"line\">\tnodeList, err := nodeLister.List(labels.Everything())</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tfmt.Println(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tfmt.Println(&quot;nodelist:&quot;, nodeList)</span><br><span class=\"line\">\t&lt;-stopper</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func onAdd(obj interface&#123;&#125;) &#123;</span><br><span class=\"line\">\tnode := obj.(*corev1.Node)</span><br><span class=\"line\">\tfmt.Println(&quot;add a node:&quot;, node.Name)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Shared指的是多个 lister 共享同一个cache，而且资源的变化会同时通知到cache和 listers。这个解释和上面图所展示的内容的是一致的，cache我们在Indexer的介绍中已经分析过了，lister 指的就是OnAdd、OnUpdate、OnDelete 这些回调函数背后的对象。</p>\n<h4 id=\"2、以下是作为-controller-使用的一个整体工作流程\"><a href=\"#2、以下是作为-controller-使用的一个整体工作流程\" class=\"headerlink\" title=\"2、以下是作为 controller 使用的一个整体工作流程\"></a>2、以下是作为 controller 使用的一个整体工作流程</h4><p>(1) 创建一个控制器</p>\n<ul>\n<li>为控制器创建 workqueue</li>\n<li>创建 informer, 为 informer 添加 callback 函数，创建 lister</li>\n</ul>\n<p>(2) 启动控制器</p>\n<ul>\n<li>启动 informer</li>\n<li>等待本地 cache sync 完成后， 启动 workers</li>\n</ul>\n<p>(3) 当收到变更事件后，执行 callback </p>\n<ul>\n<li>等待事件触发</li>\n<li>从事件中获取变更的 Object</li>\n<li>做一些必要的检查</li>\n<li>生成 object key，一般是 namespace/name 的形式</li>\n<li>将 key 放入 workqueue 中</li>\n</ul>\n<p>(4) worker loop</p>\n<ul>\n<li>等待从 workqueue 中获取到 item，一般为 object key</li>\n<li>用 object key 通过 lister 从本地 cache 中获取到真正的 object 对象</li>\n<li>做一些检查</li>\n<li>执行真正的业务逻辑</li>\n<li>处理下一个 item</li>\n</ul>\n<p>下面是自定义 controller 使用的一个参考：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">var (</span><br><span class=\"line\">    masterURL  string</span><br><span class=\"line\">    kubeconfig string</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func init() &#123;</span><br><span class=\"line\">    flag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;Path to a kubeconfig. Only required if out-of-cluster.&quot;)</span><br><span class=\"line\">    flag.StringVar(&amp;masterURL, &quot;master&quot;, &quot;&quot;, &quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.&quot;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">    flag.Parse()</span><br><span class=\"line\"></span><br><span class=\"line\">    stopCh := signals.SetupSignalHandler()</span><br><span class=\"line\"></span><br><span class=\"line\">    cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        glog.Fatalf(&quot;Error building kubeconfig: %s&quot;, err.Error())</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    kubeClient, err := kubernetes.NewForConfig(cfg)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        glog.Fatalf(&quot;Error building kubernetes clientset: %s&quot;, err.Error())</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client</span><br><span class=\"line\">    // informer watch apiserver,每隔 30 秒 resync 一次(list)</span><br><span class=\"line\">    kubeInformerFactory := informers.NewSharedInformerFactory(kubeClient, time.Second*30)</span><br><span class=\"line\"></span><br><span class=\"line\">    controller := controller.NewController(kubeClient, kubeInformerFactory.Core().V1().Nodes())</span><br><span class=\"line\"></span><br><span class=\"line\">    //  启动 informer</span><br><span class=\"line\">    go kubeInformerFactory.Start(stopCh)</span><br><span class=\"line\"></span><br><span class=\"line\">\t // start controller </span><br><span class=\"line\">    if err = controller.Run(2, stopCh); err != nil &#123;</span><br><span class=\"line\">        glog.Fatalf(&quot;Error running controller: %s&quot;, err.Error())</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// NewController returns a new network controller</span><br><span class=\"line\">func NewController(</span><br><span class=\"line\">    kubeclientset kubernetes.Interface,</span><br><span class=\"line\">    networkclientset clientset.Interface,</span><br><span class=\"line\">    networkInformer informers.NetworkInformer) *Controller &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">    // Create event broadcaster</span><br><span class=\"line\">    // Add sample-controller types to the default Kubernetes Scheme so Events can be</span><br><span class=\"line\">    // logged for sample-controller types.</span><br><span class=\"line\">    utilruntime.Must(networkscheme.AddToScheme(scheme.Scheme))</span><br><span class=\"line\">    glog.V(4).Info(&quot;Creating event broadcaster&quot;)</span><br><span class=\"line\">    eventBroadcaster := record.NewBroadcaster()</span><br><span class=\"line\">    eventBroadcaster.StartLogging(glog.Infof)</span><br><span class=\"line\">    eventBroadcaster.StartRecordingToSink(&amp;typedcorev1.EventSinkImpl&#123;Interface: kubeclientset.CoreV1().Events(&quot;&quot;)&#125;)</span><br><span class=\"line\">    recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource&#123;Component: controllerAgentName&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    controller := &amp;Controller&#123;</span><br><span class=\"line\">        kubeclientset:    kubeclientset,</span><br><span class=\"line\">        networkclientset: networkclientset,</span><br><span class=\"line\">        networksLister:   networkInformer.Lister(),</span><br><span class=\"line\">        networksSynced:   networkInformer.Informer().HasSynced,</span><br><span class=\"line\">        workqueue:        workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;Networks&quot;),</span><br><span class=\"line\">        recorder:         recorder,</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    glog.Info(&quot;Setting up event handlers&quot;)</span><br><span class=\"line\">    // Set up an event handler for when Network resources change</span><br><span class=\"line\">    networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123;</span><br><span class=\"line\">        AddFunc: controller.enqueueNetwork,</span><br><span class=\"line\">        UpdateFunc: func(old, new interface&#123;&#125;) &#123;</span><br><span class=\"line\">            oldNetwork := old.(*samplecrdv1.Network)</span><br><span class=\"line\">            newNetwork := new.(*samplecrdv1.Network)</span><br><span class=\"line\">            if oldNetwork.ResourceVersion == newNetwork.ResourceVersion &#123;</span><br><span class=\"line\">                // Periodic resync will send update events for all known Networks.</span><br><span class=\"line\">                // Two different versions of the same Network will always have different RVs.</span><br><span class=\"line\">                return</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            controller.enqueueNetwork(new)</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        DeleteFunc: controller.enqueueNetworkForDelete,</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">    return controller</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>自定义 controller 的详细使用方法可以参考：<a href=\"https://github.com/resouer/k8s-controller-custom-resource\" target=\"_blank\" rel=\"noopener\">k8s-controller-custom-resource</a></p>\n<h4 id=\"四、使用中的一些问题\"><a href=\"#四、使用中的一些问题\" class=\"headerlink\" title=\"四、使用中的一些问题\"></a>四、使用中的一些问题</h4><h5 id=\"1、Informer-二级缓存中的同步问题\"><a href=\"#1、Informer-二级缓存中的同步问题\" class=\"headerlink\" title=\"1、Informer 二级缓存中的同步问题\"></a>1、Informer 二级缓存中的同步问题</h5><p>虽然 Informer 和 Kubernetes 之间没有 resync 机制，但 Informer 内部的这两级缓存 DeltaIFIFO 和 LocalStore 之间会存在 resync 机制，k8s 中 kube-controller-manager 的 StatefulSetController 中使用了两级缓存的 resync 机制（如下图所示），我们在生产环境中发现 sts 创建后过了很久 pod 才会创建，主要是由于 StatefulSetController 的两级缓存之间 30s 会同步一次，由于  StatefulSetController watch 到变化后就会把对应的 sts 放入 DeltaIFIFO 中，且每隔30s会把 LocalStore 中全部的 sts 重新入一遍 DeltaIFIFO，入队时会做一些处理，过滤掉一些不需要重复入队列的 sts，若间隔的 30s 内没有处理完队列中所有的 sts，则待处理队列中始终存在未处理完的 sts，并且在同步过程中产生的 sts 会加的队列的尾部，新加入队尾的 sts 只能等到前面的 sts 处理完成（也就是 resync 完成）才会被处理，所以导致的现象就是 sts 创建后过了很久 pod 才会创建。</p>\n<p>优化的方法就是去掉二级缓存的同步策略（将 setInformer.Informer().AddEventHandlerWithResyncPeriod() 改为 informer.AddEventHandler()）或者调大同步周期，但是在研究 kube-controller-manager 其他 controller 时发现并不是所有的 controller 都有同步策略，社区也有相关的 issue 反馈了这一问题，<a href=\"https://github.com/kubernetes/kubernetes/pull/75622\" target=\"_blank\" rel=\"noopener\">Remove resync period for sset controller</a>，社区也会在以后的版本中去掉两级缓存之间的 resync 策略。</p>\n<p><code>k8s.io/kubernetes/pkg/controller/statefulset/stateful_set.go</code></p>\n<p><img src=\"http://cdn.tianfeiyu.com/informer-2.png\" alt=\"kube-controller-manager sts controller\"></p>\n<h5 id=\"2、使用-Informer-如何监听所有资源对象？\"><a href=\"#2、使用-Informer-如何监听所有资源对象？\" class=\"headerlink\" title=\"2、使用 Informer 如何监听所有资源对象？\"></a>2、使用 Informer 如何监听所有资源对象？</h5><p>一个 Informer 实例只能监听一种 resource，每个 resource 需要创建对应的 Informer 实例。</p>\n<h5 id=\"3、为什么不是使用-workqueue？\"><a href=\"#3、为什么不是使用-workqueue？\" class=\"headerlink\" title=\"3、为什么不是使用 workqueue？\"></a>3、为什么不是使用 workqueue？</h5><p>建议使用 RateLimitingQueue，它相比普通的 workqueue 多了以下的功能: </p>\n<ul>\n<li>限流：可以限制一个 item 被 reenqueued 的次数。</li>\n<li>防止 hot loop：它保证了一个 item 被 reenqueued 后，不会马上被处理。</li>\n</ul>\n<h4 id=\"五、总结\"><a href=\"#五、总结\" class=\"headerlink\" title=\"五、总结\"></a>五、总结</h4><p>本文介绍了 client-go 包中核心组件 Informer 的原理以及使用方法，Informer 主要功能是缓存对象到本地以及根据对应的事件类型触发已注册好的 ResourceEventHandler，其主要用在访问 k8s apiserver 的客户端和 operator 中。</p>\n<p>参考：</p>\n<p><a href=\"https://mp.weixin.qq.com/s?__biz=MzU1OTAzNzc5MQ==&amp;mid=2247484052&amp;idx=1&amp;sn=cec9f4a1ee0d21c5b2c51bd147b8af59&amp;chksm=fc1c2ea4cb6ba7b283eef5ac4a45985437c648361831bc3e6dd5f38053be1968b3389386e415&amp;scene=21#wechat_redirect\" target=\"_blank\" rel=\"noopener\">如何用 client-go 拓展 Kubernetes 的 API</a></p>\n<p><a href=\"https://www.kubernetes.org.cn/2693.html\" target=\"_blank\" rel=\"noopener\">https://www.kubernetes.org.cn/2693.html</a></p>\n<p><a href=\"https://studygolang.com/articles/9270\" target=\"_blank\" rel=\"noopener\">Kubernetes 大咖秀徐超《使用 client-go 控制原生及拓展的 Kubernetes API》</a></p>\n<p><a href=\"https://github.com/kubernetes/kubernetes/issues/71165\" target=\"_blank\" rel=\"noopener\">Use prometheus conventions for workqueue metrics</a></p>\n<p><a href=\"https://blog.csdn.net/weixin_42663840/article/details/81482553#%E9%99%90%E9%80%9F%E9%98%9F%E5%88%97\" target=\"_blank\" rel=\"noopener\">深入浅出kubernetes之client-go的workqueue</a></p>\n<p><a href=\"https://gianarb.it/blog/kubernetes-shared-informer\" target=\"_blank\" rel=\"noopener\">https://gianarb.it/blog/kubernetes-shared-informer</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/59660536\" target=\"_blank\" rel=\"noopener\">理解 K8S 的设计精髓之 List-Watch机制和Informer模块</a></p>\n<p><a href=\"https://ranler.org/notes/file/528\" target=\"_blank\" rel=\"noopener\">https://ranler.org/notes/file/528</a></p>\n<p><a href=\"https://yq.aliyun.com/articles/688485\" target=\"_blank\" rel=\"noopener\">Kubernetes Client-go Informer 源码分析</a></p>\n"},{"title":"etcd 启用 https","date":"2017-03-15T13:32:00.000Z","type":"etcd","_content":"* 1， 生成 TLS 秘钥对\n* 2，拷贝密钥对到所有节点\n* 3，配置 etcd 使用证书\n* 4，测试 etcd 是否正常\n* 5，配置 kube-apiserver 使用 CA 连接 etcd\n* 6，测试 kube-apiserver\n* 7，未解决的问题\n\nSSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。\n\n因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。\n\n[Generate self-signed certificates 官方参考文档](https://coreos.com/os/docs/latest/generate-self-signed-certificates.html)\n\n官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 [easy-rsa](https://github.com/OpenVPN/easy-rsa)。以下步骤遵循官方文档：\n\n## 1， 生成 TLS 秘钥对\n\n生成步骤：\n\n* 1，下载 cfssl\n* 2，初始化证书颁发机构\n* 3，配置 CA 选项\n* 4，生成服务器端证书\n* 5，生成对等证书\n* 6，生成客户端证书\n\n想深入了解 HTTPS 的看这里：\n\n* [聊聊HTTPS和SSL/TLS协议](http://www.techug.com/post/https-ssl-tls.html)\n* [数字证书CA及扫盲](http://blog.jobbole.com/104919/)\n* [互联网加密及OpenSSL介绍和简单使用](https://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8)\n* [SSL双向认证和单向认证的区别](http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html)\n\n##### 1，下载 cfssl\n\n    mkdir ~/bin\n    curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\n    curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\n    chmod +x ~/bin/{cfssl,cfssljson}\n    export PATH=$PATH:~/bin\n\n##### 2，初始化证书颁发机构\n\n```\nmkdir ~/cfssl\ncd ~/cfssl\ncfssl print-defaults config > ca-config.json\ncfssl print-defaults csr > ca-csr.json\n```\n\n证书类型介绍：\n\n* client certificate  用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。\n* server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。\n* peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。\n\n##### 3，配置 CA 选项\n\n```\n$ cat << EOF > ca-config.json\n\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"43800h\"\n        },\n        \"profiles\": {\n            \"server\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\"\n                ]\n            },\n            \"client\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"client auth\"\n                ]\n            },\n            \"peer\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ]\n            }\n        }\n    }\n}\n\n$ cat << EOF > ca-csr.json\n\n{\n    \"CN\": \"My own CA\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"US\",\n            \"L\": \"CA\",\n            \"O\": \"My Company Name\",\n            \"ST\": \"San Francisco\",\n            \"OU\": \"Org Unit 1\",\n            \"OU\": \"Org Unit 2\"\n        }\n    ]\n}\n\n生成 CA 证书：\n\n$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n\n将会生成以下几个文件：\n\nca-key.pem\nca.csr\nca.pem\n\n```\n> 请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。\n\n##### 4，生成服务器端证书\n```\n$ echo '{\"CN\":\"coreos1\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=\"10.93.81.17,127.0.0.1,server\" - | cfssljson -bare server\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\nserver-key.pem\nserver.csr\nserver.pem\n```\n\n##### 5，生成对等证书\n```\n$ echo '{\"CN\":\"member1\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=\"10.93.81.17,127.0.0.1,server,member1\" - | cfssljson -bare member1\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\n\nmember1-key.pem\nmember1.csr\nmember1.pem\n\n如果有多个 etcd 成员，重复此步为每个成员生成对等证书。\n\n```\n\n##### 6，生成客户端证书\n\n```\n$ echo '{\"CN\":\"client\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\n\nclient-key.pem\nclient.csr\nclient.pem\n\n```\n至此，所有证书都已生成完毕。\n\n\n## 2，拷贝密钥对到所有节点\n* 1，拷贝密钥对到所有节点\n* 2，更新系统证书库\n\n##### 1，拷贝密钥对到所有节点\n\n\n```\n$ mkdir -pv /etc/ssl/etcd/\n$ cp ~/cfssl/* /etc/ssl/etcd/\n$ chown -R etcd:etcd /etc/ssl/etcd\n$ chmod 600 /etc/ssl/etcd/*-key.pem\n$ cp ~/cfssl/ca.pem /etc/ssl/certs/\n```\n\n##### 2，更新系统证书库\n\n```\n$ yum install ca-certificates -y\n     \n$ update-ca-trust\n        \n```\n\n## 3，配置 etcd 使用证书\n\n```\n$ etcdctl version\netcdctl version: 3.1.3\nAPI version: 3.1\n\n$ cat  /etc/etcd/etcd.conf\n\nETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\n#监听URL，用于与其他节点通讯\nETCD_LISTEN_PEER_URLS=\"https://10.93.81.17:2380\"\n\n#告知客户端的URL, 也就是服务的URL\nETCD_LISTEN_CLIENT_URLS=\"https://10.93.81.17:2379,https://10.93.81.17:4001\"\n\n#表示监听其他节点同步信号的地址\nETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://10.93.81.17:2380\"\n\n#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求\nETCD_ADVERTISE_CLIENT_URLS=\"https://10.93.81.17:2379\"\n\n#启动参数配置\nETCD_NAME=\"node1\"\nETCD_INITIAL_CLUSTER=\"node1=https://10.93.81.17:2380\"\nETCD_INITIAL_CLUSTER_STATE=\"new\"\n\n#[security]\n\nETCD_CERT_FILE=\"/etc/ssl/etcd/server.pem\"\nETCD_KEY_FILE=\"/etc/ssl/etcd/server-key.pem\"\nETCD_TRUSTED_CA_FILE=\"/etc/ssl/etcd/ca.pem\"\nETCD_CLIENT_CERT_AUTH=\"true\"\nETCD_PEER_CERT_FILE=\"/etc/ssl/etcd/member1.pem\"\nETCD_PEER_KEY_FILE=\"/etc/ssl/etcd/member1-key.pem\"\nETCD_PEER_TRUSTED_CA_FILE=\"/etc/ssl/etcd/ca.pem\"\nETCD_PEER_CLIENT_CERT_AUTH=\"true\"\n#[logging]\nETCD_DEBUG=\"true\"\nETCD_LOG_PACKAGE_LEVELS=\"etcdserver=WARNING,security=DEBUG\"\n```\n\n\n## 4，测试 etcd 是否正常\n\n```\n$ systemctl restart  etcd\n\n如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。\n\n$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health\n{\"health\": \"true\"}\n\n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list\n     \n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar  \"hello world\"\n     \n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar\n```\n\n## 5，配置 kube-apiserver 使用 CA 连接 etcd\n\n```\n$ cp /etc/ssl/etcd/*  /var/run/kubernetes/\n    \n$ chown  -R kube.kube /var/run/kubernetes/\n\n在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：\n\n--cert-dir='/var/run/kubernetes/' --etcd-cafile='/var/run/kubernetes/ca.pem' --etcd-certfile='/var/run/kubernetes/client.pem' --etcd-keyfile='/var/run/kubernetes/client-key.pem'\n\n\n```\n\n## 6，测试 kube-apiserver \n\n```\n$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy\n\n$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy\n\n$ kubectl get node\n\n$ kubectl get cs\nNAME                 STATUS      MESSAGE                                                                   ERROR\nscheduler            Healthy     ok\ncontroller-manager   Healthy     ok\netcd-0               Unhealthy   Get https://10.93.81.17:2379/health: remote error: tls: bad certificate\n\n$ ./version.sh\netcdctl version: 3.1.3\nAPI version: 3.1\nKubernetes v1.6.0-beta.1\n\n```\n\n## 7，未解决的问题\n\n##### 1，使用  `kubectl get cs ` 查看会出现如上面所示的报错： \n```\netcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate\n```\n此问题有人提交 pr 但尚未被 merge，[etcd component status check should include credentials](https://github.com/kubernetes/kubernetes/pull/39716)\n\n##### 2，使用以下命令查看到的 2380 端口是未加密的\n```\n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list  \n\n2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated\n145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379\n```\n\n参考文档：\n\n* [kubernetes + etcd ssl 支持](https://www.addops.cn/post/tls-for-kubernetes-etcd.html)\n* [Security model](https://coreos.com/etcd/docs/latest/op-guide/security.html)\n* [Enabling HTTPS in an existing etcd cluster](https://coreos.com/etcd/docs/latest/etcd-live-http-to-https-migration.html)\n","source":"_posts/etcd-enable-https.md","raw":"---\ntitle: etcd 启用 https\ndate: 2017-03-15 21:32:00\ntype: \"etcd\"\n\n---\n* 1， 生成 TLS 秘钥对\n* 2，拷贝密钥对到所有节点\n* 3，配置 etcd 使用证书\n* 4，测试 etcd 是否正常\n* 5，配置 kube-apiserver 使用 CA 连接 etcd\n* 6，测试 kube-apiserver\n* 7，未解决的问题\n\nSSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。\n\n因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。\n\n[Generate self-signed certificates 官方参考文档](https://coreos.com/os/docs/latest/generate-self-signed-certificates.html)\n\n官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 [easy-rsa](https://github.com/OpenVPN/easy-rsa)。以下步骤遵循官方文档：\n\n## 1， 生成 TLS 秘钥对\n\n生成步骤：\n\n* 1，下载 cfssl\n* 2，初始化证书颁发机构\n* 3，配置 CA 选项\n* 4，生成服务器端证书\n* 5，生成对等证书\n* 6，生成客户端证书\n\n想深入了解 HTTPS 的看这里：\n\n* [聊聊HTTPS和SSL/TLS协议](http://www.techug.com/post/https-ssl-tls.html)\n* [数字证书CA及扫盲](http://blog.jobbole.com/104919/)\n* [互联网加密及OpenSSL介绍和简单使用](https://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8)\n* [SSL双向认证和单向认证的区别](http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html)\n\n##### 1，下载 cfssl\n\n    mkdir ~/bin\n    curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\n    curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\n    chmod +x ~/bin/{cfssl,cfssljson}\n    export PATH=$PATH:~/bin\n\n##### 2，初始化证书颁发机构\n\n```\nmkdir ~/cfssl\ncd ~/cfssl\ncfssl print-defaults config > ca-config.json\ncfssl print-defaults csr > ca-csr.json\n```\n\n证书类型介绍：\n\n* client certificate  用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。\n* server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。\n* peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。\n\n##### 3，配置 CA 选项\n\n```\n$ cat << EOF > ca-config.json\n\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"43800h\"\n        },\n        \"profiles\": {\n            \"server\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\"\n                ]\n            },\n            \"client\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"client auth\"\n                ]\n            },\n            \"peer\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ]\n            }\n        }\n    }\n}\n\n$ cat << EOF > ca-csr.json\n\n{\n    \"CN\": \"My own CA\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"US\",\n            \"L\": \"CA\",\n            \"O\": \"My Company Name\",\n            \"ST\": \"San Francisco\",\n            \"OU\": \"Org Unit 1\",\n            \"OU\": \"Org Unit 2\"\n        }\n    ]\n}\n\n生成 CA 证书：\n\n$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n\n将会生成以下几个文件：\n\nca-key.pem\nca.csr\nca.pem\n\n```\n> 请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。\n\n##### 4，生成服务器端证书\n```\n$ echo '{\"CN\":\"coreos1\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=\"10.93.81.17,127.0.0.1,server\" - | cfssljson -bare server\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\nserver-key.pem\nserver.csr\nserver.pem\n```\n\n##### 5，生成对等证书\n```\n$ echo '{\"CN\":\"member1\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=\"10.93.81.17,127.0.0.1,server,member1\" - | cfssljson -bare member1\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\n\nmember1-key.pem\nmember1.csr\nmember1.pem\n\n如果有多个 etcd 成员，重复此步为每个成员生成对等证书。\n\n```\n\n##### 6，生成客户端证书\n\n```\n$ echo '{\"CN\":\"client\",\"hosts\":[\"10.93.81.17\",\"127.0.0.1\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client\n\nhosts 字段需要自定义。\n\n然后将得到以下几个文件：\n\nclient-key.pem\nclient.csr\nclient.pem\n\n```\n至此，所有证书都已生成完毕。\n\n\n## 2，拷贝密钥对到所有节点\n* 1，拷贝密钥对到所有节点\n* 2，更新系统证书库\n\n##### 1，拷贝密钥对到所有节点\n\n\n```\n$ mkdir -pv /etc/ssl/etcd/\n$ cp ~/cfssl/* /etc/ssl/etcd/\n$ chown -R etcd:etcd /etc/ssl/etcd\n$ chmod 600 /etc/ssl/etcd/*-key.pem\n$ cp ~/cfssl/ca.pem /etc/ssl/certs/\n```\n\n##### 2，更新系统证书库\n\n```\n$ yum install ca-certificates -y\n     \n$ update-ca-trust\n        \n```\n\n## 3，配置 etcd 使用证书\n\n```\n$ etcdctl version\netcdctl version: 3.1.3\nAPI version: 3.1\n\n$ cat  /etc/etcd/etcd.conf\n\nETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\n#监听URL，用于与其他节点通讯\nETCD_LISTEN_PEER_URLS=\"https://10.93.81.17:2380\"\n\n#告知客户端的URL, 也就是服务的URL\nETCD_LISTEN_CLIENT_URLS=\"https://10.93.81.17:2379,https://10.93.81.17:4001\"\n\n#表示监听其他节点同步信号的地址\nETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://10.93.81.17:2380\"\n\n#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求\nETCD_ADVERTISE_CLIENT_URLS=\"https://10.93.81.17:2379\"\n\n#启动参数配置\nETCD_NAME=\"node1\"\nETCD_INITIAL_CLUSTER=\"node1=https://10.93.81.17:2380\"\nETCD_INITIAL_CLUSTER_STATE=\"new\"\n\n#[security]\n\nETCD_CERT_FILE=\"/etc/ssl/etcd/server.pem\"\nETCD_KEY_FILE=\"/etc/ssl/etcd/server-key.pem\"\nETCD_TRUSTED_CA_FILE=\"/etc/ssl/etcd/ca.pem\"\nETCD_CLIENT_CERT_AUTH=\"true\"\nETCD_PEER_CERT_FILE=\"/etc/ssl/etcd/member1.pem\"\nETCD_PEER_KEY_FILE=\"/etc/ssl/etcd/member1-key.pem\"\nETCD_PEER_TRUSTED_CA_FILE=\"/etc/ssl/etcd/ca.pem\"\nETCD_PEER_CLIENT_CERT_AUTH=\"true\"\n#[logging]\nETCD_DEBUG=\"true\"\nETCD_LOG_PACKAGE_LEVELS=\"etcdserver=WARNING,security=DEBUG\"\n```\n\n\n## 4，测试 etcd 是否正常\n\n```\n$ systemctl restart  etcd\n\n如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。\n\n$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health\n{\"health\": \"true\"}\n\n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list\n     \n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar  \"hello world\"\n     \n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar\n```\n\n## 5，配置 kube-apiserver 使用 CA 连接 etcd\n\n```\n$ cp /etc/ssl/etcd/*  /var/run/kubernetes/\n    \n$ chown  -R kube.kube /var/run/kubernetes/\n\n在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：\n\n--cert-dir='/var/run/kubernetes/' --etcd-cafile='/var/run/kubernetes/ca.pem' --etcd-certfile='/var/run/kubernetes/client.pem' --etcd-keyfile='/var/run/kubernetes/client-key.pem'\n\n\n```\n\n## 6，测试 kube-apiserver \n\n```\n$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy\n\n$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy\n\n$ kubectl get node\n\n$ kubectl get cs\nNAME                 STATUS      MESSAGE                                                                   ERROR\nscheduler            Healthy     ok\ncontroller-manager   Healthy     ok\netcd-0               Unhealthy   Get https://10.93.81.17:2379/health: remote error: tls: bad certificate\n\n$ ./version.sh\netcdctl version: 3.1.3\nAPI version: 3.1\nKubernetes v1.6.0-beta.1\n\n```\n\n## 7，未解决的问题\n\n##### 1，使用  `kubectl get cs ` 查看会出现如上面所示的报错： \n```\netcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate\n```\n此问题有人提交 pr 但尚未被 merge，[etcd component status check should include credentials](https://github.com/kubernetes/kubernetes/pull/39716)\n\n##### 2，使用以下命令查看到的 2380 端口是未加密的\n```\n$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list  \n\n2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated\n145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379\n```\n\n参考文档：\n\n* [kubernetes + etcd ssl 支持](https://www.addops.cn/post/tls-for-kubernetes-etcd.html)\n* [Security model](https://coreos.com/etcd/docs/latest/op-guide/security.html)\n* [Enabling HTTPS in an existing etcd cluster](https://coreos.com/etcd/docs/latest/etcd-live-http-to-https-migration.html)\n","slug":"etcd-enable-https","published":1,"updated":"2019-06-01T14:26:16.307Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro5960004apwn93nc4n4r","content":"<ul>\n<li>1， 生成 TLS 秘钥对</li>\n<li>2，拷贝密钥对到所有节点</li>\n<li>3，配置 etcd 使用证书</li>\n<li>4，测试 etcd 是否正常</li>\n<li>5，配置 kube-apiserver 使用 CA 连接 etcd</li>\n<li>6，测试 kube-apiserver</li>\n<li>7，未解决的问题</li>\n</ul>\n<p>SSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。</p>\n<p>因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。</p>\n<p><a href=\"https://coreos.com/os/docs/latest/generate-self-signed-certificates.html\" target=\"_blank\" rel=\"noopener\">Generate self-signed certificates 官方参考文档</a></p>\n<p>官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 <a href=\"https://github.com/OpenVPN/easy-rsa\" target=\"_blank\" rel=\"noopener\">easy-rsa</a>。以下步骤遵循官方文档：</p>\n<h2 id=\"1，-生成-TLS-秘钥对\"><a href=\"#1，-生成-TLS-秘钥对\" class=\"headerlink\" title=\"1， 生成 TLS 秘钥对\"></a>1， 生成 TLS 秘钥对</h2><p>生成步骤：</p>\n<ul>\n<li>1，下载 cfssl</li>\n<li>2，初始化证书颁发机构</li>\n<li>3，配置 CA 选项</li>\n<li>4，生成服务器端证书</li>\n<li>5，生成对等证书</li>\n<li>6，生成客户端证书</li>\n</ul>\n<p>想深入了解 HTTPS 的看这里：</p>\n<ul>\n<li><a href=\"http://www.techug.com/post/https-ssl-tls.html\" target=\"_blank\" rel=\"noopener\">聊聊HTTPS和SSL/TLS协议</a></li>\n<li><a href=\"http://blog.jobbole.com/104919/\" target=\"_blank\" rel=\"noopener\">数字证书CA及扫盲</a></li>\n<li><a href=\"https://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8\" target=\"_blank\" rel=\"noopener\">互联网加密及OpenSSL介绍和简单使用</a></li>\n<li><a href=\"http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html\" target=\"_blank\" rel=\"noopener\">SSL双向认证和单向认证的区别</a></li>\n</ul>\n<h5 id=\"1，下载-cfssl\"><a href=\"#1，下载-cfssl\" class=\"headerlink\" title=\"1，下载 cfssl\"></a>1，下载 cfssl</h5><pre><code>mkdir ~/bin\ncurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x ~/bin/{cfssl,cfssljson}\nexport PATH=$PATH:~/bin\n</code></pre><h5 id=\"2，初始化证书颁发机构\"><a href=\"#2，初始化证书颁发机构\" class=\"headerlink\" title=\"2，初始化证书颁发机构\"></a>2，初始化证书颁发机构</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/cfssl</span><br><span class=\"line\">cd ~/cfssl</span><br><span class=\"line\">cfssl print-defaults config &gt; ca-config.json</span><br><span class=\"line\">cfssl print-defaults csr &gt; ca-csr.json</span><br></pre></td></tr></table></figure>\n<p>证书类型介绍：</p>\n<ul>\n<li>client certificate  用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。</li>\n<li>server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。</li>\n<li>peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。</li>\n</ul>\n<h5 id=\"3，配置-CA-选项\"><a href=\"#3，配置-CA-选项\" class=\"headerlink\" title=\"3，配置 CA 选项\"></a>3，配置 CA 选项</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-config.json</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;signing&quot;: &#123;</span><br><span class=\"line\">        &quot;default&quot;: &#123;</span><br><span class=\"line\">            &quot;expiry&quot;: &quot;43800h&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;profiles&quot;: &#123;</span><br><span class=\"line\">            &quot;server&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;server auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;client&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;client auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;peer&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;server auth&quot;,</span><br><span class=\"line\">                    &quot;client auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-csr.json</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;CN&quot;: &quot;My own CA&quot;,</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 2048</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;C&quot;: &quot;US&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;CA&quot;,</span><br><span class=\"line\">            &quot;O&quot;: &quot;My Company Name&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;San Francisco&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;Org Unit 1&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;Org Unit 2&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">生成 CA 证书：</span><br><span class=\"line\"></span><br><span class=\"line\">$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</span><br><span class=\"line\"></span><br><span class=\"line\">将会生成以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">ca-key.pem</span><br><span class=\"line\">ca.csr</span><br><span class=\"line\">ca.pem</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。</p>\n</blockquote>\n<h5 id=\"4，生成服务器端证书\"><a href=\"#4，生成服务器端证书\" class=\"headerlink\" title=\"4，生成服务器端证书\"></a>4，生成服务器端证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;coreos1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&quot;10.93.81.17,127.0.0.1,server&quot; - | cfssljson -bare server</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\">server-key.pem</span><br><span class=\"line\">server.csr</span><br><span class=\"line\">server.pem</span><br></pre></td></tr></table></figure>\n<h5 id=\"5，生成对等证书\"><a href=\"#5，生成对等证书\" class=\"headerlink\" title=\"5，生成对等证书\"></a>5，生成对等证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;member1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=&quot;10.93.81.17,127.0.0.1,server,member1&quot; - | cfssljson -bare member1</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">member1-key.pem</span><br><span class=\"line\">member1.csr</span><br><span class=\"line\">member1.pem</span><br><span class=\"line\"></span><br><span class=\"line\">如果有多个 etcd 成员，重复此步为每个成员生成对等证书。</span><br></pre></td></tr></table></figure>\n<h5 id=\"6，生成客户端证书\"><a href=\"#6，生成客户端证书\" class=\"headerlink\" title=\"6，生成客户端证书\"></a>6，生成客户端证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;client&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">client-key.pem</span><br><span class=\"line\">client.csr</span><br><span class=\"line\">client.pem</span><br></pre></td></tr></table></figure>\n<p>至此，所有证书都已生成完毕。</p>\n<h2 id=\"2，拷贝密钥对到所有节点\"><a href=\"#2，拷贝密钥对到所有节点\" class=\"headerlink\" title=\"2，拷贝密钥对到所有节点\"></a>2，拷贝密钥对到所有节点</h2><ul>\n<li>1，拷贝密钥对到所有节点</li>\n<li>2，更新系统证书库</li>\n</ul>\n<h5 id=\"1，拷贝密钥对到所有节点\"><a href=\"#1，拷贝密钥对到所有节点\" class=\"headerlink\" title=\"1，拷贝密钥对到所有节点\"></a>1，拷贝密钥对到所有节点</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -pv /etc/ssl/etcd/</span><br><span class=\"line\">$ cp ~/cfssl/* /etc/ssl/etcd/</span><br><span class=\"line\">$ chown -R etcd:etcd /etc/ssl/etcd</span><br><span class=\"line\">$ chmod 600 /etc/ssl/etcd/*-key.pem</span><br><span class=\"line\">$ cp ~/cfssl/ca.pem /etc/ssl/certs/</span><br></pre></td></tr></table></figure>\n<h5 id=\"2，更新系统证书库\"><a href=\"#2，更新系统证书库\" class=\"headerlink\" title=\"2，更新系统证书库\"></a>2，更新系统证书库</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yum install ca-certificates -y</span><br><span class=\"line\">     </span><br><span class=\"line\">$ update-ca-trust</span><br></pre></td></tr></table></figure>\n<h2 id=\"3，配置-etcd-使用证书\"><a href=\"#3，配置-etcd-使用证书\" class=\"headerlink\" title=\"3，配置 etcd 使用证书\"></a>3，配置 etcd 使用证书</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcdctl version</span><br><span class=\"line\">etcdctl version: 3.1.3</span><br><span class=\"line\">API version: 3.1</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat  /etc/etcd/etcd.conf</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;</span><br><span class=\"line\">#监听URL，用于与其他节点通讯</span><br><span class=\"line\">ETCD_LISTEN_PEER_URLS=&quot;https://10.93.81.17:2380&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#告知客户端的URL, 也就是服务的URL</span><br><span class=\"line\">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.93.81.17:2379,https://10.93.81.17:4001&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#表示监听其他节点同步信号的地址</span><br><span class=\"line\">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.93.81.17:2380&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求</span><br><span class=\"line\">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.93.81.17:2379&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#启动参数配置</span><br><span class=\"line\">ETCD_NAME=&quot;node1&quot;</span><br><span class=\"line\">ETCD_INITIAL_CLUSTER=&quot;node1=https://10.93.81.17:2380&quot;</span><br><span class=\"line\">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#[security]</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_CERT_FILE=&quot;/etc/ssl/etcd/server.pem&quot;</span><br><span class=\"line\">ETCD_KEY_FILE=&quot;/etc/ssl/etcd/server-key.pem&quot;</span><br><span class=\"line\">ETCD_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;</span><br><span class=\"line\">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class=\"line\">ETCD_PEER_CERT_FILE=&quot;/etc/ssl/etcd/member1.pem&quot;</span><br><span class=\"line\">ETCD_PEER_KEY_FILE=&quot;/etc/ssl/etcd/member1-key.pem&quot;</span><br><span class=\"line\">ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;</span><br><span class=\"line\">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class=\"line\">#[logging]</span><br><span class=\"line\">ETCD_DEBUG=&quot;true&quot;</span><br><span class=\"line\">ETCD_LOG_PACKAGE_LEVELS=&quot;etcdserver=WARNING,security=DEBUG&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4，测试-etcd-是否正常\"><a href=\"#4，测试-etcd-是否正常\" class=\"headerlink\" title=\"4，测试 etcd 是否正常\"></a>4，测试 etcd 是否正常</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart  etcd</span><br><span class=\"line\"></span><br><span class=\"line\">如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health</span><br><span class=\"line\">&#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list</span><br><span class=\"line\">     </span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar  &quot;hello world&quot;</span><br><span class=\"line\">     </span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar</span><br></pre></td></tr></table></figure>\n<h2 id=\"5，配置-kube-apiserver-使用-CA-连接-etcd\"><a href=\"#5，配置-kube-apiserver-使用-CA-连接-etcd\" class=\"headerlink\" title=\"5，配置 kube-apiserver 使用 CA 连接 etcd\"></a>5，配置 kube-apiserver 使用 CA 连接 etcd</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp /etc/ssl/etcd/*  /var/run/kubernetes/</span><br><span class=\"line\">    </span><br><span class=\"line\">$ chown  -R kube.kube /var/run/kubernetes/</span><br><span class=\"line\"></span><br><span class=\"line\">在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：</span><br><span class=\"line\"></span><br><span class=\"line\">--cert-dir=&apos;/var/run/kubernetes/&apos; --etcd-cafile=&apos;/var/run/kubernetes/ca.pem&apos; --etcd-certfile=&apos;/var/run/kubernetes/client.pem&apos; --etcd-keyfile=&apos;/var/run/kubernetes/client-key.pem&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"6，测试-kube-apiserver\"><a href=\"#6，测试-kube-apiserver\" class=\"headerlink\" title=\"6，测试 kube-apiserver\"></a>6，测试 kube-apiserver</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get node</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get cs</span><br><span class=\"line\">NAME                 STATUS      MESSAGE                                                                   ERROR</span><br><span class=\"line\">scheduler            Healthy     ok</span><br><span class=\"line\">controller-manager   Healthy     ok</span><br><span class=\"line\">etcd-0               Unhealthy   Get https://10.93.81.17:2379/health: remote error: tls: bad certificate</span><br><span class=\"line\"></span><br><span class=\"line\">$ ./version.sh</span><br><span class=\"line\">etcdctl version: 3.1.3</span><br><span class=\"line\">API version: 3.1</span><br><span class=\"line\">Kubernetes v1.6.0-beta.1</span><br></pre></td></tr></table></figure>\n<h2 id=\"7，未解决的问题\"><a href=\"#7，未解决的问题\" class=\"headerlink\" title=\"7，未解决的问题\"></a>7，未解决的问题</h2><h5 id=\"1，使用-kubectl-get-cs-查看会出现如上面所示的报错：\"><a href=\"#1，使用-kubectl-get-cs-查看会出现如上面所示的报错：\" class=\"headerlink\" title=\"1，使用  kubectl get cs 查看会出现如上面所示的报错：\"></a>1，使用  <code>kubectl get cs</code> 查看会出现如上面所示的报错：</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">etcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate</span><br></pre></td></tr></table></figure>\n<p>此问题有人提交 pr 但尚未被 merge，<a href=\"https://github.com/kubernetes/kubernetes/pull/39716\" target=\"_blank\" rel=\"noopener\">etcd component status check should include credentials</a></p>\n<h5 id=\"2，使用以下命令查看到的-2380-端口是未加密的\"><a href=\"#2，使用以下命令查看到的-2380-端口是未加密的\" class=\"headerlink\" title=\"2，使用以下命令查看到的 2380 端口是未加密的\"></a>2，使用以下命令查看到的 2380 端口是未加密的</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list  </span><br><span class=\"line\"></span><br><span class=\"line\">2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated</span><br><span class=\"line\">145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379</span><br></pre></td></tr></table></figure>\n<p>参考文档：</p>\n<ul>\n<li><a href=\"https://www.addops.cn/post/tls-for-kubernetes-etcd.html\" target=\"_blank\" rel=\"noopener\">kubernetes + etcd ssl 支持</a></li>\n<li><a href=\"https://coreos.com/etcd/docs/latest/op-guide/security.html\" target=\"_blank\" rel=\"noopener\">Security model</a></li>\n<li><a href=\"https://coreos.com/etcd/docs/latest/etcd-live-http-to-https-migration.html\" target=\"_blank\" rel=\"noopener\">Enabling HTTPS in an existing etcd cluster</a></li>\n</ul>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<ul>\n<li>1， 生成 TLS 秘钥对</li>\n<li>2，拷贝密钥对到所有节点</li>\n<li>3，配置 etcd 使用证书</li>\n<li>4，测试 etcd 是否正常</li>\n<li>5，配置 kube-apiserver 使用 CA 连接 etcd</li>\n<li>6，测试 kube-apiserver</li>\n<li>7，未解决的问题</li>\n</ul>\n<p>SSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。</p>\n<p>因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。</p>\n<p><a href=\"https://coreos.com/os/docs/latest/generate-self-signed-certificates.html\" target=\"_blank\" rel=\"noopener\">Generate self-signed certificates 官方参考文档</a></p>\n<p>官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 <a href=\"https://github.com/OpenVPN/easy-rsa\" target=\"_blank\" rel=\"noopener\">easy-rsa</a>。以下步骤遵循官方文档：</p>\n<h2 id=\"1，-生成-TLS-秘钥对\"><a href=\"#1，-生成-TLS-秘钥对\" class=\"headerlink\" title=\"1， 生成 TLS 秘钥对\"></a>1， 生成 TLS 秘钥对</h2><p>生成步骤：</p>\n<ul>\n<li>1，下载 cfssl</li>\n<li>2，初始化证书颁发机构</li>\n<li>3，配置 CA 选项</li>\n<li>4，生成服务器端证书</li>\n<li>5，生成对等证书</li>\n<li>6，生成客户端证书</li>\n</ul>\n<p>想深入了解 HTTPS 的看这里：</p>\n<ul>\n<li><a href=\"http://www.techug.com/post/https-ssl-tls.html\" target=\"_blank\" rel=\"noopener\">聊聊HTTPS和SSL/TLS协议</a></li>\n<li><a href=\"http://blog.jobbole.com/104919/\" target=\"_blank\" rel=\"noopener\">数字证书CA及扫盲</a></li>\n<li><a href=\"https://mritd.me/2016/07/02/%E4%BA%92%E8%81%94%E7%BD%91%E5%8A%A0%E5%AF%86%E5%8F%8AOpenSSL%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8\" target=\"_blank\" rel=\"noopener\">互联网加密及OpenSSL介绍和简单使用</a></li>\n<li><a href=\"http://www.cnblogs.com/Michael-Kong/archive/2012/08/16/SSL%E8%AF%81%E4%B9%A6%E5%8E%9F%E7%90%86.html\" target=\"_blank\" rel=\"noopener\">SSL双向认证和单向认证的区别</a></li>\n</ul>\n<h5 id=\"1，下载-cfssl\"><a href=\"#1，下载-cfssl\" class=\"headerlink\" title=\"1，下载 cfssl\"></a>1，下载 cfssl</h5><pre><code>mkdir ~/bin\ncurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x ~/bin/{cfssl,cfssljson}\nexport PATH=$PATH:~/bin\n</code></pre><h5 id=\"2，初始化证书颁发机构\"><a href=\"#2，初始化证书颁发机构\" class=\"headerlink\" title=\"2，初始化证书颁发机构\"></a>2，初始化证书颁发机构</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir ~/cfssl</span><br><span class=\"line\">cd ~/cfssl</span><br><span class=\"line\">cfssl print-defaults config &gt; ca-config.json</span><br><span class=\"line\">cfssl print-defaults csr &gt; ca-csr.json</span><br></pre></td></tr></table></figure>\n<p>证书类型介绍：</p>\n<ul>\n<li>client certificate  用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。</li>\n<li>server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。</li>\n<li>peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。</li>\n</ul>\n<h5 id=\"3，配置-CA-选项\"><a href=\"#3，配置-CA-选项\" class=\"headerlink\" title=\"3，配置 CA 选项\"></a>3，配置 CA 选项</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-config.json</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;signing&quot;: &#123;</span><br><span class=\"line\">        &quot;default&quot;: &#123;</span><br><span class=\"line\">            &quot;expiry&quot;: &quot;43800h&quot;</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        &quot;profiles&quot;: &#123;</span><br><span class=\"line\">            &quot;server&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;server auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;client&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;client auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">            &quot;peer&quot;: &#123;</span><br><span class=\"line\">                &quot;expiry&quot;: &quot;43800h&quot;,</span><br><span class=\"line\">                &quot;usages&quot;: [</span><br><span class=\"line\">                    &quot;signing&quot;,</span><br><span class=\"line\">                    &quot;key encipherment&quot;,</span><br><span class=\"line\">                    &quot;server auth&quot;,</span><br><span class=\"line\">                    &quot;client auth&quot;</span><br><span class=\"line\">                ]</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-csr.json</span><br><span class=\"line\"></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;CN&quot;: &quot;My own CA&quot;,</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 2048</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;C&quot;: &quot;US&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;CA&quot;,</span><br><span class=\"line\">            &quot;O&quot;: &quot;My Company Name&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;San Francisco&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;Org Unit 1&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;Org Unit 2&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">生成 CA 证书：</span><br><span class=\"line\"></span><br><span class=\"line\">$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</span><br><span class=\"line\"></span><br><span class=\"line\">将会生成以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">ca-key.pem</span><br><span class=\"line\">ca.csr</span><br><span class=\"line\">ca.pem</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。</p>\n</blockquote>\n<h5 id=\"4，生成服务器端证书\"><a href=\"#4，生成服务器端证书\" class=\"headerlink\" title=\"4，生成服务器端证书\"></a>4，生成服务器端证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;coreos1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&quot;10.93.81.17,127.0.0.1,server&quot; - | cfssljson -bare server</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\">server-key.pem</span><br><span class=\"line\">server.csr</span><br><span class=\"line\">server.pem</span><br></pre></td></tr></table></figure>\n<h5 id=\"5，生成对等证书\"><a href=\"#5，生成对等证书\" class=\"headerlink\" title=\"5，生成对等证书\"></a>5，生成对等证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;member1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=&quot;10.93.81.17,127.0.0.1,server,member1&quot; - | cfssljson -bare member1</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">member1-key.pem</span><br><span class=\"line\">member1.csr</span><br><span class=\"line\">member1.pem</span><br><span class=\"line\"></span><br><span class=\"line\">如果有多个 etcd 成员，重复此步为每个成员生成对等证书。</span><br></pre></td></tr></table></figure>\n<h5 id=\"6，生成客户端证书\"><a href=\"#6，生成客户端证书\" class=\"headerlink\" title=\"6，生成客户端证书\"></a>6，生成客户端证书</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo &apos;&#123;&quot;CN&quot;:&quot;client&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client</span><br><span class=\"line\"></span><br><span class=\"line\">hosts 字段需要自定义。</span><br><span class=\"line\"></span><br><span class=\"line\">然后将得到以下几个文件：</span><br><span class=\"line\"></span><br><span class=\"line\">client-key.pem</span><br><span class=\"line\">client.csr</span><br><span class=\"line\">client.pem</span><br></pre></td></tr></table></figure>\n<p>至此，所有证书都已生成完毕。</p>\n<h2 id=\"2，拷贝密钥对到所有节点\"><a href=\"#2，拷贝密钥对到所有节点\" class=\"headerlink\" title=\"2，拷贝密钥对到所有节点\"></a>2，拷贝密钥对到所有节点</h2><ul>\n<li>1，拷贝密钥对到所有节点</li>\n<li>2，更新系统证书库</li>\n</ul>\n<h5 id=\"1，拷贝密钥对到所有节点\"><a href=\"#1，拷贝密钥对到所有节点\" class=\"headerlink\" title=\"1，拷贝密钥对到所有节点\"></a>1，拷贝密钥对到所有节点</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -pv /etc/ssl/etcd/</span><br><span class=\"line\">$ cp ~/cfssl/* /etc/ssl/etcd/</span><br><span class=\"line\">$ chown -R etcd:etcd /etc/ssl/etcd</span><br><span class=\"line\">$ chmod 600 /etc/ssl/etcd/*-key.pem</span><br><span class=\"line\">$ cp ~/cfssl/ca.pem /etc/ssl/certs/</span><br></pre></td></tr></table></figure>\n<h5 id=\"2，更新系统证书库\"><a href=\"#2，更新系统证书库\" class=\"headerlink\" title=\"2，更新系统证书库\"></a>2，更新系统证书库</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ yum install ca-certificates -y</span><br><span class=\"line\">     </span><br><span class=\"line\">$ update-ca-trust</span><br></pre></td></tr></table></figure>\n<h2 id=\"3，配置-etcd-使用证书\"><a href=\"#3，配置-etcd-使用证书\" class=\"headerlink\" title=\"3，配置 etcd 使用证书\"></a>3，配置 etcd 使用证书</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcdctl version</span><br><span class=\"line\">etcdctl version: 3.1.3</span><br><span class=\"line\">API version: 3.1</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat  /etc/etcd/etcd.conf</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;</span><br><span class=\"line\">#监听URL，用于与其他节点通讯</span><br><span class=\"line\">ETCD_LISTEN_PEER_URLS=&quot;https://10.93.81.17:2380&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#告知客户端的URL, 也就是服务的URL</span><br><span class=\"line\">ETCD_LISTEN_CLIENT_URLS=&quot;https://10.93.81.17:2379,https://10.93.81.17:4001&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#表示监听其他节点同步信号的地址</span><br><span class=\"line\">ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.93.81.17:2380&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求</span><br><span class=\"line\">ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.93.81.17:2379&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#启动参数配置</span><br><span class=\"line\">ETCD_NAME=&quot;node1&quot;</span><br><span class=\"line\">ETCD_INITIAL_CLUSTER=&quot;node1=https://10.93.81.17:2380&quot;</span><br><span class=\"line\">ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">#[security]</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_CERT_FILE=&quot;/etc/ssl/etcd/server.pem&quot;</span><br><span class=\"line\">ETCD_KEY_FILE=&quot;/etc/ssl/etcd/server-key.pem&quot;</span><br><span class=\"line\">ETCD_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;</span><br><span class=\"line\">ETCD_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class=\"line\">ETCD_PEER_CERT_FILE=&quot;/etc/ssl/etcd/member1.pem&quot;</span><br><span class=\"line\">ETCD_PEER_KEY_FILE=&quot;/etc/ssl/etcd/member1-key.pem&quot;</span><br><span class=\"line\">ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;</span><br><span class=\"line\">ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;</span><br><span class=\"line\">#[logging]</span><br><span class=\"line\">ETCD_DEBUG=&quot;true&quot;</span><br><span class=\"line\">ETCD_LOG_PACKAGE_LEVELS=&quot;etcdserver=WARNING,security=DEBUG&quot;</span><br></pre></td></tr></table></figure>\n<h2 id=\"4，测试-etcd-是否正常\"><a href=\"#4，测试-etcd-是否正常\" class=\"headerlink\" title=\"4，测试 etcd 是否正常\"></a>4，测试 etcd 是否正常</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart  etcd</span><br><span class=\"line\"></span><br><span class=\"line\">如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。</span><br><span class=\"line\"></span><br><span class=\"line\">$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health</span><br><span class=\"line\">&#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list</span><br><span class=\"line\">     </span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar  &quot;hello world&quot;</span><br><span class=\"line\">     </span><br><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar</span><br></pre></td></tr></table></figure>\n<h2 id=\"5，配置-kube-apiserver-使用-CA-连接-etcd\"><a href=\"#5，配置-kube-apiserver-使用-CA-连接-etcd\" class=\"headerlink\" title=\"5，配置 kube-apiserver 使用 CA 连接 etcd\"></a>5，配置 kube-apiserver 使用 CA 连接 etcd</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp /etc/ssl/etcd/*  /var/run/kubernetes/</span><br><span class=\"line\">    </span><br><span class=\"line\">$ chown  -R kube.kube /var/run/kubernetes/</span><br><span class=\"line\"></span><br><span class=\"line\">在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：</span><br><span class=\"line\"></span><br><span class=\"line\">--cert-dir=&apos;/var/run/kubernetes/&apos; --etcd-cafile=&apos;/var/run/kubernetes/ca.pem&apos; --etcd-certfile=&apos;/var/run/kubernetes/client.pem&apos; --etcd-keyfile=&apos;/var/run/kubernetes/client-key.pem&apos;</span><br></pre></td></tr></table></figure>\n<h2 id=\"6，测试-kube-apiserver\"><a href=\"#6，测试-kube-apiserver\" class=\"headerlink\" title=\"6，测试 kube-apiserver\"></a>6，测试 kube-apiserver</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get node</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get cs</span><br><span class=\"line\">NAME                 STATUS      MESSAGE                                                                   ERROR</span><br><span class=\"line\">scheduler            Healthy     ok</span><br><span class=\"line\">controller-manager   Healthy     ok</span><br><span class=\"line\">etcd-0               Unhealthy   Get https://10.93.81.17:2379/health: remote error: tls: bad certificate</span><br><span class=\"line\"></span><br><span class=\"line\">$ ./version.sh</span><br><span class=\"line\">etcdctl version: 3.1.3</span><br><span class=\"line\">API version: 3.1</span><br><span class=\"line\">Kubernetes v1.6.0-beta.1</span><br></pre></td></tr></table></figure>\n<h2 id=\"7，未解决的问题\"><a href=\"#7，未解决的问题\" class=\"headerlink\" title=\"7，未解决的问题\"></a>7，未解决的问题</h2><h5 id=\"1，使用-kubectl-get-cs-查看会出现如上面所示的报错：\"><a href=\"#1，使用-kubectl-get-cs-查看会出现如上面所示的报错：\" class=\"headerlink\" title=\"1，使用  kubectl get cs 查看会出现如上面所示的报错：\"></a>1，使用  <code>kubectl get cs</code> 查看会出现如上面所示的报错：</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">etcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate</span><br></pre></td></tr></table></figure>\n<p>此问题有人提交 pr 但尚未被 merge，<a href=\"https://github.com/kubernetes/kubernetes/pull/39716\" target=\"_blank\" rel=\"noopener\">etcd component status check should include credentials</a></p>\n<h5 id=\"2，使用以下命令查看到的-2380-端口是未加密的\"><a href=\"#2，使用以下命令查看到的-2380-端口是未加密的\" class=\"headerlink\" title=\"2，使用以下命令查看到的 2380 端口是未加密的\"></a>2，使用以下命令查看到的 2380 端口是未加密的</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list  </span><br><span class=\"line\"></span><br><span class=\"line\">2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated</span><br><span class=\"line\">145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379</span><br></pre></td></tr></table></figure>\n<p>参考文档：</p>\n<ul>\n<li><a href=\"https://www.addops.cn/post/tls-for-kubernetes-etcd.html\" target=\"_blank\" rel=\"noopener\">kubernetes + etcd ssl 支持</a></li>\n<li><a href=\"https://coreos.com/etcd/docs/latest/op-guide/security.html\" target=\"_blank\" rel=\"noopener\">Security model</a></li>\n<li><a href=\"https://coreos.com/etcd/docs/latest/etcd-live-http-to-https-migration.html\" target=\"_blank\" rel=\"noopener\">Enabling HTTPS in an existing etcd cluster</a></li>\n</ul>\n"},{"title":"etcd 性能测试与调优","date":"2019-10-08T06:50:30.000Z","type":"etcd","_content":"\netcd 是一个分布式一致性键值存储。其主要功能有服务注册与发现、消息发布与订阅、负载均衡、分布式通知与协调、分布式锁、分布式队列、集群监控与leader 选举等。\n\n\n\n### etcd 性能优化\n\n> 官方文档原文：https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md\n>\n> 译文参考：https://skyao.gitbooks.io/learning-etcd3/content/documentation/op-guide/performance.html\n\n#### 理解 etcd 的性能\n\n决定 etcd 性能的关键因素，包括：\n\n- 延迟(latency)：延迟是完成操作的时间。\n\n- 吞吐量(throughput)：吞吐量是在某个时间期间之内完成操作的总数量。 当 etcd 接收并发客户端请求时，通常平均延迟随着总体吞吐量增加而增加。\n\n在通常的云环境，比如 Google Compute Engine (GCE) 标准的 n-4 或者 AWS 上相当的机器类型，一个三成员 etcd 集群在轻负载下可以在低于1毫秒内完成一个请求，并在重负载下可以每秒完成超过 30000 个请求。\n\netcd 使用 Raft 一致性算法来在成员之间复制请求并达成一致。一致性性能，特别是提交延迟，受限于两个物理约束：**网络IO延迟和磁盘IO延迟**。完成一个etcd请求的最小时间是成员之间的网络往返时延(Round Trip Time / RTT)，加需要提交数据到持久化存储的 fdatasync 时间。在一个数据中心内的 RTT 可能有数百毫秒。在美国典型的 RTT 是大概 50ms, 而在大陆之间可以慢到400ms。旋转硬盘(注：指传统机械硬盘)的典型 fdatasync 延迟是大概 10ms。对于 SSD 硬盘, 延迟通常低于 1ms。为了提高吞吐量, etcd 将多个请求打包在一起并提交给 Raft。这个批量策略让 etcd 在重负载试获得高吞吐量。也有其他子系统影响到 etcd 的整体性能。每个序列化的 etcd 请求必须通过 etcd 的 boltdb 支持的(boltdb-backed) MVCC 存储引擎,它通常需要10微秒来完成。etcd 定期递增快照它最近实施的请求，将他们和之前在磁盘上的快照合并。这个过程可能导致延迟尖峰(latency spike)。虽然在SSD上这通常不是问题，在HDD上它可能加倍可观察到的延迟。而且，进行中的压缩可以影响 etcd 的性能。幸运的是，压缩通常无足轻重，因为压缩是错开的，因此它不和常规请求竞争资源。RPC 系统，gRPC，为 etcd 提供定义良好，可扩展的 API，但是它也引入了额外的延迟，尤其是本地读取。\n\nEtcd 的默认配置在本地网络环境（localhost）下通常能够运行的很好，因为延迟很低。然而，当跨数据中心部署 Etcd 或网络延时很高时，etcd 的心跳间隔或选举超时时间等参数需要根据实际情况进行调整。\n\n网络并不是导致延时的唯一来源。不论是 Follower 还是 Leader，其请求和响应都受磁盘 I/O 延时的影响。每个 timeout 都代表从请求发起到成功返回响应的总时间。\n\n#### 时间参数\n\nEtcd 底层的分布式一致性协议依赖两个时间参数来保证节点之间能够在部分节点掉钱的情况下依然能够正确处理主节点的选举。第一个参数就是所谓的心跳间隔，即主节点通知从节点它还是领导者的频率。实践数据表明，该参数应该设置成节点之间 RTT 的时间。Etcd 的心跳间隔默认是 100 毫秒。第二个参数是选举超时时间，即从节点等待多久没收到主节点的心跳就尝试去竞选领导者。Etcd 的选举超时时间默认是 1000 毫秒。\n\n调整这些参数值是有条件的，此消波长。心跳间隔值推荐设置为临近节点间 RTT 的最大值，通常是 0.5~1.5 倍 RTT 值。如果心跳间隔设得太短，那么 Etcd 就会发送没必要的心跳信息，从而增加 CPU 和网络资源的消耗；如果设得太长，就会导致选举等待时间的超时。如果选举等待时间设置的过长，就会导致节点异常检测时间过长。评估 RTT 值的最简单的方法是使用 ping 的操作。\n\n选举超时时间应该基于心跳间隔和节点之间的平均 RTT 值。选举超时必须至少是 RTT 10 倍的时间以便对网络波动。例如，如果 RTT 的值是 10 毫秒，那么选举超时时间必须至少是 100 毫秒。选举超时时间的上线是 50000 毫秒（50 秒），这个时间只能只用于全球范围内分布式部署的 Etcd 集群。美国大陆的一个 RTT 的合理时间大约是 130 毫秒，美国和日本的 RTT 大约是 350~400 毫秒。如果算上网络波动和重试的时间，那么 5 秒是一次全球 RTT 的安全上线。因为选举超时时间应该是心跳包广播时间的 10 倍，所以 50 秒的选举超时时间是全局分布式部署 Etcd 的合理上线值。\n\n心跳间隔和选举超时时间的值对同一个 Etcd 集群的所有节点都生效，如果各个节点都不同的话，就会导致集群发生不可预知的不稳定性。Etcd 启动时通过传入启动参数或环境变量覆盖默认值，单位是毫秒。示例代码具体如下：\n\n\n\n```\n$ etcd --heartbeat-interval=100 --election-timeout=500\n\n# 环境变量值\n$ ETCD_HEARTBEAT_INTERVAL=100 ETCD_ELECTION_TIMEOUT=500 etcd\n```\n\n#### 快照\n\nEtcd 总是向日志文件中追加 key，这样一来，日志文件会随着 key 的改动而线性增长。当 Etcd 集群使用较少时，保存完整的日志历史记录是没问题的，但如果 Etcd 集群规模比较大时，那么集群就会携带很大的日志文件。为了避免携带庞大的日志文件，Etcd 需要做周期性的快照。快照提供了一种通过保存系统的当前状态并移除旧日志文件的方式来压缩日志文件。\n\n##### 快照调优\n\n为 v2 后端存储创建快照的代价是很高的，所以只用当参数累积到一定的数量时，Etcd 才会创建快照文件。默认情况下，修改数量达到 10000 时才会建立快照。如果 Etcd 的内存使用和磁盘使用过高，那么应该尝试调低快照触发的阈值，具体请参考如下命令。\n\n启动参数：\n\n```\n$ etcd --snapshot-count=5000\n```\n\n环境变量：\n\n```\n$ ETCD_SNAPSHOT_COUNT=5000 etcd\n```\n\n#### 磁盘\n\netcd 的存储目录分为 snapshot 和 wal，他们写入的方式是不同的，snapshot 是内存直接 dump file。而 wal 是顺序追加写，对于这两种方式系统调优的方式是不同的，snapshot 可以通过增加 io 平滑写来提高磁盘 io 能力，而 wal 可以通过降低 pagecache 的方式提前写入时序。因此对于不同的场景，可以考虑将 snap 与 wal 进行分盘，放在两块 SSD 盘上，提高整体的 IO 效率，这种方式可以提升etcd 20%左右的性能。\n\netcd 集群对磁盘 I/O 的延时非常敏感，因为 Etcd 必须持久化它的日志，当其他 I/O 密集型的进程也在占用磁盘 I/O 的带宽时，就会导致 fsync 时延非常高。这将导致 Etcd 丢失心跳包、请求超时或暂时性的 Leader 丢失。这时可以适当为 Etcd 服务赋予更高的磁盘 I/O 权限，让 Etcd 更稳定的运行。在 Linux 系统中，磁盘 I/O 权限可以通过 ionice 命令进行调整。\n\nnux 默认 IO 调度器使用 CFQ 调度算法，支持用 ionice 命令为程序指定 IO 调度策略和优先级，IO 调度策略分为三种：\n\n- Idle ：其他进程没有磁盘 IO 时，才进行磁盘 IO\n- Best Effort：缺省调度策略，可以设置0-7的优先级，数值越小优先级越高，同优先级的进程采用 round-robin算法调度；\n- Real Time ：立即访问磁盘，无视其它进程 IO\n- None 即Best Effort，进程未指定策略和优先级时显示为none，会使用依据cpu nice设置计算出优先级\n\n\n\nLinux 中 etcd  的磁盘优先级可以使用 `ionice` 配置：\n\n```\n$ ionice -c2 -n0 -p `pgrep etcd`\n```\n\n#### 网络\n\netcd 中比较复杂的是网络的调优，因此大量的网络请求会在 peer 节点之间转发，而且整体网络吞吐也很大，但是还是再次强调不建议大家调整系统参数，大家可以通过修改 etcd 的 `--heartbeat-interval` 与 `--election-timeout` 启动参数来适当提高高吞吐网络下 etcd 的集群鲁棒性，通常同步吞吐在100MB左右的集群可以考虑将 `--heartbeat-interval` 设置为 300ms-500ms，`--election-timeout` 可以设置在 5000ms 左右。此外官方还有基于 TC 的网络优先传输方案，也是一个比较适用的调优手段。\n\n\n\n如果 etcd 的 Leader 服务大量并发客户端，这就会导致 follower 的请求的处理被延迟因为网络延迟。follower 的send buffer中能看到错误的列表，如下所示：\n\n```\ndropped MsgProp to 247ae21ff9436b2d since streamMsg's sending buffer is full\n\ndropped MsgAppResp to 247ae21ff9436b2d since streamMsg's sending buffer is full\n```\n\n这些错误可以通过提高 Leader 的网络优先级来提高 follower 的请求的响应。可以通过流量控制机制来提高:\n\n```\n// 针对 2379、2380 端口放行\n$ tc qdisc add dev eth0 root handle 1: prio bands 3\n$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip sport 2380 0xffff flowid 1:1\n$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip dport 2380 0xffff flowid 1:1\n$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip sport 2379 0xffff flowid 1:1\n$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip dport 2379 0xffff flowid 1:1\n\n// 查看现有的队列\n$ tc -s qdisc ls dev enp0s8\nqdisc prio 1: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1\n Sent 258578 bytes 923 pkt (dropped 0, overlimits 0 requeues 0)\n backlog 0b 0p requeues 0\n\n\n// 删除队列\n$ tc qdisc del dev enp0s8 root\n```\n\n#### 数据规模\n\netcd 的硬盘存储上限（默认是 2GB）,当 etcd 数据量超过默认 quota 值后便不再接受写请求，可以通过设置 `--quota-backend-bytes` 参数来增加存储大小,`quota-backend-bytes` 默认值为 0，即使用默认 quota 为 2GB，上限值为 8 GB，具体说明可参考官方文档：[dev-guide/limit.md](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/limit.md)。\n\n\n\n```\nThe default storage size limit is 2GB, configurable with `--quota-backend-bytes` flag. 8GB is a suggested maximum size for normal environments and etcd warns at startup if the configured value exceeds it.\n```\n\n\n\n以下摘自 [当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？](https://mp.weixin.qq.com/s/skjNwU6Rdsn2qWN2KHU9zg)\n\n> 阿里进行了深入研究了 etcd 内部的实现原理，并发现了影响 etcd 扩展性的一个关键问题在底层 bbolt db 的 page 页面分配算法上：随着 etcd 中存储的数据量的增长，bbolt db 中线性查找“连续长度为 n 的 page 存储页面”的性能显著下降。\n>\n> 为了解决该问题，我们设计了基于 segregrated hashmap 的空闲页面管理算法，hashmap 以连续 page 大小为 key, 连续页面起始 page id 为  value。通过查这个 segregrated hashmap 实现 O(1) 的空闲 page 查找，极大地提高了性能。在释放块时，新算法尝试和地址相邻的 page 合并，并更新 segregrated hashmap。更详细的算法分析可以见已发表在[CNCF 博客的博文](https://www.cncf.io/blog/2019/05/09/performance-optimization-of-etcd-in-web-scale-data-scenario/)。\n>\n> 通过这个算法改进，我们可以将 etcd 的存储空间从推荐的 2GB 扩展到 100GB，极大地提高了 etcd 存储数据的规模，并且读写无显著延迟增长。\n>\n> pull request ： [https://github.com/etcd-io/bbolt/pull/141 ](https://github.com/etcd-io/bbolt/pull/141)\n\n目前社区已发布的 v3.4 系列版本并没有说明支持数据规模可达 100 G。\n\n\n\n### etcd 性能测试\n\n> 测试环境：本机 mac 使用 virtualbox 安装 vm，所有 etcd 实例都是运行在在 vm 中的 docker 上\n\n参考官方文档：https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md\n\n\n\n安装 etcd 压测工具 [benchmark](https://github.com/etcd-io/etcd/tree/master/tools/benchmark)：\n\n```\n$ go get go.etcd.io/etcd/tools/benchmark\n# GOPATH should be set\n$ ls $GOPATH/bin\nbenchmark\n```\n\n本文仅对 etcd v3.3.10 以及 v3.4.1 进行压测。\n\n#### 部署 etcd 集群\n\n以下为脚本示例：\n\n```\n#!/bin/bash\n\ndocker ps -a | grep etcd | grep -v k8s\ndocker rm -f etcd\n\nETCD_VERSION=3.3.10\nTOKEN=my-etcd-token\nCLUSTER_STATE=new\nNAME_1=etcd-node-0\nNAME_2=etcd-node-1\nNAME_3=etcd-node-2\nHOST_1=192.168.74.36\nHOST_2=192.168.74.36\nHOST_3=192.168.74.36\nCLUSTER=${NAME_1}=http://${HOST_1}:23801,${NAME_2}=http://${HOST_2}:23802,${NAME_3}=http://${HOST_3}:23803\n\n# 对于节点1\nTHIS_NAME=${NAME_1}\nTHIS_IP=${HOST_1}\nsudo docker run -d --net=host --name ${THIS_NAME} k8s.gcr.io/etcd:${ETCD_VERSION} \\\n    /usr/local/bin/etcd \\\n    --data-dir=data.etcd --name ${THIS_NAME} \\\n    --initial-advertise-peer-urls http://${THIS_IP}:23801 --listen-peer-urls http://${THIS_IP}:23801 \\\n    --advertise-client-urls http://${THIS_IP}:23791 --listen-client-urls http://${THIS_IP}:23791 \\\n    --initial-cluster ${CLUSTER} \\\n    --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# 对于节点2\nTHIS_NAME=${NAME_2}\nTHIS_IP=${HOST_2}\nsudo docker run -d --net=host --name ${THIS_NAME} k8s.gcr.io/etcd:${ETCD_VERSION} \\\n    /usr/local/bin/etcd \\\n    --data-dir=data.etcd --name ${THIS_NAME} \\\n    --initial-advertise-peer-urls http://${THIS_IP}:23802 --listen-peer-urls http://${THIS_IP}:23802 \\\n    --advertise-client-urls http://${THIS_IP}:23792 --listen-client-urls http://${THIS_IP}:23792 \\\n    --initial-cluster ${CLUSTER} \\\n    --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# 对于节点3\nTHIS_NAME=${NAME_3}\nTHIS_IP=${HOST_3}\nsudo docker run -d --net=host --name ${THIS_NAME} k8s.gcr.io/etcd:${ETCD_VERSION} \\\n    /usr/local/bin/etcd \\\n    --data-dir=data.etcd --name ${THIS_NAME} \\\n    --initial-advertise-peer-urls http://${THIS_IP}:23803 --listen-peer-urls http://${THIS_IP}:23803 \\\n    --advertise-client-urls http://${THIS_IP}:23793 --listen-client-urls http://${THIS_IP}:23793 \\\n    --initial-cluster ${CLUSTER} \\\n    --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n```\n\n#### 压测\n\n本文主要对不同场景下 etcd 的读写操作进行测试，尽管环境有限，但在不同场景下 etcd 的表现还是有区别的。对于写入测试，按照官方文档的测试方法指定不同数量的客户端和连接数以及 key 的大小，对于读取操作，分别测试了线性化读取以及串行化读取，由于 etcd 是强一致性的，其默认读取测试就是线性化读取。\n\n\n\n##### etcd v3.3.10\n\n**写入测试**\n\n```\n// 查看 leader\n$ etcdctl member list\n\n// leader\n$ benchmark --endpoints=\"http://192.168.74.36:23791\" --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791\" --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256\n\n\n// 所有 members\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\" --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256\n```\n\n\n\n| key 数量 | Key 大小 | Value的大小 | 连接数量 | 客户端数量 | 目标 etcd 服务器 | 平均写入 QPS | 每请求平均延迟 | Average server RSS | 调整磁盘IO优先级 | 调整网络带宽和优先级 |\n| -------- | -------- | ----------- | -------- | ---------- | ---------------- | ------------ | -------------- | ------------------ | ---------------- | -------------------- |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | 75           | 50.0ms         | -                  | 否               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | 77           | 46.5ms         | -                  | 是               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | -            | -              | -                  | 是               | 是                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | 1144         | 1697.5ms       | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | 1185         | 1541.8ms       | -                  | 是               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | -            | -              | -                  | 是               | 是                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | 73           | 49.6ms         | -                  | 否               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | 80           | 48.5ms         | -                  | 是               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | -            | -              | -                  | 是               | 是                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | 1132         | 1649.1ms       | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | 1198         | 1536.8ms         | -                  | 是               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | -            | -              | -                  | 是               | 是                   |\n\n\n\n**读取测试**\n\n```\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=1 --clients=1  range foo --consistency=l --total=10000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=1 --clients=1  range foo --consistency=s --total=10000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=100 --clients=1000  range foo --consistency=l --total=100000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=100 --clients=1000  range foo --consistency=s --total=100000\n```\n\n\n\n| key 数量 | Key 大小 | Value的大小 | 连接数量 | 客户端数量 | 一致性(线性化/串行化) | 每请求平均延迟 | 平均读取 QPS |\n| -------- | -------- | ----------- | -------- | ---------- | --------------------- | -------------- | ------------ |\n| 10,000   | 8        | 256         | 1        | 1          | Linearizable          | 11.5ms         | 740          |\n| 10,000   | 8        | 256         | 1        | 1          | Serializable          | 3.5ms          | 2146         |\n| 100,000  | 8        | 256         | 100      | 1000       | Linearizable          | 647.3ms        | 3376         |\n| 100,000  | 8        | 256         | 100      | 1000       | Serializable          | 546.9ms        | 4060         |\n\n\n\n##### etcd v3.4.1\n\n**写入测试**\n\n```\n// 查看 etcd leader\n$ etcdctl  --write-out=table --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23801\" endpoint status\n\n// leader\n$ benchmark --endpoints=\"http://192.168.74.36:23791\" --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791\" --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256\n\n\n// 所有 members\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\" --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256\n```\n\n\n\n| key 数量 | Key 大小 | Value的大小 | 连接数量 | 客户端数量 | 目标 etcd 服务器 | 平均写入 QPS | 每请求平均延迟 | Average server RSS | 调整磁盘IO优先级 | 调整网络带宽和优先级 |\n| -------- | -------- | ----------- | -------- | ---------- | ---------------- | ------------ | -------------- | ------------------ | ---------------- | -------------------- |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | 75           | 50.0ms         | -                  | 否               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | 322          | 13.2ms         | -                  | 是               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | -            | -              | -                  | 是               | 是                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | 1871         | 1207.7ms       | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | 2239         | 992.4ms        | -                  | 是               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | -            | -              | -                  | 是               | 是                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | 326          | 13.4ms         | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 1        | 1          | 所有 members     | 352          | 12.6ms         | -                  | 是               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | -            | -              | -                  | 是               | 是                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | 1132         | 1649.1ms       | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | 1198         | 1536.8ms       | -                  | 是               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | -            | -              | -                  | 是               | 是                   |\n\n**读取测试**\n\n```\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=1 --clients=1  range foo --consistency=l --total=10000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=1 --clients=1  range foo --consistency=s --total=10000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=100 --clients=1000  range foo --consistency=l --total=100000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=100 --clients=1000  range foo --consistency=s --total=100000\n```\n\n\n\n| key 数量 | Key 大小 | Value的大小 | 连接数量 | 客户端数量 | 一致性(线性化/串行化) | 每请求平均延迟(99%) | 平均读取 QPS |\n| -------- | -------- | ----------- | -------- | ---------- | --------------------- | ------------------- | ------------ |\n| 10,000   | 8        | 256         | 1        | 1          | Linearizable          | 36.2ms              | 319          |\n| 10,000   | 8        | 256         | 1        | 1          | Serializable          | 34.4ms              | 916          |\n| 100,000  | 8        | 256         | 100      | 1000       | Linearizable          | 1302.7ms            | 1680         |\n| 100,000  | 8        | 256         | 100      | 1000       | Serializable          | 1097.6ms            | 2401         |\n\n\n\n> 由于仅在本地进行测试，所受网络带宽影响不大，所以仅调整 io。\n\n\n\n### 分析\n\n可以看到，测试结果中写入操作与以上列出的几种因素关联比较大。读取指标的时候，串行化要比线性化要好，但为了一致性，线性化(Linearizable)读取请求要通过集群成员的法定人数来获取最新的数据。串行化(Serializable)读取请求比线性化读取要廉价一些，因为他们是通过任意单台 etcd 服务器来提供服务，而不是成员的法定人数，代价是可能提供过期数据。\n\n本文在力所能及的范围内对 etcd 的性能进行了一定的评估，所得到的数据并不能作为最终的参考数据，应当根据自己的环境进行评估，结合以上性能优化的方法得到最终的结论。\n\n\n\n参考：\n\n[Raft一致性算法论文的中文翻译](https://github.com/maemual/raft-zh_cn)\n\n[etcd 在超大规模数据场景下的性能优化](https://www.infoq.cn/article/Dit9YCy2-ziDrLfFeQzq)\n\n[当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？](https://mp.weixin.qq.com/s/skjNwU6Rdsn2qWN2KHU9zg)\n\n[Everything you should know about etcd](https://yq.aliyun.com/articles/388546)\n\n[etcd2 与 etcd3 相比](http://dockone.io/article/801)\n\n[etcd使用经验总结](https://alexstocks.github.io/html/etcd.html)\n\n[Understanding performance](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md)\n","source":"_posts/etcd_improvements.md","raw":"---\ntitle: etcd 性能测试与调优\ndate: 2019-10-08 14:50:30\ntags: [\"etcd\",\"improvements\"]\ntype: \"etcd\"\n\n---\n\netcd 是一个分布式一致性键值存储。其主要功能有服务注册与发现、消息发布与订阅、负载均衡、分布式通知与协调、分布式锁、分布式队列、集群监控与leader 选举等。\n\n\n\n### etcd 性能优化\n\n> 官方文档原文：https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md\n>\n> 译文参考：https://skyao.gitbooks.io/learning-etcd3/content/documentation/op-guide/performance.html\n\n#### 理解 etcd 的性能\n\n决定 etcd 性能的关键因素，包括：\n\n- 延迟(latency)：延迟是完成操作的时间。\n\n- 吞吐量(throughput)：吞吐量是在某个时间期间之内完成操作的总数量。 当 etcd 接收并发客户端请求时，通常平均延迟随着总体吞吐量增加而增加。\n\n在通常的云环境，比如 Google Compute Engine (GCE) 标准的 n-4 或者 AWS 上相当的机器类型，一个三成员 etcd 集群在轻负载下可以在低于1毫秒内完成一个请求，并在重负载下可以每秒完成超过 30000 个请求。\n\netcd 使用 Raft 一致性算法来在成员之间复制请求并达成一致。一致性性能，特别是提交延迟，受限于两个物理约束：**网络IO延迟和磁盘IO延迟**。完成一个etcd请求的最小时间是成员之间的网络往返时延(Round Trip Time / RTT)，加需要提交数据到持久化存储的 fdatasync 时间。在一个数据中心内的 RTT 可能有数百毫秒。在美国典型的 RTT 是大概 50ms, 而在大陆之间可以慢到400ms。旋转硬盘(注：指传统机械硬盘)的典型 fdatasync 延迟是大概 10ms。对于 SSD 硬盘, 延迟通常低于 1ms。为了提高吞吐量, etcd 将多个请求打包在一起并提交给 Raft。这个批量策略让 etcd 在重负载试获得高吞吐量。也有其他子系统影响到 etcd 的整体性能。每个序列化的 etcd 请求必须通过 etcd 的 boltdb 支持的(boltdb-backed) MVCC 存储引擎,它通常需要10微秒来完成。etcd 定期递增快照它最近实施的请求，将他们和之前在磁盘上的快照合并。这个过程可能导致延迟尖峰(latency spike)。虽然在SSD上这通常不是问题，在HDD上它可能加倍可观察到的延迟。而且，进行中的压缩可以影响 etcd 的性能。幸运的是，压缩通常无足轻重，因为压缩是错开的，因此它不和常规请求竞争资源。RPC 系统，gRPC，为 etcd 提供定义良好，可扩展的 API，但是它也引入了额外的延迟，尤其是本地读取。\n\nEtcd 的默认配置在本地网络环境（localhost）下通常能够运行的很好，因为延迟很低。然而，当跨数据中心部署 Etcd 或网络延时很高时，etcd 的心跳间隔或选举超时时间等参数需要根据实际情况进行调整。\n\n网络并不是导致延时的唯一来源。不论是 Follower 还是 Leader，其请求和响应都受磁盘 I/O 延时的影响。每个 timeout 都代表从请求发起到成功返回响应的总时间。\n\n#### 时间参数\n\nEtcd 底层的分布式一致性协议依赖两个时间参数来保证节点之间能够在部分节点掉钱的情况下依然能够正确处理主节点的选举。第一个参数就是所谓的心跳间隔，即主节点通知从节点它还是领导者的频率。实践数据表明，该参数应该设置成节点之间 RTT 的时间。Etcd 的心跳间隔默认是 100 毫秒。第二个参数是选举超时时间，即从节点等待多久没收到主节点的心跳就尝试去竞选领导者。Etcd 的选举超时时间默认是 1000 毫秒。\n\n调整这些参数值是有条件的，此消波长。心跳间隔值推荐设置为临近节点间 RTT 的最大值，通常是 0.5~1.5 倍 RTT 值。如果心跳间隔设得太短，那么 Etcd 就会发送没必要的心跳信息，从而增加 CPU 和网络资源的消耗；如果设得太长，就会导致选举等待时间的超时。如果选举等待时间设置的过长，就会导致节点异常检测时间过长。评估 RTT 值的最简单的方法是使用 ping 的操作。\n\n选举超时时间应该基于心跳间隔和节点之间的平均 RTT 值。选举超时必须至少是 RTT 10 倍的时间以便对网络波动。例如，如果 RTT 的值是 10 毫秒，那么选举超时时间必须至少是 100 毫秒。选举超时时间的上线是 50000 毫秒（50 秒），这个时间只能只用于全球范围内分布式部署的 Etcd 集群。美国大陆的一个 RTT 的合理时间大约是 130 毫秒，美国和日本的 RTT 大约是 350~400 毫秒。如果算上网络波动和重试的时间，那么 5 秒是一次全球 RTT 的安全上线。因为选举超时时间应该是心跳包广播时间的 10 倍，所以 50 秒的选举超时时间是全局分布式部署 Etcd 的合理上线值。\n\n心跳间隔和选举超时时间的值对同一个 Etcd 集群的所有节点都生效，如果各个节点都不同的话，就会导致集群发生不可预知的不稳定性。Etcd 启动时通过传入启动参数或环境变量覆盖默认值，单位是毫秒。示例代码具体如下：\n\n\n\n```\n$ etcd --heartbeat-interval=100 --election-timeout=500\n\n# 环境变量值\n$ ETCD_HEARTBEAT_INTERVAL=100 ETCD_ELECTION_TIMEOUT=500 etcd\n```\n\n#### 快照\n\nEtcd 总是向日志文件中追加 key，这样一来，日志文件会随着 key 的改动而线性增长。当 Etcd 集群使用较少时，保存完整的日志历史记录是没问题的，但如果 Etcd 集群规模比较大时，那么集群就会携带很大的日志文件。为了避免携带庞大的日志文件，Etcd 需要做周期性的快照。快照提供了一种通过保存系统的当前状态并移除旧日志文件的方式来压缩日志文件。\n\n##### 快照调优\n\n为 v2 后端存储创建快照的代价是很高的，所以只用当参数累积到一定的数量时，Etcd 才会创建快照文件。默认情况下，修改数量达到 10000 时才会建立快照。如果 Etcd 的内存使用和磁盘使用过高，那么应该尝试调低快照触发的阈值，具体请参考如下命令。\n\n启动参数：\n\n```\n$ etcd --snapshot-count=5000\n```\n\n环境变量：\n\n```\n$ ETCD_SNAPSHOT_COUNT=5000 etcd\n```\n\n#### 磁盘\n\netcd 的存储目录分为 snapshot 和 wal，他们写入的方式是不同的，snapshot 是内存直接 dump file。而 wal 是顺序追加写，对于这两种方式系统调优的方式是不同的，snapshot 可以通过增加 io 平滑写来提高磁盘 io 能力，而 wal 可以通过降低 pagecache 的方式提前写入时序。因此对于不同的场景，可以考虑将 snap 与 wal 进行分盘，放在两块 SSD 盘上，提高整体的 IO 效率，这种方式可以提升etcd 20%左右的性能。\n\netcd 集群对磁盘 I/O 的延时非常敏感，因为 Etcd 必须持久化它的日志，当其他 I/O 密集型的进程也在占用磁盘 I/O 的带宽时，就会导致 fsync 时延非常高。这将导致 Etcd 丢失心跳包、请求超时或暂时性的 Leader 丢失。这时可以适当为 Etcd 服务赋予更高的磁盘 I/O 权限，让 Etcd 更稳定的运行。在 Linux 系统中，磁盘 I/O 权限可以通过 ionice 命令进行调整。\n\nnux 默认 IO 调度器使用 CFQ 调度算法，支持用 ionice 命令为程序指定 IO 调度策略和优先级，IO 调度策略分为三种：\n\n- Idle ：其他进程没有磁盘 IO 时，才进行磁盘 IO\n- Best Effort：缺省调度策略，可以设置0-7的优先级，数值越小优先级越高，同优先级的进程采用 round-robin算法调度；\n- Real Time ：立即访问磁盘，无视其它进程 IO\n- None 即Best Effort，进程未指定策略和优先级时显示为none，会使用依据cpu nice设置计算出优先级\n\n\n\nLinux 中 etcd  的磁盘优先级可以使用 `ionice` 配置：\n\n```\n$ ionice -c2 -n0 -p `pgrep etcd`\n```\n\n#### 网络\n\netcd 中比较复杂的是网络的调优，因此大量的网络请求会在 peer 节点之间转发，而且整体网络吞吐也很大，但是还是再次强调不建议大家调整系统参数，大家可以通过修改 etcd 的 `--heartbeat-interval` 与 `--election-timeout` 启动参数来适当提高高吞吐网络下 etcd 的集群鲁棒性，通常同步吞吐在100MB左右的集群可以考虑将 `--heartbeat-interval` 设置为 300ms-500ms，`--election-timeout` 可以设置在 5000ms 左右。此外官方还有基于 TC 的网络优先传输方案，也是一个比较适用的调优手段。\n\n\n\n如果 etcd 的 Leader 服务大量并发客户端，这就会导致 follower 的请求的处理被延迟因为网络延迟。follower 的send buffer中能看到错误的列表，如下所示：\n\n```\ndropped MsgProp to 247ae21ff9436b2d since streamMsg's sending buffer is full\n\ndropped MsgAppResp to 247ae21ff9436b2d since streamMsg's sending buffer is full\n```\n\n这些错误可以通过提高 Leader 的网络优先级来提高 follower 的请求的响应。可以通过流量控制机制来提高:\n\n```\n// 针对 2379、2380 端口放行\n$ tc qdisc add dev eth0 root handle 1: prio bands 3\n$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip sport 2380 0xffff flowid 1:1\n$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip dport 2380 0xffff flowid 1:1\n$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip sport 2379 0xffff flowid 1:1\n$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip dport 2379 0xffff flowid 1:1\n\n// 查看现有的队列\n$ tc -s qdisc ls dev enp0s8\nqdisc prio 1: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1\n Sent 258578 bytes 923 pkt (dropped 0, overlimits 0 requeues 0)\n backlog 0b 0p requeues 0\n\n\n// 删除队列\n$ tc qdisc del dev enp0s8 root\n```\n\n#### 数据规模\n\netcd 的硬盘存储上限（默认是 2GB）,当 etcd 数据量超过默认 quota 值后便不再接受写请求，可以通过设置 `--quota-backend-bytes` 参数来增加存储大小,`quota-backend-bytes` 默认值为 0，即使用默认 quota 为 2GB，上限值为 8 GB，具体说明可参考官方文档：[dev-guide/limit.md](https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/limit.md)。\n\n\n\n```\nThe default storage size limit is 2GB, configurable with `--quota-backend-bytes` flag. 8GB is a suggested maximum size for normal environments and etcd warns at startup if the configured value exceeds it.\n```\n\n\n\n以下摘自 [当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？](https://mp.weixin.qq.com/s/skjNwU6Rdsn2qWN2KHU9zg)\n\n> 阿里进行了深入研究了 etcd 内部的实现原理，并发现了影响 etcd 扩展性的一个关键问题在底层 bbolt db 的 page 页面分配算法上：随着 etcd 中存储的数据量的增长，bbolt db 中线性查找“连续长度为 n 的 page 存储页面”的性能显著下降。\n>\n> 为了解决该问题，我们设计了基于 segregrated hashmap 的空闲页面管理算法，hashmap 以连续 page 大小为 key, 连续页面起始 page id 为  value。通过查这个 segregrated hashmap 实现 O(1) 的空闲 page 查找，极大地提高了性能。在释放块时，新算法尝试和地址相邻的 page 合并，并更新 segregrated hashmap。更详细的算法分析可以见已发表在[CNCF 博客的博文](https://www.cncf.io/blog/2019/05/09/performance-optimization-of-etcd-in-web-scale-data-scenario/)。\n>\n> 通过这个算法改进，我们可以将 etcd 的存储空间从推荐的 2GB 扩展到 100GB，极大地提高了 etcd 存储数据的规模，并且读写无显著延迟增长。\n>\n> pull request ： [https://github.com/etcd-io/bbolt/pull/141 ](https://github.com/etcd-io/bbolt/pull/141)\n\n目前社区已发布的 v3.4 系列版本并没有说明支持数据规模可达 100 G。\n\n\n\n### etcd 性能测试\n\n> 测试环境：本机 mac 使用 virtualbox 安装 vm，所有 etcd 实例都是运行在在 vm 中的 docker 上\n\n参考官方文档：https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md\n\n\n\n安装 etcd 压测工具 [benchmark](https://github.com/etcd-io/etcd/tree/master/tools/benchmark)：\n\n```\n$ go get go.etcd.io/etcd/tools/benchmark\n# GOPATH should be set\n$ ls $GOPATH/bin\nbenchmark\n```\n\n本文仅对 etcd v3.3.10 以及 v3.4.1 进行压测。\n\n#### 部署 etcd 集群\n\n以下为脚本示例：\n\n```\n#!/bin/bash\n\ndocker ps -a | grep etcd | grep -v k8s\ndocker rm -f etcd\n\nETCD_VERSION=3.3.10\nTOKEN=my-etcd-token\nCLUSTER_STATE=new\nNAME_1=etcd-node-0\nNAME_2=etcd-node-1\nNAME_3=etcd-node-2\nHOST_1=192.168.74.36\nHOST_2=192.168.74.36\nHOST_3=192.168.74.36\nCLUSTER=${NAME_1}=http://${HOST_1}:23801,${NAME_2}=http://${HOST_2}:23802,${NAME_3}=http://${HOST_3}:23803\n\n# 对于节点1\nTHIS_NAME=${NAME_1}\nTHIS_IP=${HOST_1}\nsudo docker run -d --net=host --name ${THIS_NAME} k8s.gcr.io/etcd:${ETCD_VERSION} \\\n    /usr/local/bin/etcd \\\n    --data-dir=data.etcd --name ${THIS_NAME} \\\n    --initial-advertise-peer-urls http://${THIS_IP}:23801 --listen-peer-urls http://${THIS_IP}:23801 \\\n    --advertise-client-urls http://${THIS_IP}:23791 --listen-client-urls http://${THIS_IP}:23791 \\\n    --initial-cluster ${CLUSTER} \\\n    --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# 对于节点2\nTHIS_NAME=${NAME_2}\nTHIS_IP=${HOST_2}\nsudo docker run -d --net=host --name ${THIS_NAME} k8s.gcr.io/etcd:${ETCD_VERSION} \\\n    /usr/local/bin/etcd \\\n    --data-dir=data.etcd --name ${THIS_NAME} \\\n    --initial-advertise-peer-urls http://${THIS_IP}:23802 --listen-peer-urls http://${THIS_IP}:23802 \\\n    --advertise-client-urls http://${THIS_IP}:23792 --listen-client-urls http://${THIS_IP}:23792 \\\n    --initial-cluster ${CLUSTER} \\\n    --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n\n# 对于节点3\nTHIS_NAME=${NAME_3}\nTHIS_IP=${HOST_3}\nsudo docker run -d --net=host --name ${THIS_NAME} k8s.gcr.io/etcd:${ETCD_VERSION} \\\n    /usr/local/bin/etcd \\\n    --data-dir=data.etcd --name ${THIS_NAME} \\\n    --initial-advertise-peer-urls http://${THIS_IP}:23803 --listen-peer-urls http://${THIS_IP}:23803 \\\n    --advertise-client-urls http://${THIS_IP}:23793 --listen-client-urls http://${THIS_IP}:23793 \\\n    --initial-cluster ${CLUSTER} \\\n    --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}\n```\n\n#### 压测\n\n本文主要对不同场景下 etcd 的读写操作进行测试，尽管环境有限，但在不同场景下 etcd 的表现还是有区别的。对于写入测试，按照官方文档的测试方法指定不同数量的客户端和连接数以及 key 的大小，对于读取操作，分别测试了线性化读取以及串行化读取，由于 etcd 是强一致性的，其默认读取测试就是线性化读取。\n\n\n\n##### etcd v3.3.10\n\n**写入测试**\n\n```\n// 查看 leader\n$ etcdctl member list\n\n// leader\n$ benchmark --endpoints=\"http://192.168.74.36:23791\" --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791\" --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256\n\n\n// 所有 members\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\" --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256\n```\n\n\n\n| key 数量 | Key 大小 | Value的大小 | 连接数量 | 客户端数量 | 目标 etcd 服务器 | 平均写入 QPS | 每请求平均延迟 | Average server RSS | 调整磁盘IO优先级 | 调整网络带宽和优先级 |\n| -------- | -------- | ----------- | -------- | ---------- | ---------------- | ------------ | -------------- | ------------------ | ---------------- | -------------------- |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | 75           | 50.0ms         | -                  | 否               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | 77           | 46.5ms         | -                  | 是               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | -            | -              | -                  | 是               | 是                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | 1144         | 1697.5ms       | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | 1185         | 1541.8ms       | -                  | 是               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | -            | -              | -                  | 是               | 是                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | 73           | 49.6ms         | -                  | 否               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | 80           | 48.5ms         | -                  | 是               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | -            | -              | -                  | 是               | 是                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | 1132         | 1649.1ms       | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | 1198         | 1536.8ms         | -                  | 是               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | -            | -              | -                  | 是               | 是                   |\n\n\n\n**读取测试**\n\n```\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=1 --clients=1  range foo --consistency=l --total=10000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=1 --clients=1  range foo --consistency=s --total=10000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=100 --clients=1000  range foo --consistency=l --total=100000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=100 --clients=1000  range foo --consistency=s --total=100000\n```\n\n\n\n| key 数量 | Key 大小 | Value的大小 | 连接数量 | 客户端数量 | 一致性(线性化/串行化) | 每请求平均延迟 | 平均读取 QPS |\n| -------- | -------- | ----------- | -------- | ---------- | --------------------- | -------------- | ------------ |\n| 10,000   | 8        | 256         | 1        | 1          | Linearizable          | 11.5ms         | 740          |\n| 10,000   | 8        | 256         | 1        | 1          | Serializable          | 3.5ms          | 2146         |\n| 100,000  | 8        | 256         | 100      | 1000       | Linearizable          | 647.3ms        | 3376         |\n| 100,000  | 8        | 256         | 100      | 1000       | Serializable          | 546.9ms        | 4060         |\n\n\n\n##### etcd v3.4.1\n\n**写入测试**\n\n```\n// 查看 etcd leader\n$ etcdctl  --write-out=table --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23801\" endpoint status\n\n// leader\n$ benchmark --endpoints=\"http://192.168.74.36:23791\" --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791\" --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256\n\n\n// 所有 members\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\" --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256\n```\n\n\n\n| key 数量 | Key 大小 | Value的大小 | 连接数量 | 客户端数量 | 目标 etcd 服务器 | 平均写入 QPS | 每请求平均延迟 | Average server RSS | 调整磁盘IO优先级 | 调整网络带宽和优先级 |\n| -------- | -------- | ----------- | -------- | ---------- | ---------------- | ------------ | -------------- | ------------------ | ---------------- | -------------------- |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | 75           | 50.0ms         | -                  | 否               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | 322          | 13.2ms         | -                  | 是               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 只有主           | -            | -              | -                  | 是               | 是                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | 1871         | 1207.7ms       | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | 2239         | 992.4ms        | -                  | 是               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | 只有主           | -            | -              | -                  | 是               | 是                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | 326          | 13.4ms         | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 1        | 1          | 所有 members     | 352          | 12.6ms         | -                  | 是               | 否                   |\n| 10,000   | 8        | 256         | 1        | 1          | 所有 members     | -            | -              | -                  | 是               | 是                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | 1132         | 1649.1ms       | -                  | 否               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | 1198         | 1536.8ms       | -                  | 是               | 否                   |\n| 100,000  | 8        | 256         | 100      | 1000       | all members      | -            | -              | -                  | 是               | 是                   |\n\n**读取测试**\n\n```\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=1 --clients=1  range foo --consistency=l --total=10000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=1 --clients=1  range foo --consistency=s --total=10000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=100 --clients=1000  range foo --consistency=l --total=100000\n\n$ benchmark --endpoints=\"http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793\"  --conns=100 --clients=1000  range foo --consistency=s --total=100000\n```\n\n\n\n| key 数量 | Key 大小 | Value的大小 | 连接数量 | 客户端数量 | 一致性(线性化/串行化) | 每请求平均延迟(99%) | 平均读取 QPS |\n| -------- | -------- | ----------- | -------- | ---------- | --------------------- | ------------------- | ------------ |\n| 10,000   | 8        | 256         | 1        | 1          | Linearizable          | 36.2ms              | 319          |\n| 10,000   | 8        | 256         | 1        | 1          | Serializable          | 34.4ms              | 916          |\n| 100,000  | 8        | 256         | 100      | 1000       | Linearizable          | 1302.7ms            | 1680         |\n| 100,000  | 8        | 256         | 100      | 1000       | Serializable          | 1097.6ms            | 2401         |\n\n\n\n> 由于仅在本地进行测试，所受网络带宽影响不大，所以仅调整 io。\n\n\n\n### 分析\n\n可以看到，测试结果中写入操作与以上列出的几种因素关联比较大。读取指标的时候，串行化要比线性化要好，但为了一致性，线性化(Linearizable)读取请求要通过集群成员的法定人数来获取最新的数据。串行化(Serializable)读取请求比线性化读取要廉价一些，因为他们是通过任意单台 etcd 服务器来提供服务，而不是成员的法定人数，代价是可能提供过期数据。\n\n本文在力所能及的范围内对 etcd 的性能进行了一定的评估，所得到的数据并不能作为最终的参考数据，应当根据自己的环境进行评估，结合以上性能优化的方法得到最终的结论。\n\n\n\n参考：\n\n[Raft一致性算法论文的中文翻译](https://github.com/maemual/raft-zh_cn)\n\n[etcd 在超大规模数据场景下的性能优化](https://www.infoq.cn/article/Dit9YCy2-ziDrLfFeQzq)\n\n[当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？](https://mp.weixin.qq.com/s/skjNwU6Rdsn2qWN2KHU9zg)\n\n[Everything you should know about etcd](https://yq.aliyun.com/articles/388546)\n\n[etcd2 与 etcd3 相比](http://dockone.io/article/801)\n\n[etcd使用经验总结](https://alexstocks.github.io/html/etcd.html)\n\n[Understanding performance](https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md)\n","slug":"etcd_improvements","published":1,"updated":"2019-10-08T08:11:13.643Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro5960005apwnte4gfbr0","content":"<p>etcd 是一个分布式一致性键值存储。其主要功能有服务注册与发现、消息发布与订阅、负载均衡、分布式通知与协调、分布式锁、分布式队列、集群监控与leader 选举等。</p>\n<h3 id=\"etcd-性能优化\"><a href=\"#etcd-性能优化\" class=\"headerlink\" title=\"etcd 性能优化\"></a>etcd 性能优化</h3><blockquote>\n<p>官方文档原文：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md\" target=\"_blank\" rel=\"noopener\">https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md</a></p>\n<p>译文参考：<a href=\"https://skyao.gitbooks.io/learning-etcd3/content/documentation/op-guide/performance.html\" target=\"_blank\" rel=\"noopener\">https://skyao.gitbooks.io/learning-etcd3/content/documentation/op-guide/performance.html</a></p>\n</blockquote>\n<h4 id=\"理解-etcd-的性能\"><a href=\"#理解-etcd-的性能\" class=\"headerlink\" title=\"理解 etcd 的性能\"></a>理解 etcd 的性能</h4><p>决定 etcd 性能的关键因素，包括：</p>\n<ul>\n<li><p>延迟(latency)：延迟是完成操作的时间。</p>\n</li>\n<li><p>吞吐量(throughput)：吞吐量是在某个时间期间之内完成操作的总数量。 当 etcd 接收并发客户端请求时，通常平均延迟随着总体吞吐量增加而增加。</p>\n</li>\n</ul>\n<p>在通常的云环境，比如 Google Compute Engine (GCE) 标准的 n-4 或者 AWS 上相当的机器类型，一个三成员 etcd 集群在轻负载下可以在低于1毫秒内完成一个请求，并在重负载下可以每秒完成超过 30000 个请求。</p>\n<p>etcd 使用 Raft 一致性算法来在成员之间复制请求并达成一致。一致性性能，特别是提交延迟，受限于两个物理约束：<strong>网络IO延迟和磁盘IO延迟</strong>。完成一个etcd请求的最小时间是成员之间的网络往返时延(Round Trip Time / RTT)，加需要提交数据到持久化存储的 fdatasync 时间。在一个数据中心内的 RTT 可能有数百毫秒。在美国典型的 RTT 是大概 50ms, 而在大陆之间可以慢到400ms。旋转硬盘(注：指传统机械硬盘)的典型 fdatasync 延迟是大概 10ms。对于 SSD 硬盘, 延迟通常低于 1ms。为了提高吞吐量, etcd 将多个请求打包在一起并提交给 Raft。这个批量策略让 etcd 在重负载试获得高吞吐量。也有其他子系统影响到 etcd 的整体性能。每个序列化的 etcd 请求必须通过 etcd 的 boltdb 支持的(boltdb-backed) MVCC 存储引擎,它通常需要10微秒来完成。etcd 定期递增快照它最近实施的请求，将他们和之前在磁盘上的快照合并。这个过程可能导致延迟尖峰(latency spike)。虽然在SSD上这通常不是问题，在HDD上它可能加倍可观察到的延迟。而且，进行中的压缩可以影响 etcd 的性能。幸运的是，压缩通常无足轻重，因为压缩是错开的，因此它不和常规请求竞争资源。RPC 系统，gRPC，为 etcd 提供定义良好，可扩展的 API，但是它也引入了额外的延迟，尤其是本地读取。</p>\n<p>Etcd 的默认配置在本地网络环境（localhost）下通常能够运行的很好，因为延迟很低。然而，当跨数据中心部署 Etcd 或网络延时很高时，etcd 的心跳间隔或选举超时时间等参数需要根据实际情况进行调整。</p>\n<p>网络并不是导致延时的唯一来源。不论是 Follower 还是 Leader，其请求和响应都受磁盘 I/O 延时的影响。每个 timeout 都代表从请求发起到成功返回响应的总时间。</p>\n<h4 id=\"时间参数\"><a href=\"#时间参数\" class=\"headerlink\" title=\"时间参数\"></a>时间参数</h4><p>Etcd 底层的分布式一致性协议依赖两个时间参数来保证节点之间能够在部分节点掉钱的情况下依然能够正确处理主节点的选举。第一个参数就是所谓的心跳间隔，即主节点通知从节点它还是领导者的频率。实践数据表明，该参数应该设置成节点之间 RTT 的时间。Etcd 的心跳间隔默认是 100 毫秒。第二个参数是选举超时时间，即从节点等待多久没收到主节点的心跳就尝试去竞选领导者。Etcd 的选举超时时间默认是 1000 毫秒。</p>\n<p>调整这些参数值是有条件的，此消波长。心跳间隔值推荐设置为临近节点间 RTT 的最大值，通常是 0.5~1.5 倍 RTT 值。如果心跳间隔设得太短，那么 Etcd 就会发送没必要的心跳信息，从而增加 CPU 和网络资源的消耗；如果设得太长，就会导致选举等待时间的超时。如果选举等待时间设置的过长，就会导致节点异常检测时间过长。评估 RTT 值的最简单的方法是使用 ping 的操作。</p>\n<p>选举超时时间应该基于心跳间隔和节点之间的平均 RTT 值。选举超时必须至少是 RTT 10 倍的时间以便对网络波动。例如，如果 RTT 的值是 10 毫秒，那么选举超时时间必须至少是 100 毫秒。选举超时时间的上线是 50000 毫秒（50 秒），这个时间只能只用于全球范围内分布式部署的 Etcd 集群。美国大陆的一个 RTT 的合理时间大约是 130 毫秒，美国和日本的 RTT 大约是 350~400 毫秒。如果算上网络波动和重试的时间，那么 5 秒是一次全球 RTT 的安全上线。因为选举超时时间应该是心跳包广播时间的 10 倍，所以 50 秒的选举超时时间是全局分布式部署 Etcd 的合理上线值。</p>\n<p>心跳间隔和选举超时时间的值对同一个 Etcd 集群的所有节点都生效，如果各个节点都不同的话，就会导致集群发生不可预知的不稳定性。Etcd 启动时通过传入启动参数或环境变量覆盖默认值，单位是毫秒。示例代码具体如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcd --heartbeat-interval=100 --election-timeout=500</span><br><span class=\"line\"></span><br><span class=\"line\"># 环境变量值</span><br><span class=\"line\">$ ETCD_HEARTBEAT_INTERVAL=100 ETCD_ELECTION_TIMEOUT=500 etcd</span><br></pre></td></tr></table></figure>\n<h4 id=\"快照\"><a href=\"#快照\" class=\"headerlink\" title=\"快照\"></a>快照</h4><p>Etcd 总是向日志文件中追加 key，这样一来，日志文件会随着 key 的改动而线性增长。当 Etcd 集群使用较少时，保存完整的日志历史记录是没问题的，但如果 Etcd 集群规模比较大时，那么集群就会携带很大的日志文件。为了避免携带庞大的日志文件，Etcd 需要做周期性的快照。快照提供了一种通过保存系统的当前状态并移除旧日志文件的方式来压缩日志文件。</p>\n<h5 id=\"快照调优\"><a href=\"#快照调优\" class=\"headerlink\" title=\"快照调优\"></a>快照调优</h5><p>为 v2 后端存储创建快照的代价是很高的，所以只用当参数累积到一定的数量时，Etcd 才会创建快照文件。默认情况下，修改数量达到 10000 时才会建立快照。如果 Etcd 的内存使用和磁盘使用过高，那么应该尝试调低快照触发的阈值，具体请参考如下命令。</p>\n<p>启动参数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcd --snapshot-count=5000</span><br></pre></td></tr></table></figure>\n<p>环境变量：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ETCD_SNAPSHOT_COUNT=5000 etcd</span><br></pre></td></tr></table></figure>\n<h4 id=\"磁盘\"><a href=\"#磁盘\" class=\"headerlink\" title=\"磁盘\"></a>磁盘</h4><p>etcd 的存储目录分为 snapshot 和 wal，他们写入的方式是不同的，snapshot 是内存直接 dump file。而 wal 是顺序追加写，对于这两种方式系统调优的方式是不同的，snapshot 可以通过增加 io 平滑写来提高磁盘 io 能力，而 wal 可以通过降低 pagecache 的方式提前写入时序。因此对于不同的场景，可以考虑将 snap 与 wal 进行分盘，放在两块 SSD 盘上，提高整体的 IO 效率，这种方式可以提升etcd 20%左右的性能。</p>\n<p>etcd 集群对磁盘 I/O 的延时非常敏感，因为 Etcd 必须持久化它的日志，当其他 I/O 密集型的进程也在占用磁盘 I/O 的带宽时，就会导致 fsync 时延非常高。这将导致 Etcd 丢失心跳包、请求超时或暂时性的 Leader 丢失。这时可以适当为 Etcd 服务赋予更高的磁盘 I/O 权限，让 Etcd 更稳定的运行。在 Linux 系统中，磁盘 I/O 权限可以通过 ionice 命令进行调整。</p>\n<p>nux 默认 IO 调度器使用 CFQ 调度算法，支持用 ionice 命令为程序指定 IO 调度策略和优先级，IO 调度策略分为三种：</p>\n<ul>\n<li>Idle ：其他进程没有磁盘 IO 时，才进行磁盘 IO</li>\n<li>Best Effort：缺省调度策略，可以设置0-7的优先级，数值越小优先级越高，同优先级的进程采用 round-robin算法调度；</li>\n<li>Real Time ：立即访问磁盘，无视其它进程 IO</li>\n<li>None 即Best Effort，进程未指定策略和优先级时显示为none，会使用依据cpu nice设置计算出优先级</li>\n</ul>\n<p>Linux 中 etcd  的磁盘优先级可以使用 <code>ionice</code> 配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ionice -c2 -n0 -p `pgrep etcd`</span><br></pre></td></tr></table></figure>\n<h4 id=\"网络\"><a href=\"#网络\" class=\"headerlink\" title=\"网络\"></a>网络</h4><p>etcd 中比较复杂的是网络的调优，因此大量的网络请求会在 peer 节点之间转发，而且整体网络吞吐也很大，但是还是再次强调不建议大家调整系统参数，大家可以通过修改 etcd 的 <code>--heartbeat-interval</code> 与 <code>--election-timeout</code> 启动参数来适当提高高吞吐网络下 etcd 的集群鲁棒性，通常同步吞吐在100MB左右的集群可以考虑将 <code>--heartbeat-interval</code> 设置为 300ms-500ms，<code>--election-timeout</code> 可以设置在 5000ms 左右。此外官方还有基于 TC 的网络优先传输方案，也是一个比较适用的调优手段。</p>\n<p>如果 etcd 的 Leader 服务大量并发客户端，这就会导致 follower 的请求的处理被延迟因为网络延迟。follower 的send buffer中能看到错误的列表，如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dropped MsgProp to 247ae21ff9436b2d since streamMsg&apos;s sending buffer is full</span><br><span class=\"line\"></span><br><span class=\"line\">dropped MsgAppResp to 247ae21ff9436b2d since streamMsg&apos;s sending buffer is full</span><br></pre></td></tr></table></figure>\n<p>这些错误可以通过提高 Leader 的网络优先级来提高 follower 的请求的响应。可以通过流量控制机制来提高:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 针对 2379、2380 端口放行</span><br><span class=\"line\">$ tc qdisc add dev eth0 root handle 1: prio bands 3</span><br><span class=\"line\">$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip sport 2380 0xffff flowid 1:1</span><br><span class=\"line\">$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip dport 2380 0xffff flowid 1:1</span><br><span class=\"line\">$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip sport 2379 0xffff flowid 1:1</span><br><span class=\"line\">$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip dport 2379 0xffff flowid 1:1</span><br><span class=\"line\"></span><br><span class=\"line\">// 查看现有的队列</span><br><span class=\"line\">$ tc -s qdisc ls dev enp0s8</span><br><span class=\"line\">qdisc prio 1: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1</span><br><span class=\"line\"> Sent 258578 bytes 923 pkt (dropped 0, overlimits 0 requeues 0)</span><br><span class=\"line\"> backlog 0b 0p requeues 0</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 删除队列</span><br><span class=\"line\">$ tc qdisc del dev enp0s8 root</span><br></pre></td></tr></table></figure>\n<h4 id=\"数据规模\"><a href=\"#数据规模\" class=\"headerlink\" title=\"数据规模\"></a>数据规模</h4><p>etcd 的硬盘存储上限（默认是 2GB）,当 etcd 数据量超过默认 quota 值后便不再接受写请求，可以通过设置 <code>--quota-backend-bytes</code> 参数来增加存储大小,<code>quota-backend-bytes</code> 默认值为 0，即使用默认 quota 为 2GB，上限值为 8 GB，具体说明可参考官方文档：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/limit.md\" target=\"_blank\" rel=\"noopener\">dev-guide/limit.md</a>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">The default storage size limit is 2GB, configurable with `--quota-backend-bytes` flag. 8GB is a suggested maximum size for normal environments and etcd warns at startup if the configured value exceeds it.</span><br></pre></td></tr></table></figure>\n<p>以下摘自 <a href=\"https://mp.weixin.qq.com/s/skjNwU6Rdsn2qWN2KHU9zg\" target=\"_blank\" rel=\"noopener\">当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？</a></p>\n<blockquote>\n<p>阿里进行了深入研究了 etcd 内部的实现原理，并发现了影响 etcd 扩展性的一个关键问题在底层 bbolt db 的 page 页面分配算法上：随着 etcd 中存储的数据量的增长，bbolt db 中线性查找“连续长度为 n 的 page 存储页面”的性能显著下降。</p>\n<p>为了解决该问题，我们设计了基于 segregrated hashmap 的空闲页面管理算法，hashmap 以连续 page 大小为 key, 连续页面起始 page id 为  value。通过查这个 segregrated hashmap 实现 O(1) 的空闲 page 查找，极大地提高了性能。在释放块时，新算法尝试和地址相邻的 page 合并，并更新 segregrated hashmap。更详细的算法分析可以见已发表在<a href=\"https://www.cncf.io/blog/2019/05/09/performance-optimization-of-etcd-in-web-scale-data-scenario/\" target=\"_blank\" rel=\"noopener\">CNCF 博客的博文</a>。</p>\n<p>通过这个算法改进，我们可以将 etcd 的存储空间从推荐的 2GB 扩展到 100GB，极大地提高了 etcd 存储数据的规模，并且读写无显著延迟增长。</p>\n<p>pull request ： <a href=\"https://github.com/etcd-io/bbolt/pull/141\" target=\"_blank\" rel=\"noopener\">https://github.com/etcd-io/bbolt/pull/141 </a></p>\n</blockquote>\n<p>目前社区已发布的 v3.4 系列版本并没有说明支持数据规模可达 100 G。</p>\n<h3 id=\"etcd-性能测试\"><a href=\"#etcd-性能测试\" class=\"headerlink\" title=\"etcd 性能测试\"></a>etcd 性能测试</h3><blockquote>\n<p>测试环境：本机 mac 使用 virtualbox 安装 vm，所有 etcd 实例都是运行在在 vm 中的 docker 上</p>\n</blockquote>\n<p>参考官方文档：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md\" target=\"_blank\" rel=\"noopener\">https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md</a></p>\n<p>安装 etcd 压测工具 <a href=\"https://github.com/etcd-io/etcd/tree/master/tools/benchmark\" target=\"_blank\" rel=\"noopener\">benchmark</a>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ go get go.etcd.io/etcd/tools/benchmark</span><br><span class=\"line\"># GOPATH should be set</span><br><span class=\"line\">$ ls $GOPATH/bin</span><br><span class=\"line\">benchmark</span><br></pre></td></tr></table></figure>\n<p>本文仅对 etcd v3.3.10 以及 v3.4.1 进行压测。</p>\n<h4 id=\"部署-etcd-集群\"><a href=\"#部署-etcd-集群\" class=\"headerlink\" title=\"部署 etcd 集群\"></a>部署 etcd 集群</h4><p>以下为脚本示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">docker ps -a | grep etcd | grep -v k8s</span><br><span class=\"line\">docker rm -f etcd</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_VERSION=3.3.10</span><br><span class=\"line\">TOKEN=my-etcd-token</span><br><span class=\"line\">CLUSTER_STATE=new</span><br><span class=\"line\">NAME_1=etcd-node-0</span><br><span class=\"line\">NAME_2=etcd-node-1</span><br><span class=\"line\">NAME_3=etcd-node-2</span><br><span class=\"line\">HOST_1=192.168.74.36</span><br><span class=\"line\">HOST_2=192.168.74.36</span><br><span class=\"line\">HOST_3=192.168.74.36</span><br><span class=\"line\">CLUSTER=$&#123;NAME_1&#125;=http://$&#123;HOST_1&#125;:23801,$&#123;NAME_2&#125;=http://$&#123;HOST_2&#125;:23802,$&#123;NAME_3&#125;=http://$&#123;HOST_3&#125;:23803</span><br><span class=\"line\"></span><br><span class=\"line\"># 对于节点1</span><br><span class=\"line\">THIS_NAME=$&#123;NAME_1&#125;</span><br><span class=\"line\">THIS_IP=$&#123;HOST_1&#125;</span><br><span class=\"line\">sudo docker run -d --net=host --name $&#123;THIS_NAME&#125; k8s.gcr.io/etcd:$&#123;ETCD_VERSION&#125; \\</span><br><span class=\"line\">    /usr/local/bin/etcd \\</span><br><span class=\"line\">    --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \\</span><br><span class=\"line\">    --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:23801 --listen-peer-urls http://$&#123;THIS_IP&#125;:23801 \\</span><br><span class=\"line\">    --advertise-client-urls http://$&#123;THIS_IP&#125;:23791 --listen-client-urls http://$&#123;THIS_IP&#125;:23791 \\</span><br><span class=\"line\">    --initial-cluster $&#123;CLUSTER&#125; \\</span><br><span class=\"line\">    --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># 对于节点2</span><br><span class=\"line\">THIS_NAME=$&#123;NAME_2&#125;</span><br><span class=\"line\">THIS_IP=$&#123;HOST_2&#125;</span><br><span class=\"line\">sudo docker run -d --net=host --name $&#123;THIS_NAME&#125; k8s.gcr.io/etcd:$&#123;ETCD_VERSION&#125; \\</span><br><span class=\"line\">    /usr/local/bin/etcd \\</span><br><span class=\"line\">    --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \\</span><br><span class=\"line\">    --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:23802 --listen-peer-urls http://$&#123;THIS_IP&#125;:23802 \\</span><br><span class=\"line\">    --advertise-client-urls http://$&#123;THIS_IP&#125;:23792 --listen-client-urls http://$&#123;THIS_IP&#125;:23792 \\</span><br><span class=\"line\">    --initial-cluster $&#123;CLUSTER&#125; \\</span><br><span class=\"line\">    --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># 对于节点3</span><br><span class=\"line\">THIS_NAME=$&#123;NAME_3&#125;</span><br><span class=\"line\">THIS_IP=$&#123;HOST_3&#125;</span><br><span class=\"line\">sudo docker run -d --net=host --name $&#123;THIS_NAME&#125; k8s.gcr.io/etcd:$&#123;ETCD_VERSION&#125; \\</span><br><span class=\"line\">    /usr/local/bin/etcd \\</span><br><span class=\"line\">    --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \\</span><br><span class=\"line\">    --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:23803 --listen-peer-urls http://$&#123;THIS_IP&#125;:23803 \\</span><br><span class=\"line\">    --advertise-client-urls http://$&#123;THIS_IP&#125;:23793 --listen-client-urls http://$&#123;THIS_IP&#125;:23793 \\</span><br><span class=\"line\">    --initial-cluster $&#123;CLUSTER&#125; \\</span><br><span class=\"line\">    --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"压测\"><a href=\"#压测\" class=\"headerlink\" title=\"压测\"></a>压测</h4><p>本文主要对不同场景下 etcd 的读写操作进行测试，尽管环境有限，但在不同场景下 etcd 的表现还是有区别的。对于写入测试，按照官方文档的测试方法指定不同数量的客户端和连接数以及 key 的大小，对于读取操作，分别测试了线性化读取以及串行化读取，由于 etcd 是强一致性的，其默认读取测试就是线性化读取。</p>\n<h5 id=\"etcd-v3-3-10\"><a href=\"#etcd-v3-3-10\" class=\"headerlink\" title=\"etcd v3.3.10\"></a>etcd v3.3.10</h5><p><strong>写入测试</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看 leader</span><br><span class=\"line\">$ etcdctl member list</span><br><span class=\"line\"></span><br><span class=\"line\">// leader</span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 所有 members</span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>key 数量</th>\n<th>Key 大小</th>\n<th>Value的大小</th>\n<th>连接数量</th>\n<th>客户端数量</th>\n<th>目标 etcd 服务器</th>\n<th>平均写入 QPS</th>\n<th>每请求平均延迟</th>\n<th>Average server RSS</th>\n<th>调整磁盘IO优先级</th>\n<th>调整网络带宽和优先级</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>75</td>\n<td>50.0ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>77</td>\n<td>46.5ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>1144</td>\n<td>1697.5ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>1185</td>\n<td>1541.8ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>73</td>\n<td>49.6ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>80</td>\n<td>48.5ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>1132</td>\n<td>1649.1ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>1198</td>\n<td>1536.8ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n</tbody>\n</table>\n<p><strong>读取测试</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=1 --clients=1  range foo --consistency=l --total=10000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=1 --clients=1  range foo --consistency=s --total=10000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=100 --clients=1000  range foo --consistency=l --total=100000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=100 --clients=1000  range foo --consistency=s --total=100000</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>key 数量</th>\n<th>Key 大小</th>\n<th>Value的大小</th>\n<th>连接数量</th>\n<th>客户端数量</th>\n<th>一致性(线性化/串行化)</th>\n<th>每请求平均延迟</th>\n<th>平均读取 QPS</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>Linearizable</td>\n<td>11.5ms</td>\n<td>740</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>Serializable</td>\n<td>3.5ms</td>\n<td>2146</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>Linearizable</td>\n<td>647.3ms</td>\n<td>3376</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>Serializable</td>\n<td>546.9ms</td>\n<td>4060</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"etcd-v3-4-1\"><a href=\"#etcd-v3-4-1\" class=\"headerlink\" title=\"etcd v3.4.1\"></a>etcd v3.4.1</h5><p><strong>写入测试</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看 etcd leader</span><br><span class=\"line\">$ etcdctl  --write-out=table --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23801&quot; endpoint status</span><br><span class=\"line\"></span><br><span class=\"line\">// leader</span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 所有 members</span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>key 数量</th>\n<th>Key 大小</th>\n<th>Value的大小</th>\n<th>连接数量</th>\n<th>客户端数量</th>\n<th>目标 etcd 服务器</th>\n<th>平均写入 QPS</th>\n<th>每请求平均延迟</th>\n<th>Average server RSS</th>\n<th>调整磁盘IO优先级</th>\n<th>调整网络带宽和优先级</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>75</td>\n<td>50.0ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>322</td>\n<td>13.2ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>1871</td>\n<td>1207.7ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>2239</td>\n<td>992.4ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>326</td>\n<td>13.4ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>352</td>\n<td>12.6ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>1132</td>\n<td>1649.1ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>1198</td>\n<td>1536.8ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n</tbody>\n</table>\n<p><strong>读取测试</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=1 --clients=1  range foo --consistency=l --total=10000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=1 --clients=1  range foo --consistency=s --total=10000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=100 --clients=1000  range foo --consistency=l --total=100000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=100 --clients=1000  range foo --consistency=s --total=100000</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>key 数量</th>\n<th>Key 大小</th>\n<th>Value的大小</th>\n<th>连接数量</th>\n<th>客户端数量</th>\n<th>一致性(线性化/串行化)</th>\n<th>每请求平均延迟(99%)</th>\n<th>平均读取 QPS</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>Linearizable</td>\n<td>36.2ms</td>\n<td>319</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>Serializable</td>\n<td>34.4ms</td>\n<td>916</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>Linearizable</td>\n<td>1302.7ms</td>\n<td>1680</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>Serializable</td>\n<td>1097.6ms</td>\n<td>2401</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>由于仅在本地进行测试，所受网络带宽影响不大，所以仅调整 io。</p>\n</blockquote>\n<h3 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h3><p>可以看到，测试结果中写入操作与以上列出的几种因素关联比较大。读取指标的时候，串行化要比线性化要好，但为了一致性，线性化(Linearizable)读取请求要通过集群成员的法定人数来获取最新的数据。串行化(Serializable)读取请求比线性化读取要廉价一些，因为他们是通过任意单台 etcd 服务器来提供服务，而不是成员的法定人数，代价是可能提供过期数据。</p>\n<p>本文在力所能及的范围内对 etcd 的性能进行了一定的评估，所得到的数据并不能作为最终的参考数据，应当根据自己的环境进行评估，结合以上性能优化的方法得到最终的结论。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/maemual/raft-zh_cn\" target=\"_blank\" rel=\"noopener\">Raft一致性算法论文的中文翻译</a></p>\n<p><a href=\"https://www.infoq.cn/article/Dit9YCy2-ziDrLfFeQzq\" target=\"_blank\" rel=\"noopener\">etcd 在超大规模数据场景下的性能优化</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/skjNwU6Rdsn2qWN2KHU9zg\" target=\"_blank\" rel=\"noopener\">当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？</a></p>\n<p><a href=\"https://yq.aliyun.com/articles/388546\" target=\"_blank\" rel=\"noopener\">Everything you should know about etcd</a></p>\n<p><a href=\"http://dockone.io/article/801\" target=\"_blank\" rel=\"noopener\">etcd2 与 etcd3 相比</a></p>\n<p><a href=\"https://alexstocks.github.io/html/etcd.html\" target=\"_blank\" rel=\"noopener\">etcd使用经验总结</a></p>\n<p><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md\" target=\"_blank\" rel=\"noopener\">Understanding performance</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>etcd 是一个分布式一致性键值存储。其主要功能有服务注册与发现、消息发布与订阅、负载均衡、分布式通知与协调、分布式锁、分布式队列、集群监控与leader 选举等。</p>\n<h3 id=\"etcd-性能优化\"><a href=\"#etcd-性能优化\" class=\"headerlink\" title=\"etcd 性能优化\"></a>etcd 性能优化</h3><blockquote>\n<p>官方文档原文：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md\" target=\"_blank\" rel=\"noopener\">https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md</a></p>\n<p>译文参考：<a href=\"https://skyao.gitbooks.io/learning-etcd3/content/documentation/op-guide/performance.html\" target=\"_blank\" rel=\"noopener\">https://skyao.gitbooks.io/learning-etcd3/content/documentation/op-guide/performance.html</a></p>\n</blockquote>\n<h4 id=\"理解-etcd-的性能\"><a href=\"#理解-etcd-的性能\" class=\"headerlink\" title=\"理解 etcd 的性能\"></a>理解 etcd 的性能</h4><p>决定 etcd 性能的关键因素，包括：</p>\n<ul>\n<li><p>延迟(latency)：延迟是完成操作的时间。</p>\n</li>\n<li><p>吞吐量(throughput)：吞吐量是在某个时间期间之内完成操作的总数量。 当 etcd 接收并发客户端请求时，通常平均延迟随着总体吞吐量增加而增加。</p>\n</li>\n</ul>\n<p>在通常的云环境，比如 Google Compute Engine (GCE) 标准的 n-4 或者 AWS 上相当的机器类型，一个三成员 etcd 集群在轻负载下可以在低于1毫秒内完成一个请求，并在重负载下可以每秒完成超过 30000 个请求。</p>\n<p>etcd 使用 Raft 一致性算法来在成员之间复制请求并达成一致。一致性性能，特别是提交延迟，受限于两个物理约束：<strong>网络IO延迟和磁盘IO延迟</strong>。完成一个etcd请求的最小时间是成员之间的网络往返时延(Round Trip Time / RTT)，加需要提交数据到持久化存储的 fdatasync 时间。在一个数据中心内的 RTT 可能有数百毫秒。在美国典型的 RTT 是大概 50ms, 而在大陆之间可以慢到400ms。旋转硬盘(注：指传统机械硬盘)的典型 fdatasync 延迟是大概 10ms。对于 SSD 硬盘, 延迟通常低于 1ms。为了提高吞吐量, etcd 将多个请求打包在一起并提交给 Raft。这个批量策略让 etcd 在重负载试获得高吞吐量。也有其他子系统影响到 etcd 的整体性能。每个序列化的 etcd 请求必须通过 etcd 的 boltdb 支持的(boltdb-backed) MVCC 存储引擎,它通常需要10微秒来完成。etcd 定期递增快照它最近实施的请求，将他们和之前在磁盘上的快照合并。这个过程可能导致延迟尖峰(latency spike)。虽然在SSD上这通常不是问题，在HDD上它可能加倍可观察到的延迟。而且，进行中的压缩可以影响 etcd 的性能。幸运的是，压缩通常无足轻重，因为压缩是错开的，因此它不和常规请求竞争资源。RPC 系统，gRPC，为 etcd 提供定义良好，可扩展的 API，但是它也引入了额外的延迟，尤其是本地读取。</p>\n<p>Etcd 的默认配置在本地网络环境（localhost）下通常能够运行的很好，因为延迟很低。然而，当跨数据中心部署 Etcd 或网络延时很高时，etcd 的心跳间隔或选举超时时间等参数需要根据实际情况进行调整。</p>\n<p>网络并不是导致延时的唯一来源。不论是 Follower 还是 Leader，其请求和响应都受磁盘 I/O 延时的影响。每个 timeout 都代表从请求发起到成功返回响应的总时间。</p>\n<h4 id=\"时间参数\"><a href=\"#时间参数\" class=\"headerlink\" title=\"时间参数\"></a>时间参数</h4><p>Etcd 底层的分布式一致性协议依赖两个时间参数来保证节点之间能够在部分节点掉钱的情况下依然能够正确处理主节点的选举。第一个参数就是所谓的心跳间隔，即主节点通知从节点它还是领导者的频率。实践数据表明，该参数应该设置成节点之间 RTT 的时间。Etcd 的心跳间隔默认是 100 毫秒。第二个参数是选举超时时间，即从节点等待多久没收到主节点的心跳就尝试去竞选领导者。Etcd 的选举超时时间默认是 1000 毫秒。</p>\n<p>调整这些参数值是有条件的，此消波长。心跳间隔值推荐设置为临近节点间 RTT 的最大值，通常是 0.5~1.5 倍 RTT 值。如果心跳间隔设得太短，那么 Etcd 就会发送没必要的心跳信息，从而增加 CPU 和网络资源的消耗；如果设得太长，就会导致选举等待时间的超时。如果选举等待时间设置的过长，就会导致节点异常检测时间过长。评估 RTT 值的最简单的方法是使用 ping 的操作。</p>\n<p>选举超时时间应该基于心跳间隔和节点之间的平均 RTT 值。选举超时必须至少是 RTT 10 倍的时间以便对网络波动。例如，如果 RTT 的值是 10 毫秒，那么选举超时时间必须至少是 100 毫秒。选举超时时间的上线是 50000 毫秒（50 秒），这个时间只能只用于全球范围内分布式部署的 Etcd 集群。美国大陆的一个 RTT 的合理时间大约是 130 毫秒，美国和日本的 RTT 大约是 350~400 毫秒。如果算上网络波动和重试的时间，那么 5 秒是一次全球 RTT 的安全上线。因为选举超时时间应该是心跳包广播时间的 10 倍，所以 50 秒的选举超时时间是全局分布式部署 Etcd 的合理上线值。</p>\n<p>心跳间隔和选举超时时间的值对同一个 Etcd 集群的所有节点都生效，如果各个节点都不同的话，就会导致集群发生不可预知的不稳定性。Etcd 启动时通过传入启动参数或环境变量覆盖默认值，单位是毫秒。示例代码具体如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcd --heartbeat-interval=100 --election-timeout=500</span><br><span class=\"line\"></span><br><span class=\"line\"># 环境变量值</span><br><span class=\"line\">$ ETCD_HEARTBEAT_INTERVAL=100 ETCD_ELECTION_TIMEOUT=500 etcd</span><br></pre></td></tr></table></figure>\n<h4 id=\"快照\"><a href=\"#快照\" class=\"headerlink\" title=\"快照\"></a>快照</h4><p>Etcd 总是向日志文件中追加 key，这样一来，日志文件会随着 key 的改动而线性增长。当 Etcd 集群使用较少时，保存完整的日志历史记录是没问题的，但如果 Etcd 集群规模比较大时，那么集群就会携带很大的日志文件。为了避免携带庞大的日志文件，Etcd 需要做周期性的快照。快照提供了一种通过保存系统的当前状态并移除旧日志文件的方式来压缩日志文件。</p>\n<h5 id=\"快照调优\"><a href=\"#快照调优\" class=\"headerlink\" title=\"快照调优\"></a>快照调优</h5><p>为 v2 后端存储创建快照的代价是很高的，所以只用当参数累积到一定的数量时，Etcd 才会创建快照文件。默认情况下，修改数量达到 10000 时才会建立快照。如果 Etcd 的内存使用和磁盘使用过高，那么应该尝试调低快照触发的阈值，具体请参考如下命令。</p>\n<p>启动参数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ etcd --snapshot-count=5000</span><br></pre></td></tr></table></figure>\n<p>环境变量：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ETCD_SNAPSHOT_COUNT=5000 etcd</span><br></pre></td></tr></table></figure>\n<h4 id=\"磁盘\"><a href=\"#磁盘\" class=\"headerlink\" title=\"磁盘\"></a>磁盘</h4><p>etcd 的存储目录分为 snapshot 和 wal，他们写入的方式是不同的，snapshot 是内存直接 dump file。而 wal 是顺序追加写，对于这两种方式系统调优的方式是不同的，snapshot 可以通过增加 io 平滑写来提高磁盘 io 能力，而 wal 可以通过降低 pagecache 的方式提前写入时序。因此对于不同的场景，可以考虑将 snap 与 wal 进行分盘，放在两块 SSD 盘上，提高整体的 IO 效率，这种方式可以提升etcd 20%左右的性能。</p>\n<p>etcd 集群对磁盘 I/O 的延时非常敏感，因为 Etcd 必须持久化它的日志，当其他 I/O 密集型的进程也在占用磁盘 I/O 的带宽时，就会导致 fsync 时延非常高。这将导致 Etcd 丢失心跳包、请求超时或暂时性的 Leader 丢失。这时可以适当为 Etcd 服务赋予更高的磁盘 I/O 权限，让 Etcd 更稳定的运行。在 Linux 系统中，磁盘 I/O 权限可以通过 ionice 命令进行调整。</p>\n<p>nux 默认 IO 调度器使用 CFQ 调度算法，支持用 ionice 命令为程序指定 IO 调度策略和优先级，IO 调度策略分为三种：</p>\n<ul>\n<li>Idle ：其他进程没有磁盘 IO 时，才进行磁盘 IO</li>\n<li>Best Effort：缺省调度策略，可以设置0-7的优先级，数值越小优先级越高，同优先级的进程采用 round-robin算法调度；</li>\n<li>Real Time ：立即访问磁盘，无视其它进程 IO</li>\n<li>None 即Best Effort，进程未指定策略和优先级时显示为none，会使用依据cpu nice设置计算出优先级</li>\n</ul>\n<p>Linux 中 etcd  的磁盘优先级可以使用 <code>ionice</code> 配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ ionice -c2 -n0 -p `pgrep etcd`</span><br></pre></td></tr></table></figure>\n<h4 id=\"网络\"><a href=\"#网络\" class=\"headerlink\" title=\"网络\"></a>网络</h4><p>etcd 中比较复杂的是网络的调优，因此大量的网络请求会在 peer 节点之间转发，而且整体网络吞吐也很大，但是还是再次强调不建议大家调整系统参数，大家可以通过修改 etcd 的 <code>--heartbeat-interval</code> 与 <code>--election-timeout</code> 启动参数来适当提高高吞吐网络下 etcd 的集群鲁棒性，通常同步吞吐在100MB左右的集群可以考虑将 <code>--heartbeat-interval</code> 设置为 300ms-500ms，<code>--election-timeout</code> 可以设置在 5000ms 左右。此外官方还有基于 TC 的网络优先传输方案，也是一个比较适用的调优手段。</p>\n<p>如果 etcd 的 Leader 服务大量并发客户端，这就会导致 follower 的请求的处理被延迟因为网络延迟。follower 的send buffer中能看到错误的列表，如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dropped MsgProp to 247ae21ff9436b2d since streamMsg&apos;s sending buffer is full</span><br><span class=\"line\"></span><br><span class=\"line\">dropped MsgAppResp to 247ae21ff9436b2d since streamMsg&apos;s sending buffer is full</span><br></pre></td></tr></table></figure>\n<p>这些错误可以通过提高 Leader 的网络优先级来提高 follower 的请求的响应。可以通过流量控制机制来提高:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 针对 2379、2380 端口放行</span><br><span class=\"line\">$ tc qdisc add dev eth0 root handle 1: prio bands 3</span><br><span class=\"line\">$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip sport 2380 0xffff flowid 1:1</span><br><span class=\"line\">$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip dport 2380 0xffff flowid 1:1</span><br><span class=\"line\">$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip sport 2379 0xffff flowid 1:1</span><br><span class=\"line\">$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip dport 2379 0xffff flowid 1:1</span><br><span class=\"line\"></span><br><span class=\"line\">// 查看现有的队列</span><br><span class=\"line\">$ tc -s qdisc ls dev enp0s8</span><br><span class=\"line\">qdisc prio 1: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1</span><br><span class=\"line\"> Sent 258578 bytes 923 pkt (dropped 0, overlimits 0 requeues 0)</span><br><span class=\"line\"> backlog 0b 0p requeues 0</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 删除队列</span><br><span class=\"line\">$ tc qdisc del dev enp0s8 root</span><br></pre></td></tr></table></figure>\n<h4 id=\"数据规模\"><a href=\"#数据规模\" class=\"headerlink\" title=\"数据规模\"></a>数据规模</h4><p>etcd 的硬盘存储上限（默认是 2GB）,当 etcd 数据量超过默认 quota 值后便不再接受写请求，可以通过设置 <code>--quota-backend-bytes</code> 参数来增加存储大小,<code>quota-backend-bytes</code> 默认值为 0，即使用默认 quota 为 2GB，上限值为 8 GB，具体说明可参考官方文档：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/dev-guide/limit.md\" target=\"_blank\" rel=\"noopener\">dev-guide/limit.md</a>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">The default storage size limit is 2GB, configurable with `--quota-backend-bytes` flag. 8GB is a suggested maximum size for normal environments and etcd warns at startup if the configured value exceeds it.</span><br></pre></td></tr></table></figure>\n<p>以下摘自 <a href=\"https://mp.weixin.qq.com/s/skjNwU6Rdsn2qWN2KHU9zg\" target=\"_blank\" rel=\"noopener\">当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？</a></p>\n<blockquote>\n<p>阿里进行了深入研究了 etcd 内部的实现原理，并发现了影响 etcd 扩展性的一个关键问题在底层 bbolt db 的 page 页面分配算法上：随着 etcd 中存储的数据量的增长，bbolt db 中线性查找“连续长度为 n 的 page 存储页面”的性能显著下降。</p>\n<p>为了解决该问题，我们设计了基于 segregrated hashmap 的空闲页面管理算法，hashmap 以连续 page 大小为 key, 连续页面起始 page id 为  value。通过查这个 segregrated hashmap 实现 O(1) 的空闲 page 查找，极大地提高了性能。在释放块时，新算法尝试和地址相邻的 page 合并，并更新 segregrated hashmap。更详细的算法分析可以见已发表在<a href=\"https://www.cncf.io/blog/2019/05/09/performance-optimization-of-etcd-in-web-scale-data-scenario/\" target=\"_blank\" rel=\"noopener\">CNCF 博客的博文</a>。</p>\n<p>通过这个算法改进，我们可以将 etcd 的存储空间从推荐的 2GB 扩展到 100GB，极大地提高了 etcd 存储数据的规模，并且读写无显著延迟增长。</p>\n<p>pull request ： <a href=\"https://github.com/etcd-io/bbolt/pull/141\" target=\"_blank\" rel=\"noopener\">https://github.com/etcd-io/bbolt/pull/141 </a></p>\n</blockquote>\n<p>目前社区已发布的 v3.4 系列版本并没有说明支持数据规模可达 100 G。</p>\n<h3 id=\"etcd-性能测试\"><a href=\"#etcd-性能测试\" class=\"headerlink\" title=\"etcd 性能测试\"></a>etcd 性能测试</h3><blockquote>\n<p>测试环境：本机 mac 使用 virtualbox 安装 vm，所有 etcd 实例都是运行在在 vm 中的 docker 上</p>\n</blockquote>\n<p>参考官方文档：<a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md\" target=\"_blank\" rel=\"noopener\">https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md</a></p>\n<p>安装 etcd 压测工具 <a href=\"https://github.com/etcd-io/etcd/tree/master/tools/benchmark\" target=\"_blank\" rel=\"noopener\">benchmark</a>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ go get go.etcd.io/etcd/tools/benchmark</span><br><span class=\"line\"># GOPATH should be set</span><br><span class=\"line\">$ ls $GOPATH/bin</span><br><span class=\"line\">benchmark</span><br></pre></td></tr></table></figure>\n<p>本文仅对 etcd v3.3.10 以及 v3.4.1 进行压测。</p>\n<h4 id=\"部署-etcd-集群\"><a href=\"#部署-etcd-集群\" class=\"headerlink\" title=\"部署 etcd 集群\"></a>部署 etcd 集群</h4><p>以下为脚本示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">docker ps -a | grep etcd | grep -v k8s</span><br><span class=\"line\">docker rm -f etcd</span><br><span class=\"line\"></span><br><span class=\"line\">ETCD_VERSION=3.3.10</span><br><span class=\"line\">TOKEN=my-etcd-token</span><br><span class=\"line\">CLUSTER_STATE=new</span><br><span class=\"line\">NAME_1=etcd-node-0</span><br><span class=\"line\">NAME_2=etcd-node-1</span><br><span class=\"line\">NAME_3=etcd-node-2</span><br><span class=\"line\">HOST_1=192.168.74.36</span><br><span class=\"line\">HOST_2=192.168.74.36</span><br><span class=\"line\">HOST_3=192.168.74.36</span><br><span class=\"line\">CLUSTER=$&#123;NAME_1&#125;=http://$&#123;HOST_1&#125;:23801,$&#123;NAME_2&#125;=http://$&#123;HOST_2&#125;:23802,$&#123;NAME_3&#125;=http://$&#123;HOST_3&#125;:23803</span><br><span class=\"line\"></span><br><span class=\"line\"># 对于节点1</span><br><span class=\"line\">THIS_NAME=$&#123;NAME_1&#125;</span><br><span class=\"line\">THIS_IP=$&#123;HOST_1&#125;</span><br><span class=\"line\">sudo docker run -d --net=host --name $&#123;THIS_NAME&#125; k8s.gcr.io/etcd:$&#123;ETCD_VERSION&#125; \\</span><br><span class=\"line\">    /usr/local/bin/etcd \\</span><br><span class=\"line\">    --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \\</span><br><span class=\"line\">    --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:23801 --listen-peer-urls http://$&#123;THIS_IP&#125;:23801 \\</span><br><span class=\"line\">    --advertise-client-urls http://$&#123;THIS_IP&#125;:23791 --listen-client-urls http://$&#123;THIS_IP&#125;:23791 \\</span><br><span class=\"line\">    --initial-cluster $&#123;CLUSTER&#125; \\</span><br><span class=\"line\">    --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># 对于节点2</span><br><span class=\"line\">THIS_NAME=$&#123;NAME_2&#125;</span><br><span class=\"line\">THIS_IP=$&#123;HOST_2&#125;</span><br><span class=\"line\">sudo docker run -d --net=host --name $&#123;THIS_NAME&#125; k8s.gcr.io/etcd:$&#123;ETCD_VERSION&#125; \\</span><br><span class=\"line\">    /usr/local/bin/etcd \\</span><br><span class=\"line\">    --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \\</span><br><span class=\"line\">    --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:23802 --listen-peer-urls http://$&#123;THIS_IP&#125;:23802 \\</span><br><span class=\"line\">    --advertise-client-urls http://$&#123;THIS_IP&#125;:23792 --listen-client-urls http://$&#123;THIS_IP&#125;:23792 \\</span><br><span class=\"line\">    --initial-cluster $&#123;CLUSTER&#125; \\</span><br><span class=\"line\">    --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># 对于节点3</span><br><span class=\"line\">THIS_NAME=$&#123;NAME_3&#125;</span><br><span class=\"line\">THIS_IP=$&#123;HOST_3&#125;</span><br><span class=\"line\">sudo docker run -d --net=host --name $&#123;THIS_NAME&#125; k8s.gcr.io/etcd:$&#123;ETCD_VERSION&#125; \\</span><br><span class=\"line\">    /usr/local/bin/etcd \\</span><br><span class=\"line\">    --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \\</span><br><span class=\"line\">    --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:23803 --listen-peer-urls http://$&#123;THIS_IP&#125;:23803 \\</span><br><span class=\"line\">    --advertise-client-urls http://$&#123;THIS_IP&#125;:23793 --listen-client-urls http://$&#123;THIS_IP&#125;:23793 \\</span><br><span class=\"line\">    --initial-cluster $&#123;CLUSTER&#125; \\</span><br><span class=\"line\">    --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"压测\"><a href=\"#压测\" class=\"headerlink\" title=\"压测\"></a>压测</h4><p>本文主要对不同场景下 etcd 的读写操作进行测试，尽管环境有限，但在不同场景下 etcd 的表现还是有区别的。对于写入测试，按照官方文档的测试方法指定不同数量的客户端和连接数以及 key 的大小，对于读取操作，分别测试了线性化读取以及串行化读取，由于 etcd 是强一致性的，其默认读取测试就是线性化读取。</p>\n<h5 id=\"etcd-v3-3-10\"><a href=\"#etcd-v3-3-10\" class=\"headerlink\" title=\"etcd v3.3.10\"></a>etcd v3.3.10</h5><p><strong>写入测试</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看 leader</span><br><span class=\"line\">$ etcdctl member list</span><br><span class=\"line\"></span><br><span class=\"line\">// leader</span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 所有 members</span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>key 数量</th>\n<th>Key 大小</th>\n<th>Value的大小</th>\n<th>连接数量</th>\n<th>客户端数量</th>\n<th>目标 etcd 服务器</th>\n<th>平均写入 QPS</th>\n<th>每请求平均延迟</th>\n<th>Average server RSS</th>\n<th>调整磁盘IO优先级</th>\n<th>调整网络带宽和优先级</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>75</td>\n<td>50.0ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>77</td>\n<td>46.5ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>1144</td>\n<td>1697.5ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>1185</td>\n<td>1541.8ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>73</td>\n<td>49.6ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>80</td>\n<td>48.5ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>1132</td>\n<td>1649.1ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>1198</td>\n<td>1536.8ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n</tbody>\n</table>\n<p><strong>读取测试</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=1 --clients=1  range foo --consistency=l --total=10000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=1 --clients=1  range foo --consistency=s --total=10000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=100 --clients=1000  range foo --consistency=l --total=100000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=100 --clients=1000  range foo --consistency=s --total=100000</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>key 数量</th>\n<th>Key 大小</th>\n<th>Value的大小</th>\n<th>连接数量</th>\n<th>客户端数量</th>\n<th>一致性(线性化/串行化)</th>\n<th>每请求平均延迟</th>\n<th>平均读取 QPS</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>Linearizable</td>\n<td>11.5ms</td>\n<td>740</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>Serializable</td>\n<td>3.5ms</td>\n<td>2146</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>Linearizable</td>\n<td>647.3ms</td>\n<td>3376</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>Serializable</td>\n<td>546.9ms</td>\n<td>4060</td>\n</tr>\n</tbody>\n</table>\n<h5 id=\"etcd-v3-4-1\"><a href=\"#etcd-v3-4-1\" class=\"headerlink\" title=\"etcd v3.4.1\"></a>etcd v3.4.1</h5><p><strong>写入测试</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看 etcd leader</span><br><span class=\"line\">$ etcdctl  --write-out=table --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23801&quot; endpoint status</span><br><span class=\"line\"></span><br><span class=\"line\">// leader</span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 所有 members</span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>key 数量</th>\n<th>Key 大小</th>\n<th>Value的大小</th>\n<th>连接数量</th>\n<th>客户端数量</th>\n<th>目标 etcd 服务器</th>\n<th>平均写入 QPS</th>\n<th>每请求平均延迟</th>\n<th>Average server RSS</th>\n<th>调整磁盘IO优先级</th>\n<th>调整网络带宽和优先级</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>75</td>\n<td>50.0ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>322</td>\n<td>13.2ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>只有主</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>1871</td>\n<td>1207.7ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>2239</td>\n<td>992.4ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>只有主</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>326</td>\n<td>13.4ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>352</td>\n<td>12.6ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>所有 members</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>1132</td>\n<td>1649.1ms</td>\n<td>-</td>\n<td>否</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>1198</td>\n<td>1536.8ms</td>\n<td>-</td>\n<td>是</td>\n<td>否</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>all members</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n<td>是</td>\n<td>是</td>\n</tr>\n</tbody>\n</table>\n<p><strong>读取测试</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=1 --clients=1  range foo --consistency=l --total=10000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=1 --clients=1  range foo --consistency=s --total=10000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=100 --clients=1000  range foo --consistency=l --total=100000</span><br><span class=\"line\"></span><br><span class=\"line\">$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot;  --conns=100 --clients=1000  range foo --consistency=s --total=100000</span><br></pre></td></tr></table></figure>\n<table>\n<thead>\n<tr>\n<th>key 数量</th>\n<th>Key 大小</th>\n<th>Value的大小</th>\n<th>连接数量</th>\n<th>客户端数量</th>\n<th>一致性(线性化/串行化)</th>\n<th>每请求平均延迟(99%)</th>\n<th>平均读取 QPS</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>Linearizable</td>\n<td>36.2ms</td>\n<td>319</td>\n</tr>\n<tr>\n<td>10,000</td>\n<td>8</td>\n<td>256</td>\n<td>1</td>\n<td>1</td>\n<td>Serializable</td>\n<td>34.4ms</td>\n<td>916</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>Linearizable</td>\n<td>1302.7ms</td>\n<td>1680</td>\n</tr>\n<tr>\n<td>100,000</td>\n<td>8</td>\n<td>256</td>\n<td>100</td>\n<td>1000</td>\n<td>Serializable</td>\n<td>1097.6ms</td>\n<td>2401</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>由于仅在本地进行测试，所受网络带宽影响不大，所以仅调整 io。</p>\n</blockquote>\n<h3 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h3><p>可以看到，测试结果中写入操作与以上列出的几种因素关联比较大。读取指标的时候，串行化要比线性化要好，但为了一致性，线性化(Linearizable)读取请求要通过集群成员的法定人数来获取最新的数据。串行化(Serializable)读取请求比线性化读取要廉价一些，因为他们是通过任意单台 etcd 服务器来提供服务，而不是成员的法定人数，代价是可能提供过期数据。</p>\n<p>本文在力所能及的范围内对 etcd 的性能进行了一定的评估，所得到的数据并不能作为最终的参考数据，应当根据自己的环境进行评估，结合以上性能优化的方法得到最终的结论。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/maemual/raft-zh_cn\" target=\"_blank\" rel=\"noopener\">Raft一致性算法论文的中文翻译</a></p>\n<p><a href=\"https://www.infoq.cn/article/Dit9YCy2-ziDrLfFeQzq\" target=\"_blank\" rel=\"noopener\">etcd 在超大规模数据场景下的性能优化</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/skjNwU6Rdsn2qWN2KHU9zg\" target=\"_blank\" rel=\"noopener\">当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？</a></p>\n<p><a href=\"https://yq.aliyun.com/articles/388546\" target=\"_blank\" rel=\"noopener\">Everything you should know about etcd</a></p>\n<p><a href=\"http://dockone.io/article/801\" target=\"_blank\" rel=\"noopener\">etcd2 与 etcd3 相比</a></p>\n<p><a href=\"https://alexstocks.github.io/html/etcd.html\" target=\"_blank\" rel=\"noopener\">etcd使用经验总结</a></p>\n<p><a href=\"https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md\" target=\"_blank\" rel=\"noopener\">Understanding performance</a></p>\n"},{"title":"etcd 备份与恢复","date":"2017-03-02T10:04:00.000Z","type":"etcd","_content":"\n**[etcd](https://github.com/coreos/etcd)** 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。\n\netcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。\n\n    # etcdctl --version\n    etcdctl version: 3.0.4\n    API version: 2\n\n\n官方指出 etcd v2 和 v3 的数据不能混合存放，[support backup of v2 and v3 stores](https://github.com/coreos/etcd/issues/7002) 。\n\n\n**特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复\n若使用 v2 备份数据时存在 v3 的数据则恢复失败**\n\n### 对于 API 2 备份与恢复方法   \n[官方 v2 admin guide](https://github.com/coreos/etcd/blob/master/Documentation/v2/admin_guide.md#disaster-recovery)\n\n\netcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中：\n* snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。\n\n* wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。\n\n\n    # etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup\n\n    # etcd -data-dir=/home/etcd_backup/  -force-new-cluster\n\n\n恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。\n\n### 对于 API 3 备份与恢复方法  \n[官方 v3 admin guide](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md)\n\n在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。\n\n在命令行设置：\n\n\t# export ETCDCTL_API=3\n\t\n备份数据：\n\n\t# etcdctl --endpoints localhost:2379 snapshot save snapshot.db\n\n恢复：\n\n\t# etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data\n\n> 恢复后的文件需要修改权限为 etcd:etcd\n> --name:重新指定一个数据目录，可以不指定，默认为 default.etcd\n> --data-dir：指定数据目录\n> 建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir\n\netcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 \n\n[官方文档翻译](https://skyao.gitbooks.io/leaning-etcd3/content/documentation/op-guide/recovery.html)\n","source":"_posts/etcd-backup.md","raw":"---\ntitle: etcd 备份与恢复\ndate: 2017-03-02 18:04:00\ntype: \"etcd\"\n\n---\n\n**[etcd](https://github.com/coreos/etcd)** 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。\n\netcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。\n\n    # etcdctl --version\n    etcdctl version: 3.0.4\n    API version: 2\n\n\n官方指出 etcd v2 和 v3 的数据不能混合存放，[support backup of v2 and v3 stores](https://github.com/coreos/etcd/issues/7002) 。\n\n\n**特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复\n若使用 v2 备份数据时存在 v3 的数据则恢复失败**\n\n### 对于 API 2 备份与恢复方法   \n[官方 v2 admin guide](https://github.com/coreos/etcd/blob/master/Documentation/v2/admin_guide.md#disaster-recovery)\n\n\netcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中：\n* snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。\n\n* wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。\n\n\n    # etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup\n\n    # etcd -data-dir=/home/etcd_backup/  -force-new-cluster\n\n\n恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。\n\n### 对于 API 3 备份与恢复方法  \n[官方 v3 admin guide](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md)\n\n在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。\n\n在命令行设置：\n\n\t# export ETCDCTL_API=3\n\t\n备份数据：\n\n\t# etcdctl --endpoints localhost:2379 snapshot save snapshot.db\n\n恢复：\n\n\t# etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data\n\n> 恢复后的文件需要修改权限为 etcd:etcd\n> --name:重新指定一个数据目录，可以不指定，默认为 default.etcd\n> --data-dir：指定数据目录\n> 建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir\n\netcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 \n\n[官方文档翻译](https://skyao.gitbooks.io/leaning-etcd3/content/documentation/op-guide/recovery.html)\n","slug":"etcd-backup","published":1,"updated":"2019-06-01T14:26:16.307Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro5970007apwnv29b0g4u","content":"<p><strong><a href=\"https://github.com/coreos/etcd\" target=\"_blank\" rel=\"noopener\">etcd</a></strong> 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。</p>\n<p>etcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。</p>\n<pre><code># etcdctl --version\netcdctl version: 3.0.4\nAPI version: 2\n</code></pre><p>官方指出 etcd v2 和 v3 的数据不能混合存放，<a href=\"https://github.com/coreos/etcd/issues/7002\" target=\"_blank\" rel=\"noopener\">support backup of v2 and v3 stores</a> 。</p>\n<p><strong>特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复<br>若使用 v2 备份数据时存在 v3 的数据则恢复失败</strong></p>\n<h3 id=\"对于-API-2-备份与恢复方法\"><a href=\"#对于-API-2-备份与恢复方法\" class=\"headerlink\" title=\"对于 API 2 备份与恢复方法\"></a>对于 API 2 备份与恢复方法</h3><p><a href=\"https://github.com/coreos/etcd/blob/master/Documentation/v2/admin_guide.md#disaster-recovery\" target=\"_blank\" rel=\"noopener\">官方 v2 admin guide</a></p>\n<p>etcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中：</p>\n<ul>\n<li><p>snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。</p>\n</li>\n<li><p>wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。</p>\n</li>\n</ul>\n<pre><code># etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup\n\n# etcd -data-dir=/home/etcd_backup/  -force-new-cluster\n</code></pre><p>恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。</p>\n<h3 id=\"对于-API-3-备份与恢复方法\"><a href=\"#对于-API-3-备份与恢复方法\" class=\"headerlink\" title=\"对于 API 3 备份与恢复方法\"></a>对于 API 3 备份与恢复方法</h3><p><a href=\"https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">官方 v3 admin guide</a></p>\n<p>在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。</p>\n<p>在命令行设置：</p>\n<pre><code># export ETCDCTL_API=3\n</code></pre><p>备份数据：</p>\n<pre><code># etcdctl --endpoints localhost:2379 snapshot save snapshot.db\n</code></pre><p>恢复：</p>\n<pre><code># etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data\n</code></pre><blockquote>\n<p>恢复后的文件需要修改权限为 etcd:etcd<br>–name:重新指定一个数据目录，可以不指定，默认为 default.etcd<br>–data-dir：指定数据目录<br>建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir</p>\n</blockquote>\n<p>etcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 </p>\n<p><a href=\"https://skyao.gitbooks.io/leaning-etcd3/content/documentation/op-guide/recovery.html\" target=\"_blank\" rel=\"noopener\">官方文档翻译</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p><strong><a href=\"https://github.com/coreos/etcd\" target=\"_blank\" rel=\"noopener\">etcd</a></strong> 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。</p>\n<p>etcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。</p>\n<pre><code># etcdctl --version\netcdctl version: 3.0.4\nAPI version: 2\n</code></pre><p>官方指出 etcd v2 和 v3 的数据不能混合存放，<a href=\"https://github.com/coreos/etcd/issues/7002\" target=\"_blank\" rel=\"noopener\">support backup of v2 and v3 stores</a> 。</p>\n<p><strong>特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复<br>若使用 v2 备份数据时存在 v3 的数据则恢复失败</strong></p>\n<h3 id=\"对于-API-2-备份与恢复方法\"><a href=\"#对于-API-2-备份与恢复方法\" class=\"headerlink\" title=\"对于 API 2 备份与恢复方法\"></a>对于 API 2 备份与恢复方法</h3><p><a href=\"https://github.com/coreos/etcd/blob/master/Documentation/v2/admin_guide.md#disaster-recovery\" target=\"_blank\" rel=\"noopener\">官方 v2 admin guide</a></p>\n<p>etcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中：</p>\n<ul>\n<li><p>snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。</p>\n</li>\n<li><p>wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。</p>\n</li>\n</ul>\n<pre><code># etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup\n\n# etcd -data-dir=/home/etcd_backup/  -force-new-cluster\n</code></pre><p>恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。</p>\n<h3 id=\"对于-API-3-备份与恢复方法\"><a href=\"#对于-API-3-备份与恢复方法\" class=\"headerlink\" title=\"对于 API 3 备份与恢复方法\"></a>对于 API 3 备份与恢复方法</h3><p><a href=\"https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md\" target=\"_blank\" rel=\"noopener\">官方 v3 admin guide</a></p>\n<p>在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。</p>\n<p>在命令行设置：</p>\n<pre><code># export ETCDCTL_API=3\n</code></pre><p>备份数据：</p>\n<pre><code># etcdctl --endpoints localhost:2379 snapshot save snapshot.db\n</code></pre><p>恢复：</p>\n<pre><code># etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data\n</code></pre><blockquote>\n<p>恢复后的文件需要修改权限为 etcd:etcd<br>–name:重新指定一个数据目录，可以不指定，默认为 default.etcd<br>–data-dir：指定数据目录<br>建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir</p>\n</blockquote>\n<p>etcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 </p>\n<p><a href=\"https://skyao.gitbooks.io/leaning-etcd3/content/documentation/op-guide/recovery.html\" target=\"_blank\" rel=\"noopener\">官方文档翻译</a></p>\n"},{"title":"使用 Go Modules 管理依赖","date":"2019-06-22T12:49:30.000Z","type":"go module","_content":"Go Modules 是 Go 语言的一种依赖管理方式，该  feature 是在 Go 1.11 版本中出现的，由于最近在做的项目中，团队都开始使用 go module 来替代以前的 Godep，Kubernetes 也从 v1.15 开始采用 go module 来进行包管理，所以有必要了解一下 go module。go module 相比于原来的 Godep，go module 在打包、编译等多个环节上有着明显的速度优势，并且能够在任意操作系统上方便的复现依赖包，更重要的是 go module 本身的设计使得自身被其他项目引用变得更加容易，这也是 Kubernetes 项目向框架化演进的又一个重要体现。\n\n使用 go module 管理依赖后会在项目根目录下生成两个文件  go.mod 和 go.sum。\n\n`go.mod` 中会记录当前项目的所依赖，文件格式如下所示：\n\n```\nmodule github.com/gosoon/audit-webhook\n\ngo 1.12\n\nrequire (\n\tgithub.com/elastic/go-elasticsearch v0.0.0\n\tgithub.com/gorilla/mux v1.7.2\n\tgithub.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81\n)\n```\n\n`go.sum`记录每个依赖库的版本和哈希值，文件格式如下所示：\n\n```\ngithub.com/elastic/go-elasticsearch v0.0.0 h1:Pd5fqOuBxKxv83b0+xOAJDAkziWYwFinWnBO0y+TZaA=\ngithub.com/elastic/go-elasticsearch v0.0.0/go.mod h1:TkBSJBuTyFdBnrNqoPc54FN0vKf5c04IdM4zuStJ7xg=\ngithub.com/gorilla/mux v1.7.2 h1:zoNxOV7WjqXptQOVngLmcSQgXmgk4NMz1HibBchjl/I=\ngithub.com/gorilla/mux v1.7.2/go.mod h1:1lud6UwP+6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=\ngithub.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81 h1:JP0LU0ajeawW2xySrbhDqtSUfVWohZ505Q4LXo+hCmg=\ngithub.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81/go.mod h1:1e0N9vBl2wPF6qYa+JCRNIZnhxSkXkOJfD2iFw3eOfg=\n```\n\n#### 一、如何启用 go module 功能\n\n(1) go 版本 >= v1.11\n\n(2) 设置`GO111MODULE`环境变量\n\n要使用`go module` 首先要设置`GO111MODULE=on`，`GO111MODULE` 有三个值，off、on、auto，off 和 on 即关闭和开启，auto 则会根据当前目录下是否有 go.mod 文件来判断是否使用 modules 功能。无论使用哪种模式，module 功能默认不在 GOPATH 目录下查找依赖文件，所以使用 modules 功能时请设置好代理。\n\n在使用 go module 时，将 `GO111MODULE` 全局环境变量设置为 off，在需要使用的时候再开启，避免在已有项目中意外引入  go module。\n\n```\n$ echo export GO111MODULE=off >> ~/.zshrc\nor\n$ echo export GO111MODULE=off >> ~/.bashrc\n```\n\ngo mod 命令的使用：\n\n```\ndownload    download modules to local cache (下载依赖的module到本地cache))\nedit        edit go.mod from tools or scripts (编辑go.mod文件)\ngraph       print module requirement graph (打印模块依赖图))\ninit        initialize new module in current directory (在当前文件夹下初始化一个新的module, 创建go.mod文件))\ntidy        add missing and remove unused modules (增加丢失的module，去掉未使用的module)\nvendor      make vendored copy of dependencies (将依赖复制到vendor下)\nverify      verify dependencies have expected content (校验依赖)\nwhy         explain why packages or modules are needed (解释为什么需要依赖)\n```\n\n#### 二、使用 go module 功能\n\n对于新建项目使用 go module：\n\n```\n$ export GO111MODULE=on\n\t\n$ go mod init github.com/you/hello\n\t\n...\n// go build 会将项目的依赖添加到 go.mod 中\n$ go build \n```\n\n\n\n对于已有项目要改为使用 go module：\n\n```\n$ export GO111MODULE=on\n\n// 创建一个空的 go.mod 文件\n$ go mod init .\n\n// 查找依赖并记录在 go.mod 文件中\n$ go get ./...\n\n```\n\n> go.mod 文件必须要提交到 git 仓库，但 go.sum 文件可以不用提交到 git 仓库(gi t忽略文件 .gitignore 中设置一下)。\n\n\n#### 三、项目的打包\n\n首先需要使用 `go mod vendor` 将项目所有的依赖下载到本地 vendor 目录中然后进行编译，下面是一个参考： \n\n```\n#!/bin/bash\n\nexport GO111MODULE=\"on\"\nexport GOPROXY=\"https://goproxy.io\"\nexport CGO_ENABLED=\"0\"\nexport GOOS=\"linux\"\nexport GOARCH=amd64\n\ngo mod vendor\ngo build -ldflags \"-s -w\" -a -installsuffix cgo -o audit-webhook .\n```\n\n\n\n#### 四、注意事项\n\n1、依赖下载\n\ngo module 默认不在 GOPATH 目录下查找依赖文件，其首先会在`$GOPATH/pkg/mod`中查找有没有所需要的依赖，没有的直接会进行下载。可以使用 `go mod download`下载好所需要的依赖，依赖默认会下载到`$GOPATH/pkg/mod`中，其他项目也会使用缓存的 module。\n\n2、国内无法访问的依赖\n\n使用 Go 的其他包管理工具 godep、govendor、glide、dep 等都避免不了翻墙的问题，Go Modules 也是一样，但在`go.mod`中可以使用`replace`将特定的库替换成其他库：\n\n```\nreplace (\n\tgolang.org/x/text v0.3.0 => github.com/golang/text v0.3.0\n)\n```\n\n或者也可以在其他机器上使用 `go mod download`下载好所需要的依赖，然后再传输到本机。\n\n\n参考：\n\nhttps://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/2019-03-19-go-modules.md\n\nhttps://blog.golang.org/using-go-modules\n","source":"_posts/golang_modules.md","raw":"---\ntitle: 使用 Go Modules 管理依赖\ndate: 2019-06-22 20:49:30\ntags: [\"go module\"]\ntype: \"go module\"\n\n---\nGo Modules 是 Go 语言的一种依赖管理方式，该  feature 是在 Go 1.11 版本中出现的，由于最近在做的项目中，团队都开始使用 go module 来替代以前的 Godep，Kubernetes 也从 v1.15 开始采用 go module 来进行包管理，所以有必要了解一下 go module。go module 相比于原来的 Godep，go module 在打包、编译等多个环节上有着明显的速度优势，并且能够在任意操作系统上方便的复现依赖包，更重要的是 go module 本身的设计使得自身被其他项目引用变得更加容易，这也是 Kubernetes 项目向框架化演进的又一个重要体现。\n\n使用 go module 管理依赖后会在项目根目录下生成两个文件  go.mod 和 go.sum。\n\n`go.mod` 中会记录当前项目的所依赖，文件格式如下所示：\n\n```\nmodule github.com/gosoon/audit-webhook\n\ngo 1.12\n\nrequire (\n\tgithub.com/elastic/go-elasticsearch v0.0.0\n\tgithub.com/gorilla/mux v1.7.2\n\tgithub.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81\n)\n```\n\n`go.sum`记录每个依赖库的版本和哈希值，文件格式如下所示：\n\n```\ngithub.com/elastic/go-elasticsearch v0.0.0 h1:Pd5fqOuBxKxv83b0+xOAJDAkziWYwFinWnBO0y+TZaA=\ngithub.com/elastic/go-elasticsearch v0.0.0/go.mod h1:TkBSJBuTyFdBnrNqoPc54FN0vKf5c04IdM4zuStJ7xg=\ngithub.com/gorilla/mux v1.7.2 h1:zoNxOV7WjqXptQOVngLmcSQgXmgk4NMz1HibBchjl/I=\ngithub.com/gorilla/mux v1.7.2/go.mod h1:1lud6UwP+6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=\ngithub.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81 h1:JP0LU0ajeawW2xySrbhDqtSUfVWohZ505Q4LXo+hCmg=\ngithub.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81/go.mod h1:1e0N9vBl2wPF6qYa+JCRNIZnhxSkXkOJfD2iFw3eOfg=\n```\n\n#### 一、如何启用 go module 功能\n\n(1) go 版本 >= v1.11\n\n(2) 设置`GO111MODULE`环境变量\n\n要使用`go module` 首先要设置`GO111MODULE=on`，`GO111MODULE` 有三个值，off、on、auto，off 和 on 即关闭和开启，auto 则会根据当前目录下是否有 go.mod 文件来判断是否使用 modules 功能。无论使用哪种模式，module 功能默认不在 GOPATH 目录下查找依赖文件，所以使用 modules 功能时请设置好代理。\n\n在使用 go module 时，将 `GO111MODULE` 全局环境变量设置为 off，在需要使用的时候再开启，避免在已有项目中意外引入  go module。\n\n```\n$ echo export GO111MODULE=off >> ~/.zshrc\nor\n$ echo export GO111MODULE=off >> ~/.bashrc\n```\n\ngo mod 命令的使用：\n\n```\ndownload    download modules to local cache (下载依赖的module到本地cache))\nedit        edit go.mod from tools or scripts (编辑go.mod文件)\ngraph       print module requirement graph (打印模块依赖图))\ninit        initialize new module in current directory (在当前文件夹下初始化一个新的module, 创建go.mod文件))\ntidy        add missing and remove unused modules (增加丢失的module，去掉未使用的module)\nvendor      make vendored copy of dependencies (将依赖复制到vendor下)\nverify      verify dependencies have expected content (校验依赖)\nwhy         explain why packages or modules are needed (解释为什么需要依赖)\n```\n\n#### 二、使用 go module 功能\n\n对于新建项目使用 go module：\n\n```\n$ export GO111MODULE=on\n\t\n$ go mod init github.com/you/hello\n\t\n...\n// go build 会将项目的依赖添加到 go.mod 中\n$ go build \n```\n\n\n\n对于已有项目要改为使用 go module：\n\n```\n$ export GO111MODULE=on\n\n// 创建一个空的 go.mod 文件\n$ go mod init .\n\n// 查找依赖并记录在 go.mod 文件中\n$ go get ./...\n\n```\n\n> go.mod 文件必须要提交到 git 仓库，但 go.sum 文件可以不用提交到 git 仓库(gi t忽略文件 .gitignore 中设置一下)。\n\n\n#### 三、项目的打包\n\n首先需要使用 `go mod vendor` 将项目所有的依赖下载到本地 vendor 目录中然后进行编译，下面是一个参考： \n\n```\n#!/bin/bash\n\nexport GO111MODULE=\"on\"\nexport GOPROXY=\"https://goproxy.io\"\nexport CGO_ENABLED=\"0\"\nexport GOOS=\"linux\"\nexport GOARCH=amd64\n\ngo mod vendor\ngo build -ldflags \"-s -w\" -a -installsuffix cgo -o audit-webhook .\n```\n\n\n\n#### 四、注意事项\n\n1、依赖下载\n\ngo module 默认不在 GOPATH 目录下查找依赖文件，其首先会在`$GOPATH/pkg/mod`中查找有没有所需要的依赖，没有的直接会进行下载。可以使用 `go mod download`下载好所需要的依赖，依赖默认会下载到`$GOPATH/pkg/mod`中，其他项目也会使用缓存的 module。\n\n2、国内无法访问的依赖\n\n使用 Go 的其他包管理工具 godep、govendor、glide、dep 等都避免不了翻墙的问题，Go Modules 也是一样，但在`go.mod`中可以使用`replace`将特定的库替换成其他库：\n\n```\nreplace (\n\tgolang.org/x/text v0.3.0 => github.com/golang/text v0.3.0\n)\n```\n\n或者也可以在其他机器上使用 `go mod download`下载好所需要的依赖，然后再传输到本机。\n\n\n参考：\n\nhttps://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/2019-03-19-go-modules.md\n\nhttps://blog.golang.org/using-go-modules\n","slug":"golang_modules","published":1,"updated":"2019-06-22T12:49:34.781Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro5980008apwnvcp843r8","content":"<p>Go Modules 是 Go 语言的一种依赖管理方式，该  feature 是在 Go 1.11 版本中出现的，由于最近在做的项目中，团队都开始使用 go module 来替代以前的 Godep，Kubernetes 也从 v1.15 开始采用 go module 来进行包管理，所以有必要了解一下 go module。go module 相比于原来的 Godep，go module 在打包、编译等多个环节上有着明显的速度优势，并且能够在任意操作系统上方便的复现依赖包，更重要的是 go module 本身的设计使得自身被其他项目引用变得更加容易，这也是 Kubernetes 项目向框架化演进的又一个重要体现。</p>\n<p>使用 go module 管理依赖后会在项目根目录下生成两个文件  go.mod 和 go.sum。</p>\n<p><code>go.mod</code> 中会记录当前项目的所依赖，文件格式如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">module github.com/gosoon/audit-webhook</span><br><span class=\"line\"></span><br><span class=\"line\">go 1.12</span><br><span class=\"line\"></span><br><span class=\"line\">require (</span><br><span class=\"line\">\tgithub.com/elastic/go-elasticsearch v0.0.0</span><br><span class=\"line\">\tgithub.com/gorilla/mux v1.7.2</span><br><span class=\"line\">\tgithub.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p><code>go.sum</code>记录每个依赖库的版本和哈希值，文件格式如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">github.com/elastic/go-elasticsearch v0.0.0 h1:Pd5fqOuBxKxv83b0+xOAJDAkziWYwFinWnBO0y+TZaA=</span><br><span class=\"line\">github.com/elastic/go-elasticsearch v0.0.0/go.mod h1:TkBSJBuTyFdBnrNqoPc54FN0vKf5c04IdM4zuStJ7xg=</span><br><span class=\"line\">github.com/gorilla/mux v1.7.2 h1:zoNxOV7WjqXptQOVngLmcSQgXmgk4NMz1HibBchjl/I=</span><br><span class=\"line\">github.com/gorilla/mux v1.7.2/go.mod h1:1lud6UwP+6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=</span><br><span class=\"line\">github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81 h1:JP0LU0ajeawW2xySrbhDqtSUfVWohZ505Q4LXo+hCmg=</span><br><span class=\"line\">github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81/go.mod h1:1e0N9vBl2wPF6qYa+JCRNIZnhxSkXkOJfD2iFw3eOfg=</span><br></pre></td></tr></table></figure>\n<h4 id=\"一、如何启用-go-module-功能\"><a href=\"#一、如何启用-go-module-功能\" class=\"headerlink\" title=\"一、如何启用 go module 功能\"></a>一、如何启用 go module 功能</h4><p>(1) go 版本 &gt;= v1.11</p>\n<p>(2) 设置<code>GO111MODULE</code>环境变量</p>\n<p>要使用<code>go module</code> 首先要设置<code>GO111MODULE=on</code>，<code>GO111MODULE</code> 有三个值，off、on、auto，off 和 on 即关闭和开启，auto 则会根据当前目录下是否有 go.mod 文件来判断是否使用 modules 功能。无论使用哪种模式，module 功能默认不在 GOPATH 目录下查找依赖文件，所以使用 modules 功能时请设置好代理。</p>\n<p>在使用 go module 时，将 <code>GO111MODULE</code> 全局环境变量设置为 off，在需要使用的时候再开启，避免在已有项目中意外引入  go module。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo export GO111MODULE=off &gt;&gt; ~/.zshrc</span><br><span class=\"line\">or</span><br><span class=\"line\">$ echo export GO111MODULE=off &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>\n<p>go mod 命令的使用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">download    download modules to local cache (下载依赖的module到本地cache))</span><br><span class=\"line\">edit        edit go.mod from tools or scripts (编辑go.mod文件)</span><br><span class=\"line\">graph       print module requirement graph (打印模块依赖图))</span><br><span class=\"line\">init        initialize new module in current directory (在当前文件夹下初始化一个新的module, 创建go.mod文件))</span><br><span class=\"line\">tidy        add missing and remove unused modules (增加丢失的module，去掉未使用的module)</span><br><span class=\"line\">vendor      make vendored copy of dependencies (将依赖复制到vendor下)</span><br><span class=\"line\">verify      verify dependencies have expected content (校验依赖)</span><br><span class=\"line\">why         explain why packages or modules are needed (解释为什么需要依赖)</span><br></pre></td></tr></table></figure>\n<h4 id=\"二、使用-go-module-功能\"><a href=\"#二、使用-go-module-功能\" class=\"headerlink\" title=\"二、使用 go module 功能\"></a>二、使用 go module 功能</h4><p>对于新建项目使用 go module：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ export GO111MODULE=on</span><br><span class=\"line\">\t</span><br><span class=\"line\">$ go mod init github.com/you/hello</span><br><span class=\"line\">\t</span><br><span class=\"line\">...</span><br><span class=\"line\">// go build 会将项目的依赖添加到 go.mod 中</span><br><span class=\"line\">$ go build</span><br></pre></td></tr></table></figure>\n<p>对于已有项目要改为使用 go module：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ export GO111MODULE=on</span><br><span class=\"line\"></span><br><span class=\"line\">// 创建一个空的 go.mod 文件</span><br><span class=\"line\">$ go mod init .</span><br><span class=\"line\"></span><br><span class=\"line\">// 查找依赖并记录在 go.mod 文件中</span><br><span class=\"line\">$ go get ./...</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>go.mod 文件必须要提交到 git 仓库，但 go.sum 文件可以不用提交到 git 仓库(gi t忽略文件 .gitignore 中设置一下)。</p>\n</blockquote>\n<h4 id=\"三、项目的打包\"><a href=\"#三、项目的打包\" class=\"headerlink\" title=\"三、项目的打包\"></a>三、项目的打包</h4><p>首先需要使用 <code>go mod vendor</code> 将项目所有的依赖下载到本地 vendor 目录中然后进行编译，下面是一个参考： </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">export GO111MODULE=&quot;on&quot;</span><br><span class=\"line\">export GOPROXY=&quot;https://goproxy.io&quot;</span><br><span class=\"line\">export CGO_ENABLED=&quot;0&quot;</span><br><span class=\"line\">export GOOS=&quot;linux&quot;</span><br><span class=\"line\">export GOARCH=amd64</span><br><span class=\"line\"></span><br><span class=\"line\">go mod vendor</span><br><span class=\"line\">go build -ldflags &quot;-s -w&quot; -a -installsuffix cgo -o audit-webhook .</span><br></pre></td></tr></table></figure>\n<h4 id=\"四、注意事项\"><a href=\"#四、注意事项\" class=\"headerlink\" title=\"四、注意事项\"></a>四、注意事项</h4><p>1、依赖下载</p>\n<p>go module 默认不在 GOPATH 目录下查找依赖文件，其首先会在<code>$GOPATH/pkg/mod</code>中查找有没有所需要的依赖，没有的直接会进行下载。可以使用 <code>go mod download</code>下载好所需要的依赖，依赖默认会下载到<code>$GOPATH/pkg/mod</code>中，其他项目也会使用缓存的 module。</p>\n<p>2、国内无法访问的依赖</p>\n<p>使用 Go 的其他包管理工具 godep、govendor、glide、dep 等都避免不了翻墙的问题，Go Modules 也是一样，但在<code>go.mod</code>中可以使用<code>replace</code>将特定的库替换成其他库：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">replace (</span><br><span class=\"line\">\tgolang.org/x/text v0.3.0 =&gt; github.com/golang/text v0.3.0</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p>或者也可以在其他机器上使用 <code>go mod download</code>下载好所需要的依赖，然后再传输到本机。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/2019-03-19-go-modules.md\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/2019-03-19-go-modules.md</a></p>\n<p><a href=\"https://blog.golang.org/using-go-modules\" target=\"_blank\" rel=\"noopener\">https://blog.golang.org/using-go-modules</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>Go Modules 是 Go 语言的一种依赖管理方式，该  feature 是在 Go 1.11 版本中出现的，由于最近在做的项目中，团队都开始使用 go module 来替代以前的 Godep，Kubernetes 也从 v1.15 开始采用 go module 来进行包管理，所以有必要了解一下 go module。go module 相比于原来的 Godep，go module 在打包、编译等多个环节上有着明显的速度优势，并且能够在任意操作系统上方便的复现依赖包，更重要的是 go module 本身的设计使得自身被其他项目引用变得更加容易，这也是 Kubernetes 项目向框架化演进的又一个重要体现。</p>\n<p>使用 go module 管理依赖后会在项目根目录下生成两个文件  go.mod 和 go.sum。</p>\n<p><code>go.mod</code> 中会记录当前项目的所依赖，文件格式如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">module github.com/gosoon/audit-webhook</span><br><span class=\"line\"></span><br><span class=\"line\">go 1.12</span><br><span class=\"line\"></span><br><span class=\"line\">require (</span><br><span class=\"line\">\tgithub.com/elastic/go-elasticsearch v0.0.0</span><br><span class=\"line\">\tgithub.com/gorilla/mux v1.7.2</span><br><span class=\"line\">\tgithub.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p><code>go.sum</code>记录每个依赖库的版本和哈希值，文件格式如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">github.com/elastic/go-elasticsearch v0.0.0 h1:Pd5fqOuBxKxv83b0+xOAJDAkziWYwFinWnBO0y+TZaA=</span><br><span class=\"line\">github.com/elastic/go-elasticsearch v0.0.0/go.mod h1:TkBSJBuTyFdBnrNqoPc54FN0vKf5c04IdM4zuStJ7xg=</span><br><span class=\"line\">github.com/gorilla/mux v1.7.2 h1:zoNxOV7WjqXptQOVngLmcSQgXmgk4NMz1HibBchjl/I=</span><br><span class=\"line\">github.com/gorilla/mux v1.7.2/go.mod h1:1lud6UwP+6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=</span><br><span class=\"line\">github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81 h1:JP0LU0ajeawW2xySrbhDqtSUfVWohZ505Q4LXo+hCmg=</span><br><span class=\"line\">github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81/go.mod h1:1e0N9vBl2wPF6qYa+JCRNIZnhxSkXkOJfD2iFw3eOfg=</span><br></pre></td></tr></table></figure>\n<h4 id=\"一、如何启用-go-module-功能\"><a href=\"#一、如何启用-go-module-功能\" class=\"headerlink\" title=\"一、如何启用 go module 功能\"></a>一、如何启用 go module 功能</h4><p>(1) go 版本 &gt;= v1.11</p>\n<p>(2) 设置<code>GO111MODULE</code>环境变量</p>\n<p>要使用<code>go module</code> 首先要设置<code>GO111MODULE=on</code>，<code>GO111MODULE</code> 有三个值，off、on、auto，off 和 on 即关闭和开启，auto 则会根据当前目录下是否有 go.mod 文件来判断是否使用 modules 功能。无论使用哪种模式，module 功能默认不在 GOPATH 目录下查找依赖文件，所以使用 modules 功能时请设置好代理。</p>\n<p>在使用 go module 时，将 <code>GO111MODULE</code> 全局环境变量设置为 off，在需要使用的时候再开启，避免在已有项目中意外引入  go module。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ echo export GO111MODULE=off &gt;&gt; ~/.zshrc</span><br><span class=\"line\">or</span><br><span class=\"line\">$ echo export GO111MODULE=off &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>\n<p>go mod 命令的使用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">download    download modules to local cache (下载依赖的module到本地cache))</span><br><span class=\"line\">edit        edit go.mod from tools or scripts (编辑go.mod文件)</span><br><span class=\"line\">graph       print module requirement graph (打印模块依赖图))</span><br><span class=\"line\">init        initialize new module in current directory (在当前文件夹下初始化一个新的module, 创建go.mod文件))</span><br><span class=\"line\">tidy        add missing and remove unused modules (增加丢失的module，去掉未使用的module)</span><br><span class=\"line\">vendor      make vendored copy of dependencies (将依赖复制到vendor下)</span><br><span class=\"line\">verify      verify dependencies have expected content (校验依赖)</span><br><span class=\"line\">why         explain why packages or modules are needed (解释为什么需要依赖)</span><br></pre></td></tr></table></figure>\n<h4 id=\"二、使用-go-module-功能\"><a href=\"#二、使用-go-module-功能\" class=\"headerlink\" title=\"二、使用 go module 功能\"></a>二、使用 go module 功能</h4><p>对于新建项目使用 go module：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ export GO111MODULE=on</span><br><span class=\"line\">\t</span><br><span class=\"line\">$ go mod init github.com/you/hello</span><br><span class=\"line\">\t</span><br><span class=\"line\">...</span><br><span class=\"line\">// go build 会将项目的依赖添加到 go.mod 中</span><br><span class=\"line\">$ go build</span><br></pre></td></tr></table></figure>\n<p>对于已有项目要改为使用 go module：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ export GO111MODULE=on</span><br><span class=\"line\"></span><br><span class=\"line\">// 创建一个空的 go.mod 文件</span><br><span class=\"line\">$ go mod init .</span><br><span class=\"line\"></span><br><span class=\"line\">// 查找依赖并记录在 go.mod 文件中</span><br><span class=\"line\">$ go get ./...</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>go.mod 文件必须要提交到 git 仓库，但 go.sum 文件可以不用提交到 git 仓库(gi t忽略文件 .gitignore 中设置一下)。</p>\n</blockquote>\n<h4 id=\"三、项目的打包\"><a href=\"#三、项目的打包\" class=\"headerlink\" title=\"三、项目的打包\"></a>三、项目的打包</h4><p>首先需要使用 <code>go mod vendor</code> 将项目所有的依赖下载到本地 vendor 目录中然后进行编译，下面是一个参考： </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">export GO111MODULE=&quot;on&quot;</span><br><span class=\"line\">export GOPROXY=&quot;https://goproxy.io&quot;</span><br><span class=\"line\">export CGO_ENABLED=&quot;0&quot;</span><br><span class=\"line\">export GOOS=&quot;linux&quot;</span><br><span class=\"line\">export GOARCH=amd64</span><br><span class=\"line\"></span><br><span class=\"line\">go mod vendor</span><br><span class=\"line\">go build -ldflags &quot;-s -w&quot; -a -installsuffix cgo -o audit-webhook .</span><br></pre></td></tr></table></figure>\n<h4 id=\"四、注意事项\"><a href=\"#四、注意事项\" class=\"headerlink\" title=\"四、注意事项\"></a>四、注意事项</h4><p>1、依赖下载</p>\n<p>go module 默认不在 GOPATH 目录下查找依赖文件，其首先会在<code>$GOPATH/pkg/mod</code>中查找有没有所需要的依赖，没有的直接会进行下载。可以使用 <code>go mod download</code>下载好所需要的依赖，依赖默认会下载到<code>$GOPATH/pkg/mod</code>中，其他项目也会使用缓存的 module。</p>\n<p>2、国内无法访问的依赖</p>\n<p>使用 Go 的其他包管理工具 godep、govendor、glide、dep 等都避免不了翻墙的问题，Go Modules 也是一样，但在<code>go.mod</code>中可以使用<code>replace</code>将特定的库替换成其他库：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">replace (</span><br><span class=\"line\">\tgolang.org/x/text v0.3.0 =&gt; github.com/golang/text v0.3.0</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n<p>或者也可以在其他机器上使用 <code>go mod download</code>下载好所需要的依赖，然后再传输到本机。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/2019-03-19-go-modules.md\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/2019-03-19-go-modules.md</a></p>\n<p><a href=\"https://blog.golang.org/using-go-modules\" target=\"_blank\" rel=\"noopener\">https://blog.golang.org/using-go-modules</a></p>\n"},{"title":"k8s 中定时任务的实现","date":"2019-02-16T13:59:30.000Z","type":"wait","_content":"k8s 中有许多优秀的包都可以在平时的开发中借鉴与使用，比如，任务的定时轮询、高可用的实现、日志处理、缓存使用等都是独立的包，可以直接引用。本篇文章会介绍 k8s 中定时任务的实现，k8s 中定时任务都是通过 wait 包实现的，wait 包在 k8s 的多个组件中都有用到，以下是 wait 包在 kubelet 中的几处使用：\n\n\n```\t\t\nfunc run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh <-chan struct{}) (err error) {\n\t\t...\n\t\t// kubelet 每5分钟一次从 apiserver 获取证书\n\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t\n\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t...\n}\n\n...\n\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration,   kubeDeps *kubelet.Dependencies, enableServer bool) {\n    // 持续监听 pod 的变化\n    go wait.Until(func() {\n        k.Run(podCfg.Updates())\n    }, 0, wait.NeverStop)\n    ...\n}\n```\n\ngolang 中可以通过 time.Ticker 实现定时任务的执行，但在 k8s 中用了更原生的方式，使用 time.Timer 实现的。time.Ticker 和 time.Timer 的使用区别如下：\n\n- ticker 只要定义完成，从此刻开始计时，不需要任何其他的操作，每隔固定时间都会自动触发。\n- timer 定时器是到了固定时间后会执行一次，仅执行一次\n- 如果 timer 定时器要每隔间隔的时间执行，实现 ticker 的效果，使用 `func (t *Timer) Reset(d Duration) bool`\n \n一个示例：\n\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\nfunc main() {\n\tvar wg sync.WaitGroup\n\n\ttimer1 := time.NewTimer(2 * time.Second)\n\tticker1 := time.NewTicker(2 * time.Second)\n\n\twg.Add(1)\n\tgo func(t *time.Ticker) {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\t<-t.C\n\t\t\tfmt.Println(\"exec ticker\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\t}\n\t}(ticker1)\n\n\twg.Add(1)\n\tgo func(t *time.Timer) {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\t<-t.C\n\t\t\tfmt.Println(\"exec timer\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\t\tt.Reset(2 * time.Second)\n\t\t}\n\t}(timer1)\n\t\n\twg.Wait()\n}\n\n```\n\n### 一、wait 包中的核心代码\n\n\n核心代码（k8s.io/apimachinery/pkg/util/wait/wait.go）：\n\n\n```\nfunc JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh <-chan struct{}) {\n\tvar t *time.Timer\n\tvar sawTimeout bool\n\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tjitteredPeriod := period\n\t\tif jitterFactor > 0.0 {\n\t\t\tjitteredPeriod = Jitter(period, jitterFactor)\n\t\t}\n\n\t\tif !sliding {\n\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)\n\t\t}\n\n\t\tfunc() {\n\t\t\tdefer runtime.HandleCrash()\n\t\t\tf()\n\t\t}()\n\n\t\tif sliding {\n\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)\n\t\t}\n\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn\n\t\tcase <-t.C:\n\t\t\tsawTimeout = true\n\t\t}\n\t}\n}\n\n...\n\nfunc resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer {\n    if t == nil {\n        return time.NewTimer(d)\n    }\n    if !t.Stop() && !sawTimeout {\n        <-t.C\n    }\n    t.Reset(d)\n    return t\n}\n```\n\n几个关键点的说明：\n\n- 1、如果 sliding 为 true，则在 f() 运行之后计算周期。如果为 false，那么 period 包含 f() 的执行时间。\n- 2、在 golang 中 select 没有优先级选择，为了避免额外执行 f(),在每次循环开始后会先判断 stopCh chan。\n\nk8s 中 wait 包其实是对 time.Timer 做了一层封装实现。\n\n### 二、wait 包常用的方法\n\n##### 1、定期执行一个函数，永不停止，可以使用 Forever 方法：\n\nfunc Forever(f func(), period time.Duration)\n\n##### 2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：\n\nfunc Until(f func(), period time.Duration, stopCh <-chan struct{})\n\n上面的第三个参数 stopCh 就是用于退出无限循环的标志，停止的时候我们 close 掉这个 chan 就可以了。\n\n##### 3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：\n\nfunc Poll(interval, timeout time.Duration, condition ConditionFunc)\n\n这个函数会以 interval 为间隔，不断去检查 condition 条件是否为真，如果为真则可以继续后续处理；如果指定了 timeout 参数，则该函数也可以只常识指定的时间。\n\n##### 4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：\n\nPollUntil(interval time.Duration, condition ConditionFunc, stopCh <-chan struct{}) error\n\n此外还有 PollImmediate 、 PollInfinite 和 PollImmediateInfinite 方法。\n\n### 三、总结\n\n本篇文章主要讲了 k8s 中定时任务的实现与对应包（wait）中方法的使用。通过阅读 k8s 的源代码，可以发现 k8s 中许多功能的实现也都是我们需要在平时工作中用的，其大部分包的性能都是经过大规模考验的，通过使用其相关的工具包不仅能学到大量的编程技巧也能避免自己造轮子。\n\n","source":"_posts/k8s-crontab.md","raw":"---\ntitle: k8s 中定时任务的实现\ndate: 2019-02-16 21:59:30\ntags: [\"crontab\",\"wait\",\"k8s\"]\ntype: \"wait\"\n\n---\nk8s 中有许多优秀的包都可以在平时的开发中借鉴与使用，比如，任务的定时轮询、高可用的实现、日志处理、缓存使用等都是独立的包，可以直接引用。本篇文章会介绍 k8s 中定时任务的实现，k8s 中定时任务都是通过 wait 包实现的，wait 包在 k8s 的多个组件中都有用到，以下是 wait 包在 kubelet 中的几处使用：\n\n\n```\t\t\nfunc run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh <-chan struct{}) (err error) {\n\t\t...\n\t\t// kubelet 每5分钟一次从 apiserver 获取证书\n\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t\n\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t...\n}\n\n...\n\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration,   kubeDeps *kubelet.Dependencies, enableServer bool) {\n    // 持续监听 pod 的变化\n    go wait.Until(func() {\n        k.Run(podCfg.Updates())\n    }, 0, wait.NeverStop)\n    ...\n}\n```\n\ngolang 中可以通过 time.Ticker 实现定时任务的执行，但在 k8s 中用了更原生的方式，使用 time.Timer 实现的。time.Ticker 和 time.Timer 的使用区别如下：\n\n- ticker 只要定义完成，从此刻开始计时，不需要任何其他的操作，每隔固定时间都会自动触发。\n- timer 定时器是到了固定时间后会执行一次，仅执行一次\n- 如果 timer 定时器要每隔间隔的时间执行，实现 ticker 的效果，使用 `func (t *Timer) Reset(d Duration) bool`\n \n一个示例：\n\n\n```\npackage main\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\nfunc main() {\n\tvar wg sync.WaitGroup\n\n\ttimer1 := time.NewTimer(2 * time.Second)\n\tticker1 := time.NewTicker(2 * time.Second)\n\n\twg.Add(1)\n\tgo func(t *time.Ticker) {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\t<-t.C\n\t\t\tfmt.Println(\"exec ticker\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\t}\n\t}(ticker1)\n\n\twg.Add(1)\n\tgo func(t *time.Timer) {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\t<-t.C\n\t\t\tfmt.Println(\"exec timer\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\t\tt.Reset(2 * time.Second)\n\t\t}\n\t}(timer1)\n\t\n\twg.Wait()\n}\n\n```\n\n### 一、wait 包中的核心代码\n\n\n核心代码（k8s.io/apimachinery/pkg/util/wait/wait.go）：\n\n\n```\nfunc JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh <-chan struct{}) {\n\tvar t *time.Timer\n\tvar sawTimeout bool\n\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\n\t\tjitteredPeriod := period\n\t\tif jitterFactor > 0.0 {\n\t\t\tjitteredPeriod = Jitter(period, jitterFactor)\n\t\t}\n\n\t\tif !sliding {\n\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)\n\t\t}\n\n\t\tfunc() {\n\t\t\tdefer runtime.HandleCrash()\n\t\t\tf()\n\t\t}()\n\n\t\tif sliding {\n\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)\n\t\t}\n\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\treturn\n\t\tcase <-t.C:\n\t\t\tsawTimeout = true\n\t\t}\n\t}\n}\n\n...\n\nfunc resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer {\n    if t == nil {\n        return time.NewTimer(d)\n    }\n    if !t.Stop() && !sawTimeout {\n        <-t.C\n    }\n    t.Reset(d)\n    return t\n}\n```\n\n几个关键点的说明：\n\n- 1、如果 sliding 为 true，则在 f() 运行之后计算周期。如果为 false，那么 period 包含 f() 的执行时间。\n- 2、在 golang 中 select 没有优先级选择，为了避免额外执行 f(),在每次循环开始后会先判断 stopCh chan。\n\nk8s 中 wait 包其实是对 time.Timer 做了一层封装实现。\n\n### 二、wait 包常用的方法\n\n##### 1、定期执行一个函数，永不停止，可以使用 Forever 方法：\n\nfunc Forever(f func(), period time.Duration)\n\n##### 2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：\n\nfunc Until(f func(), period time.Duration, stopCh <-chan struct{})\n\n上面的第三个参数 stopCh 就是用于退出无限循环的标志，停止的时候我们 close 掉这个 chan 就可以了。\n\n##### 3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：\n\nfunc Poll(interval, timeout time.Duration, condition ConditionFunc)\n\n这个函数会以 interval 为间隔，不断去检查 condition 条件是否为真，如果为真则可以继续后续处理；如果指定了 timeout 参数，则该函数也可以只常识指定的时间。\n\n##### 4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：\n\nPollUntil(interval time.Duration, condition ConditionFunc, stopCh <-chan struct{}) error\n\n此外还有 PollImmediate 、 PollInfinite 和 PollImmediateInfinite 方法。\n\n### 三、总结\n\n本篇文章主要讲了 k8s 中定时任务的实现与对应包（wait）中方法的使用。通过阅读 k8s 的源代码，可以发现 k8s 中许多功能的实现也都是我们需要在平时工作中用的，其大部分包的性能都是经过大规模考验的，通过使用其相关的工具包不仅能学到大量的编程技巧也能避免自己造轮子。\n\n","slug":"k8s-crontab","published":1,"updated":"2019-06-01T14:26:16.307Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro599000aapwni8fjlsrl","content":"<p>k8s 中有许多优秀的包都可以在平时的开发中借鉴与使用，比如，任务的定时轮询、高可用的实现、日志处理、缓存使用等都是独立的包，可以直接引用。本篇文章会介绍 k8s 中定时任务的实现，k8s 中定时任务都是通过 wait 包实现的，wait 包在 k8s 的多个组件中都有用到，以下是 wait 包在 kubelet 中的几处使用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// kubelet 每5分钟一次从 apiserver 获取证书</span><br><span class=\"line\">\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration,   kubeDeps *kubelet.Dependencies, enableServer bool) &#123;</span><br><span class=\"line\">    // 持续监听 pod 的变化</span><br><span class=\"line\">    go wait.Until(func() &#123;</span><br><span class=\"line\">        k.Run(podCfg.Updates())</span><br><span class=\"line\">    &#125;, 0, wait.NeverStop)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>golang 中可以通过 time.Ticker 实现定时任务的执行，但在 k8s 中用了更原生的方式，使用 time.Timer 实现的。time.Ticker 和 time.Timer 的使用区别如下：</p>\n<ul>\n<li>ticker 只要定义完成，从此刻开始计时，不需要任何其他的操作，每隔固定时间都会自动触发。</li>\n<li>timer 定时器是到了固定时间后会执行一次，仅执行一次</li>\n<li>如果 timer 定时器要每隔间隔的时间执行，实现 ticker 的效果，使用 <code>func (t *Timer) Reset(d Duration) bool</code></li>\n</ul>\n<p>一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;fmt&quot;</span><br><span class=\"line\">\t&quot;sync&quot;</span><br><span class=\"line\">\t&quot;time&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tvar wg sync.WaitGroup</span><br><span class=\"line\"></span><br><span class=\"line\">\ttimer1 := time.NewTimer(2 * time.Second)</span><br><span class=\"line\">\tticker1 := time.NewTicker(2 * time.Second)</span><br><span class=\"line\"></span><br><span class=\"line\">\twg.Add(1)</span><br><span class=\"line\">\tgo func(t *time.Ticker) &#123;</span><br><span class=\"line\">\t\tdefer wg.Done()</span><br><span class=\"line\">\t\tfor &#123;</span><br><span class=\"line\">\t\t\t&lt;-t.C</span><br><span class=\"line\">\t\t\tfmt.Println(&quot;exec ticker&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;(ticker1)</span><br><span class=\"line\"></span><br><span class=\"line\">\twg.Add(1)</span><br><span class=\"line\">\tgo func(t *time.Timer) &#123;</span><br><span class=\"line\">\t\tdefer wg.Done()</span><br><span class=\"line\">\t\tfor &#123;</span><br><span class=\"line\">\t\t\t&lt;-t.C</span><br><span class=\"line\">\t\t\tfmt.Println(&quot;exec timer&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;))</span><br><span class=\"line\">\t\t\tt.Reset(2 * time.Second)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;(timer1)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\twg.Wait()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"一、wait-包中的核心代码\"><a href=\"#一、wait-包中的核心代码\" class=\"headerlink\" title=\"一、wait 包中的核心代码\"></a>一、wait 包中的核心代码</h3><p>核心代码（k8s.io/apimachinery/pkg/util/wait/wait.go）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh &lt;-chan struct&#123;&#125;) &#123;</span><br><span class=\"line\">\tvar t *time.Timer</span><br><span class=\"line\">\tvar sawTimeout bool</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tselect &#123;</span><br><span class=\"line\">\t\tcase &lt;-stopCh:</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\tdefault:</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tjitteredPeriod := period</span><br><span class=\"line\">\t\tif jitterFactor &gt; 0.0 &#123;</span><br><span class=\"line\">\t\t\tjitteredPeriod = Jitter(period, jitterFactor)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif !sliding &#123;</span><br><span class=\"line\">\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfunc() &#123;</span><br><span class=\"line\">\t\t\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t\t\tf()</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif sliding &#123;</span><br><span class=\"line\">\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tselect &#123;</span><br><span class=\"line\">\t\tcase &lt;-stopCh:</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\tcase &lt;-t.C:</span><br><span class=\"line\">\t\t\tsawTimeout = true</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">func resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer &#123;</span><br><span class=\"line\">    if t == nil &#123;</span><br><span class=\"line\">        return time.NewTimer(d)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if !t.Stop() &amp;&amp; !sawTimeout &#123;</span><br><span class=\"line\">        &lt;-t.C</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    t.Reset(d)</span><br><span class=\"line\">    return t</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>几个关键点的说明：</p>\n<ul>\n<li>1、如果 sliding 为 true，则在 f() 运行之后计算周期。如果为 false，那么 period 包含 f() 的执行时间。</li>\n<li>2、在 golang 中 select 没有优先级选择，为了避免额外执行 f(),在每次循环开始后会先判断 stopCh chan。</li>\n</ul>\n<p>k8s 中 wait 包其实是对 time.Timer 做了一层封装实现。</p>\n<h3 id=\"二、wait-包常用的方法\"><a href=\"#二、wait-包常用的方法\" class=\"headerlink\" title=\"二、wait 包常用的方法\"></a>二、wait 包常用的方法</h3><h5 id=\"1、定期执行一个函数，永不停止，可以使用-Forever-方法：\"><a href=\"#1、定期执行一个函数，永不停止，可以使用-Forever-方法：\" class=\"headerlink\" title=\"1、定期执行一个函数，永不停止，可以使用 Forever 方法：\"></a>1、定期执行一个函数，永不停止，可以使用 Forever 方法：</h5><p>func Forever(f func(), period time.Duration)</p>\n<h5 id=\"2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的-chan-即可，方法定义如下：\"><a href=\"#2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的-chan-即可，方法定义如下：\" class=\"headerlink\" title=\"2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：\"></a>2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：</h5><p>func Until(f func(), period time.Duration, stopCh &lt;-chan struct{})</p>\n<p>上面的第三个参数 stopCh 就是用于退出无限循环的标志，停止的时候我们 close 掉这个 chan 就可以了。</p>\n<h5 id=\"3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用-Poll-方法：\"><a href=\"#3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用-Poll-方法：\" class=\"headerlink\" title=\"3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：\"></a>3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：</h5><p>func Poll(interval, timeout time.Duration, condition ConditionFunc)</p>\n<p>这个函数会以 interval 为间隔，不断去检查 condition 条件是否为真，如果为真则可以继续后续处理；如果指定了 timeout 参数，则该函数也可以只常识指定的时间。</p>\n<h5 id=\"4、PollUntil-方法和上面的类似，但是没有-timeout-参数，多了一个-stopCh-参数，如下所示：\"><a href=\"#4、PollUntil-方法和上面的类似，但是没有-timeout-参数，多了一个-stopCh-参数，如下所示：\" class=\"headerlink\" title=\"4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：\"></a>4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：</h5><p>PollUntil(interval time.Duration, condition ConditionFunc, stopCh &lt;-chan struct{}) error</p>\n<p>此外还有 PollImmediate 、 PollInfinite 和 PollImmediateInfinite 方法。</p>\n<h3 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h3><p>本篇文章主要讲了 k8s 中定时任务的实现与对应包（wait）中方法的使用。通过阅读 k8s 的源代码，可以发现 k8s 中许多功能的实现也都是我们需要在平时工作中用的，其大部分包的性能都是经过大规模考验的，通过使用其相关的工具包不仅能学到大量的编程技巧也能避免自己造轮子。</p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"https://abelsu7.top/2019/05/08/crontab-intro/\">Linux 定时任务与 crontab 简介</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>k8s 中有许多优秀的包都可以在平时的开发中借鉴与使用，比如，任务的定时轮询、高可用的实现、日志处理、缓存使用等都是独立的包，可以直接引用。本篇文章会介绍 k8s 中定时任务的实现，k8s 中定时任务都是通过 wait 包实现的，wait 包在 k8s 的多个组件中都有用到，以下是 wait 包在 kubelet 中的几处使用：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// kubelet 每5分钟一次从 apiserver 获取证书</span><br><span class=\"line\">\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tcloseAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration,   kubeDeps *kubelet.Dependencies, enableServer bool) &#123;</span><br><span class=\"line\">    // 持续监听 pod 的变化</span><br><span class=\"line\">    go wait.Until(func() &#123;</span><br><span class=\"line\">        k.Run(podCfg.Updates())</span><br><span class=\"line\">    &#125;, 0, wait.NeverStop)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>golang 中可以通过 time.Ticker 实现定时任务的执行，但在 k8s 中用了更原生的方式，使用 time.Timer 实现的。time.Ticker 和 time.Timer 的使用区别如下：</p>\n<ul>\n<li>ticker 只要定义完成，从此刻开始计时，不需要任何其他的操作，每隔固定时间都会自动触发。</li>\n<li>timer 定时器是到了固定时间后会执行一次，仅执行一次</li>\n<li>如果 timer 定时器要每隔间隔的时间执行，实现 ticker 的效果，使用 <code>func (t *Timer) Reset(d Duration) bool</code></li>\n</ul>\n<p>一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;fmt&quot;</span><br><span class=\"line\">\t&quot;sync&quot;</span><br><span class=\"line\">\t&quot;time&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tvar wg sync.WaitGroup</span><br><span class=\"line\"></span><br><span class=\"line\">\ttimer1 := time.NewTimer(2 * time.Second)</span><br><span class=\"line\">\tticker1 := time.NewTicker(2 * time.Second)</span><br><span class=\"line\"></span><br><span class=\"line\">\twg.Add(1)</span><br><span class=\"line\">\tgo func(t *time.Ticker) &#123;</span><br><span class=\"line\">\t\tdefer wg.Done()</span><br><span class=\"line\">\t\tfor &#123;</span><br><span class=\"line\">\t\t\t&lt;-t.C</span><br><span class=\"line\">\t\t\tfmt.Println(&quot;exec ticker&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;(ticker1)</span><br><span class=\"line\"></span><br><span class=\"line\">\twg.Add(1)</span><br><span class=\"line\">\tgo func(t *time.Timer) &#123;</span><br><span class=\"line\">\t\tdefer wg.Done()</span><br><span class=\"line\">\t\tfor &#123;</span><br><span class=\"line\">\t\t\t&lt;-t.C</span><br><span class=\"line\">\t\t\tfmt.Println(&quot;exec timer&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;))</span><br><span class=\"line\">\t\t\tt.Reset(2 * time.Second)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;(timer1)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\twg.Wait()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"一、wait-包中的核心代码\"><a href=\"#一、wait-包中的核心代码\" class=\"headerlink\" title=\"一、wait 包中的核心代码\"></a>一、wait 包中的核心代码</h3><p>核心代码（k8s.io/apimachinery/pkg/util/wait/wait.go）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh &lt;-chan struct&#123;&#125;) &#123;</span><br><span class=\"line\">\tvar t *time.Timer</span><br><span class=\"line\">\tvar sawTimeout bool</span><br><span class=\"line\"></span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tselect &#123;</span><br><span class=\"line\">\t\tcase &lt;-stopCh:</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\tdefault:</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tjitteredPeriod := period</span><br><span class=\"line\">\t\tif jitterFactor &gt; 0.0 &#123;</span><br><span class=\"line\">\t\t\tjitteredPeriod = Jitter(period, jitterFactor)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif !sliding &#123;</span><br><span class=\"line\">\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tfunc() &#123;</span><br><span class=\"line\">\t\t\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t\t\tf()</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif sliding &#123;</span><br><span class=\"line\">\t\t\tt = resetOrReuseTimer(t, jitteredPeriod, sawTimeout)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tselect &#123;</span><br><span class=\"line\">\t\tcase &lt;-stopCh:</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\tcase &lt;-t.C:</span><br><span class=\"line\">\t\t\tsawTimeout = true</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">func resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer &#123;</span><br><span class=\"line\">    if t == nil &#123;</span><br><span class=\"line\">        return time.NewTimer(d)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if !t.Stop() &amp;&amp; !sawTimeout &#123;</span><br><span class=\"line\">        &lt;-t.C</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    t.Reset(d)</span><br><span class=\"line\">    return t</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>几个关键点的说明：</p>\n<ul>\n<li>1、如果 sliding 为 true，则在 f() 运行之后计算周期。如果为 false，那么 period 包含 f() 的执行时间。</li>\n<li>2、在 golang 中 select 没有优先级选择，为了避免额外执行 f(),在每次循环开始后会先判断 stopCh chan。</li>\n</ul>\n<p>k8s 中 wait 包其实是对 time.Timer 做了一层封装实现。</p>\n<h3 id=\"二、wait-包常用的方法\"><a href=\"#二、wait-包常用的方法\" class=\"headerlink\" title=\"二、wait 包常用的方法\"></a>二、wait 包常用的方法</h3><h5 id=\"1、定期执行一个函数，永不停止，可以使用-Forever-方法：\"><a href=\"#1、定期执行一个函数，永不停止，可以使用-Forever-方法：\" class=\"headerlink\" title=\"1、定期执行一个函数，永不停止，可以使用 Forever 方法：\"></a>1、定期执行一个函数，永不停止，可以使用 Forever 方法：</h5><p>func Forever(f func(), period time.Duration)</p>\n<h5 id=\"2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的-chan-即可，方法定义如下：\"><a href=\"#2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的-chan-即可，方法定义如下：\" class=\"headerlink\" title=\"2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：\"></a>2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：</h5><p>func Until(f func(), period time.Duration, stopCh &lt;-chan struct{})</p>\n<p>上面的第三个参数 stopCh 就是用于退出无限循环的标志，停止的时候我们 close 掉这个 chan 就可以了。</p>\n<h5 id=\"3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用-Poll-方法：\"><a href=\"#3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用-Poll-方法：\" class=\"headerlink\" title=\"3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：\"></a>3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：</h5><p>func Poll(interval, timeout time.Duration, condition ConditionFunc)</p>\n<p>这个函数会以 interval 为间隔，不断去检查 condition 条件是否为真，如果为真则可以继续后续处理；如果指定了 timeout 参数，则该函数也可以只常识指定的时间。</p>\n<h5 id=\"4、PollUntil-方法和上面的类似，但是没有-timeout-参数，多了一个-stopCh-参数，如下所示：\"><a href=\"#4、PollUntil-方法和上面的类似，但是没有-timeout-参数，多了一个-stopCh-参数，如下所示：\" class=\"headerlink\" title=\"4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：\"></a>4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：</h5><p>PollUntil(interval time.Duration, condition ConditionFunc, stopCh &lt;-chan struct{}) error</p>\n<p>此外还有 PollImmediate 、 PollInfinite 和 PollImmediateInfinite 方法。</p>\n<h3 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h3><p>本篇文章主要讲了 k8s 中定时任务的实现与对应包（wait）中方法的使用。通过阅读 k8s 的源代码，可以发现 k8s 中许多功能的实现也都是我们需要在平时工作中用的，其大部分包的性能都是经过大规模考验的，通过使用其相关的工具包不仅能学到大量的编程技巧也能避免自己造轮子。</p>\n"},{"title":"kubernetes 审计日志功能","date":"2019-01-30T08:26:30.000Z","type":"audit","_content":"审计日志可以记录所有对 apiserver 接口的调用，让我们能够非常清晰的知道集群到底发生了什么事情，通过记录的日志可以查到所发生的事件、操作的用户和时间。kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。\n\n> kubernetes feature-gates 中的功能 Alpha 版本默认为 false，到 Beta 版本时默认为 true，所以 v1.8 会默认启用审计日志的功能。\n\n\n### 一、审计日志的策略\n\n#### 1、日志记录阶段\n\nkube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有：\n\n- RequestReceived - apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。\n- ResponseStarted - 在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。\n- ResponseComplete - 当响应 body 发送完并且不再发送数据。\n- Panic - 当有 panic 发生时生成。\n\n也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成。\n\n#### 2、日志记录级别\n\n当前支持的日志记录级别有：\n\n- None - 不记录日志。\n- Metadata - 只记录 Request 的一些 metadata (例如 user, timestamp, resource, verb 等)，但不记录 Request 或 Response 的body。\n- Request - 记录 Request 的 metadata 和 body。\n- RequestResponse - 最全记录方式，会记录所有的 metadata、Request 和 Response 的 body。\n\n#### 3、日志记录策略\n\n在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。\n\n- 一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段\n- 不要记录所有的资源，不要记录一个资源的所有子资源\n- 系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录\n- 对一些认证信息（secerts、configmaps、token 等）的 body 不记录 \n\nk8s 审计日志的一个示例：\n\n```\n{\n  \"kind\": \"EventList\",\n  \"apiVersion\": \"audit.k8s.io/v1beta1\",\n  \"Items\": [\n    {\n      \"Level\": \"Request\",\n      \"AuditID\": \"793e7ae2-5ca7-4ad3-a632-19708d2f8265\",\n      \"Stage\": \"RequestReceived\",\n      \"RequestURI\": \"/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0\",\n      \"Verb\": \"get\",\n      \"User\": {\n        \"Username\": \"system:unsecured\",\n        \"UID\": \"\",\n        \"Groups\": [\n          \"system:masters\",\n          \"system:authenticated\"\n        ],\n        \"Extra\": null\n      },\n      \"ImpersonatedUser\": null,\n      \"SourceIPs\": [\n        \"192.168.1.11\"\n      ],\n      \"UserAgent\": \"kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler\",\n      \"ObjectRef\": {\n        \"Resource\": \"pods\",\n        \"Namespace\": \"default\",\n        \"Name\": \"test-pre-sf-de7cc-0\",\n        \"UID\": \"\",\n        \"APIGroup\": \"\",\n        \"APIVersion\": \"v1\",\n        \"ResourceVersion\": \"\",\n        \"Subresource\": \"\"\n      },\n      \"ResponseStatus\": null,\n      \"RequestObject\": null,\n      \"ResponseObject\": null,\n      \"RequestReceivedTimestamp\": \"2019-01-11T06:51:43.528703Z\",\n      \"StageTimestamp\": \"2019-01-11T06:51:43.528703Z\",\n      \"Annotations\": null\n    }\n    ]\n}\n```\n\n### 二、启用审计日志\n\n当前的审计日志支持两种收集方式：保存为日志文件和调用自定义的 webhook，在 v1.13 中还支持动态的 webhook。\n\n#### 1、将审计日志以 json 格式保存到本地文件\n\napiserver 配置文件的 KUBE_API_ARGS 中需要添加如下参数：\n```\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json\n```\n\n日志保存到本地后再通过 fluentd 等其他组件进行收集。\n还有其他几个选项可以指定保留审计日志文件的最大天数、文件的最大数量、文件的大小等。\n\n#### 2、将审计日志打到后端指定的 webhook\n\n```\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig\n```\n\nwebhook 配置文件实际上是一个 kubeconfig，apiserver 会将审计日志发送 到指定的 webhook 后，webhook 接收到日志后可以再分发到 kafka 或其他组件进行收集。\n\n`audit-webhook-kubeconfig` 示例：\n```\napiVersion: v1\nclusters:\n- cluster:\n    server: http://127.0.0.1:8081/audit/webhook\n  name: metric\ncontexts:\n- context:\n    cluster: metric\n    user: \"\"\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers: []\n```\n\n前面提到过，apiserver 的每一个请求会记录三个阶段的审计日志，但是在实际中并不是需要所有的审计日志，官方也说明了启用审计日志会增加 apiserver 对内存的使用量。\n\n> Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration.\n\n\n`audit-policy.yaml` 配置示例：\n\n```\napiVersion: audit.k8s.io/v1\nkind: Policy\n# ResponseStarted 阶段不记录\nomitStages:\n  - \"ResponseStarted\"\nrules:\n  # 记录用户对 pod 和 statefulset 的操作\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      resources: [\"pods\",\"pods/status\"]\n    - group: \"apps\"\n      resources: [\"statefulsets\",\"statefulsets/scale\"]\n  # kube-controller-manager、kube-scheduler 等已经认证过身份的请求不需要记录\n  - level: None\n    userGroups: [\"system:authenticated\"]\n    nonResourceURLs:\n    - \"/api*\"\n    - \"/version\"\n  # 对 config、secret、token 等认证信息不记录请求体和返回体\n  - level: Metadata\n    resources:\n    - group: \"\" # core API group\n      resources: [\"secrets\", \"configmaps\"]\n```\n\n官方提供两个参考示例：\n\n- [Use fluentd to collect and distribute audit events from log file](https://kubernetes.io/zh/docs/tasks/debug-application-cluster/audit/#%E6%97%A5%E5%BF%97%E9%80%89%E6%8B%A9%E5%99%A8%E7%A4%BA%E4%BE%8B)\n- [Use logstash to collect and distribute audit events from webhook backend](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend)\n\n#### 3、subresource 说明\n\n\nkubernetes 每个资源对象都有 subresource,通过调用 master 的 api 可以获取 kubernetes 中所有的 resource 以及对应的 subresource,比如 pod 有 logs、exec 等 subresource。\n\n\n![](https://upload-images.jianshu.io/upload_images/1262158-4450a01f65b3f76d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n```\n获取所有 resource（ 1.10 之后使用）：\n$ curl  127.0.0.1:8080/openapi/v2\n```\n\n参考：[https://kubernetes.io/docs/concepts/overview/kubernetes-api/](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)\n\n### 三、webhook 的一个简单示例\n\n```\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/emicklei/go-restful\"\n\t\"github.com/gosoon/glog\"\n\t\"k8s.io/apiserver/pkg/apis/audit\"\n)\n\nfunc main() {\n\t// NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter)\n\tcontainer := restful.NewContainer()\n\tws := new(restful.WebService)\n\tws.Path(\"/audit\").\n\t\tConsumes(restful.MIME_JSON).\n\t\tProduces(restful.MIME_JSON)\n\tws.Route(ws.POST(\"/webhook\").To(AuditWebhook))\n\n\t//WebService ws2被添加到container2中\n\tcontainer.Add(ws)\n\tserver := &http.Server{\n\t\tAddr:    \":8081\",\n\t\tHandler: container,\n\t}\n\t//go consumer()\n\tlog.Fatal(server.ListenAndServe())\n}\n\nfunc AuditWebhook(req *restful.Request, resp *restful.Response) {\n\tbody, err := ioutil.ReadAll(req.Request.Body)\n\tif err != nil {\n\t\tglog.Errorf(\"read body err is: %v\", err)\n\t}\n\tvar eventList audit.EventList\n\terr = json.Unmarshal(body, &eventList)\n\tif err != nil {\n\t\tglog.Errorf(\"unmarshal failed with:%v,body is :\\n\", err, string(body))\n\t\treturn\n\t}\n\tfor _, event := range eventList.Items {\n\t\tjsonBytes, err := json.Marshal(event)\n\t\tif err != nil {\n\t\t\tglog.Infof(\"marshal failed with:%v,event is \\n %+v\", err, event)\n\t\t}\n\t\t// 消费日志\n\t\tasyncProducer(string(jsonBytes))\n\t}\n\tresp.AddHeader(\"Content-Type\", \"application/json\")\n\tresp.WriteEntity(\"success\")\n}\n```\n\n> 完整代码请参考：[https://github.com/gosoon/k8s-audit-webhook](https://github.com/gosoon/k8s-audit-webhook)\n\n\n### 四、总结\n\n本文主要介绍了 kubernetes 的日志审计功能，kubernetes 最近也被爆出多个安全漏洞，安全问题是每个团队不可忽视的，kubernetes 虽然被多数公司用作私有云，但日志审计也是不可或缺的。\n\n\n----\n\n参考：\n[https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n[ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/)\n[阿里云 Kubernetes 审计日志方案](https://yq.aliyun.com/articles/686982?utm_content=g_1000040449)\n\n","source":"_posts/k8s-audit-webhook.md","raw":"---\ntitle: kubernetes 审计日志功能\ndate: 2019-01-30 16:26:30\ntags: [\"audit\",\"log\"]\ntype: \"audit\"\n\n---\n审计日志可以记录所有对 apiserver 接口的调用，让我们能够非常清晰的知道集群到底发生了什么事情，通过记录的日志可以查到所发生的事件、操作的用户和时间。kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。\n\n> kubernetes feature-gates 中的功能 Alpha 版本默认为 false，到 Beta 版本时默认为 true，所以 v1.8 会默认启用审计日志的功能。\n\n\n### 一、审计日志的策略\n\n#### 1、日志记录阶段\n\nkube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有：\n\n- RequestReceived - apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。\n- ResponseStarted - 在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。\n- ResponseComplete - 当响应 body 发送完并且不再发送数据。\n- Panic - 当有 panic 发生时生成。\n\n也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成。\n\n#### 2、日志记录级别\n\n当前支持的日志记录级别有：\n\n- None - 不记录日志。\n- Metadata - 只记录 Request 的一些 metadata (例如 user, timestamp, resource, verb 等)，但不记录 Request 或 Response 的body。\n- Request - 记录 Request 的 metadata 和 body。\n- RequestResponse - 最全记录方式，会记录所有的 metadata、Request 和 Response 的 body。\n\n#### 3、日志记录策略\n\n在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。\n\n- 一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段\n- 不要记录所有的资源，不要记录一个资源的所有子资源\n- 系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录\n- 对一些认证信息（secerts、configmaps、token 等）的 body 不记录 \n\nk8s 审计日志的一个示例：\n\n```\n{\n  \"kind\": \"EventList\",\n  \"apiVersion\": \"audit.k8s.io/v1beta1\",\n  \"Items\": [\n    {\n      \"Level\": \"Request\",\n      \"AuditID\": \"793e7ae2-5ca7-4ad3-a632-19708d2f8265\",\n      \"Stage\": \"RequestReceived\",\n      \"RequestURI\": \"/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0\",\n      \"Verb\": \"get\",\n      \"User\": {\n        \"Username\": \"system:unsecured\",\n        \"UID\": \"\",\n        \"Groups\": [\n          \"system:masters\",\n          \"system:authenticated\"\n        ],\n        \"Extra\": null\n      },\n      \"ImpersonatedUser\": null,\n      \"SourceIPs\": [\n        \"192.168.1.11\"\n      ],\n      \"UserAgent\": \"kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler\",\n      \"ObjectRef\": {\n        \"Resource\": \"pods\",\n        \"Namespace\": \"default\",\n        \"Name\": \"test-pre-sf-de7cc-0\",\n        \"UID\": \"\",\n        \"APIGroup\": \"\",\n        \"APIVersion\": \"v1\",\n        \"ResourceVersion\": \"\",\n        \"Subresource\": \"\"\n      },\n      \"ResponseStatus\": null,\n      \"RequestObject\": null,\n      \"ResponseObject\": null,\n      \"RequestReceivedTimestamp\": \"2019-01-11T06:51:43.528703Z\",\n      \"StageTimestamp\": \"2019-01-11T06:51:43.528703Z\",\n      \"Annotations\": null\n    }\n    ]\n}\n```\n\n### 二、启用审计日志\n\n当前的审计日志支持两种收集方式：保存为日志文件和调用自定义的 webhook，在 v1.13 中还支持动态的 webhook。\n\n#### 1、将审计日志以 json 格式保存到本地文件\n\napiserver 配置文件的 KUBE_API_ARGS 中需要添加如下参数：\n```\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json\n```\n\n日志保存到本地后再通过 fluentd 等其他组件进行收集。\n还有其他几个选项可以指定保留审计日志文件的最大天数、文件的最大数量、文件的大小等。\n\n#### 2、将审计日志打到后端指定的 webhook\n\n```\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig\n```\n\nwebhook 配置文件实际上是一个 kubeconfig，apiserver 会将审计日志发送 到指定的 webhook 后，webhook 接收到日志后可以再分发到 kafka 或其他组件进行收集。\n\n`audit-webhook-kubeconfig` 示例：\n```\napiVersion: v1\nclusters:\n- cluster:\n    server: http://127.0.0.1:8081/audit/webhook\n  name: metric\ncontexts:\n- context:\n    cluster: metric\n    user: \"\"\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers: []\n```\n\n前面提到过，apiserver 的每一个请求会记录三个阶段的审计日志，但是在实际中并不是需要所有的审计日志，官方也说明了启用审计日志会增加 apiserver 对内存的使用量。\n\n> Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration.\n\n\n`audit-policy.yaml` 配置示例：\n\n```\napiVersion: audit.k8s.io/v1\nkind: Policy\n# ResponseStarted 阶段不记录\nomitStages:\n  - \"ResponseStarted\"\nrules:\n  # 记录用户对 pod 和 statefulset 的操作\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      resources: [\"pods\",\"pods/status\"]\n    - group: \"apps\"\n      resources: [\"statefulsets\",\"statefulsets/scale\"]\n  # kube-controller-manager、kube-scheduler 等已经认证过身份的请求不需要记录\n  - level: None\n    userGroups: [\"system:authenticated\"]\n    nonResourceURLs:\n    - \"/api*\"\n    - \"/version\"\n  # 对 config、secret、token 等认证信息不记录请求体和返回体\n  - level: Metadata\n    resources:\n    - group: \"\" # core API group\n      resources: [\"secrets\", \"configmaps\"]\n```\n\n官方提供两个参考示例：\n\n- [Use fluentd to collect and distribute audit events from log file](https://kubernetes.io/zh/docs/tasks/debug-application-cluster/audit/#%E6%97%A5%E5%BF%97%E9%80%89%E6%8B%A9%E5%99%A8%E7%A4%BA%E4%BE%8B)\n- [Use logstash to collect and distribute audit events from webhook backend](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend)\n\n#### 3、subresource 说明\n\n\nkubernetes 每个资源对象都有 subresource,通过调用 master 的 api 可以获取 kubernetes 中所有的 resource 以及对应的 subresource,比如 pod 有 logs、exec 等 subresource。\n\n\n![](https://upload-images.jianshu.io/upload_images/1262158-4450a01f65b3f76d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n```\n获取所有 resource（ 1.10 之后使用）：\n$ curl  127.0.0.1:8080/openapi/v2\n```\n\n参考：[https://kubernetes.io/docs/concepts/overview/kubernetes-api/](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)\n\n### 三、webhook 的一个简单示例\n\n```\npackage main\n\nimport (\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"net/http\"\n\n\t\"github.com/emicklei/go-restful\"\n\t\"github.com/gosoon/glog\"\n\t\"k8s.io/apiserver/pkg/apis/audit\"\n)\n\nfunc main() {\n\t// NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter)\n\tcontainer := restful.NewContainer()\n\tws := new(restful.WebService)\n\tws.Path(\"/audit\").\n\t\tConsumes(restful.MIME_JSON).\n\t\tProduces(restful.MIME_JSON)\n\tws.Route(ws.POST(\"/webhook\").To(AuditWebhook))\n\n\t//WebService ws2被添加到container2中\n\tcontainer.Add(ws)\n\tserver := &http.Server{\n\t\tAddr:    \":8081\",\n\t\tHandler: container,\n\t}\n\t//go consumer()\n\tlog.Fatal(server.ListenAndServe())\n}\n\nfunc AuditWebhook(req *restful.Request, resp *restful.Response) {\n\tbody, err := ioutil.ReadAll(req.Request.Body)\n\tif err != nil {\n\t\tglog.Errorf(\"read body err is: %v\", err)\n\t}\n\tvar eventList audit.EventList\n\terr = json.Unmarshal(body, &eventList)\n\tif err != nil {\n\t\tglog.Errorf(\"unmarshal failed with:%v,body is :\\n\", err, string(body))\n\t\treturn\n\t}\n\tfor _, event := range eventList.Items {\n\t\tjsonBytes, err := json.Marshal(event)\n\t\tif err != nil {\n\t\t\tglog.Infof(\"marshal failed with:%v,event is \\n %+v\", err, event)\n\t\t}\n\t\t// 消费日志\n\t\tasyncProducer(string(jsonBytes))\n\t}\n\tresp.AddHeader(\"Content-Type\", \"application/json\")\n\tresp.WriteEntity(\"success\")\n}\n```\n\n> 完整代码请参考：[https://github.com/gosoon/k8s-audit-webhook](https://github.com/gosoon/k8s-audit-webhook)\n\n\n### 四、总结\n\n本文主要介绍了 kubernetes 的日志审计功能，kubernetes 最近也被爆出多个安全漏洞，安全问题是每个团队不可忽视的，kubernetes 虽然被多数公司用作私有云，但日志审计也是不可或缺的。\n\n\n----\n\n参考：\n[https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n[ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/)\n[阿里云 Kubernetes 审计日志方案](https://yq.aliyun.com/articles/686982?utm_content=g_1000040449)\n\n","slug":"k8s-audit-webhook","published":1,"updated":"2019-06-01T14:26:16.307Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59a000capwn0txujkah","content":"<p>审计日志可以记录所有对 apiserver 接口的调用，让我们能够非常清晰的知道集群到底发生了什么事情，通过记录的日志可以查到所发生的事件、操作的用户和时间。kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。</p>\n<blockquote>\n<p>kubernetes feature-gates 中的功能 Alpha 版本默认为 false，到 Beta 版本时默认为 true，所以 v1.8 会默认启用审计日志的功能。</p>\n</blockquote>\n<h3 id=\"一、审计日志的策略\"><a href=\"#一、审计日志的策略\" class=\"headerlink\" title=\"一、审计日志的策略\"></a>一、审计日志的策略</h3><h4 id=\"1、日志记录阶段\"><a href=\"#1、日志记录阶段\" class=\"headerlink\" title=\"1、日志记录阶段\"></a>1、日志记录阶段</h4><p>kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有：</p>\n<ul>\n<li>RequestReceived - apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。</li>\n<li>ResponseStarted - 在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。</li>\n<li>ResponseComplete - 当响应 body 发送完并且不再发送数据。</li>\n<li>Panic - 当有 panic 发生时生成。</li>\n</ul>\n<p>也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成。</p>\n<h4 id=\"2、日志记录级别\"><a href=\"#2、日志记录级别\" class=\"headerlink\" title=\"2、日志记录级别\"></a>2、日志记录级别</h4><p>当前支持的日志记录级别有：</p>\n<ul>\n<li>None - 不记录日志。</li>\n<li>Metadata - 只记录 Request 的一些 metadata (例如 user, timestamp, resource, verb 等)，但不记录 Request 或 Response 的body。</li>\n<li>Request - 记录 Request 的 metadata 和 body。</li>\n<li>RequestResponse - 最全记录方式，会记录所有的 metadata、Request 和 Response 的 body。</li>\n</ul>\n<h4 id=\"3、日志记录策略\"><a href=\"#3、日志记录策略\" class=\"headerlink\" title=\"3、日志记录策略\"></a>3、日志记录策略</h4><p>在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。</p>\n<ul>\n<li>一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段</li>\n<li>不要记录所有的资源，不要记录一个资源的所有子资源</li>\n<li>系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录</li>\n<li>对一些认证信息（secerts、configmaps、token 等）的 body 不记录 </li>\n</ul>\n<p>k8s 审计日志的一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;kind&quot;: &quot;EventList&quot;,</span><br><span class=\"line\">  &quot;apiVersion&quot;: &quot;audit.k8s.io/v1beta1&quot;,</span><br><span class=\"line\">  &quot;Items&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;Level&quot;: &quot;Request&quot;,</span><br><span class=\"line\">      &quot;AuditID&quot;: &quot;793e7ae2-5ca7-4ad3-a632-19708d2f8265&quot;,</span><br><span class=\"line\">      &quot;Stage&quot;: &quot;RequestReceived&quot;,</span><br><span class=\"line\">      &quot;RequestURI&quot;: &quot;/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0&quot;,</span><br><span class=\"line\">      &quot;Verb&quot;: &quot;get&quot;,</span><br><span class=\"line\">      &quot;User&quot;: &#123;</span><br><span class=\"line\">        &quot;Username&quot;: &quot;system:unsecured&quot;,</span><br><span class=\"line\">        &quot;UID&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;Groups&quot;: [</span><br><span class=\"line\">          &quot;system:masters&quot;,</span><br><span class=\"line\">          &quot;system:authenticated&quot;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;Extra&quot;: null</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;ImpersonatedUser&quot;: null,</span><br><span class=\"line\">      &quot;SourceIPs&quot;: [</span><br><span class=\"line\">        &quot;192.168.1.11&quot;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;UserAgent&quot;: &quot;kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler&quot;,</span><br><span class=\"line\">      &quot;ObjectRef&quot;: &#123;</span><br><span class=\"line\">        &quot;Resource&quot;: &quot;pods&quot;,</span><br><span class=\"line\">        &quot;Namespace&quot;: &quot;default&quot;,</span><br><span class=\"line\">        &quot;Name&quot;: &quot;test-pre-sf-de7cc-0&quot;,</span><br><span class=\"line\">        &quot;UID&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;APIGroup&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;APIVersion&quot;: &quot;v1&quot;,</span><br><span class=\"line\">        &quot;ResourceVersion&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;Subresource&quot;: &quot;&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;ResponseStatus&quot;: null,</span><br><span class=\"line\">      &quot;RequestObject&quot;: null,</span><br><span class=\"line\">      &quot;ResponseObject&quot;: null,</span><br><span class=\"line\">      &quot;RequestReceivedTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;,</span><br><span class=\"line\">      &quot;StageTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;,</span><br><span class=\"line\">      &quot;Annotations&quot;: null</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"二、启用审计日志\"><a href=\"#二、启用审计日志\" class=\"headerlink\" title=\"二、启用审计日志\"></a>二、启用审计日志</h3><p>当前的审计日志支持两种收集方式：保存为日志文件和调用自定义的 webhook，在 v1.13 中还支持动态的 webhook。</p>\n<h4 id=\"1、将审计日志以-json-格式保存到本地文件\"><a href=\"#1、将审计日志以-json-格式保存到本地文件\" class=\"headerlink\" title=\"1、将审计日志以 json 格式保存到本地文件\"></a>1、将审计日志以 json 格式保存到本地文件</h4><p>apiserver 配置文件的 KUBE_API_ARGS 中需要添加如下参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json</span><br></pre></td></tr></table></figure></p>\n<p>日志保存到本地后再通过 fluentd 等其他组件进行收集。<br>还有其他几个选项可以指定保留审计日志文件的最大天数、文件的最大数量、文件的大小等。</p>\n<h4 id=\"2、将审计日志打到后端指定的-webhook\"><a href=\"#2、将审计日志打到后端指定的-webhook\" class=\"headerlink\" title=\"2、将审计日志打到后端指定的 webhook\"></a>2、将审计日志打到后端指定的 webhook</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig</span><br></pre></td></tr></table></figure>\n<p>webhook 配置文件实际上是一个 kubeconfig，apiserver 会将审计日志发送 到指定的 webhook 后，webhook 接收到日志后可以再分发到 kafka 或其他组件进行收集。</p>\n<p><code>audit-webhook-kubeconfig</code> 示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    server: http://127.0.0.1:8081/audit/webhook</span><br><span class=\"line\">  name: metric</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: metric</span><br><span class=\"line\">    user: &quot;&quot;</span><br><span class=\"line\">  name: default-context</span><br><span class=\"line\">current-context: default-context</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">users: []</span><br></pre></td></tr></table></figure></p>\n<p>前面提到过，apiserver 的每一个请求会记录三个阶段的审计日志，但是在实际中并不是需要所有的审计日志，官方也说明了启用审计日志会增加 apiserver 对内存的使用量。</p>\n<blockquote>\n<p>Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration.</p>\n</blockquote>\n<p><code>audit-policy.yaml</code> 配置示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: audit.k8s.io/v1</span><br><span class=\"line\">kind: Policy</span><br><span class=\"line\"># ResponseStarted 阶段不记录</span><br><span class=\"line\">omitStages:</span><br><span class=\"line\">  - &quot;ResponseStarted&quot;</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  # 记录用户对 pod 和 statefulset 的操作</span><br><span class=\"line\">  - level: RequestResponse</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">    - group: &quot;&quot;</span><br><span class=\"line\">      resources: [&quot;pods&quot;,&quot;pods/status&quot;]</span><br><span class=\"line\">    - group: &quot;apps&quot;</span><br><span class=\"line\">      resources: [&quot;statefulsets&quot;,&quot;statefulsets/scale&quot;]</span><br><span class=\"line\">  # kube-controller-manager、kube-scheduler 等已经认证过身份的请求不需要记录</span><br><span class=\"line\">  - level: None</span><br><span class=\"line\">    userGroups: [&quot;system:authenticated&quot;]</span><br><span class=\"line\">    nonResourceURLs:</span><br><span class=\"line\">    - &quot;/api*&quot;</span><br><span class=\"line\">    - &quot;/version&quot;</span><br><span class=\"line\">  # 对 config、secret、token 等认证信息不记录请求体和返回体</span><br><span class=\"line\">  - level: Metadata</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">    - group: &quot;&quot; # core API group</span><br><span class=\"line\">      resources: [&quot;secrets&quot;, &quot;configmaps&quot;]</span><br></pre></td></tr></table></figure>\n<p>官方提供两个参考示例：</p>\n<ul>\n<li><a href=\"https://kubernetes.io/zh/docs/tasks/debug-application-cluster/audit/#%E6%97%A5%E5%BF%97%E9%80%89%E6%8B%A9%E5%99%A8%E7%A4%BA%E4%BE%8B\" target=\"_blank\" rel=\"noopener\">Use fluentd to collect and distribute audit events from log file</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend\" target=\"_blank\" rel=\"noopener\">Use logstash to collect and distribute audit events from webhook backend</a></li>\n</ul>\n<h4 id=\"3、subresource-说明\"><a href=\"#3、subresource-说明\" class=\"headerlink\" title=\"3、subresource 说明\"></a>3、subresource 说明</h4><p>kubernetes 每个资源对象都有 subresource,通过调用 master 的 api 可以获取 kubernetes 中所有的 resource 以及对应的 subresource,比如 pod 有 logs、exec 等 subresource。</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-4450a01f65b3f76d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">获取所有 resource（ 1.10 之后使用）：</span><br><span class=\"line\">$ curl  127.0.0.1:8080/openapi/v2</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://kubernetes.io/docs/concepts/overview/kubernetes-api/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/overview/kubernetes-api/</a></p>\n<h3 id=\"三、webhook-的一个简单示例\"><a href=\"#三、webhook-的一个简单示例\" class=\"headerlink\" title=\"三、webhook 的一个简单示例\"></a>三、webhook 的一个简单示例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;encoding/json&quot;</span><br><span class=\"line\">\t&quot;io/ioutil&quot;</span><br><span class=\"line\">\t&quot;log&quot;</span><br><span class=\"line\">\t&quot;net/http&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&quot;github.com/emicklei/go-restful&quot;</span><br><span class=\"line\">\t&quot;github.com/gosoon/glog&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apiserver/pkg/apis/audit&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\t// NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter)</span><br><span class=\"line\">\tcontainer := restful.NewContainer()</span><br><span class=\"line\">\tws := new(restful.WebService)</span><br><span class=\"line\">\tws.Path(&quot;/audit&quot;).</span><br><span class=\"line\">\t\tConsumes(restful.MIME_JSON).</span><br><span class=\"line\">\t\tProduces(restful.MIME_JSON)</span><br><span class=\"line\">\tws.Route(ws.POST(&quot;/webhook&quot;).To(AuditWebhook))</span><br><span class=\"line\"></span><br><span class=\"line\">\t//WebService ws2被添加到container2中</span><br><span class=\"line\">\tcontainer.Add(ws)</span><br><span class=\"line\">\tserver := &amp;http.Server&#123;</span><br><span class=\"line\">\t\tAddr:    &quot;:8081&quot;,</span><br><span class=\"line\">\t\tHandler: container,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t//go consumer()</span><br><span class=\"line\">\tlog.Fatal(server.ListenAndServe())</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func AuditWebhook(req *restful.Request, resp *restful.Response) &#123;</span><br><span class=\"line\">\tbody, err := ioutil.ReadAll(req.Request.Body)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;read body err is: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tvar eventList audit.EventList</span><br><span class=\"line\">\terr = json.Unmarshal(body, &amp;eventList)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;unmarshal failed with:%v,body is :\\n&quot;, err, string(body))</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tfor _, event := range eventList.Items &#123;</span><br><span class=\"line\">\t\tjsonBytes, err := json.Marshal(event)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Infof(&quot;marshal failed with:%v,event is \\n %+v&quot;, err, event)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 消费日志</span><br><span class=\"line\">\t\tasyncProducer(string(jsonBytes))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tresp.AddHeader(&quot;Content-Type&quot;, &quot;application/json&quot;)</span><br><span class=\"line\">\tresp.WriteEntity(&quot;success&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>完整代码请参考：<a href=\"https://github.com/gosoon/k8s-audit-webhook\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/k8s-audit-webhook</a></p>\n</blockquote>\n<h3 id=\"四、总结\"><a href=\"#四、总结\" class=\"headerlink\" title=\"四、总结\"></a>四、总结</h3><p>本文主要介绍了 kubernetes 的日志审计功能，kubernetes 最近也被爆出多个安全漏洞，安全问题是每个团队不可忽视的，kubernetes 虽然被多数公司用作私有云，但日志审计也是不可或缺的。</p>\n<hr>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</a><br><a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/audit/\" target=\"_blank\" rel=\"noopener\">ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/</a><br><a href=\"https://yq.aliyun.com/articles/686982?utm_content=g_1000040449\" target=\"_blank\" rel=\"noopener\">阿里云 Kubernetes 审计日志方案</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>审计日志可以记录所有对 apiserver 接口的调用，让我们能够非常清晰的知道集群到底发生了什么事情，通过记录的日志可以查到所发生的事件、操作的用户和时间。kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。</p>\n<blockquote>\n<p>kubernetes feature-gates 中的功能 Alpha 版本默认为 false，到 Beta 版本时默认为 true，所以 v1.8 会默认启用审计日志的功能。</p>\n</blockquote>\n<h3 id=\"一、审计日志的策略\"><a href=\"#一、审计日志的策略\" class=\"headerlink\" title=\"一、审计日志的策略\"></a>一、审计日志的策略</h3><h4 id=\"1、日志记录阶段\"><a href=\"#1、日志记录阶段\" class=\"headerlink\" title=\"1、日志记录阶段\"></a>1、日志记录阶段</h4><p>kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有：</p>\n<ul>\n<li>RequestReceived - apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。</li>\n<li>ResponseStarted - 在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。</li>\n<li>ResponseComplete - 当响应 body 发送完并且不再发送数据。</li>\n<li>Panic - 当有 panic 发生时生成。</li>\n</ul>\n<p>也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成。</p>\n<h4 id=\"2、日志记录级别\"><a href=\"#2、日志记录级别\" class=\"headerlink\" title=\"2、日志记录级别\"></a>2、日志记录级别</h4><p>当前支持的日志记录级别有：</p>\n<ul>\n<li>None - 不记录日志。</li>\n<li>Metadata - 只记录 Request 的一些 metadata (例如 user, timestamp, resource, verb 等)，但不记录 Request 或 Response 的body。</li>\n<li>Request - 记录 Request 的 metadata 和 body。</li>\n<li>RequestResponse - 最全记录方式，会记录所有的 metadata、Request 和 Response 的 body。</li>\n</ul>\n<h4 id=\"3、日志记录策略\"><a href=\"#3、日志记录策略\" class=\"headerlink\" title=\"3、日志记录策略\"></a>3、日志记录策略</h4><p>在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。</p>\n<ul>\n<li>一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段</li>\n<li>不要记录所有的资源，不要记录一个资源的所有子资源</li>\n<li>系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录</li>\n<li>对一些认证信息（secerts、configmaps、token 等）的 body 不记录 </li>\n</ul>\n<p>k8s 审计日志的一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;kind&quot;: &quot;EventList&quot;,</span><br><span class=\"line\">  &quot;apiVersion&quot;: &quot;audit.k8s.io/v1beta1&quot;,</span><br><span class=\"line\">  &quot;Items&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;Level&quot;: &quot;Request&quot;,</span><br><span class=\"line\">      &quot;AuditID&quot;: &quot;793e7ae2-5ca7-4ad3-a632-19708d2f8265&quot;,</span><br><span class=\"line\">      &quot;Stage&quot;: &quot;RequestReceived&quot;,</span><br><span class=\"line\">      &quot;RequestURI&quot;: &quot;/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0&quot;,</span><br><span class=\"line\">      &quot;Verb&quot;: &quot;get&quot;,</span><br><span class=\"line\">      &quot;User&quot;: &#123;</span><br><span class=\"line\">        &quot;Username&quot;: &quot;system:unsecured&quot;,</span><br><span class=\"line\">        &quot;UID&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;Groups&quot;: [</span><br><span class=\"line\">          &quot;system:masters&quot;,</span><br><span class=\"line\">          &quot;system:authenticated&quot;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;Extra&quot;: null</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;ImpersonatedUser&quot;: null,</span><br><span class=\"line\">      &quot;SourceIPs&quot;: [</span><br><span class=\"line\">        &quot;192.168.1.11&quot;</span><br><span class=\"line\">      ],</span><br><span class=\"line\">      &quot;UserAgent&quot;: &quot;kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler&quot;,</span><br><span class=\"line\">      &quot;ObjectRef&quot;: &#123;</span><br><span class=\"line\">        &quot;Resource&quot;: &quot;pods&quot;,</span><br><span class=\"line\">        &quot;Namespace&quot;: &quot;default&quot;,</span><br><span class=\"line\">        &quot;Name&quot;: &quot;test-pre-sf-de7cc-0&quot;,</span><br><span class=\"line\">        &quot;UID&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;APIGroup&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;APIVersion&quot;: &quot;v1&quot;,</span><br><span class=\"line\">        &quot;ResourceVersion&quot;: &quot;&quot;,</span><br><span class=\"line\">        &quot;Subresource&quot;: &quot;&quot;</span><br><span class=\"line\">      &#125;,</span><br><span class=\"line\">      &quot;ResponseStatus&quot;: null,</span><br><span class=\"line\">      &quot;RequestObject&quot;: null,</span><br><span class=\"line\">      &quot;ResponseObject&quot;: null,</span><br><span class=\"line\">      &quot;RequestReceivedTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;,</span><br><span class=\"line\">      &quot;StageTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;,</span><br><span class=\"line\">      &quot;Annotations&quot;: null</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"二、启用审计日志\"><a href=\"#二、启用审计日志\" class=\"headerlink\" title=\"二、启用审计日志\"></a>二、启用审计日志</h3><p>当前的审计日志支持两种收集方式：保存为日志文件和调用自定义的 webhook，在 v1.13 中还支持动态的 webhook。</p>\n<h4 id=\"1、将审计日志以-json-格式保存到本地文件\"><a href=\"#1、将审计日志以-json-格式保存到本地文件\" class=\"headerlink\" title=\"1、将审计日志以 json 格式保存到本地文件\"></a>1、将审计日志以 json 格式保存到本地文件</h4><p>apiserver 配置文件的 KUBE_API_ARGS 中需要添加如下参数：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json</span><br></pre></td></tr></table></figure></p>\n<p>日志保存到本地后再通过 fluentd 等其他组件进行收集。<br>还有其他几个选项可以指定保留审计日志文件的最大天数、文件的最大数量、文件的大小等。</p>\n<h4 id=\"2、将审计日志打到后端指定的-webhook\"><a href=\"#2、将审计日志打到后端指定的-webhook\" class=\"headerlink\" title=\"2、将审计日志打到后端指定的 webhook\"></a>2、将审计日志打到后端指定的 webhook</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig</span><br></pre></td></tr></table></figure>\n<p>webhook 配置文件实际上是一个 kubeconfig，apiserver 会将审计日志发送 到指定的 webhook 后，webhook 接收到日志后可以再分发到 kafka 或其他组件进行收集。</p>\n<p><code>audit-webhook-kubeconfig</code> 示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    server: http://127.0.0.1:8081/audit/webhook</span><br><span class=\"line\">  name: metric</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: metric</span><br><span class=\"line\">    user: &quot;&quot;</span><br><span class=\"line\">  name: default-context</span><br><span class=\"line\">current-context: default-context</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">users: []</span><br></pre></td></tr></table></figure></p>\n<p>前面提到过，apiserver 的每一个请求会记录三个阶段的审计日志，但是在实际中并不是需要所有的审计日志，官方也说明了启用审计日志会增加 apiserver 对内存的使用量。</p>\n<blockquote>\n<p>Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration.</p>\n</blockquote>\n<p><code>audit-policy.yaml</code> 配置示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: audit.k8s.io/v1</span><br><span class=\"line\">kind: Policy</span><br><span class=\"line\"># ResponseStarted 阶段不记录</span><br><span class=\"line\">omitStages:</span><br><span class=\"line\">  - &quot;ResponseStarted&quot;</span><br><span class=\"line\">rules:</span><br><span class=\"line\">  # 记录用户对 pod 和 statefulset 的操作</span><br><span class=\"line\">  - level: RequestResponse</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">    - group: &quot;&quot;</span><br><span class=\"line\">      resources: [&quot;pods&quot;,&quot;pods/status&quot;]</span><br><span class=\"line\">    - group: &quot;apps&quot;</span><br><span class=\"line\">      resources: [&quot;statefulsets&quot;,&quot;statefulsets/scale&quot;]</span><br><span class=\"line\">  # kube-controller-manager、kube-scheduler 等已经认证过身份的请求不需要记录</span><br><span class=\"line\">  - level: None</span><br><span class=\"line\">    userGroups: [&quot;system:authenticated&quot;]</span><br><span class=\"line\">    nonResourceURLs:</span><br><span class=\"line\">    - &quot;/api*&quot;</span><br><span class=\"line\">    - &quot;/version&quot;</span><br><span class=\"line\">  # 对 config、secret、token 等认证信息不记录请求体和返回体</span><br><span class=\"line\">  - level: Metadata</span><br><span class=\"line\">    resources:</span><br><span class=\"line\">    - group: &quot;&quot; # core API group</span><br><span class=\"line\">      resources: [&quot;secrets&quot;, &quot;configmaps&quot;]</span><br></pre></td></tr></table></figure>\n<p>官方提供两个参考示例：</p>\n<ul>\n<li><a href=\"https://kubernetes.io/zh/docs/tasks/debug-application-cluster/audit/#%E6%97%A5%E5%BF%97%E9%80%89%E6%8B%A9%E5%99%A8%E7%A4%BA%E4%BE%8B\" target=\"_blank\" rel=\"noopener\">Use fluentd to collect and distribute audit events from log file</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#use-logstash-to-collect-and-distribute-audit-events-from-webhook-backend\" target=\"_blank\" rel=\"noopener\">Use logstash to collect and distribute audit events from webhook backend</a></li>\n</ul>\n<h4 id=\"3、subresource-说明\"><a href=\"#3、subresource-说明\" class=\"headerlink\" title=\"3、subresource 说明\"></a>3、subresource 说明</h4><p>kubernetes 每个资源对象都有 subresource,通过调用 master 的 api 可以获取 kubernetes 中所有的 resource 以及对应的 subresource,比如 pod 有 logs、exec 等 subresource。</p>\n<p><img src=\"https://upload-images.jianshu.io/upload_images/1262158-4450a01f65b3f76d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"\"></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">获取所有 resource（ 1.10 之后使用）：</span><br><span class=\"line\">$ curl  127.0.0.1:8080/openapi/v2</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://kubernetes.io/docs/concepts/overview/kubernetes-api/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/overview/kubernetes-api/</a></p>\n<h3 id=\"三、webhook-的一个简单示例\"><a href=\"#三、webhook-的一个简单示例\" class=\"headerlink\" title=\"三、webhook 的一个简单示例\"></a>三、webhook 的一个简单示例</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;encoding/json&quot;</span><br><span class=\"line\">\t&quot;io/ioutil&quot;</span><br><span class=\"line\">\t&quot;log&quot;</span><br><span class=\"line\">\t&quot;net/http&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&quot;github.com/emicklei/go-restful&quot;</span><br><span class=\"line\">\t&quot;github.com/gosoon/glog&quot;</span><br><span class=\"line\">\t&quot;k8s.io/apiserver/pkg/apis/audit&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\t// NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter)</span><br><span class=\"line\">\tcontainer := restful.NewContainer()</span><br><span class=\"line\">\tws := new(restful.WebService)</span><br><span class=\"line\">\tws.Path(&quot;/audit&quot;).</span><br><span class=\"line\">\t\tConsumes(restful.MIME_JSON).</span><br><span class=\"line\">\t\tProduces(restful.MIME_JSON)</span><br><span class=\"line\">\tws.Route(ws.POST(&quot;/webhook&quot;).To(AuditWebhook))</span><br><span class=\"line\"></span><br><span class=\"line\">\t//WebService ws2被添加到container2中</span><br><span class=\"line\">\tcontainer.Add(ws)</span><br><span class=\"line\">\tserver := &amp;http.Server&#123;</span><br><span class=\"line\">\t\tAddr:    &quot;:8081&quot;,</span><br><span class=\"line\">\t\tHandler: container,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t//go consumer()</span><br><span class=\"line\">\tlog.Fatal(server.ListenAndServe())</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func AuditWebhook(req *restful.Request, resp *restful.Response) &#123;</span><br><span class=\"line\">\tbody, err := ioutil.ReadAll(req.Request.Body)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;read body err is: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tvar eventList audit.EventList</span><br><span class=\"line\">\terr = json.Unmarshal(body, &amp;eventList)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;unmarshal failed with:%v,body is :\\n&quot;, err, string(body))</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tfor _, event := range eventList.Items &#123;</span><br><span class=\"line\">\t\tjsonBytes, err := json.Marshal(event)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Infof(&quot;marshal failed with:%v,event is \\n %+v&quot;, err, event)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 消费日志</span><br><span class=\"line\">\t\tasyncProducer(string(jsonBytes))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tresp.AddHeader(&quot;Content-Type&quot;, &quot;application/json&quot;)</span><br><span class=\"line\">\tresp.WriteEntity(&quot;success&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>完整代码请参考：<a href=\"https://github.com/gosoon/k8s-audit-webhook\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/k8s-audit-webhook</a></p>\n</blockquote>\n<h3 id=\"四、总结\"><a href=\"#四、总结\" class=\"headerlink\" title=\"四、总结\"></a>四、总结</h3><p>本文主要介绍了 kubernetes 的日志审计功能，kubernetes 最近也被爆出多个安全漏洞，安全问题是每个团队不可忽视的，kubernetes 虽然被多数公司用作私有云，但日志审计也是不可或缺的。</p>\n<hr>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</a><br><a href=\"https://kubernetes.io/docs/tasks/debug-application-cluster/audit/\" target=\"_blank\" rel=\"noopener\">ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/</a><br><a href=\"https://yq.aliyun.com/articles/686982?utm_content=g_1000040449\" target=\"_blank\" rel=\"noopener\">阿里云 Kubernetes 审计日志方案</a></p>\n"},{"title":"浅析 kubernetes 的认证与鉴权机制","date":"2019-08-18T13:20:30.000Z","type":"Authentication,Authorization,RBAC","_content":"\n笔者最初接触 kubernetes 时使用的是 v1.4 版本，集群间的通信仅使用 8080 端口，认证与鉴权机制还未得到完善，到后来开始使用 static token 作为认证机制，直到 v1.6 时才开始使用 TLS 认证。随着社区的发展，kubernetes 的认证与鉴权机制已经越来越完善，新版本已经全面趋于 TLS + RBAC 配置，但其认证与鉴权机制也极其复杂，本文将会带你一步步了解。\n\n\n\nkubernetes 集群的所有操作基本上都是通过 apiserver 这个组件进行的，它提供 HTTP RESTful 形式的 API 供集群内外客户端调用。kubernetes 对于访问 API 来说提供了三个步骤的安全措施：认证、授权、准入控制，当用户使用 kubectl，client-go 或者 REST API 请求 apiserver 时，都要经过以上三个步骤的校验。认证解决的问题是识别用户的身份，鉴权是为了解决用户有哪些权限，准入控制是作用于 kubernetes 中的对象，通过合理的权限管理，能够保证系统的安全可靠。认证授权过程只存在 HTTPS 形式的 API 中，也就是说，如果客户端使用 HTTP 连接到 apiserver，是不会进行认证授权的，然而 apiserver 的非安全认证端口 8080 已经在 v1.12 中废弃了，未来将全面使用 HTTPS。\n\n\n\n![](http://cdn.tianfeiyu.com/image-20190818210224777.png)\n\n\n\n\n\n首先来看一下 kubernetes 中的认证、授权以及访问控制机制。\n\n\n\n### kubernetes 的认证机制(Authentication)\n\nkubernetes 目前所有的认证策略如下所示：\n\n- X509 client certs\n- Static Token File\n- Bootstrap Tokens\n- Static Password File\n- Service Account Tokens\n- OpenId Connect Tokens\n- Webhook Token Authentication\n- Authticating Proxy\n- Anonymous requests\n- User impersonation\n- Client-go credential plugins \n\n\n\n可以看到，kubernetes 的认证机制非常多，要想一个个搞清楚也绝非易事，本文仅分析几个比较重要且使用广泛的认证机制。\n\n#### X509 client certs\n\nX509是一种数字证书的格式标准，现在 HTTPS 依赖的 SSL 证书使用的就是使用的 X509 格式。X509 客户端证书认证方式是 kubernetes 所有认证中使用最多的一种，相对来说也是最安全的一种，kubernetes 的一些部署工具 kubeadm、minkube 等都是基于证书的认证方式。客户端证书认证叫作 TLS 双向认证，也就是服务器客户端互相验证证书的正确性，在都正确的情况下协调通信加密方案。目前最常用的 X509 证书制作工具有 openssl、cfssl 等。\n\n#### Service Account Tokens\n\n有些情况下，我们希望在 pod 内部访问 apiserver，获取集群的信息，甚至对集群进行改动。针对这种情况，kubernetes 提供了一种特殊的认证方式：serviceaccounts。 serviceaccounts 是面向 namespace 的，每个 namespace 创建的时候，kubernetes 会自动在这个 namespace 下面创建一个默认的 serviceaccounts；并且这个 serviceaccounts 只能访问该 namespace 的资源。serviceaccounts 和 pod、service、deployment 一样是 kubernetes 集群中的一种资源，用户也可以创建自己的 serviceaccounts。\n\nserviceaccounts 主要包含了三个内容：namespace、token 和 ca，每个 serviceaccounts 中都对应一个 secrets，namespace、token 和 ca 信息都是保存在 secrets 中且都通过 base64 编码的。namespace 指定了 pod 所在的 namespace，ca 用于验证 apiserver 的证书，token 用作身份验证，它们都通过 mount 的方式保存在 pod 的文件系统中，其三者都是保存在 `/var/run/secrets/kubernetes.io/serviceaccount/`目录下。\n\n\n\n关于 serviceaccounts 的配置可以参考官方的 [Configure Service Accounts for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/) 文档。\n\n\n\n> 认证机制的官方文档，请参考：https://kubernetes.io/docs/reference/access-authn-authz/authentication/\n\n\n\n##### 小结：\n\nkubernetes 中有多种认证方式，上面讲了最常使用的两种认证方式，X509 client certs 认证方式是用在一些客户端访问 apiserver 以及集群组件之间访问时使用，比如 kubectl 请求 apiserver 时。serviceaccounts 是用在 pod 中访问 apiserver 时进行认证的，比如使用自定义 controller 时。 \n\n\n\n认证解决的问题是识别用户的身份，那 kubernetes 中都有哪几种用户？目前 kubernetes 中的用户分为内部用户和外部用户，内部用户指在 kubernetes 集群中的 pod 要访问 apiserver 时所使用的，也就是 serviceaccounts，内部用户需要在 kubernetes 中创建。外部用户指 kubectl 以及一些客户端工具访问 apiserver 时所需要认证的用户，此类用户嵌入在客户端的证书中。\n\n\n\n### kubernetes 的鉴权机制(Authorization)\n\nkubernetes 目前支持如下四种鉴权机制：\n\n- Node\n- ABAC\n- RBAC\n- Webhook\n\n\n\n下面仅介绍两种最常使用的鉴权机制：\n\n#### Node\n\n仅 v1.7 版本以上支持 Node 授权，配合 NodeRestriction 准入控制来限制 kubelet，使其仅可访问 node、endpoint、pod、service 以及 secret、configmap、pv、pvc 等相关的资源，在 apiserver 中使用以下配置来开启 node 的鉴权机制：\n\n```\nKUBE_ADMISSION_CONTROL=\"...,NodeRestriction,...\"\n\nKUBE_API_ARGS=\"...,--authorization-mode=Node,...\"\n```\n\n\n\n#### RBAC\n\nRBAC（Role-Based Access Control）是 kubernetes 中负责完成授权，是基于角色的访问控制，通过自定义角色并将角色和特定的 user，group，serviceaccounts 关联起来已达到权限控制的目的。\n\n\n\nRBAC 中有三个比较重要的概念：\n\n- Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限；\n\n- Subject：被作用者，包括 user，group，serviceaccounts，通俗来讲就是认证机制中所识别的用户；\n\n- RoleBinding：定义了“被作用者”和“角色”的绑定关系，也就是将用户以及操作权限进行绑定；\n\n\n\nRBAC 其实就是通过创建角色(Role），通过 RoleBinding 将被作用者（subject）和角色（Role）进行绑定。下图是 RBAC 中的几种绑定关系：\n\n![rbac](http://cdn.tianfeiyu.com/rback.png)\n\n\n\n\n\n> 鉴权机制的官方文档，请参考：https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules\n\n\n\n\n\n### 准入控制(Admission Control)\n\n准入控制是请求的最后一个步骤，准入控制有许多内置的模块，可以作用于对象的 \"CREATE\"、\"UPDATE\"、\"DELETE\"、\"CONNECT\" 四个阶段。在这一过程中，如果任一准入控制模块拒绝，那么请求立刻被拒绝。一旦请求通过所有的准入控制器后就会写入对象存储中。\n\n\n\n准入控制是在 apiserver 中进行配置的：\n\n```\nKUBE_ADMISSION_CONTROL=\"--enable-admission-plugins=NamespaceLifecycle,LimitRanger,...MutatingAdmissionWebhook,ValidatingAdmissionWebhook,NodeRestriction...\"\n```\n\n准入控制的配置是有序的，不同的顺序会影响 kubernetes 的性能，建议使用官方的配置。\n\n\n\n若需要对 kubernetes 中的对象做一些扩展，可以使用准入控制，比如：创建 pod 时添加 initContainer 或者校验字段等。准入控制最常使用的扩展方式就是 [admission webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks)，以前写过一篇类似的文章，可以参考：[http://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/](http://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/)。\n\n\n\n>  准入控制更详细的文档，请参考：https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\n\n\n\n#### 小结：\n\n上文已经说了 kubernetes 中有两种用户，一种是内置用户被称为 serviceaccounts，一种外部用户，嵌入在客户端的证书中，那么 kubernetes 中有哪些证书链以及内嵌的用户如何与 RBAC 结合呢？\n\n\n\n### kubernetes 中的证书链\n\n笔者通过自己的研究及实践经验发现，在目前主流版本的 kubernetes 集群中，有四条重要的 CA 证书链，而在大多数生产环境中，则至少需要两条 CA 证书链。\n\n\n\n- apiserver CA 证书链：主要用于 kubernetes 内部组件互相访问以及外部客户端访问 apiserver 使用\n- etcd CA 证书链：主要用于 etcd 节点之间的访问以及 apiserver 访问 etcd 使用\n- extension apiserver CA 证书链：用于访问 extension apiserver 使用，比如 metrics-server\n- kubelet CA 信任链：用于 apiserver 访问 kubelet 时使用\n- 其他证书链：admission webhook 证书链、audit webhook 证书链，用于 apiserver 访问 webhook 时使用\n\n\n\n以上这几套 CA 证书链中，apiserver CA 证书链和 etcd CA 证书链是必要的。extension apiserver 的 CA 证书链只有在使用时才会用到，且不可与 apiserver CA 证书链相同。kubelet 的 CA 证书链不是必要的，根据部署的实际情况可以和 apiserver CA 证书链公用。\n\n\n\n#### 证书中的内嵌用户如何与 RBAC 配置进行结合\n\n\n\n##### 证书中的内嵌用户\n\n以下是 kubelet 的证书请求文件（CSR）：\n\n```\n{\n  \"CN\": \"system:node:<nodeName>\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:nodes\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\n```\n\n\n\n- “CN”：Common Name，从证书中提取该字段作为请求的用户名 (User Name)；\n- “O”：Organization，从证书中提取该字段作为请求用户所属的组 (Group)；\n\nkubernetes 使用 X509 证书中 CN(Common Name) 以及 O(Organization) 字段对应 kubernetes 中的 user 和 group，即 RBAC 中的 subject，而 kubernetes 也为多个组件内置了 Role 以及 RoleBinding，巧妙的将 Authentication 和 RBAC Authorization 结合到了一起。\n\n\n\n查看 kubernetes 中内置的 RBAC：\n\n```\n$ kubectl get clusterrole\n\n$ kubectl get clusterrolebinding\n```\n\n\n\n下面是 kubernetes 中核心组件内置的 user 和 group，在为每个组件生成证书时需要在其 CSR 中使用对应的 CN 和 O 字段。 \n\n![csr](http://cdn.tianfeiyu.com/image-20190723194531004.png)\n\n\n\n### 访问 apiserver 的几种方式\n\n通过上文可以知道访问 apiserver 时需要通过认证、鉴权以及访问控制三个步骤，认证的方式可以使用  serviceaccounts 和 X509 证书，鉴权的方式使用 RBAC，访问控制若没有特殊需求可以不使用。\n\n\n\nserviceaccounts 是 kubernetes 针对 pod 内访问 apiserver 提供的认证方式，那可以用在外部 client 端吗？答案是可以的，serviceaccounts 最终是通过 ca + token 的方式访问的，你只要创建一个 serviceaccounts 并从对应的 secrets 中获取 ca + token 即可访问 apiserver。那使用证书认证的方式可以在 pod 内访问 apiserver 吗？当然也可以，不过创建证书比 serviceaccounts 麻烦，证书默认是用于内置组件访问 apiserver 使用的。不论哪种方式，你都需要为其创建 RBAC 配置。\n\n\n\n所以在 TLS +RBAC 模式下，访问 apiserver 目前有两种方式：\n\n- 使用 serviceaccounts + RBAC ：需要创建 serviceaccounts 以及关联对应的 RBAC(ca + token + RBAC)\n- 使用证书 + RBAC：需要用到 ca、client、client-key 以及关联对应的 RBAC(ca + client-key + client-cert + RBAC)\n\n\n\n### 总结\n\n本文主要讲述了 kubernetes 中的认证(Authentication)以及鉴权(Authorization)机制，其复杂性主要体现在部署 kubernetes 集群时组件之间的认证以及在集群中为附加组件配置正确的权限，希望通过本节你可以了解到 kubernetes 中的组件需要哪些权限认证以及如何为相关组件配置正确的权限。\n\n\n\n参考： \n\nControlling Access to the Kubernetes API：https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/\n\nadmission controllers：https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\n\nkubelet 配置权限认证：https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/  \n\nmaster-node communication：https://kubernetes.io/docs/concepts/architecture/master-node-communication/\n\nkubernetes 数字证书体系浅析：https://mp.weixin.qq.com/s/iXuDbPKjSc65t_9Y1--bog\n","source":"_posts/k8s_auth_rbac.md","raw":"---\ntitle: 浅析 kubernetes 的认证与鉴权机制\ndate: 2019-08-18 21:20:30\ntags: [\"Authentication\",\"Authorization\",\"RBAC\"]\ntype: \"Authentication,Authorization,RBAC\"\n\n---\n\n笔者最初接触 kubernetes 时使用的是 v1.4 版本，集群间的通信仅使用 8080 端口，认证与鉴权机制还未得到完善，到后来开始使用 static token 作为认证机制，直到 v1.6 时才开始使用 TLS 认证。随着社区的发展，kubernetes 的认证与鉴权机制已经越来越完善，新版本已经全面趋于 TLS + RBAC 配置，但其认证与鉴权机制也极其复杂，本文将会带你一步步了解。\n\n\n\nkubernetes 集群的所有操作基本上都是通过 apiserver 这个组件进行的，它提供 HTTP RESTful 形式的 API 供集群内外客户端调用。kubernetes 对于访问 API 来说提供了三个步骤的安全措施：认证、授权、准入控制，当用户使用 kubectl，client-go 或者 REST API 请求 apiserver 时，都要经过以上三个步骤的校验。认证解决的问题是识别用户的身份，鉴权是为了解决用户有哪些权限，准入控制是作用于 kubernetes 中的对象，通过合理的权限管理，能够保证系统的安全可靠。认证授权过程只存在 HTTPS 形式的 API 中，也就是说，如果客户端使用 HTTP 连接到 apiserver，是不会进行认证授权的，然而 apiserver 的非安全认证端口 8080 已经在 v1.12 中废弃了，未来将全面使用 HTTPS。\n\n\n\n![](http://cdn.tianfeiyu.com/image-20190818210224777.png)\n\n\n\n\n\n首先来看一下 kubernetes 中的认证、授权以及访问控制机制。\n\n\n\n### kubernetes 的认证机制(Authentication)\n\nkubernetes 目前所有的认证策略如下所示：\n\n- X509 client certs\n- Static Token File\n- Bootstrap Tokens\n- Static Password File\n- Service Account Tokens\n- OpenId Connect Tokens\n- Webhook Token Authentication\n- Authticating Proxy\n- Anonymous requests\n- User impersonation\n- Client-go credential plugins \n\n\n\n可以看到，kubernetes 的认证机制非常多，要想一个个搞清楚也绝非易事，本文仅分析几个比较重要且使用广泛的认证机制。\n\n#### X509 client certs\n\nX509是一种数字证书的格式标准，现在 HTTPS 依赖的 SSL 证书使用的就是使用的 X509 格式。X509 客户端证书认证方式是 kubernetes 所有认证中使用最多的一种，相对来说也是最安全的一种，kubernetes 的一些部署工具 kubeadm、minkube 等都是基于证书的认证方式。客户端证书认证叫作 TLS 双向认证，也就是服务器客户端互相验证证书的正确性，在都正确的情况下协调通信加密方案。目前最常用的 X509 证书制作工具有 openssl、cfssl 等。\n\n#### Service Account Tokens\n\n有些情况下，我们希望在 pod 内部访问 apiserver，获取集群的信息，甚至对集群进行改动。针对这种情况，kubernetes 提供了一种特殊的认证方式：serviceaccounts。 serviceaccounts 是面向 namespace 的，每个 namespace 创建的时候，kubernetes 会自动在这个 namespace 下面创建一个默认的 serviceaccounts；并且这个 serviceaccounts 只能访问该 namespace 的资源。serviceaccounts 和 pod、service、deployment 一样是 kubernetes 集群中的一种资源，用户也可以创建自己的 serviceaccounts。\n\nserviceaccounts 主要包含了三个内容：namespace、token 和 ca，每个 serviceaccounts 中都对应一个 secrets，namespace、token 和 ca 信息都是保存在 secrets 中且都通过 base64 编码的。namespace 指定了 pod 所在的 namespace，ca 用于验证 apiserver 的证书，token 用作身份验证，它们都通过 mount 的方式保存在 pod 的文件系统中，其三者都是保存在 `/var/run/secrets/kubernetes.io/serviceaccount/`目录下。\n\n\n\n关于 serviceaccounts 的配置可以参考官方的 [Configure Service Accounts for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/) 文档。\n\n\n\n> 认证机制的官方文档，请参考：https://kubernetes.io/docs/reference/access-authn-authz/authentication/\n\n\n\n##### 小结：\n\nkubernetes 中有多种认证方式，上面讲了最常使用的两种认证方式，X509 client certs 认证方式是用在一些客户端访问 apiserver 以及集群组件之间访问时使用，比如 kubectl 请求 apiserver 时。serviceaccounts 是用在 pod 中访问 apiserver 时进行认证的，比如使用自定义 controller 时。 \n\n\n\n认证解决的问题是识别用户的身份，那 kubernetes 中都有哪几种用户？目前 kubernetes 中的用户分为内部用户和外部用户，内部用户指在 kubernetes 集群中的 pod 要访问 apiserver 时所使用的，也就是 serviceaccounts，内部用户需要在 kubernetes 中创建。外部用户指 kubectl 以及一些客户端工具访问 apiserver 时所需要认证的用户，此类用户嵌入在客户端的证书中。\n\n\n\n### kubernetes 的鉴权机制(Authorization)\n\nkubernetes 目前支持如下四种鉴权机制：\n\n- Node\n- ABAC\n- RBAC\n- Webhook\n\n\n\n下面仅介绍两种最常使用的鉴权机制：\n\n#### Node\n\n仅 v1.7 版本以上支持 Node 授权，配合 NodeRestriction 准入控制来限制 kubelet，使其仅可访问 node、endpoint、pod、service 以及 secret、configmap、pv、pvc 等相关的资源，在 apiserver 中使用以下配置来开启 node 的鉴权机制：\n\n```\nKUBE_ADMISSION_CONTROL=\"...,NodeRestriction,...\"\n\nKUBE_API_ARGS=\"...,--authorization-mode=Node,...\"\n```\n\n\n\n#### RBAC\n\nRBAC（Role-Based Access Control）是 kubernetes 中负责完成授权，是基于角色的访问控制，通过自定义角色并将角色和特定的 user，group，serviceaccounts 关联起来已达到权限控制的目的。\n\n\n\nRBAC 中有三个比较重要的概念：\n\n- Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限；\n\n- Subject：被作用者，包括 user，group，serviceaccounts，通俗来讲就是认证机制中所识别的用户；\n\n- RoleBinding：定义了“被作用者”和“角色”的绑定关系，也就是将用户以及操作权限进行绑定；\n\n\n\nRBAC 其实就是通过创建角色(Role），通过 RoleBinding 将被作用者（subject）和角色（Role）进行绑定。下图是 RBAC 中的几种绑定关系：\n\n![rbac](http://cdn.tianfeiyu.com/rback.png)\n\n\n\n\n\n> 鉴权机制的官方文档，请参考：https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules\n\n\n\n\n\n### 准入控制(Admission Control)\n\n准入控制是请求的最后一个步骤，准入控制有许多内置的模块，可以作用于对象的 \"CREATE\"、\"UPDATE\"、\"DELETE\"、\"CONNECT\" 四个阶段。在这一过程中，如果任一准入控制模块拒绝，那么请求立刻被拒绝。一旦请求通过所有的准入控制器后就会写入对象存储中。\n\n\n\n准入控制是在 apiserver 中进行配置的：\n\n```\nKUBE_ADMISSION_CONTROL=\"--enable-admission-plugins=NamespaceLifecycle,LimitRanger,...MutatingAdmissionWebhook,ValidatingAdmissionWebhook,NodeRestriction...\"\n```\n\n准入控制的配置是有序的，不同的顺序会影响 kubernetes 的性能，建议使用官方的配置。\n\n\n\n若需要对 kubernetes 中的对象做一些扩展，可以使用准入控制，比如：创建 pod 时添加 initContainer 或者校验字段等。准入控制最常使用的扩展方式就是 [admission webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks)，以前写过一篇类似的文章，可以参考：[http://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/](http://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/)。\n\n\n\n>  准入控制更详细的文档，请参考：https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\n\n\n\n#### 小结：\n\n上文已经说了 kubernetes 中有两种用户，一种是内置用户被称为 serviceaccounts，一种外部用户，嵌入在客户端的证书中，那么 kubernetes 中有哪些证书链以及内嵌的用户如何与 RBAC 结合呢？\n\n\n\n### kubernetes 中的证书链\n\n笔者通过自己的研究及实践经验发现，在目前主流版本的 kubernetes 集群中，有四条重要的 CA 证书链，而在大多数生产环境中，则至少需要两条 CA 证书链。\n\n\n\n- apiserver CA 证书链：主要用于 kubernetes 内部组件互相访问以及外部客户端访问 apiserver 使用\n- etcd CA 证书链：主要用于 etcd 节点之间的访问以及 apiserver 访问 etcd 使用\n- extension apiserver CA 证书链：用于访问 extension apiserver 使用，比如 metrics-server\n- kubelet CA 信任链：用于 apiserver 访问 kubelet 时使用\n- 其他证书链：admission webhook 证书链、audit webhook 证书链，用于 apiserver 访问 webhook 时使用\n\n\n\n以上这几套 CA 证书链中，apiserver CA 证书链和 etcd CA 证书链是必要的。extension apiserver 的 CA 证书链只有在使用时才会用到，且不可与 apiserver CA 证书链相同。kubelet 的 CA 证书链不是必要的，根据部署的实际情况可以和 apiserver CA 证书链公用。\n\n\n\n#### 证书中的内嵌用户如何与 RBAC 配置进行结合\n\n\n\n##### 证书中的内嵌用户\n\n以下是 kubelet 的证书请求文件（CSR）：\n\n```\n{\n  \"CN\": \"system:node:<nodeName>\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:nodes\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\n```\n\n\n\n- “CN”：Common Name，从证书中提取该字段作为请求的用户名 (User Name)；\n- “O”：Organization，从证书中提取该字段作为请求用户所属的组 (Group)；\n\nkubernetes 使用 X509 证书中 CN(Common Name) 以及 O(Organization) 字段对应 kubernetes 中的 user 和 group，即 RBAC 中的 subject，而 kubernetes 也为多个组件内置了 Role 以及 RoleBinding，巧妙的将 Authentication 和 RBAC Authorization 结合到了一起。\n\n\n\n查看 kubernetes 中内置的 RBAC：\n\n```\n$ kubectl get clusterrole\n\n$ kubectl get clusterrolebinding\n```\n\n\n\n下面是 kubernetes 中核心组件内置的 user 和 group，在为每个组件生成证书时需要在其 CSR 中使用对应的 CN 和 O 字段。 \n\n![csr](http://cdn.tianfeiyu.com/image-20190723194531004.png)\n\n\n\n### 访问 apiserver 的几种方式\n\n通过上文可以知道访问 apiserver 时需要通过认证、鉴权以及访问控制三个步骤，认证的方式可以使用  serviceaccounts 和 X509 证书，鉴权的方式使用 RBAC，访问控制若没有特殊需求可以不使用。\n\n\n\nserviceaccounts 是 kubernetes 针对 pod 内访问 apiserver 提供的认证方式，那可以用在外部 client 端吗？答案是可以的，serviceaccounts 最终是通过 ca + token 的方式访问的，你只要创建一个 serviceaccounts 并从对应的 secrets 中获取 ca + token 即可访问 apiserver。那使用证书认证的方式可以在 pod 内访问 apiserver 吗？当然也可以，不过创建证书比 serviceaccounts 麻烦，证书默认是用于内置组件访问 apiserver 使用的。不论哪种方式，你都需要为其创建 RBAC 配置。\n\n\n\n所以在 TLS +RBAC 模式下，访问 apiserver 目前有两种方式：\n\n- 使用 serviceaccounts + RBAC ：需要创建 serviceaccounts 以及关联对应的 RBAC(ca + token + RBAC)\n- 使用证书 + RBAC：需要用到 ca、client、client-key 以及关联对应的 RBAC(ca + client-key + client-cert + RBAC)\n\n\n\n### 总结\n\n本文主要讲述了 kubernetes 中的认证(Authentication)以及鉴权(Authorization)机制，其复杂性主要体现在部署 kubernetes 集群时组件之间的认证以及在集群中为附加组件配置正确的权限，希望通过本节你可以了解到 kubernetes 中的组件需要哪些权限认证以及如何为相关组件配置正确的权限。\n\n\n\n参考： \n\nControlling Access to the Kubernetes API：https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/\n\nadmission controllers：https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\n\nkubelet 配置权限认证：https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/  \n\nmaster-node communication：https://kubernetes.io/docs/concepts/architecture/master-node-communication/\n\nkubernetes 数字证书体系浅析：https://mp.weixin.qq.com/s/iXuDbPKjSc65t_9Y1--bog\n","slug":"k8s_auth_rbac","published":1,"updated":"2019-08-18T13:26:08.396Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59c000fapwn06p4tnjb","content":"<p>笔者最初接触 kubernetes 时使用的是 v1.4 版本，集群间的通信仅使用 8080 端口，认证与鉴权机制还未得到完善，到后来开始使用 static token 作为认证机制，直到 v1.6 时才开始使用 TLS 认证。随着社区的发展，kubernetes 的认证与鉴权机制已经越来越完善，新版本已经全面趋于 TLS + RBAC 配置，但其认证与鉴权机制也极其复杂，本文将会带你一步步了解。</p>\n<p>kubernetes 集群的所有操作基本上都是通过 apiserver 这个组件进行的，它提供 HTTP RESTful 形式的 API 供集群内外客户端调用。kubernetes 对于访问 API 来说提供了三个步骤的安全措施：认证、授权、准入控制，当用户使用 kubectl，client-go 或者 REST API 请求 apiserver 时，都要经过以上三个步骤的校验。认证解决的问题是识别用户的身份，鉴权是为了解决用户有哪些权限，准入控制是作用于 kubernetes 中的对象，通过合理的权限管理，能够保证系统的安全可靠。认证授权过程只存在 HTTPS 形式的 API 中，也就是说，如果客户端使用 HTTP 连接到 apiserver，是不会进行认证授权的，然而 apiserver 的非安全认证端口 8080 已经在 v1.12 中废弃了，未来将全面使用 HTTPS。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20190818210224777.png\" alt=\"\"></p>\n<p>首先来看一下 kubernetes 中的认证、授权以及访问控制机制。</p>\n<h3 id=\"kubernetes-的认证机制-Authentication\"><a href=\"#kubernetes-的认证机制-Authentication\" class=\"headerlink\" title=\"kubernetes 的认证机制(Authentication)\"></a>kubernetes 的认证机制(Authentication)</h3><p>kubernetes 目前所有的认证策略如下所示：</p>\n<ul>\n<li>X509 client certs</li>\n<li>Static Token File</li>\n<li>Bootstrap Tokens</li>\n<li>Static Password File</li>\n<li>Service Account Tokens</li>\n<li>OpenId Connect Tokens</li>\n<li>Webhook Token Authentication</li>\n<li>Authticating Proxy</li>\n<li>Anonymous requests</li>\n<li>User impersonation</li>\n<li>Client-go credential plugins </li>\n</ul>\n<p>可以看到，kubernetes 的认证机制非常多，要想一个个搞清楚也绝非易事，本文仅分析几个比较重要且使用广泛的认证机制。</p>\n<h4 id=\"X509-client-certs\"><a href=\"#X509-client-certs\" class=\"headerlink\" title=\"X509 client certs\"></a>X509 client certs</h4><p>X509是一种数字证书的格式标准，现在 HTTPS 依赖的 SSL 证书使用的就是使用的 X509 格式。X509 客户端证书认证方式是 kubernetes 所有认证中使用最多的一种，相对来说也是最安全的一种，kubernetes 的一些部署工具 kubeadm、minkube 等都是基于证书的认证方式。客户端证书认证叫作 TLS 双向认证，也就是服务器客户端互相验证证书的正确性，在都正确的情况下协调通信加密方案。目前最常用的 X509 证书制作工具有 openssl、cfssl 等。</p>\n<h4 id=\"Service-Account-Tokens\"><a href=\"#Service-Account-Tokens\" class=\"headerlink\" title=\"Service Account Tokens\"></a>Service Account Tokens</h4><p>有些情况下，我们希望在 pod 内部访问 apiserver，获取集群的信息，甚至对集群进行改动。针对这种情况，kubernetes 提供了一种特殊的认证方式：serviceaccounts。 serviceaccounts 是面向 namespace 的，每个 namespace 创建的时候，kubernetes 会自动在这个 namespace 下面创建一个默认的 serviceaccounts；并且这个 serviceaccounts 只能访问该 namespace 的资源。serviceaccounts 和 pod、service、deployment 一样是 kubernetes 集群中的一种资源，用户也可以创建自己的 serviceaccounts。</p>\n<p>serviceaccounts 主要包含了三个内容：namespace、token 和 ca，每个 serviceaccounts 中都对应一个 secrets，namespace、token 和 ca 信息都是保存在 secrets 中且都通过 base64 编码的。namespace 指定了 pod 所在的 namespace，ca 用于验证 apiserver 的证书，token 用作身份验证，它们都通过 mount 的方式保存在 pod 的文件系统中，其三者都是保存在 <code>/var/run/secrets/kubernetes.io/serviceaccount/</code>目录下。</p>\n<p>关于 serviceaccounts 的配置可以参考官方的 <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\" target=\"_blank\" rel=\"noopener\">Configure Service Accounts for Pods</a> 文档。</p>\n<blockquote>\n<p>认证机制的官方文档，请参考：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/authentication/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/authentication/</a></p>\n</blockquote>\n<h5 id=\"小结：\"><a href=\"#小结：\" class=\"headerlink\" title=\"小结：\"></a>小结：</h5><p>kubernetes 中有多种认证方式，上面讲了最常使用的两种认证方式，X509 client certs 认证方式是用在一些客户端访问 apiserver 以及集群组件之间访问时使用，比如 kubectl 请求 apiserver 时。serviceaccounts 是用在 pod 中访问 apiserver 时进行认证的，比如使用自定义 controller 时。 </p>\n<p>认证解决的问题是识别用户的身份，那 kubernetes 中都有哪几种用户？目前 kubernetes 中的用户分为内部用户和外部用户，内部用户指在 kubernetes 集群中的 pod 要访问 apiserver 时所使用的，也就是 serviceaccounts，内部用户需要在 kubernetes 中创建。外部用户指 kubectl 以及一些客户端工具访问 apiserver 时所需要认证的用户，此类用户嵌入在客户端的证书中。</p>\n<h3 id=\"kubernetes-的鉴权机制-Authorization\"><a href=\"#kubernetes-的鉴权机制-Authorization\" class=\"headerlink\" title=\"kubernetes 的鉴权机制(Authorization)\"></a>kubernetes 的鉴权机制(Authorization)</h3><p>kubernetes 目前支持如下四种鉴权机制：</p>\n<ul>\n<li>Node</li>\n<li>ABAC</li>\n<li>RBAC</li>\n<li>Webhook</li>\n</ul>\n<p>下面仅介绍两种最常使用的鉴权机制：</p>\n<h4 id=\"Node\"><a href=\"#Node\" class=\"headerlink\" title=\"Node\"></a>Node</h4><p>仅 v1.7 版本以上支持 Node 授权，配合 NodeRestriction 准入控制来限制 kubelet，使其仅可访问 node、endpoint、pod、service 以及 secret、configmap、pv、pvc 等相关的资源，在 apiserver 中使用以下配置来开启 node 的鉴权机制：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KUBE_ADMISSION_CONTROL=&quot;...,NodeRestriction,...&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">KUBE_API_ARGS=&quot;...,--authorization-mode=Node,...&quot;</span><br></pre></td></tr></table></figure>\n<h4 id=\"RBAC\"><a href=\"#RBAC\" class=\"headerlink\" title=\"RBAC\"></a>RBAC</h4><p>RBAC（Role-Based Access Control）是 kubernetes 中负责完成授权，是基于角色的访问控制，通过自定义角色并将角色和特定的 user，group，serviceaccounts 关联起来已达到权限控制的目的。</p>\n<p>RBAC 中有三个比较重要的概念：</p>\n<ul>\n<li><p>Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限；</p>\n</li>\n<li><p>Subject：被作用者，包括 user，group，serviceaccounts，通俗来讲就是认证机制中所识别的用户；</p>\n</li>\n<li><p>RoleBinding：定义了“被作用者”和“角色”的绑定关系，也就是将用户以及操作权限进行绑定；</p>\n</li>\n</ul>\n<p>RBAC 其实就是通过创建角色(Role），通过 RoleBinding 将被作用者（subject）和角色（Role）进行绑定。下图是 RBAC 中的几种绑定关系：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/rback.png\" alt=\"rbac\"></p>\n<blockquote>\n<p>鉴权机制的官方文档，请参考：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules</a></p>\n</blockquote>\n<h3 id=\"准入控制-Admission-Control\"><a href=\"#准入控制-Admission-Control\" class=\"headerlink\" title=\"准入控制(Admission Control)\"></a>准入控制(Admission Control)</h3><p>准入控制是请求的最后一个步骤，准入控制有许多内置的模块，可以作用于对象的 “CREATE”、”UPDATE”、”DELETE”、”CONNECT” 四个阶段。在这一过程中，如果任一准入控制模块拒绝，那么请求立刻被拒绝。一旦请求通过所有的准入控制器后就会写入对象存储中。</p>\n<p>准入控制是在 apiserver 中进行配置的：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KUBE_ADMISSION_CONTROL=&quot;--enable-admission-plugins=NamespaceLifecycle,LimitRanger,...MutatingAdmissionWebhook,ValidatingAdmissionWebhook,NodeRestriction...&quot;</span><br></pre></td></tr></table></figure>\n<p>准入控制的配置是有序的，不同的顺序会影响 kubernetes 的性能，建议使用官方的配置。</p>\n<p>若需要对 kubernetes 中的对象做一些扩展，可以使用准入控制，比如：创建 pod 时添加 initContainer 或者校验字段等。准入控制最常使用的扩展方式就是 <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks\" target=\"_blank\" rel=\"noopener\">admission webhooks</a>，以前写过一篇类似的文章，可以参考：<a href=\"http://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/\" target=\"_blank\" rel=\"noopener\">http://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/</a>。</p>\n<blockquote>\n<p> 准入控制更详细的文档，请参考：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</a></p>\n</blockquote>\n<h4 id=\"小结：-1\"><a href=\"#小结：-1\" class=\"headerlink\" title=\"小结：\"></a>小结：</h4><p>上文已经说了 kubernetes 中有两种用户，一种是内置用户被称为 serviceaccounts，一种外部用户，嵌入在客户端的证书中，那么 kubernetes 中有哪些证书链以及内嵌的用户如何与 RBAC 结合呢？</p>\n<h3 id=\"kubernetes-中的证书链\"><a href=\"#kubernetes-中的证书链\" class=\"headerlink\" title=\"kubernetes 中的证书链\"></a>kubernetes 中的证书链</h3><p>笔者通过自己的研究及实践经验发现，在目前主流版本的 kubernetes 集群中，有四条重要的 CA 证书链，而在大多数生产环境中，则至少需要两条 CA 证书链。</p>\n<ul>\n<li>apiserver CA 证书链：主要用于 kubernetes 内部组件互相访问以及外部客户端访问 apiserver 使用</li>\n<li>etcd CA 证书链：主要用于 etcd 节点之间的访问以及 apiserver 访问 etcd 使用</li>\n<li>extension apiserver CA 证书链：用于访问 extension apiserver 使用，比如 metrics-server</li>\n<li>kubelet CA 信任链：用于 apiserver 访问 kubelet 时使用</li>\n<li>其他证书链：admission webhook 证书链、audit webhook 证书链，用于 apiserver 访问 webhook 时使用</li>\n</ul>\n<p>以上这几套 CA 证书链中，apiserver CA 证书链和 etcd CA 证书链是必要的。extension apiserver 的 CA 证书链只有在使用时才会用到，且不可与 apiserver CA 证书链相同。kubelet 的 CA 证书链不是必要的，根据部署的实际情况可以和 apiserver CA 证书链公用。</p>\n<h4 id=\"证书中的内嵌用户如何与-RBAC-配置进行结合\"><a href=\"#证书中的内嵌用户如何与-RBAC-配置进行结合\" class=\"headerlink\" title=\"证书中的内嵌用户如何与 RBAC 配置进行结合\"></a>证书中的内嵌用户如何与 RBAC 配置进行结合</h4><h5 id=\"证书中的内嵌用户\"><a href=\"#证书中的内嵌用户\" class=\"headerlink\" title=\"证书中的内嵌用户\"></a>证书中的内嵌用户</h5><p>以下是 kubelet 的证书请求文件（CSR）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:node:&lt;nodeName&gt;&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:nodes&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>“CN”：Common Name，从证书中提取该字段作为请求的用户名 (User Name)；</li>\n<li>“O”：Organization，从证书中提取该字段作为请求用户所属的组 (Group)；</li>\n</ul>\n<p>kubernetes 使用 X509 证书中 CN(Common Name) 以及 O(Organization) 字段对应 kubernetes 中的 user 和 group，即 RBAC 中的 subject，而 kubernetes 也为多个组件内置了 Role 以及 RoleBinding，巧妙的将 Authentication 和 RBAC Authorization 结合到了一起。</p>\n<p>查看 kubernetes 中内置的 RBAC：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get clusterrole</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get clusterrolebinding</span><br></pre></td></tr></table></figure>\n<p>下面是 kubernetes 中核心组件内置的 user 和 group，在为每个组件生成证书时需要在其 CSR 中使用对应的 CN 和 O 字段。 </p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20190723194531004.png\" alt=\"csr\"></p>\n<h3 id=\"访问-apiserver-的几种方式\"><a href=\"#访问-apiserver-的几种方式\" class=\"headerlink\" title=\"访问 apiserver 的几种方式\"></a>访问 apiserver 的几种方式</h3><p>通过上文可以知道访问 apiserver 时需要通过认证、鉴权以及访问控制三个步骤，认证的方式可以使用  serviceaccounts 和 X509 证书，鉴权的方式使用 RBAC，访问控制若没有特殊需求可以不使用。</p>\n<p>serviceaccounts 是 kubernetes 针对 pod 内访问 apiserver 提供的认证方式，那可以用在外部 client 端吗？答案是可以的，serviceaccounts 最终是通过 ca + token 的方式访问的，你只要创建一个 serviceaccounts 并从对应的 secrets 中获取 ca + token 即可访问 apiserver。那使用证书认证的方式可以在 pod 内访问 apiserver 吗？当然也可以，不过创建证书比 serviceaccounts 麻烦，证书默认是用于内置组件访问 apiserver 使用的。不论哪种方式，你都需要为其创建 RBAC 配置。</p>\n<p>所以在 TLS +RBAC 模式下，访问 apiserver 目前有两种方式：</p>\n<ul>\n<li>使用 serviceaccounts + RBAC ：需要创建 serviceaccounts 以及关联对应的 RBAC(ca + token + RBAC)</li>\n<li>使用证书 + RBAC：需要用到 ca、client、client-key 以及关联对应的 RBAC(ca + client-key + client-cert + RBAC)</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要讲述了 kubernetes 中的认证(Authentication)以及鉴权(Authorization)机制，其复杂性主要体现在部署 kubernetes 集群时组件之间的认证以及在集群中为附加组件配置正确的权限，希望通过本节你可以了解到 kubernetes 中的组件需要哪些权限认证以及如何为相关组件配置正确的权限。</p>\n<p>参考： </p>\n<p>Controlling Access to the Kubernetes API：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/</a></p>\n<p>admission controllers：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</a></p>\n<p>kubelet 配置权限认证：<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</a>  </p>\n<p>master-node communication：<a href=\"https://kubernetes.io/docs/concepts/architecture/master-node-communication/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/architecture/master-node-communication/</a></p>\n<p>kubernetes 数字证书体系浅析：<a href=\"https://mp.weixin.qq.com/s/iXuDbPKjSc65t_9Y1--bog\" target=\"_blank\" rel=\"noopener\">https://mp.weixin.qq.com/s/iXuDbPKjSc65t_9Y1--bog</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>笔者最初接触 kubernetes 时使用的是 v1.4 版本，集群间的通信仅使用 8080 端口，认证与鉴权机制还未得到完善，到后来开始使用 static token 作为认证机制，直到 v1.6 时才开始使用 TLS 认证。随着社区的发展，kubernetes 的认证与鉴权机制已经越来越完善，新版本已经全面趋于 TLS + RBAC 配置，但其认证与鉴权机制也极其复杂，本文将会带你一步步了解。</p>\n<p>kubernetes 集群的所有操作基本上都是通过 apiserver 这个组件进行的，它提供 HTTP RESTful 形式的 API 供集群内外客户端调用。kubernetes 对于访问 API 来说提供了三个步骤的安全措施：认证、授权、准入控制，当用户使用 kubectl，client-go 或者 REST API 请求 apiserver 时，都要经过以上三个步骤的校验。认证解决的问题是识别用户的身份，鉴权是为了解决用户有哪些权限，准入控制是作用于 kubernetes 中的对象，通过合理的权限管理，能够保证系统的安全可靠。认证授权过程只存在 HTTPS 形式的 API 中，也就是说，如果客户端使用 HTTP 连接到 apiserver，是不会进行认证授权的，然而 apiserver 的非安全认证端口 8080 已经在 v1.12 中废弃了，未来将全面使用 HTTPS。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20190818210224777.png\" alt=\"\"></p>\n<p>首先来看一下 kubernetes 中的认证、授权以及访问控制机制。</p>\n<h3 id=\"kubernetes-的认证机制-Authentication\"><a href=\"#kubernetes-的认证机制-Authentication\" class=\"headerlink\" title=\"kubernetes 的认证机制(Authentication)\"></a>kubernetes 的认证机制(Authentication)</h3><p>kubernetes 目前所有的认证策略如下所示：</p>\n<ul>\n<li>X509 client certs</li>\n<li>Static Token File</li>\n<li>Bootstrap Tokens</li>\n<li>Static Password File</li>\n<li>Service Account Tokens</li>\n<li>OpenId Connect Tokens</li>\n<li>Webhook Token Authentication</li>\n<li>Authticating Proxy</li>\n<li>Anonymous requests</li>\n<li>User impersonation</li>\n<li>Client-go credential plugins </li>\n</ul>\n<p>可以看到，kubernetes 的认证机制非常多，要想一个个搞清楚也绝非易事，本文仅分析几个比较重要且使用广泛的认证机制。</p>\n<h4 id=\"X509-client-certs\"><a href=\"#X509-client-certs\" class=\"headerlink\" title=\"X509 client certs\"></a>X509 client certs</h4><p>X509是一种数字证书的格式标准，现在 HTTPS 依赖的 SSL 证书使用的就是使用的 X509 格式。X509 客户端证书认证方式是 kubernetes 所有认证中使用最多的一种，相对来说也是最安全的一种，kubernetes 的一些部署工具 kubeadm、minkube 等都是基于证书的认证方式。客户端证书认证叫作 TLS 双向认证，也就是服务器客户端互相验证证书的正确性，在都正确的情况下协调通信加密方案。目前最常用的 X509 证书制作工具有 openssl、cfssl 等。</p>\n<h4 id=\"Service-Account-Tokens\"><a href=\"#Service-Account-Tokens\" class=\"headerlink\" title=\"Service Account Tokens\"></a>Service Account Tokens</h4><p>有些情况下，我们希望在 pod 内部访问 apiserver，获取集群的信息，甚至对集群进行改动。针对这种情况，kubernetes 提供了一种特殊的认证方式：serviceaccounts。 serviceaccounts 是面向 namespace 的，每个 namespace 创建的时候，kubernetes 会自动在这个 namespace 下面创建一个默认的 serviceaccounts；并且这个 serviceaccounts 只能访问该 namespace 的资源。serviceaccounts 和 pod、service、deployment 一样是 kubernetes 集群中的一种资源，用户也可以创建自己的 serviceaccounts。</p>\n<p>serviceaccounts 主要包含了三个内容：namespace、token 和 ca，每个 serviceaccounts 中都对应一个 secrets，namespace、token 和 ca 信息都是保存在 secrets 中且都通过 base64 编码的。namespace 指定了 pod 所在的 namespace，ca 用于验证 apiserver 的证书，token 用作身份验证，它们都通过 mount 的方式保存在 pod 的文件系统中，其三者都是保存在 <code>/var/run/secrets/kubernetes.io/serviceaccount/</code>目录下。</p>\n<p>关于 serviceaccounts 的配置可以参考官方的 <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\" target=\"_blank\" rel=\"noopener\">Configure Service Accounts for Pods</a> 文档。</p>\n<blockquote>\n<p>认证机制的官方文档，请参考：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/authentication/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/authentication/</a></p>\n</blockquote>\n<h5 id=\"小结：\"><a href=\"#小结：\" class=\"headerlink\" title=\"小结：\"></a>小结：</h5><p>kubernetes 中有多种认证方式，上面讲了最常使用的两种认证方式，X509 client certs 认证方式是用在一些客户端访问 apiserver 以及集群组件之间访问时使用，比如 kubectl 请求 apiserver 时。serviceaccounts 是用在 pod 中访问 apiserver 时进行认证的，比如使用自定义 controller 时。 </p>\n<p>认证解决的问题是识别用户的身份，那 kubernetes 中都有哪几种用户？目前 kubernetes 中的用户分为内部用户和外部用户，内部用户指在 kubernetes 集群中的 pod 要访问 apiserver 时所使用的，也就是 serviceaccounts，内部用户需要在 kubernetes 中创建。外部用户指 kubectl 以及一些客户端工具访问 apiserver 时所需要认证的用户，此类用户嵌入在客户端的证书中。</p>\n<h3 id=\"kubernetes-的鉴权机制-Authorization\"><a href=\"#kubernetes-的鉴权机制-Authorization\" class=\"headerlink\" title=\"kubernetes 的鉴权机制(Authorization)\"></a>kubernetes 的鉴权机制(Authorization)</h3><p>kubernetes 目前支持如下四种鉴权机制：</p>\n<ul>\n<li>Node</li>\n<li>ABAC</li>\n<li>RBAC</li>\n<li>Webhook</li>\n</ul>\n<p>下面仅介绍两种最常使用的鉴权机制：</p>\n<h4 id=\"Node\"><a href=\"#Node\" class=\"headerlink\" title=\"Node\"></a>Node</h4><p>仅 v1.7 版本以上支持 Node 授权，配合 NodeRestriction 准入控制来限制 kubelet，使其仅可访问 node、endpoint、pod、service 以及 secret、configmap、pv、pvc 等相关的资源，在 apiserver 中使用以下配置来开启 node 的鉴权机制：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KUBE_ADMISSION_CONTROL=&quot;...,NodeRestriction,...&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">KUBE_API_ARGS=&quot;...,--authorization-mode=Node,...&quot;</span><br></pre></td></tr></table></figure>\n<h4 id=\"RBAC\"><a href=\"#RBAC\" class=\"headerlink\" title=\"RBAC\"></a>RBAC</h4><p>RBAC（Role-Based Access Control）是 kubernetes 中负责完成授权，是基于角色的访问控制，通过自定义角色并将角色和特定的 user，group，serviceaccounts 关联起来已达到权限控制的目的。</p>\n<p>RBAC 中有三个比较重要的概念：</p>\n<ul>\n<li><p>Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限；</p>\n</li>\n<li><p>Subject：被作用者，包括 user，group，serviceaccounts，通俗来讲就是认证机制中所识别的用户；</p>\n</li>\n<li><p>RoleBinding：定义了“被作用者”和“角色”的绑定关系，也就是将用户以及操作权限进行绑定；</p>\n</li>\n</ul>\n<p>RBAC 其实就是通过创建角色(Role），通过 RoleBinding 将被作用者（subject）和角色（Role）进行绑定。下图是 RBAC 中的几种绑定关系：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/rback.png\" alt=\"rbac\"></p>\n<blockquote>\n<p>鉴权机制的官方文档，请参考：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules</a></p>\n</blockquote>\n<h3 id=\"准入控制-Admission-Control\"><a href=\"#准入控制-Admission-Control\" class=\"headerlink\" title=\"准入控制(Admission Control)\"></a>准入控制(Admission Control)</h3><p>准入控制是请求的最后一个步骤，准入控制有许多内置的模块，可以作用于对象的 “CREATE”、”UPDATE”、”DELETE”、”CONNECT” 四个阶段。在这一过程中，如果任一准入控制模块拒绝，那么请求立刻被拒绝。一旦请求通过所有的准入控制器后就会写入对象存储中。</p>\n<p>准入控制是在 apiserver 中进行配置的：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">KUBE_ADMISSION_CONTROL=&quot;--enable-admission-plugins=NamespaceLifecycle,LimitRanger,...MutatingAdmissionWebhook,ValidatingAdmissionWebhook,NodeRestriction...&quot;</span><br></pre></td></tr></table></figure>\n<p>准入控制的配置是有序的，不同的顺序会影响 kubernetes 的性能，建议使用官方的配置。</p>\n<p>若需要对 kubernetes 中的对象做一些扩展，可以使用准入控制，比如：创建 pod 时添加 initContainer 或者校验字段等。准入控制最常使用的扩展方式就是 <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks\" target=\"_blank\" rel=\"noopener\">admission webhooks</a>，以前写过一篇类似的文章，可以参考：<a href=\"http://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/\" target=\"_blank\" rel=\"noopener\">http://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/</a>。</p>\n<blockquote>\n<p> 准入控制更详细的文档，请参考：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</a></p>\n</blockquote>\n<h4 id=\"小结：-1\"><a href=\"#小结：-1\" class=\"headerlink\" title=\"小结：\"></a>小结：</h4><p>上文已经说了 kubernetes 中有两种用户，一种是内置用户被称为 serviceaccounts，一种外部用户，嵌入在客户端的证书中，那么 kubernetes 中有哪些证书链以及内嵌的用户如何与 RBAC 结合呢？</p>\n<h3 id=\"kubernetes-中的证书链\"><a href=\"#kubernetes-中的证书链\" class=\"headerlink\" title=\"kubernetes 中的证书链\"></a>kubernetes 中的证书链</h3><p>笔者通过自己的研究及实践经验发现，在目前主流版本的 kubernetes 集群中，有四条重要的 CA 证书链，而在大多数生产环境中，则至少需要两条 CA 证书链。</p>\n<ul>\n<li>apiserver CA 证书链：主要用于 kubernetes 内部组件互相访问以及外部客户端访问 apiserver 使用</li>\n<li>etcd CA 证书链：主要用于 etcd 节点之间的访问以及 apiserver 访问 etcd 使用</li>\n<li>extension apiserver CA 证书链：用于访问 extension apiserver 使用，比如 metrics-server</li>\n<li>kubelet CA 信任链：用于 apiserver 访问 kubelet 时使用</li>\n<li>其他证书链：admission webhook 证书链、audit webhook 证书链，用于 apiserver 访问 webhook 时使用</li>\n</ul>\n<p>以上这几套 CA 证书链中，apiserver CA 证书链和 etcd CA 证书链是必要的。extension apiserver 的 CA 证书链只有在使用时才会用到，且不可与 apiserver CA 证书链相同。kubelet 的 CA 证书链不是必要的，根据部署的实际情况可以和 apiserver CA 证书链公用。</p>\n<h4 id=\"证书中的内嵌用户如何与-RBAC-配置进行结合\"><a href=\"#证书中的内嵌用户如何与-RBAC-配置进行结合\" class=\"headerlink\" title=\"证书中的内嵌用户如何与 RBAC 配置进行结合\"></a>证书中的内嵌用户如何与 RBAC 配置进行结合</h4><h5 id=\"证书中的内嵌用户\"><a href=\"#证书中的内嵌用户\" class=\"headerlink\" title=\"证书中的内嵌用户\"></a>证书中的内嵌用户</h5><p>以下是 kubelet 的证书请求文件（CSR）：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:node:&lt;nodeName&gt;&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:nodes&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>“CN”：Common Name，从证书中提取该字段作为请求的用户名 (User Name)；</li>\n<li>“O”：Organization，从证书中提取该字段作为请求用户所属的组 (Group)；</li>\n</ul>\n<p>kubernetes 使用 X509 证书中 CN(Common Name) 以及 O(Organization) 字段对应 kubernetes 中的 user 和 group，即 RBAC 中的 subject，而 kubernetes 也为多个组件内置了 Role 以及 RoleBinding，巧妙的将 Authentication 和 RBAC Authorization 结合到了一起。</p>\n<p>查看 kubernetes 中内置的 RBAC：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get clusterrole</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get clusterrolebinding</span><br></pre></td></tr></table></figure>\n<p>下面是 kubernetes 中核心组件内置的 user 和 group，在为每个组件生成证书时需要在其 CSR 中使用对应的 CN 和 O 字段。 </p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20190723194531004.png\" alt=\"csr\"></p>\n<h3 id=\"访问-apiserver-的几种方式\"><a href=\"#访问-apiserver-的几种方式\" class=\"headerlink\" title=\"访问 apiserver 的几种方式\"></a>访问 apiserver 的几种方式</h3><p>通过上文可以知道访问 apiserver 时需要通过认证、鉴权以及访问控制三个步骤，认证的方式可以使用  serviceaccounts 和 X509 证书，鉴权的方式使用 RBAC，访问控制若没有特殊需求可以不使用。</p>\n<p>serviceaccounts 是 kubernetes 针对 pod 内访问 apiserver 提供的认证方式，那可以用在外部 client 端吗？答案是可以的，serviceaccounts 最终是通过 ca + token 的方式访问的，你只要创建一个 serviceaccounts 并从对应的 secrets 中获取 ca + token 即可访问 apiserver。那使用证书认证的方式可以在 pod 内访问 apiserver 吗？当然也可以，不过创建证书比 serviceaccounts 麻烦，证书默认是用于内置组件访问 apiserver 使用的。不论哪种方式，你都需要为其创建 RBAC 配置。</p>\n<p>所以在 TLS +RBAC 模式下，访问 apiserver 目前有两种方式：</p>\n<ul>\n<li>使用 serviceaccounts + RBAC ：需要创建 serviceaccounts 以及关联对应的 RBAC(ca + token + RBAC)</li>\n<li>使用证书 + RBAC：需要用到 ca、client、client-key 以及关联对应的 RBAC(ca + client-key + client-cert + RBAC)</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要讲述了 kubernetes 中的认证(Authentication)以及鉴权(Authorization)机制，其复杂性主要体现在部署 kubernetes 集群时组件之间的认证以及在集群中为附加组件配置正确的权限，希望通过本节你可以了解到 kubernetes 中的组件需要哪些权限认证以及如何为相关组件配置正确的权限。</p>\n<p>参考： </p>\n<p>Controlling Access to the Kubernetes API：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/</a></p>\n<p>admission controllers：<a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</a></p>\n<p>kubelet 配置权限认证：<a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/</a>  </p>\n<p>master-node communication：<a href=\"https://kubernetes.io/docs/concepts/architecture/master-node-communication/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/architecture/master-node-communication/</a></p>\n<p>kubernetes 数字证书体系浅析：<a href=\"https://mp.weixin.qq.com/s/iXuDbPKjSc65t_9Y1--bog\" target=\"_blank\" rel=\"noopener\">https://mp.weixin.qq.com/s/iXuDbPKjSc65t_9Y1--bog</a></p>\n"},{"title":"kubernetes 自定义资源（CRD）的校验","date":"2019-07-02T03:00:00.000Z","type":"crd","_content":"\n在以前的版本若要对 apiserver 的请求做一些访问控制，必须修改 apiserver 的源代码然后重新编译部署，非常麻烦也不灵活，apiserver 也支持一些动态的准入控制器，在 apiserver 配置中看到的`ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota` 等都是 apiserver 的准入控制器，但这些都是 kubernetes 中默认内置的。在 v1.9 中，kubernetes 的动态准入控制器功能中支持了  Admission Webhooks，即用户可以以插件的方式对 apiserver 的请求做一些访问控制，要使用该功能需要自己写一个 admission webhook，apiserver 会在请求通过认证和授权之后、对象被持久化之前拦截该请求，然后调用 webhook 已达到准入控制，比如 Istio 中 sidecar 的注入就是通过这种方式实现的，在创建 Pod 阶段 apiserver 会回调 webhook 然后将 Sidecar 代理注入至用户 Pod。 本文主要介绍如何使用 AdmissionWebhook 对 CR 的校验，一般在开发 operator 过程中，都是通过对 CR 的操作实现某个功能的，若 CR 不规范可能会导致某些问题，所以对提交 CR 的校验是不可避免的一个步骤。\n\nkubernetes 目前提供了两种方式来对 CR 的校验，语法校验(`OpenAPI v3 schema`） 和语义校验\n(`validatingadmissionwebhook`）。\n\nCRD 的一个示例：\n\n```\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  # name must match the spec fields below, and be in the form: <plural>.<group>\n  name: kubernetesclusters.ecs.yun.com\nspec:\n  # group name to use for REST API: /apis/<group>/<version>\n  group: ecs.yun.com\n  # list of versions supported by this CustomResourceDefinition\n  versions:\n    - name: v1\n      # Each version can be enabled/disabled by Served flag.\n      served: true\n      # One and only one version must be marked as the storage version.\n      storage: true\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/<group>/<version>/<plural>\n    plural: kubernetesclusters\n    # singular name to be used as an alias on the CLI and for display\n    singular: kubernetescluster\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: KubernetesCluster\n\t  # listKind\n    listKind: KubernetesClusterList\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n    - ecs\n```\n\n\n\nCRD 的一个对象：\n\n```\napiVersion: ecs.yun.com/v1\nkind: KubernetesCluster\nmetadata:\n  name: test-cluster\nspec:\n  clusterType: kubernetes\n  serviceCIDR: ''\n  masterList:\n  - ip: 192.168.1.10\n  nodeList:\n  - ip: 192.168.1.11\n  privateSSHKey: ''\n  scaleUp: 0\n  scaleDown: 0\n```\n\n\n\n#### 一、OpenAPI v3 schema \n\n[OpenAPI](https://github.com/OAI/OpenAPI-Specification) 是针对 REST API 的 API 描述格式，也是一种规范。\n\n```\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kubernetesclusters.ecs.yun.com\nspec:\n  group: ecs.yun.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n  scope: Namespaced\n  names:\n    plural: kubernetesclusters\n    singular: kubernetescluster\n    kind: KubernetesCluster\n    listKind: KubernetesClusterList\n    shortNames:\n    - ecs\n  validation:\n    openAPIV3Schema:\n      properties:\n        spec:\n\t  type: object\n          required:\n          - clusterType\n          - masterList\n          - nodeList\n          properties:\n            clusterType:\n              type: string\n            scaleUp:\n              type: integer\n            scaleDown:\n              type: integer\n              minimum: 0\n```\n\n上面是使用 OpenAPI v3 检验的一个例子，OpenAPI v3 仅支持一些简单的校验规则，可以校验参数的类型，参数值的类型(支持正则)，是否为必要参数等，但若要使用与、或、非等操作对多个字段同时校验还是做不到的，所以针对一些特定场景的校验需要使用 admission webhook。 \n\n\n\n#### 二、Admission Webhooks\n\nadmission control 在 apiserver 中进行配置的，使用`--enable-admission-plugins` 或 `--admission-control`进行启用，admission control 配置的控制器列表是有顺序的，越靠前的越先执行，一旦某个控制器返回的结果是reject 的，那么整个准入控制阶段立刻结束，所以这里的配置顺序是有序的，建议使用官方的顺序配置。\n\n在 v1.9 中，admission webhook 是通过在 `--admission-control` 中配置 `ValidatingAdmissionWebhook` 或 `MutatingAdmissionWebhook` 来支持使用的，两者区别如下：\n\n- MutatingAdmissionWebhook：允许在 webhook 中对 object 进行 mutate 修改，但匹配到的 webhook **串行**执行，因为每个 webhook 都可能会 mutate object。\n- ValidatingAdmissionWebhook: 不允许在 webhook 中对 Object 进行 mutate 修改，仅返回 true 或 false。\n\n启用 admission webhook 后，每次对 CR 做 CRUD 操作时，请求就会被 apiserver 拦住，至于 CRUD 中哪些请求被拦住都是提前在 WebhookConfiguration 中配置的，然后会调用 AdmissionWebhook 进行检查是否 Admit 通过。\n\n\n\n![kubernetes API request lifecycle](http://cdn.tianfeiyu.com/1562032999173.jpg)\n\n\n\n#### 三、启用 Admission Webhooks 功能\n\n> kubernetes 版本 >= v1.9\n\n1、在 apiserver 中开启 admission webhooks\n\n在 v1.9 版本中使用的是：\n\n```shell\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota\n```\n\n在 v1.10 以后会弃用 `--admission-control`，取而代之的是  `--enable-admission-plugins`：\n\n```\n--enable-admission-plugins=NodeRestriction,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota\n```\n\n启用之后在 api-resources 可以看到：\n\n```\n# kubectl api-resources | grep admissionregistration\nmutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration\nvalidatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration\n```\n\n2、启用 `admissionregistration.k8s.io/v1alpha1` API\n\n```\n//  检查 API 是否已启用\n$ kubectl api-versions | grep admissionregistration.k8s.io\n```\n若不存在则需要在 apiserver 的配置中添加`--runtime-config=admissionregistration.k8s.io/v1alpha1`。\n\n#### 四、编写 Admission Webhook Server\n\nwebhook 其实就是一个 RESTful API 里面加上自己的一些校验逻辑。\n\n可以参考官方的示例： \n[https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.go](https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.go) \n或者 \n[https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go](https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go)\n\n\n\n> 完整代码参考：[https://github.com/gosoon/admission-webhook](https://github.com/gosoon/admission-webhook)\n\n#### 五、部署 Admission Webhook Service\n\n由于 apiserver 调用 webhook 时强制使用 TLS 认证，所以 WebhookConfiguration 中一定要配置 caBundle，也就是需要自己生成一套私有证书。\n\n生成证书的方式比较多，以下使用 openssl 生成，脚本如下所示：\n\n```\n#!/bin/bash\n\n# Generate the CA cert and private key\nopenssl req -nodes -new -x509 -days 365 -keyout ca.key -out ca.crt -subj \"/CN=admission-webhook CA\"\n\n# Generate the private key for the webhook server\nopenssl genrsa -out admission-webhook-tls.key 2048\n\n# Generate a Certificate Signing Request (CSR) for the private key, and sign it with the private key of the CA.\nopenssl req -new -key admission-webhook-tls.key -subj \"/CN=admission-webhook.ecs-system.svc\" \\\n    | openssl x509 -days 365 -req -CA ca.crt -CAkey ca.key -CAcreateserial -out admission-webhook-tls.crt\n\n# Generate pem\nopenssl base64 -A < ca.crt > ca.pem\n```\n\n生成证书后将 ca.pem 中的内容复制到 caBundle 处。\n\nValidatingWebhook yaml 文件如下：\n\n```\napiVersion: admissionregistration.k8s.io/v1beta1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: admission-webhook\nwebhooks:\n  - name: admission-webhook.ecs-system.svc  # 必须为 <svc_name>.<svc_namespace>.svc.\n    failurePolicy: Ignore\n    clientConfig:\n      service:\n        name: admission-webhook\n        namespace: ecs-system\n        path: /ecs/operator/cluster  # webhook controller\n      caBundle: xxx\n    rules:\n      - operations:   # 需要校验的方法\n        - CREATE\n        - UPDATE\n        apiGroups:    # api group\n        - ecs.yun.com\n        apiVersions:  # version\n        - v1\n        resources:    # resource\n        - kubernetesclusters\n```\n\n注意 `failurePolicy` 可以为 `Ignore`或者`Fail`，意味着如果和 webhook 通信出现问题导致调用失败，将根据 `failurePolicy`决定忽略失败（admit）还是准入失败(reject)。\n\n最后将 webhook 部署在集群中。\n\n\n\n参考：\nhttps://github.com/gosoon/admission-webhook\nhttps://banzaicloud.com/blog/k8s-admission-webhooks/\n[http://blog.fatedier.com/2019/03/20/k8s-crd/](http://blog.fatedier.com/2019/03/20/k8s-crd/)\nhttps://my.oschina.net/jxcdwangtao/blog/1591681\nhttps://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\nhttps://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use\nhttps://istio.io/zh/help/ops/setup/validation/\nhttps://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/\n","source":"_posts/k8s_crd_verify.md","raw":"---\ntitle: kubernetes 自定义资源（CRD）的校验\ndate: 2019-07-02 11:00:00\ntags: [\"crd\",\"admission control\"]\ntype: \"crd\"\n\n---\n\n在以前的版本若要对 apiserver 的请求做一些访问控制，必须修改 apiserver 的源代码然后重新编译部署，非常麻烦也不灵活，apiserver 也支持一些动态的准入控制器，在 apiserver 配置中看到的`ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota` 等都是 apiserver 的准入控制器，但这些都是 kubernetes 中默认内置的。在 v1.9 中，kubernetes 的动态准入控制器功能中支持了  Admission Webhooks，即用户可以以插件的方式对 apiserver 的请求做一些访问控制，要使用该功能需要自己写一个 admission webhook，apiserver 会在请求通过认证和授权之后、对象被持久化之前拦截该请求，然后调用 webhook 已达到准入控制，比如 Istio 中 sidecar 的注入就是通过这种方式实现的，在创建 Pod 阶段 apiserver 会回调 webhook 然后将 Sidecar 代理注入至用户 Pod。 本文主要介绍如何使用 AdmissionWebhook 对 CR 的校验，一般在开发 operator 过程中，都是通过对 CR 的操作实现某个功能的，若 CR 不规范可能会导致某些问题，所以对提交 CR 的校验是不可避免的一个步骤。\n\nkubernetes 目前提供了两种方式来对 CR 的校验，语法校验(`OpenAPI v3 schema`） 和语义校验\n(`validatingadmissionwebhook`）。\n\nCRD 的一个示例：\n\n```\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  # name must match the spec fields below, and be in the form: <plural>.<group>\n  name: kubernetesclusters.ecs.yun.com\nspec:\n  # group name to use for REST API: /apis/<group>/<version>\n  group: ecs.yun.com\n  # list of versions supported by this CustomResourceDefinition\n  versions:\n    - name: v1\n      # Each version can be enabled/disabled by Served flag.\n      served: true\n      # One and only one version must be marked as the storage version.\n      storage: true\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/<group>/<version>/<plural>\n    plural: kubernetesclusters\n    # singular name to be used as an alias on the CLI and for display\n    singular: kubernetescluster\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: KubernetesCluster\n\t  # listKind\n    listKind: KubernetesClusterList\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n    - ecs\n```\n\n\n\nCRD 的一个对象：\n\n```\napiVersion: ecs.yun.com/v1\nkind: KubernetesCluster\nmetadata:\n  name: test-cluster\nspec:\n  clusterType: kubernetes\n  serviceCIDR: ''\n  masterList:\n  - ip: 192.168.1.10\n  nodeList:\n  - ip: 192.168.1.11\n  privateSSHKey: ''\n  scaleUp: 0\n  scaleDown: 0\n```\n\n\n\n#### 一、OpenAPI v3 schema \n\n[OpenAPI](https://github.com/OAI/OpenAPI-Specification) 是针对 REST API 的 API 描述格式，也是一种规范。\n\n```\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kubernetesclusters.ecs.yun.com\nspec:\n  group: ecs.yun.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n  scope: Namespaced\n  names:\n    plural: kubernetesclusters\n    singular: kubernetescluster\n    kind: KubernetesCluster\n    listKind: KubernetesClusterList\n    shortNames:\n    - ecs\n  validation:\n    openAPIV3Schema:\n      properties:\n        spec:\n\t  type: object\n          required:\n          - clusterType\n          - masterList\n          - nodeList\n          properties:\n            clusterType:\n              type: string\n            scaleUp:\n              type: integer\n            scaleDown:\n              type: integer\n              minimum: 0\n```\n\n上面是使用 OpenAPI v3 检验的一个例子，OpenAPI v3 仅支持一些简单的校验规则，可以校验参数的类型，参数值的类型(支持正则)，是否为必要参数等，但若要使用与、或、非等操作对多个字段同时校验还是做不到的，所以针对一些特定场景的校验需要使用 admission webhook。 \n\n\n\n#### 二、Admission Webhooks\n\nadmission control 在 apiserver 中进行配置的，使用`--enable-admission-plugins` 或 `--admission-control`进行启用，admission control 配置的控制器列表是有顺序的，越靠前的越先执行，一旦某个控制器返回的结果是reject 的，那么整个准入控制阶段立刻结束，所以这里的配置顺序是有序的，建议使用官方的顺序配置。\n\n在 v1.9 中，admission webhook 是通过在 `--admission-control` 中配置 `ValidatingAdmissionWebhook` 或 `MutatingAdmissionWebhook` 来支持使用的，两者区别如下：\n\n- MutatingAdmissionWebhook：允许在 webhook 中对 object 进行 mutate 修改，但匹配到的 webhook **串行**执行，因为每个 webhook 都可能会 mutate object。\n- ValidatingAdmissionWebhook: 不允许在 webhook 中对 Object 进行 mutate 修改，仅返回 true 或 false。\n\n启用 admission webhook 后，每次对 CR 做 CRUD 操作时，请求就会被 apiserver 拦住，至于 CRUD 中哪些请求被拦住都是提前在 WebhookConfiguration 中配置的，然后会调用 AdmissionWebhook 进行检查是否 Admit 通过。\n\n\n\n![kubernetes API request lifecycle](http://cdn.tianfeiyu.com/1562032999173.jpg)\n\n\n\n#### 三、启用 Admission Webhooks 功能\n\n> kubernetes 版本 >= v1.9\n\n1、在 apiserver 中开启 admission webhooks\n\n在 v1.9 版本中使用的是：\n\n```shell\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota\n```\n\n在 v1.10 以后会弃用 `--admission-control`，取而代之的是  `--enable-admission-plugins`：\n\n```\n--enable-admission-plugins=NodeRestriction,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota\n```\n\n启用之后在 api-resources 可以看到：\n\n```\n# kubectl api-resources | grep admissionregistration\nmutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration\nvalidatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration\n```\n\n2、启用 `admissionregistration.k8s.io/v1alpha1` API\n\n```\n//  检查 API 是否已启用\n$ kubectl api-versions | grep admissionregistration.k8s.io\n```\n若不存在则需要在 apiserver 的配置中添加`--runtime-config=admissionregistration.k8s.io/v1alpha1`。\n\n#### 四、编写 Admission Webhook Server\n\nwebhook 其实就是一个 RESTful API 里面加上自己的一些校验逻辑。\n\n可以参考官方的示例： \n[https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.go](https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.go) \n或者 \n[https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go](https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go)\n\n\n\n> 完整代码参考：[https://github.com/gosoon/admission-webhook](https://github.com/gosoon/admission-webhook)\n\n#### 五、部署 Admission Webhook Service\n\n由于 apiserver 调用 webhook 时强制使用 TLS 认证，所以 WebhookConfiguration 中一定要配置 caBundle，也就是需要自己生成一套私有证书。\n\n生成证书的方式比较多，以下使用 openssl 生成，脚本如下所示：\n\n```\n#!/bin/bash\n\n# Generate the CA cert and private key\nopenssl req -nodes -new -x509 -days 365 -keyout ca.key -out ca.crt -subj \"/CN=admission-webhook CA\"\n\n# Generate the private key for the webhook server\nopenssl genrsa -out admission-webhook-tls.key 2048\n\n# Generate a Certificate Signing Request (CSR) for the private key, and sign it with the private key of the CA.\nopenssl req -new -key admission-webhook-tls.key -subj \"/CN=admission-webhook.ecs-system.svc\" \\\n    | openssl x509 -days 365 -req -CA ca.crt -CAkey ca.key -CAcreateserial -out admission-webhook-tls.crt\n\n# Generate pem\nopenssl base64 -A < ca.crt > ca.pem\n```\n\n生成证书后将 ca.pem 中的内容复制到 caBundle 处。\n\nValidatingWebhook yaml 文件如下：\n\n```\napiVersion: admissionregistration.k8s.io/v1beta1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: admission-webhook\nwebhooks:\n  - name: admission-webhook.ecs-system.svc  # 必须为 <svc_name>.<svc_namespace>.svc.\n    failurePolicy: Ignore\n    clientConfig:\n      service:\n        name: admission-webhook\n        namespace: ecs-system\n        path: /ecs/operator/cluster  # webhook controller\n      caBundle: xxx\n    rules:\n      - operations:   # 需要校验的方法\n        - CREATE\n        - UPDATE\n        apiGroups:    # api group\n        - ecs.yun.com\n        apiVersions:  # version\n        - v1\n        resources:    # resource\n        - kubernetesclusters\n```\n\n注意 `failurePolicy` 可以为 `Ignore`或者`Fail`，意味着如果和 webhook 通信出现问题导致调用失败，将根据 `failurePolicy`决定忽略失败（admit）还是准入失败(reject)。\n\n最后将 webhook 部署在集群中。\n\n\n\n参考：\nhttps://github.com/gosoon/admission-webhook\nhttps://banzaicloud.com/blog/k8s-admission-webhooks/\n[http://blog.fatedier.com/2019/03/20/k8s-crd/](http://blog.fatedier.com/2019/03/20/k8s-crd/)\nhttps://my.oschina.net/jxcdwangtao/blog/1591681\nhttps://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\nhttps://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use\nhttps://istio.io/zh/help/ops/setup/validation/\nhttps://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/\n","slug":"k8s_crd_verify","published":1,"updated":"2019-09-18T05:53:44.408Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59c000gapwnna87fzyr","content":"<p>在以前的版本若要对 apiserver 的请求做一些访问控制，必须修改 apiserver 的源代码然后重新编译部署，非常麻烦也不灵活，apiserver 也支持一些动态的准入控制器，在 apiserver 配置中看到的<code>ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota</code> 等都是 apiserver 的准入控制器，但这些都是 kubernetes 中默认内置的。在 v1.9 中，kubernetes 的动态准入控制器功能中支持了  Admission Webhooks，即用户可以以插件的方式对 apiserver 的请求做一些访问控制，要使用该功能需要自己写一个 admission webhook，apiserver 会在请求通过认证和授权之后、对象被持久化之前拦截该请求，然后调用 webhook 已达到准入控制，比如 Istio 中 sidecar 的注入就是通过这种方式实现的，在创建 Pod 阶段 apiserver 会回调 webhook 然后将 Sidecar 代理注入至用户 Pod。 本文主要介绍如何使用 AdmissionWebhook 对 CR 的校验，一般在开发 operator 过程中，都是通过对 CR 的操作实现某个功能的，若 CR 不规范可能会导致某些问题，所以对提交 CR 的校验是不可避免的一个步骤。</p>\n<p>kubernetes 目前提供了两种方式来对 CR 的校验，语法校验(<code>OpenAPI v3 schema</code>） 和语义校验<br>(<code>validatingadmissionwebhook</code>）。</p>\n<p>CRD 的一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class=\"line\">kind: CustomResourceDefinition</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;</span><br><span class=\"line\">  name: kubernetesclusters.ecs.yun.com</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><br><span class=\"line\">  group: ecs.yun.com</span><br><span class=\"line\">  # list of versions supported by this CustomResourceDefinition</span><br><span class=\"line\">  versions:</span><br><span class=\"line\">    - name: v1</span><br><span class=\"line\">      # Each version can be enabled/disabled by Served flag.</span><br><span class=\"line\">      served: true</span><br><span class=\"line\">      # One and only one version must be marked as the storage version.</span><br><span class=\"line\">      storage: true</span><br><span class=\"line\">  # either Namespaced or Cluster</span><br><span class=\"line\">  scope: Namespaced</span><br><span class=\"line\">  names:</span><br><span class=\"line\">    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><br><span class=\"line\">    plural: kubernetesclusters</span><br><span class=\"line\">    # singular name to be used as an alias on the CLI and for display</span><br><span class=\"line\">    singular: kubernetescluster</span><br><span class=\"line\">    # kind is normally the CamelCased singular type. Your resource manifests use this.</span><br><span class=\"line\">    kind: KubernetesCluster</span><br><span class=\"line\">\t  # listKind</span><br><span class=\"line\">    listKind: KubernetesClusterList</span><br><span class=\"line\">    # shortNames allow shorter string to match your resource on the CLI</span><br><span class=\"line\">    shortNames:</span><br><span class=\"line\">    - ecs</span><br></pre></td></tr></table></figure>\n<p>CRD 的一个对象：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: ecs.yun.com/v1</span><br><span class=\"line\">kind: KubernetesCluster</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: test-cluster</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  clusterType: kubernetes</span><br><span class=\"line\">  serviceCIDR: &apos;&apos;</span><br><span class=\"line\">  masterList:</span><br><span class=\"line\">  - ip: 192.168.1.10</span><br><span class=\"line\">  nodeList:</span><br><span class=\"line\">  - ip: 192.168.1.11</span><br><span class=\"line\">  privateSSHKey: &apos;&apos;</span><br><span class=\"line\">  scaleUp: 0</span><br><span class=\"line\">  scaleDown: 0</span><br></pre></td></tr></table></figure>\n<h4 id=\"一、OpenAPI-v3-schema\"><a href=\"#一、OpenAPI-v3-schema\" class=\"headerlink\" title=\"一、OpenAPI v3 schema\"></a>一、OpenAPI v3 schema</h4><p><a href=\"https://github.com/OAI/OpenAPI-Specification\" target=\"_blank\" rel=\"noopener\">OpenAPI</a> 是针对 REST API 的 API 描述格式，也是一种规范。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class=\"line\">kind: CustomResourceDefinition</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetesclusters.ecs.yun.com</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  group: ecs.yun.com</span><br><span class=\"line\">  versions:</span><br><span class=\"line\">    - name: v1</span><br><span class=\"line\">      served: true</span><br><span class=\"line\">      storage: true</span><br><span class=\"line\">  scope: Namespaced</span><br><span class=\"line\">  names:</span><br><span class=\"line\">    plural: kubernetesclusters</span><br><span class=\"line\">    singular: kubernetescluster</span><br><span class=\"line\">    kind: KubernetesCluster</span><br><span class=\"line\">    listKind: KubernetesClusterList</span><br><span class=\"line\">    shortNames:</span><br><span class=\"line\">    - ecs</span><br><span class=\"line\">  validation:</span><br><span class=\"line\">    openAPIV3Schema:</span><br><span class=\"line\">      properties:</span><br><span class=\"line\">        spec:</span><br><span class=\"line\">\t  type: object</span><br><span class=\"line\">          required:</span><br><span class=\"line\">          - clusterType</span><br><span class=\"line\">          - masterList</span><br><span class=\"line\">          - nodeList</span><br><span class=\"line\">          properties:</span><br><span class=\"line\">            clusterType:</span><br><span class=\"line\">              type: string</span><br><span class=\"line\">            scaleUp:</span><br><span class=\"line\">              type: integer</span><br><span class=\"line\">            scaleDown:</span><br><span class=\"line\">              type: integer</span><br><span class=\"line\">              minimum: 0</span><br></pre></td></tr></table></figure>\n<p>上面是使用 OpenAPI v3 检验的一个例子，OpenAPI v3 仅支持一些简单的校验规则，可以校验参数的类型，参数值的类型(支持正则)，是否为必要参数等，但若要使用与、或、非等操作对多个字段同时校验还是做不到的，所以针对一些特定场景的校验需要使用 admission webhook。 </p>\n<h4 id=\"二、Admission-Webhooks\"><a href=\"#二、Admission-Webhooks\" class=\"headerlink\" title=\"二、Admission Webhooks\"></a>二、Admission Webhooks</h4><p>admission control 在 apiserver 中进行配置的，使用<code>--enable-admission-plugins</code> 或 <code>--admission-control</code>进行启用，admission control 配置的控制器列表是有顺序的，越靠前的越先执行，一旦某个控制器返回的结果是reject 的，那么整个准入控制阶段立刻结束，所以这里的配置顺序是有序的，建议使用官方的顺序配置。</p>\n<p>在 v1.9 中，admission webhook 是通过在 <code>--admission-control</code> 中配置 <code>ValidatingAdmissionWebhook</code> 或 <code>MutatingAdmissionWebhook</code> 来支持使用的，两者区别如下：</p>\n<ul>\n<li>MutatingAdmissionWebhook：允许在 webhook 中对 object 进行 mutate 修改，但匹配到的 webhook <strong>串行</strong>执行，因为每个 webhook 都可能会 mutate object。</li>\n<li>ValidatingAdmissionWebhook: 不允许在 webhook 中对 Object 进行 mutate 修改，仅返回 true 或 false。</li>\n</ul>\n<p>启用 admission webhook 后，每次对 CR 做 CRUD 操作时，请求就会被 apiserver 拦住，至于 CRUD 中哪些请求被拦住都是提前在 WebhookConfiguration 中配置的，然后会调用 AdmissionWebhook 进行检查是否 Admit 通过。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/1562032999173.jpg\" alt=\"kubernetes API request lifecycle\"></p>\n<h4 id=\"三、启用-Admission-Webhooks-功能\"><a href=\"#三、启用-Admission-Webhooks-功能\" class=\"headerlink\" title=\"三、启用 Admission Webhooks 功能\"></a>三、启用 Admission Webhooks 功能</h4><blockquote>\n<p>kubernetes 版本 &gt;= v1.9</p>\n</blockquote>\n<p>1、在 apiserver 中开启 admission webhooks</p>\n<p>在 v1.9 版本中使用的是：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota</span><br></pre></td></tr></table></figure>\n<p>在 v1.10 以后会弃用 <code>--admission-control</code>，取而代之的是  <code>--enable-admission-plugins</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--enable-admission-plugins=NodeRestriction,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota</span><br></pre></td></tr></table></figure>\n<p>启用之后在 api-resources 可以看到：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># kubectl api-resources | grep admissionregistration</span><br><span class=\"line\">mutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration</span><br><span class=\"line\">validatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration</span><br></pre></td></tr></table></figure>\n<p>2、启用 <code>admissionregistration.k8s.io/v1alpha1</code> API</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//  检查 API 是否已启用</span><br><span class=\"line\">$ kubectl api-versions | grep admissionregistration.k8s.io</span><br></pre></td></tr></table></figure>\n<p>若不存在则需要在 apiserver 的配置中添加<code>--runtime-config=admissionregistration.k8s.io/v1alpha1</code>。</p>\n<h4 id=\"四、编写-Admission-Webhook-Server\"><a href=\"#四、编写-Admission-Webhook-Server\" class=\"headerlink\" title=\"四、编写 Admission Webhook Server\"></a>四、编写 Admission Webhook Server</h4><p>webhook 其实就是一个 RESTful API 里面加上自己的一些校验逻辑。</p>\n<p>可以参考官方的示例：<br><a href=\"https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.go\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.go</a><br>或者<br><a href=\"https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go</a></p>\n<blockquote>\n<p>完整代码参考：<a href=\"https://github.com/gosoon/admission-webhook\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/admission-webhook</a></p>\n</blockquote>\n<h4 id=\"五、部署-Admission-Webhook-Service\"><a href=\"#五、部署-Admission-Webhook-Service\" class=\"headerlink\" title=\"五、部署 Admission Webhook Service\"></a>五、部署 Admission Webhook Service</h4><p>由于 apiserver 调用 webhook 时强制使用 TLS 认证，所以 WebhookConfiguration 中一定要配置 caBundle，也就是需要自己生成一套私有证书。</p>\n<p>生成证书的方式比较多，以下使用 openssl 生成，脚本如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate the CA cert and private key</span><br><span class=\"line\">openssl req -nodes -new -x509 -days 365 -keyout ca.key -out ca.crt -subj &quot;/CN=admission-webhook CA&quot;</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate the private key for the webhook server</span><br><span class=\"line\">openssl genrsa -out admission-webhook-tls.key 2048</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate a Certificate Signing Request (CSR) for the private key, and sign it with the private key of the CA.</span><br><span class=\"line\">openssl req -new -key admission-webhook-tls.key -subj &quot;/CN=admission-webhook.ecs-system.svc&quot; \\</span><br><span class=\"line\">    | openssl x509 -days 365 -req -CA ca.crt -CAkey ca.key -CAcreateserial -out admission-webhook-tls.crt</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate pem</span><br><span class=\"line\">openssl base64 -A &lt; ca.crt &gt; ca.pem</span><br></pre></td></tr></table></figure>\n<p>生成证书后将 ca.pem 中的内容复制到 caBundle 处。</p>\n<p>ValidatingWebhook yaml 文件如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: admissionregistration.k8s.io/v1beta1</span><br><span class=\"line\">kind: ValidatingWebhookConfiguration</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: admission-webhook</span><br><span class=\"line\">webhooks:</span><br><span class=\"line\">  - name: admission-webhook.ecs-system.svc  # 必须为 &lt;svc_name&gt;.&lt;svc_namespace&gt;.svc.</span><br><span class=\"line\">    failurePolicy: Ignore</span><br><span class=\"line\">    clientConfig:</span><br><span class=\"line\">      service:</span><br><span class=\"line\">        name: admission-webhook</span><br><span class=\"line\">        namespace: ecs-system</span><br><span class=\"line\">        path: /ecs/operator/cluster  # webhook controller</span><br><span class=\"line\">      caBundle: xxx</span><br><span class=\"line\">    rules:</span><br><span class=\"line\">      - operations:   # 需要校验的方法</span><br><span class=\"line\">        - CREATE</span><br><span class=\"line\">        - UPDATE</span><br><span class=\"line\">        apiGroups:    # api group</span><br><span class=\"line\">        - ecs.yun.com</span><br><span class=\"line\">        apiVersions:  # version</span><br><span class=\"line\">        - v1</span><br><span class=\"line\">        resources:    # resource</span><br><span class=\"line\">        - kubernetesclusters</span><br></pre></td></tr></table></figure>\n<p>注意 <code>failurePolicy</code> 可以为 <code>Ignore</code>或者<code>Fail</code>，意味着如果和 webhook 通信出现问题导致调用失败，将根据 <code>failurePolicy</code>决定忽略失败（admit）还是准入失败(reject)。</p>\n<p>最后将 webhook 部署在集群中。</p>\n<p>参考：<br><a href=\"https://github.com/gosoon/admission-webhook\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/admission-webhook</a><br><a href=\"https://banzaicloud.com/blog/k8s-admission-webhooks/\" target=\"_blank\" rel=\"noopener\">https://banzaicloud.com/blog/k8s-admission-webhooks/</a><br><a href=\"http://blog.fatedier.com/2019/03/20/k8s-crd/\" target=\"_blank\" rel=\"noopener\">http://blog.fatedier.com/2019/03/20/k8s-crd/</a><br><a href=\"https://my.oschina.net/jxcdwangtao/blog/1591681\" target=\"_blank\" rel=\"noopener\">https://my.oschina.net/jxcdwangtao/blog/1591681</a><br><a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/</a><br><a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use</a><br><a href=\"https://istio.io/zh/help/ops/setup/validation/\" target=\"_blank\" rel=\"noopener\">https://istio.io/zh/help/ops/setup/validation/</a><br><a href=\"https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/</a></p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/08/06/code_generator/\">使用 code-generator 为 CustomResources 生成代码</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>在以前的版本若要对 apiserver 的请求做一些访问控制，必须修改 apiserver 的源代码然后重新编译部署，非常麻烦也不灵活，apiserver 也支持一些动态的准入控制器，在 apiserver 配置中看到的<code>ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota</code> 等都是 apiserver 的准入控制器，但这些都是 kubernetes 中默认内置的。在 v1.9 中，kubernetes 的动态准入控制器功能中支持了  Admission Webhooks，即用户可以以插件的方式对 apiserver 的请求做一些访问控制，要使用该功能需要自己写一个 admission webhook，apiserver 会在请求通过认证和授权之后、对象被持久化之前拦截该请求，然后调用 webhook 已达到准入控制，比如 Istio 中 sidecar 的注入就是通过这种方式实现的，在创建 Pod 阶段 apiserver 会回调 webhook 然后将 Sidecar 代理注入至用户 Pod。 本文主要介绍如何使用 AdmissionWebhook 对 CR 的校验，一般在开发 operator 过程中，都是通过对 CR 的操作实现某个功能的，若 CR 不规范可能会导致某些问题，所以对提交 CR 的校验是不可避免的一个步骤。</p>\n<p>kubernetes 目前提供了两种方式来对 CR 的校验，语法校验(<code>OpenAPI v3 schema</code>） 和语义校验<br>(<code>validatingadmissionwebhook</code>）。</p>\n<p>CRD 的一个示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class=\"line\">kind: CustomResourceDefinition</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;</span><br><span class=\"line\">  name: kubernetesclusters.ecs.yun.com</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><br><span class=\"line\">  group: ecs.yun.com</span><br><span class=\"line\">  # list of versions supported by this CustomResourceDefinition</span><br><span class=\"line\">  versions:</span><br><span class=\"line\">    - name: v1</span><br><span class=\"line\">      # Each version can be enabled/disabled by Served flag.</span><br><span class=\"line\">      served: true</span><br><span class=\"line\">      # One and only one version must be marked as the storage version.</span><br><span class=\"line\">      storage: true</span><br><span class=\"line\">  # either Namespaced or Cluster</span><br><span class=\"line\">  scope: Namespaced</span><br><span class=\"line\">  names:</span><br><span class=\"line\">    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><br><span class=\"line\">    plural: kubernetesclusters</span><br><span class=\"line\">    # singular name to be used as an alias on the CLI and for display</span><br><span class=\"line\">    singular: kubernetescluster</span><br><span class=\"line\">    # kind is normally the CamelCased singular type. Your resource manifests use this.</span><br><span class=\"line\">    kind: KubernetesCluster</span><br><span class=\"line\">\t  # listKind</span><br><span class=\"line\">    listKind: KubernetesClusterList</span><br><span class=\"line\">    # shortNames allow shorter string to match your resource on the CLI</span><br><span class=\"line\">    shortNames:</span><br><span class=\"line\">    - ecs</span><br></pre></td></tr></table></figure>\n<p>CRD 的一个对象：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: ecs.yun.com/v1</span><br><span class=\"line\">kind: KubernetesCluster</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: test-cluster</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  clusterType: kubernetes</span><br><span class=\"line\">  serviceCIDR: &apos;&apos;</span><br><span class=\"line\">  masterList:</span><br><span class=\"line\">  - ip: 192.168.1.10</span><br><span class=\"line\">  nodeList:</span><br><span class=\"line\">  - ip: 192.168.1.11</span><br><span class=\"line\">  privateSSHKey: &apos;&apos;</span><br><span class=\"line\">  scaleUp: 0</span><br><span class=\"line\">  scaleDown: 0</span><br></pre></td></tr></table></figure>\n<h4 id=\"一、OpenAPI-v3-schema\"><a href=\"#一、OpenAPI-v3-schema\" class=\"headerlink\" title=\"一、OpenAPI v3 schema\"></a>一、OpenAPI v3 schema</h4><p><a href=\"https://github.com/OAI/OpenAPI-Specification\" target=\"_blank\" rel=\"noopener\">OpenAPI</a> 是针对 REST API 的 API 描述格式，也是一种规范。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class=\"line\">kind: CustomResourceDefinition</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetesclusters.ecs.yun.com</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  group: ecs.yun.com</span><br><span class=\"line\">  versions:</span><br><span class=\"line\">    - name: v1</span><br><span class=\"line\">      served: true</span><br><span class=\"line\">      storage: true</span><br><span class=\"line\">  scope: Namespaced</span><br><span class=\"line\">  names:</span><br><span class=\"line\">    plural: kubernetesclusters</span><br><span class=\"line\">    singular: kubernetescluster</span><br><span class=\"line\">    kind: KubernetesCluster</span><br><span class=\"line\">    listKind: KubernetesClusterList</span><br><span class=\"line\">    shortNames:</span><br><span class=\"line\">    - ecs</span><br><span class=\"line\">  validation:</span><br><span class=\"line\">    openAPIV3Schema:</span><br><span class=\"line\">      properties:</span><br><span class=\"line\">        spec:</span><br><span class=\"line\">\t  type: object</span><br><span class=\"line\">          required:</span><br><span class=\"line\">          - clusterType</span><br><span class=\"line\">          - masterList</span><br><span class=\"line\">          - nodeList</span><br><span class=\"line\">          properties:</span><br><span class=\"line\">            clusterType:</span><br><span class=\"line\">              type: string</span><br><span class=\"line\">            scaleUp:</span><br><span class=\"line\">              type: integer</span><br><span class=\"line\">            scaleDown:</span><br><span class=\"line\">              type: integer</span><br><span class=\"line\">              minimum: 0</span><br></pre></td></tr></table></figure>\n<p>上面是使用 OpenAPI v3 检验的一个例子，OpenAPI v3 仅支持一些简单的校验规则，可以校验参数的类型，参数值的类型(支持正则)，是否为必要参数等，但若要使用与、或、非等操作对多个字段同时校验还是做不到的，所以针对一些特定场景的校验需要使用 admission webhook。 </p>\n<h4 id=\"二、Admission-Webhooks\"><a href=\"#二、Admission-Webhooks\" class=\"headerlink\" title=\"二、Admission Webhooks\"></a>二、Admission Webhooks</h4><p>admission control 在 apiserver 中进行配置的，使用<code>--enable-admission-plugins</code> 或 <code>--admission-control</code>进行启用，admission control 配置的控制器列表是有顺序的，越靠前的越先执行，一旦某个控制器返回的结果是reject 的，那么整个准入控制阶段立刻结束，所以这里的配置顺序是有序的，建议使用官方的顺序配置。</p>\n<p>在 v1.9 中，admission webhook 是通过在 <code>--admission-control</code> 中配置 <code>ValidatingAdmissionWebhook</code> 或 <code>MutatingAdmissionWebhook</code> 来支持使用的，两者区别如下：</p>\n<ul>\n<li>MutatingAdmissionWebhook：允许在 webhook 中对 object 进行 mutate 修改，但匹配到的 webhook <strong>串行</strong>执行，因为每个 webhook 都可能会 mutate object。</li>\n<li>ValidatingAdmissionWebhook: 不允许在 webhook 中对 Object 进行 mutate 修改，仅返回 true 或 false。</li>\n</ul>\n<p>启用 admission webhook 后，每次对 CR 做 CRUD 操作时，请求就会被 apiserver 拦住，至于 CRUD 中哪些请求被拦住都是提前在 WebhookConfiguration 中配置的，然后会调用 AdmissionWebhook 进行检查是否 Admit 通过。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/1562032999173.jpg\" alt=\"kubernetes API request lifecycle\"></p>\n<h4 id=\"三、启用-Admission-Webhooks-功能\"><a href=\"#三、启用-Admission-Webhooks-功能\" class=\"headerlink\" title=\"三、启用 Admission Webhooks 功能\"></a>三、启用 Admission Webhooks 功能</h4><blockquote>\n<p>kubernetes 版本 &gt;= v1.9</p>\n</blockquote>\n<p>1、在 apiserver 中开启 admission webhooks</p>\n<p>在 v1.9 版本中使用的是：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota</span><br></pre></td></tr></table></figure>\n<p>在 v1.10 以后会弃用 <code>--admission-control</code>，取而代之的是  <code>--enable-admission-plugins</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--enable-admission-plugins=NodeRestriction,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota</span><br></pre></td></tr></table></figure>\n<p>启用之后在 api-resources 可以看到：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># kubectl api-resources | grep admissionregistration</span><br><span class=\"line\">mutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration</span><br><span class=\"line\">validatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration</span><br></pre></td></tr></table></figure>\n<p>2、启用 <code>admissionregistration.k8s.io/v1alpha1</code> API</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//  检查 API 是否已启用</span><br><span class=\"line\">$ kubectl api-versions | grep admissionregistration.k8s.io</span><br></pre></td></tr></table></figure>\n<p>若不存在则需要在 apiserver 的配置中添加<code>--runtime-config=admissionregistration.k8s.io/v1alpha1</code>。</p>\n<h4 id=\"四、编写-Admission-Webhook-Server\"><a href=\"#四、编写-Admission-Webhook-Server\" class=\"headerlink\" title=\"四、编写 Admission Webhook Server\"></a>四、编写 Admission Webhook Server</h4><p>webhook 其实就是一个 RESTful API 里面加上自己的一些校验逻辑。</p>\n<p>可以参考官方的示例：<br><a href=\"https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.go\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.go</a><br>或者<br><a href=\"https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go</a></p>\n<blockquote>\n<p>完整代码参考：<a href=\"https://github.com/gosoon/admission-webhook\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/admission-webhook</a></p>\n</blockquote>\n<h4 id=\"五、部署-Admission-Webhook-Service\"><a href=\"#五、部署-Admission-Webhook-Service\" class=\"headerlink\" title=\"五、部署 Admission Webhook Service\"></a>五、部署 Admission Webhook Service</h4><p>由于 apiserver 调用 webhook 时强制使用 TLS 认证，所以 WebhookConfiguration 中一定要配置 caBundle，也就是需要自己生成一套私有证书。</p>\n<p>生成证书的方式比较多，以下使用 openssl 生成，脚本如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate the CA cert and private key</span><br><span class=\"line\">openssl req -nodes -new -x509 -days 365 -keyout ca.key -out ca.crt -subj &quot;/CN=admission-webhook CA&quot;</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate the private key for the webhook server</span><br><span class=\"line\">openssl genrsa -out admission-webhook-tls.key 2048</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate a Certificate Signing Request (CSR) for the private key, and sign it with the private key of the CA.</span><br><span class=\"line\">openssl req -new -key admission-webhook-tls.key -subj &quot;/CN=admission-webhook.ecs-system.svc&quot; \\</span><br><span class=\"line\">    | openssl x509 -days 365 -req -CA ca.crt -CAkey ca.key -CAcreateserial -out admission-webhook-tls.crt</span><br><span class=\"line\"></span><br><span class=\"line\"># Generate pem</span><br><span class=\"line\">openssl base64 -A &lt; ca.crt &gt; ca.pem</span><br></pre></td></tr></table></figure>\n<p>生成证书后将 ca.pem 中的内容复制到 caBundle 处。</p>\n<p>ValidatingWebhook yaml 文件如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: admissionregistration.k8s.io/v1beta1</span><br><span class=\"line\">kind: ValidatingWebhookConfiguration</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: admission-webhook</span><br><span class=\"line\">webhooks:</span><br><span class=\"line\">  - name: admission-webhook.ecs-system.svc  # 必须为 &lt;svc_name&gt;.&lt;svc_namespace&gt;.svc.</span><br><span class=\"line\">    failurePolicy: Ignore</span><br><span class=\"line\">    clientConfig:</span><br><span class=\"line\">      service:</span><br><span class=\"line\">        name: admission-webhook</span><br><span class=\"line\">        namespace: ecs-system</span><br><span class=\"line\">        path: /ecs/operator/cluster  # webhook controller</span><br><span class=\"line\">      caBundle: xxx</span><br><span class=\"line\">    rules:</span><br><span class=\"line\">      - operations:   # 需要校验的方法</span><br><span class=\"line\">        - CREATE</span><br><span class=\"line\">        - UPDATE</span><br><span class=\"line\">        apiGroups:    # api group</span><br><span class=\"line\">        - ecs.yun.com</span><br><span class=\"line\">        apiVersions:  # version</span><br><span class=\"line\">        - v1</span><br><span class=\"line\">        resources:    # resource</span><br><span class=\"line\">        - kubernetesclusters</span><br></pre></td></tr></table></figure>\n<p>注意 <code>failurePolicy</code> 可以为 <code>Ignore</code>或者<code>Fail</code>，意味着如果和 webhook 通信出现问题导致调用失败，将根据 <code>failurePolicy</code>决定忽略失败（admit）还是准入失败(reject)。</p>\n<p>最后将 webhook 部署在集群中。</p>\n<p>参考：<br><a href=\"https://github.com/gosoon/admission-webhook\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/admission-webhook</a><br><a href=\"https://banzaicloud.com/blog/k8s-admission-webhooks/\" target=\"_blank\" rel=\"noopener\">https://banzaicloud.com/blog/k8s-admission-webhooks/</a><br><a href=\"http://blog.fatedier.com/2019/03/20/k8s-crd/\" target=\"_blank\" rel=\"noopener\">http://blog.fatedier.com/2019/03/20/k8s-crd/</a><br><a href=\"https://my.oschina.net/jxcdwangtao/blog/1591681\" target=\"_blank\" rel=\"noopener\">https://my.oschina.net/jxcdwangtao/blog/1591681</a><br><a href=\"https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/</a><br><a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use</a><br><a href=\"https://istio.io/zh/help/ops/setup/validation/\" target=\"_blank\" rel=\"noopener\">https://istio.io/zh/help/ops/setup/validation/</a><br><a href=\"https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/</a></p>\n"},{"title":"部署 kubernetes 可视化监控组件","date":"2019-04-22T14:43:30.000Z","type":"kubernetes","_content":"\n随着 kubernetes 的大规模使用，对 kubernetes 组件及其上运行服务的监控也是非常重要的一个环节，目前开源的监控组件有很多种，例如 cAdvisor、Heapster、metrics-server、kube-state-metrics、Prometheus 等，对监控数据的可视化查看组件有 Dashboard、 Prometheus、Grafana 等，本文会介绍 kube-dashboard 和基于 prometheus 搭建数据可视化监控。\n\n> kubernetes 版本：v1.12\n\n### 一、kubernetes-dashboard 的部署\n\n#### 1、创建 kubernetes-dashboard\n\n```golang\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\n\n$ kubectl get svc -n kube-system | grep kubernetes-dashboard\nkubernetes-dashboard   ClusterIP    10.101.203.44   <none>        443/TCP    2h\n\n$ kubectl get pod -n kube-system | grep kubernetes-dashboard\nkubernetes-dashboard-65c76f6c97-8npsv    1/1     Running       0          2h\n```\n\n> 所需镜像下载地址：[k8s-system-images](https://github.com/gosoon/k8s-system-images)\n\n#### 2、使用 nodePort 方式访问 kubernetes-dashboard\n\nnodeport 的访问方式虽然有性能损失但是比较简单，kubernetes-dashboard 默认使用 clusterIP 的方式暴露服务，修改 kubernetes-dashboard svc 使用 nodePort 方式：\n\n```\n$ kubectl edit svc -n kube-system\n\t...\n  spec:\n    clusterIP: 10.101.203.44\n    externalTrafficPolicy: Cluster\n    ports:\n    - nodePort: 8004  // 添加 nodeport 端口\n      port: 443\n      protocol: TCP\n      targetPort: 8443\n    selector:\n      k8s-app: kubernetes-dashboard\n    sessionAffinity: None\n    type: NodePort   // 将 ClusterIP 修改为 NodePort\n    ...\n```\n\nnodePort 端口默认为 30000-32767，若使用其他端口，需要修改 apiserver 的启动参数 `--service-node-port-range` 来指定 nodePort 范围，如：`--service-node-port-range 8000-9000`。\n\n#### 3、创建 kubernetes-dashboard 管理员角色\n\n `kubernetes-dashboard-admin.yaml`：\n\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: dashboard-admin\n  namespace: kube-system\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: dashboard-admin\nsubjects:\n  - kind: ServiceAccount\n    name: dashboard-admin\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\n```\n\n创建角色并获取 token：\n\n```\n$ kubectl apply -f kubernetes-dashboard-admin.yaml\n\n$ kubectl describe secrets `kubectl get secret -n kube-system | grep dashboard-admin | awk '{print $1}'` -n kube-system\n\nName:         dashboard-admin-token-hrhfd\nNamespace:    kube-system\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name: dashboard-admin\n              kubernetes.io/service-account.uid: 76805bdb-6047-11e9-ba0d-525400c322d9\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1025 bytes\nnamespace:  11 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taHJoZmQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNzY4MDViZGItNjA0Ny0xMWU5LWJhMGQtNTI1NDAwYzMyMmQ5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.hJJRyp_O4sGvIULj3BhqidCkkPnD4A2AtnpkXJoEPCALaaQHC8zhCA5-nDNlo2fiEggZ02UZPwiyGxKKFPC57UlKhjTf5zYcMIhELVXlj5FdBmjzCZcCHVFF4tj_rCoOFlZi6fQ3vNCcX8CtLxX_OsH1YXaFVuUmR1gYm97hbyuO382_k3tFIPXFP3QG8zUtc_7QMkeMNEakJZLCvkW8xdlaCuC-GVAMhZl5Kq1MSthuF-8HY7KaXhvqQzfD4DQZrdQ7vf_7NG3rdvhsj8nQ__TTe1W0RjqwkQuxg5YdE4gbAsxwJjkek-N0K9HfnZhkS9WosaUaUe9pZaGZ9akqyQ\n```\n\ntoken 是访问 dashboard 需要用的。\n\n\n若没有安装 kube-proxy，可以参考官方提供使用 `kubectl proxy` 的方式访问：\n\n```\n$ kubectl proxy --address=IP --disable-filter=true\n```\n\n访问 http://IP:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login\n\n已部署 kube-proxy 的可直接访问 https://IP:nodePort \n\n![](http://cdn.tianfeiyu.com/dash-1.png)\n\n选择令牌方式使用上面生成的 token 登录。\n\n![](http://cdn.tianfeiyu.com/dash-2.png)\n\nDashboard 可以使用 Ingress、Let's Encrypt 等多种方式配置 ssl，关于 ssl 的详细配置此处不进行详解。\n\n\n### 二、部署 prometheus \n\nprometheus 作为 CNCF 生态圈中的重要一员，其活跃度仅次于 Kubernetes, 现已广泛用于 Kubernetes 集群的监控系统中。prometheus 的部署相对比较简单，社区已经有了 [kube-prometheus](<https://github.com/coreos/kube-prometheus>)，kube-prometheus 会部署包含 prometheus-operator、grafana、kube-state-metrics 等多个组件。\n\n```\n$ git clone https://github.com/coreos/kube-prometheus\n\n$ kubectl apply -f manifests/\n```\n\n为了使用简单，我也会将 prometheus 和 grafana 的端口修改为 nodePort 的方式进行暴露：\n\n```\n$ kubectl edit svc prometheus-k8s -n monitoring\n\n$ kubectl edit svc grafana -n monitoring\n\n$ kubectl get svc -n monitoring\nNAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nalertmanager-main       NodePort    10.102.81.118   <none>        9093:8007/TCP       5d1h\nalertmanager-operated   ClusterIP   None            <none>        9093/TCP,6783/TCP   5d1h\ngrafana                 NodePort    10.96.19.82     <none>        3000:8006/TCP       5d1h\nkube-state-metrics      ClusterIP   None            <none>        8443/TCP,9443/TCP   5d1h\nnode-exporter           ClusterIP   None            <none>        9100/TCP            5d1h\nprometheus-adapter      ClusterIP   10.107.103.58   <none>        443/TCP             5d1h\nprometheus-k8s          NodePort    10.110.222.41   <none>        9090:8005/TCP       5d1h\nprometheus-operated     ClusterIP   None            <none>        9090/TCP            5d1h\nprometheus-operator     ClusterIP   None            <none>        8080/TCP            5d1h\n\n$ kubectl get pod -n monitoring\nNAME                                   READY   STATUS    RESTARTS   AGE\nalertmanager-main-0                    2/2     Running   0          4d\nalertmanager-main-1                    2/2     Running   0          4d\nalertmanager-main-2                    2/2     Running   0          4d\ngrafana-9d97dfdc7-qfjts                1/1     Running   0          4d\nkube-state-metrics-74d7dcd7dc-qfz5m    4/4     Running   0          3d11h\nnode-exporter-5cdl2                    2/2     Running   0          4d\nprometheus-adapter-b7d894c9c-dvzzq     1/1     Running   0          4d\nprometheus-k8s-0                       3/3     Running   1          2d2h\nprometheus-k8s-1                       3/3     Running   1          4d\nprometheus-operator-77b8b97459-7qfxj   1/1     Running   0          4d\n```\n\n上面几个组件成功运行后就可以在页面访问 prometheus 和 ganfana ：\n\n![](http://cdn.tianfeiyu.com/dash-3.png)\n\n进入 grafana 的 web 端，默认用户名和密码均为 admin：\n\n![](http://cdn.tianfeiyu.com/dash-4.png)\n\ngrafana 支持导入其他的 Dashboard，在 grafana 官方网站可以搜到大量与 k8s 相关的 dashboard。 \n\n### 三、总结\n\n本文介绍了对 kubernetes 和容器监控比较成熟的两个方案，虽然目前开源的方案比较多，但是要形成采集、存储、展示、报警一个完成的体系还需要在使用过程中不断探索与完善。\n","source":"_posts/k8s_dashboard_prometheus.md","raw":"---\ntitle: 部署 kubernetes 可视化监控组件\ndate: 2019-04-22 22:43:30\ntags: [\"kube-dashboard\",\"prometheus\"]\ntype: \"kubernetes\"\n\n---\n\n随着 kubernetes 的大规模使用，对 kubernetes 组件及其上运行服务的监控也是非常重要的一个环节，目前开源的监控组件有很多种，例如 cAdvisor、Heapster、metrics-server、kube-state-metrics、Prometheus 等，对监控数据的可视化查看组件有 Dashboard、 Prometheus、Grafana 等，本文会介绍 kube-dashboard 和基于 prometheus 搭建数据可视化监控。\n\n> kubernetes 版本：v1.12\n\n### 一、kubernetes-dashboard 的部署\n\n#### 1、创建 kubernetes-dashboard\n\n```golang\n$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml\n\n$ kubectl get svc -n kube-system | grep kubernetes-dashboard\nkubernetes-dashboard   ClusterIP    10.101.203.44   <none>        443/TCP    2h\n\n$ kubectl get pod -n kube-system | grep kubernetes-dashboard\nkubernetes-dashboard-65c76f6c97-8npsv    1/1     Running       0          2h\n```\n\n> 所需镜像下载地址：[k8s-system-images](https://github.com/gosoon/k8s-system-images)\n\n#### 2、使用 nodePort 方式访问 kubernetes-dashboard\n\nnodeport 的访问方式虽然有性能损失但是比较简单，kubernetes-dashboard 默认使用 clusterIP 的方式暴露服务，修改 kubernetes-dashboard svc 使用 nodePort 方式：\n\n```\n$ kubectl edit svc -n kube-system\n\t...\n  spec:\n    clusterIP: 10.101.203.44\n    externalTrafficPolicy: Cluster\n    ports:\n    - nodePort: 8004  // 添加 nodeport 端口\n      port: 443\n      protocol: TCP\n      targetPort: 8443\n    selector:\n      k8s-app: kubernetes-dashboard\n    sessionAffinity: None\n    type: NodePort   // 将 ClusterIP 修改为 NodePort\n    ...\n```\n\nnodePort 端口默认为 30000-32767，若使用其他端口，需要修改 apiserver 的启动参数 `--service-node-port-range` 来指定 nodePort 范围，如：`--service-node-port-range 8000-9000`。\n\n#### 3、创建 kubernetes-dashboard 管理员角色\n\n `kubernetes-dashboard-admin.yaml`：\n\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: dashboard-admin\n  namespace: kube-system\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: dashboard-admin\nsubjects:\n  - kind: ServiceAccount\n    name: dashboard-admin\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\n```\n\n创建角色并获取 token：\n\n```\n$ kubectl apply -f kubernetes-dashboard-admin.yaml\n\n$ kubectl describe secrets `kubectl get secret -n kube-system | grep dashboard-admin | awk '{print $1}'` -n kube-system\n\nName:         dashboard-admin-token-hrhfd\nNamespace:    kube-system\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name: dashboard-admin\n              kubernetes.io/service-account.uid: 76805bdb-6047-11e9-ba0d-525400c322d9\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1025 bytes\nnamespace:  11 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taHJoZmQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNzY4MDViZGItNjA0Ny0xMWU5LWJhMGQtNTI1NDAwYzMyMmQ5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.hJJRyp_O4sGvIULj3BhqidCkkPnD4A2AtnpkXJoEPCALaaQHC8zhCA5-nDNlo2fiEggZ02UZPwiyGxKKFPC57UlKhjTf5zYcMIhELVXlj5FdBmjzCZcCHVFF4tj_rCoOFlZi6fQ3vNCcX8CtLxX_OsH1YXaFVuUmR1gYm97hbyuO382_k3tFIPXFP3QG8zUtc_7QMkeMNEakJZLCvkW8xdlaCuC-GVAMhZl5Kq1MSthuF-8HY7KaXhvqQzfD4DQZrdQ7vf_7NG3rdvhsj8nQ__TTe1W0RjqwkQuxg5YdE4gbAsxwJjkek-N0K9HfnZhkS9WosaUaUe9pZaGZ9akqyQ\n```\n\ntoken 是访问 dashboard 需要用的。\n\n\n若没有安装 kube-proxy，可以参考官方提供使用 `kubectl proxy` 的方式访问：\n\n```\n$ kubectl proxy --address=IP --disable-filter=true\n```\n\n访问 http://IP:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login\n\n已部署 kube-proxy 的可直接访问 https://IP:nodePort \n\n![](http://cdn.tianfeiyu.com/dash-1.png)\n\n选择令牌方式使用上面生成的 token 登录。\n\n![](http://cdn.tianfeiyu.com/dash-2.png)\n\nDashboard 可以使用 Ingress、Let's Encrypt 等多种方式配置 ssl，关于 ssl 的详细配置此处不进行详解。\n\n\n### 二、部署 prometheus \n\nprometheus 作为 CNCF 生态圈中的重要一员，其活跃度仅次于 Kubernetes, 现已广泛用于 Kubernetes 集群的监控系统中。prometheus 的部署相对比较简单，社区已经有了 [kube-prometheus](<https://github.com/coreos/kube-prometheus>)，kube-prometheus 会部署包含 prometheus-operator、grafana、kube-state-metrics 等多个组件。\n\n```\n$ git clone https://github.com/coreos/kube-prometheus\n\n$ kubectl apply -f manifests/\n```\n\n为了使用简单，我也会将 prometheus 和 grafana 的端口修改为 nodePort 的方式进行暴露：\n\n```\n$ kubectl edit svc prometheus-k8s -n monitoring\n\n$ kubectl edit svc grafana -n monitoring\n\n$ kubectl get svc -n monitoring\nNAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nalertmanager-main       NodePort    10.102.81.118   <none>        9093:8007/TCP       5d1h\nalertmanager-operated   ClusterIP   None            <none>        9093/TCP,6783/TCP   5d1h\ngrafana                 NodePort    10.96.19.82     <none>        3000:8006/TCP       5d1h\nkube-state-metrics      ClusterIP   None            <none>        8443/TCP,9443/TCP   5d1h\nnode-exporter           ClusterIP   None            <none>        9100/TCP            5d1h\nprometheus-adapter      ClusterIP   10.107.103.58   <none>        443/TCP             5d1h\nprometheus-k8s          NodePort    10.110.222.41   <none>        9090:8005/TCP       5d1h\nprometheus-operated     ClusterIP   None            <none>        9090/TCP            5d1h\nprometheus-operator     ClusterIP   None            <none>        8080/TCP            5d1h\n\n$ kubectl get pod -n monitoring\nNAME                                   READY   STATUS    RESTARTS   AGE\nalertmanager-main-0                    2/2     Running   0          4d\nalertmanager-main-1                    2/2     Running   0          4d\nalertmanager-main-2                    2/2     Running   0          4d\ngrafana-9d97dfdc7-qfjts                1/1     Running   0          4d\nkube-state-metrics-74d7dcd7dc-qfz5m    4/4     Running   0          3d11h\nnode-exporter-5cdl2                    2/2     Running   0          4d\nprometheus-adapter-b7d894c9c-dvzzq     1/1     Running   0          4d\nprometheus-k8s-0                       3/3     Running   1          2d2h\nprometheus-k8s-1                       3/3     Running   1          4d\nprometheus-operator-77b8b97459-7qfxj   1/1     Running   0          4d\n```\n\n上面几个组件成功运行后就可以在页面访问 prometheus 和 ganfana ：\n\n![](http://cdn.tianfeiyu.com/dash-3.png)\n\n进入 grafana 的 web 端，默认用户名和密码均为 admin：\n\n![](http://cdn.tianfeiyu.com/dash-4.png)\n\ngrafana 支持导入其他的 Dashboard，在 grafana 官方网站可以搜到大量与 k8s 相关的 dashboard。 \n\n### 三、总结\n\n本文介绍了对 kubernetes 和容器监控比较成熟的两个方案，虽然目前开源的方案比较多，但是要形成采集、存储、展示、报警一个完成的体系还需要在使用过程中不断探索与完善。\n","slug":"k8s_dashboard_prometheus","published":1,"updated":"2019-07-21T10:03:57.982Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59d000iapwn4owb2d6g","content":"<p>随着 kubernetes 的大规模使用，对 kubernetes 组件及其上运行服务的监控也是非常重要的一个环节，目前开源的监控组件有很多种，例如 cAdvisor、Heapster、metrics-server、kube-state-metrics、Prometheus 等，对监控数据的可视化查看组件有 Dashboard、 Prometheus、Grafana 等，本文会介绍 kube-dashboard 和基于 prometheus 搭建数据可视化监控。</p>\n<blockquote>\n<p>kubernetes 版本：v1.12</p>\n</blockquote>\n<h3 id=\"一、kubernetes-dashboard-的部署\"><a href=\"#一、kubernetes-dashboard-的部署\" class=\"headerlink\" title=\"一、kubernetes-dashboard 的部署\"></a>一、kubernetes-dashboard 的部署</h3><h4 id=\"1、创建-kubernetes-dashboard\"><a href=\"#1、创建-kubernetes-dashboard\" class=\"headerlink\" title=\"1、创建 kubernetes-dashboard\"></a>1、创建 kubernetes-dashboard</h4><figure class=\"highlight golang\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f https:<span class=\"comment\">//raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml</span></span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get svc -n kube-system | grep kubernetes-dashboard</span><br><span class=\"line\">kubernetes-dashboard   ClusterIP    <span class=\"number\">10.101</span><span class=\"number\">.203</span><span class=\"number\">.44</span>   &lt;none&gt;        <span class=\"number\">443</span>/TCP    <span class=\"number\">2</span>h</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get pod -n kube-system | grep kubernetes-dashboard</span><br><span class=\"line\">kubernetes-dashboard<span class=\"number\">-65</span>c76f6c97<span class=\"number\">-8</span>npsv    <span class=\"number\">1</span>/<span class=\"number\">1</span>     Running       <span class=\"number\">0</span>          <span class=\"number\">2</span>h</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>所需镜像下载地址：<a href=\"https://github.com/gosoon/k8s-system-images\" target=\"_blank\" rel=\"noopener\">k8s-system-images</a></p>\n</blockquote>\n<h4 id=\"2、使用-nodePort-方式访问-kubernetes-dashboard\"><a href=\"#2、使用-nodePort-方式访问-kubernetes-dashboard\" class=\"headerlink\" title=\"2、使用 nodePort 方式访问 kubernetes-dashboard\"></a>2、使用 nodePort 方式访问 kubernetes-dashboard</h4><p>nodeport 的访问方式虽然有性能损失但是比较简单，kubernetes-dashboard 默认使用 clusterIP 的方式暴露服务，修改 kubernetes-dashboard svc 使用 nodePort 方式：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl edit svc -n kube-system</span><br><span class=\"line\">\t...</span><br><span class=\"line\">  spec:</span><br><span class=\"line\">    clusterIP: 10.101.203.44</span><br><span class=\"line\">    externalTrafficPolicy: Cluster</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">    - nodePort: 8004  // 添加 nodeport 端口</span><br><span class=\"line\">      port: 443</span><br><span class=\"line\">      protocol: TCP</span><br><span class=\"line\">      targetPort: 8443</span><br><span class=\"line\">    selector:</span><br><span class=\"line\">      k8s-app: kubernetes-dashboard</span><br><span class=\"line\">    sessionAffinity: None</span><br><span class=\"line\">    type: NodePort   // 将 ClusterIP 修改为 NodePort</span><br><span class=\"line\">    ...</span><br></pre></td></tr></table></figure>\n<p>nodePort 端口默认为 30000-32767，若使用其他端口，需要修改 apiserver 的启动参数 <code>--service-node-port-range</code> 来指定 nodePort 范围，如：<code>--service-node-port-range 8000-9000</code>。</p>\n<h4 id=\"3、创建-kubernetes-dashboard-管理员角色\"><a href=\"#3、创建-kubernetes-dashboard-管理员角色\" class=\"headerlink\" title=\"3、创建 kubernetes-dashboard 管理员角色\"></a>3、创建 kubernetes-dashboard 管理员角色</h4><p> <code>kubernetes-dashboard-admin.yaml</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dashboard-admin</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">---</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dashboard-admin</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">  - kind: ServiceAccount</span><br><span class=\"line\">    name: dashboard-admin</span><br><span class=\"line\">    namespace: kube-system</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: cluster-admin</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>\n<p>创建角色并获取 token：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f kubernetes-dashboard-admin.yaml</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl describe secrets `kubectl get secret -n kube-system | grep dashboard-admin | awk &apos;&#123;print $1&#125;&apos;` -n kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">Name:         dashboard-admin-token-hrhfd</span><br><span class=\"line\">Namespace:    kube-system</span><br><span class=\"line\">Labels:       &lt;none&gt;</span><br><span class=\"line\">Annotations:  kubernetes.io/service-account.name: dashboard-admin</span><br><span class=\"line\">              kubernetes.io/service-account.uid: 76805bdb-6047-11e9-ba0d-525400c322d9</span><br><span class=\"line\"></span><br><span class=\"line\">Type:  kubernetes.io/service-account-token</span><br><span class=\"line\"></span><br><span class=\"line\">Data</span><br><span class=\"line\">====</span><br><span class=\"line\">ca.crt:     1025 bytes</span><br><span class=\"line\">namespace:  11 bytes</span><br><span class=\"line\">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taHJoZmQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNzY4MDViZGItNjA0Ny0xMWU5LWJhMGQtNTI1NDAwYzMyMmQ5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.hJJRyp_O4sGvIULj3BhqidCkkPnD4A2AtnpkXJoEPCALaaQHC8zhCA5-nDNlo2fiEggZ02UZPwiyGxKKFPC57UlKhjTf5zYcMIhELVXlj5FdBmjzCZcCHVFF4tj_rCoOFlZi6fQ3vNCcX8CtLxX_OsH1YXaFVuUmR1gYm97hbyuO382_k3tFIPXFP3QG8zUtc_7QMkeMNEakJZLCvkW8xdlaCuC-GVAMhZl5Kq1MSthuF-8HY7KaXhvqQzfD4DQZrdQ7vf_7NG3rdvhsj8nQ__TTe1W0RjqwkQuxg5YdE4gbAsxwJjkek-N0K9HfnZhkS9WosaUaUe9pZaGZ9akqyQ</span><br></pre></td></tr></table></figure>\n<p>token 是访问 dashboard 需要用的。</p>\n<p>若没有安装 kube-proxy，可以参考官方提供使用 <code>kubectl proxy</code> 的方式访问：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl proxy --address=IP --disable-filter=true</span><br></pre></td></tr></table></figure>\n<p>访问 <a href=\"http://IP:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login\" target=\"_blank\" rel=\"noopener\">http://IP:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login</a></p>\n<p>已部署 kube-proxy 的可直接访问 <a href=\"https://IP:nodePort\" target=\"_blank\" rel=\"noopener\">https://IP:nodePort</a> </p>\n<p><img src=\"http://cdn.tianfeiyu.com/dash-1.png\" alt=\"\"></p>\n<p>选择令牌方式使用上面生成的 token 登录。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/dash-2.png\" alt=\"\"></p>\n<p>Dashboard 可以使用 Ingress、Let’s Encrypt 等多种方式配置 ssl，关于 ssl 的详细配置此处不进行详解。</p>\n<h3 id=\"二、部署-prometheus\"><a href=\"#二、部署-prometheus\" class=\"headerlink\" title=\"二、部署 prometheus\"></a>二、部署 prometheus</h3><p>prometheus 作为 CNCF 生态圈中的重要一员，其活跃度仅次于 Kubernetes, 现已广泛用于 Kubernetes 集群的监控系统中。prometheus 的部署相对比较简单，社区已经有了 <a href=\"https://github.com/coreos/kube-prometheus\" target=\"_blank\" rel=\"noopener\">kube-prometheus</a>，kube-prometheus 会部署包含 prometheus-operator、grafana、kube-state-metrics 等多个组件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github.com/coreos/kube-prometheus</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl apply -f manifests/</span><br></pre></td></tr></table></figure>\n<p>为了使用简单，我也会将 prometheus 和 grafana 的端口修改为 nodePort 的方式进行暴露：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl edit svc prometheus-k8s -n monitoring</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl edit svc grafana -n monitoring</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get svc -n monitoring</span><br><span class=\"line\">NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE</span><br><span class=\"line\">alertmanager-main       NodePort    10.102.81.118   &lt;none&gt;        9093:8007/TCP       5d1h</span><br><span class=\"line\">alertmanager-operated   ClusterIP   None            &lt;none&gt;        9093/TCP,6783/TCP   5d1h</span><br><span class=\"line\">grafana                 NodePort    10.96.19.82     &lt;none&gt;        3000:8006/TCP       5d1h</span><br><span class=\"line\">kube-state-metrics      ClusterIP   None            &lt;none&gt;        8443/TCP,9443/TCP   5d1h</span><br><span class=\"line\">node-exporter           ClusterIP   None            &lt;none&gt;        9100/TCP            5d1h</span><br><span class=\"line\">prometheus-adapter      ClusterIP   10.107.103.58   &lt;none&gt;        443/TCP             5d1h</span><br><span class=\"line\">prometheus-k8s          NodePort    10.110.222.41   &lt;none&gt;        9090:8005/TCP       5d1h</span><br><span class=\"line\">prometheus-operated     ClusterIP   None            &lt;none&gt;        9090/TCP            5d1h</span><br><span class=\"line\">prometheus-operator     ClusterIP   None            &lt;none&gt;        8080/TCP            5d1h</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get pod -n monitoring</span><br><span class=\"line\">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">alertmanager-main-0                    2/2     Running   0          4d</span><br><span class=\"line\">alertmanager-main-1                    2/2     Running   0          4d</span><br><span class=\"line\">alertmanager-main-2                    2/2     Running   0          4d</span><br><span class=\"line\">grafana-9d97dfdc7-qfjts                1/1     Running   0          4d</span><br><span class=\"line\">kube-state-metrics-74d7dcd7dc-qfz5m    4/4     Running   0          3d11h</span><br><span class=\"line\">node-exporter-5cdl2                    2/2     Running   0          4d</span><br><span class=\"line\">prometheus-adapter-b7d894c9c-dvzzq     1/1     Running   0          4d</span><br><span class=\"line\">prometheus-k8s-0                       3/3     Running   1          2d2h</span><br><span class=\"line\">prometheus-k8s-1                       3/3     Running   1          4d</span><br><span class=\"line\">prometheus-operator-77b8b97459-7qfxj   1/1     Running   0          4d</span><br></pre></td></tr></table></figure>\n<p>上面几个组件成功运行后就可以在页面访问 prometheus 和 ganfana ：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/dash-3.png\" alt=\"\"></p>\n<p>进入 grafana 的 web 端，默认用户名和密码均为 admin：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/dash-4.png\" alt=\"\"></p>\n<p>grafana 支持导入其他的 Dashboard，在 grafana 官方网站可以搜到大量与 k8s 相关的 dashboard。 </p>\n<h3 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h3><p>本文介绍了对 kubernetes 和容器监控比较成熟的两个方案，虽然目前开源的方案比较多，但是要形成采集、存储、展示、报警一个完成的体系还需要在使用过程中不断探索与完善。</p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>随着 kubernetes 的大规模使用，对 kubernetes 组件及其上运行服务的监控也是非常重要的一个环节，目前开源的监控组件有很多种，例如 cAdvisor、Heapster、metrics-server、kube-state-metrics、Prometheus 等，对监控数据的可视化查看组件有 Dashboard、 Prometheus、Grafana 等，本文会介绍 kube-dashboard 和基于 prometheus 搭建数据可视化监控。</p>\n<blockquote>\n<p>kubernetes 版本：v1.12</p>\n</blockquote>\n<h3 id=\"一、kubernetes-dashboard-的部署\"><a href=\"#一、kubernetes-dashboard-的部署\" class=\"headerlink\" title=\"一、kubernetes-dashboard 的部署\"></a>一、kubernetes-dashboard 的部署</h3><h4 id=\"1、创建-kubernetes-dashboard\"><a href=\"#1、创建-kubernetes-dashboard\" class=\"headerlink\" title=\"1、创建 kubernetes-dashboard\"></a>1、创建 kubernetes-dashboard</h4><figure class=\"highlight golang\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f https:<span class=\"comment\">//raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml</span></span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get svc -n kube-system | grep kubernetes-dashboard</span><br><span class=\"line\">kubernetes-dashboard   ClusterIP    <span class=\"number\">10.101</span><span class=\"number\">.203</span><span class=\"number\">.44</span>   &lt;none&gt;        <span class=\"number\">443</span>/TCP    <span class=\"number\">2</span>h</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get pod -n kube-system | grep kubernetes-dashboard</span><br><span class=\"line\">kubernetes-dashboard<span class=\"number\">-65</span>c76f6c97<span class=\"number\">-8</span>npsv    <span class=\"number\">1</span>/<span class=\"number\">1</span>     Running       <span class=\"number\">0</span>          <span class=\"number\">2</span>h</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>所需镜像下载地址：<a href=\"https://github.com/gosoon/k8s-system-images\" target=\"_blank\" rel=\"noopener\">k8s-system-images</a></p>\n</blockquote>\n<h4 id=\"2、使用-nodePort-方式访问-kubernetes-dashboard\"><a href=\"#2、使用-nodePort-方式访问-kubernetes-dashboard\" class=\"headerlink\" title=\"2、使用 nodePort 方式访问 kubernetes-dashboard\"></a>2、使用 nodePort 方式访问 kubernetes-dashboard</h4><p>nodeport 的访问方式虽然有性能损失但是比较简单，kubernetes-dashboard 默认使用 clusterIP 的方式暴露服务，修改 kubernetes-dashboard svc 使用 nodePort 方式：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl edit svc -n kube-system</span><br><span class=\"line\">\t...</span><br><span class=\"line\">  spec:</span><br><span class=\"line\">    clusterIP: 10.101.203.44</span><br><span class=\"line\">    externalTrafficPolicy: Cluster</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">    - nodePort: 8004  // 添加 nodeport 端口</span><br><span class=\"line\">      port: 443</span><br><span class=\"line\">      protocol: TCP</span><br><span class=\"line\">      targetPort: 8443</span><br><span class=\"line\">    selector:</span><br><span class=\"line\">      k8s-app: kubernetes-dashboard</span><br><span class=\"line\">    sessionAffinity: None</span><br><span class=\"line\">    type: NodePort   // 将 ClusterIP 修改为 NodePort</span><br><span class=\"line\">    ...</span><br></pre></td></tr></table></figure>\n<p>nodePort 端口默认为 30000-32767，若使用其他端口，需要修改 apiserver 的启动参数 <code>--service-node-port-range</code> 来指定 nodePort 范围，如：<code>--service-node-port-range 8000-9000</code>。</p>\n<h4 id=\"3、创建-kubernetes-dashboard-管理员角色\"><a href=\"#3、创建-kubernetes-dashboard-管理员角色\" class=\"headerlink\" title=\"3、创建 kubernetes-dashboard 管理员角色\"></a>3、创建 kubernetes-dashboard 管理员角色</h4><p> <code>kubernetes-dashboard-admin.yaml</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: ServiceAccount</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dashboard-admin</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">---</span><br><span class=\"line\">kind: ClusterRoleBinding</span><br><span class=\"line\">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: dashboard-admin</span><br><span class=\"line\">subjects:</span><br><span class=\"line\">  - kind: ServiceAccount</span><br><span class=\"line\">    name: dashboard-admin</span><br><span class=\"line\">    namespace: kube-system</span><br><span class=\"line\">roleRef:</span><br><span class=\"line\">  kind: ClusterRole</span><br><span class=\"line\">  name: cluster-admin</span><br><span class=\"line\">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>\n<p>创建角色并获取 token：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f kubernetes-dashboard-admin.yaml</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl describe secrets `kubectl get secret -n kube-system | grep dashboard-admin | awk &apos;&#123;print $1&#125;&apos;` -n kube-system</span><br><span class=\"line\"></span><br><span class=\"line\">Name:         dashboard-admin-token-hrhfd</span><br><span class=\"line\">Namespace:    kube-system</span><br><span class=\"line\">Labels:       &lt;none&gt;</span><br><span class=\"line\">Annotations:  kubernetes.io/service-account.name: dashboard-admin</span><br><span class=\"line\">              kubernetes.io/service-account.uid: 76805bdb-6047-11e9-ba0d-525400c322d9</span><br><span class=\"line\"></span><br><span class=\"line\">Type:  kubernetes.io/service-account-token</span><br><span class=\"line\"></span><br><span class=\"line\">Data</span><br><span class=\"line\">====</span><br><span class=\"line\">ca.crt:     1025 bytes</span><br><span class=\"line\">namespace:  11 bytes</span><br><span class=\"line\">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taHJoZmQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNzY4MDViZGItNjA0Ny0xMWU5LWJhMGQtNTI1NDAwYzMyMmQ5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.hJJRyp_O4sGvIULj3BhqidCkkPnD4A2AtnpkXJoEPCALaaQHC8zhCA5-nDNlo2fiEggZ02UZPwiyGxKKFPC57UlKhjTf5zYcMIhELVXlj5FdBmjzCZcCHVFF4tj_rCoOFlZi6fQ3vNCcX8CtLxX_OsH1YXaFVuUmR1gYm97hbyuO382_k3tFIPXFP3QG8zUtc_7QMkeMNEakJZLCvkW8xdlaCuC-GVAMhZl5Kq1MSthuF-8HY7KaXhvqQzfD4DQZrdQ7vf_7NG3rdvhsj8nQ__TTe1W0RjqwkQuxg5YdE4gbAsxwJjkek-N0K9HfnZhkS9WosaUaUe9pZaGZ9akqyQ</span><br></pre></td></tr></table></figure>\n<p>token 是访问 dashboard 需要用的。</p>\n<p>若没有安装 kube-proxy，可以参考官方提供使用 <code>kubectl proxy</code> 的方式访问：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl proxy --address=IP --disable-filter=true</span><br></pre></td></tr></table></figure>\n<p>访问 <a href=\"http://IP:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login\" target=\"_blank\" rel=\"noopener\">http://IP:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login</a></p>\n<p>已部署 kube-proxy 的可直接访问 <a href=\"https://IP:nodePort\" target=\"_blank\" rel=\"noopener\">https://IP:nodePort</a> </p>\n<p><img src=\"http://cdn.tianfeiyu.com/dash-1.png\" alt=\"\"></p>\n<p>选择令牌方式使用上面生成的 token 登录。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/dash-2.png\" alt=\"\"></p>\n<p>Dashboard 可以使用 Ingress、Let’s Encrypt 等多种方式配置 ssl，关于 ssl 的详细配置此处不进行详解。</p>\n<h3 id=\"二、部署-prometheus\"><a href=\"#二、部署-prometheus\" class=\"headerlink\" title=\"二、部署 prometheus\"></a>二、部署 prometheus</h3><p>prometheus 作为 CNCF 生态圈中的重要一员，其活跃度仅次于 Kubernetes, 现已广泛用于 Kubernetes 集群的监控系统中。prometheus 的部署相对比较简单，社区已经有了 <a href=\"https://github.com/coreos/kube-prometheus\" target=\"_blank\" rel=\"noopener\">kube-prometheus</a>，kube-prometheus 会部署包含 prometheus-operator、grafana、kube-state-metrics 等多个组件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone https://github.com/coreos/kube-prometheus</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl apply -f manifests/</span><br></pre></td></tr></table></figure>\n<p>为了使用简单，我也会将 prometheus 和 grafana 的端口修改为 nodePort 的方式进行暴露：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl edit svc prometheus-k8s -n monitoring</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl edit svc grafana -n monitoring</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get svc -n monitoring</span><br><span class=\"line\">NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE</span><br><span class=\"line\">alertmanager-main       NodePort    10.102.81.118   &lt;none&gt;        9093:8007/TCP       5d1h</span><br><span class=\"line\">alertmanager-operated   ClusterIP   None            &lt;none&gt;        9093/TCP,6783/TCP   5d1h</span><br><span class=\"line\">grafana                 NodePort    10.96.19.82     &lt;none&gt;        3000:8006/TCP       5d1h</span><br><span class=\"line\">kube-state-metrics      ClusterIP   None            &lt;none&gt;        8443/TCP,9443/TCP   5d1h</span><br><span class=\"line\">node-exporter           ClusterIP   None            &lt;none&gt;        9100/TCP            5d1h</span><br><span class=\"line\">prometheus-adapter      ClusterIP   10.107.103.58   &lt;none&gt;        443/TCP             5d1h</span><br><span class=\"line\">prometheus-k8s          NodePort    10.110.222.41   &lt;none&gt;        9090:8005/TCP       5d1h</span><br><span class=\"line\">prometheus-operated     ClusterIP   None            &lt;none&gt;        9090/TCP            5d1h</span><br><span class=\"line\">prometheus-operator     ClusterIP   None            &lt;none&gt;        8080/TCP            5d1h</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get pod -n monitoring</span><br><span class=\"line\">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">alertmanager-main-0                    2/2     Running   0          4d</span><br><span class=\"line\">alertmanager-main-1                    2/2     Running   0          4d</span><br><span class=\"line\">alertmanager-main-2                    2/2     Running   0          4d</span><br><span class=\"line\">grafana-9d97dfdc7-qfjts                1/1     Running   0          4d</span><br><span class=\"line\">kube-state-metrics-74d7dcd7dc-qfz5m    4/4     Running   0          3d11h</span><br><span class=\"line\">node-exporter-5cdl2                    2/2     Running   0          4d</span><br><span class=\"line\">prometheus-adapter-b7d894c9c-dvzzq     1/1     Running   0          4d</span><br><span class=\"line\">prometheus-k8s-0                       3/3     Running   1          2d2h</span><br><span class=\"line\">prometheus-k8s-1                       3/3     Running   1          4d</span><br><span class=\"line\">prometheus-operator-77b8b97459-7qfxj   1/1     Running   0          4d</span><br></pre></td></tr></table></figure>\n<p>上面几个组件成功运行后就可以在页面访问 prometheus 和 ganfana ：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/dash-3.png\" alt=\"\"></p>\n<p>进入 grafana 的 web 端，默认用户名和密码均为 admin：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/dash-4.png\" alt=\"\"></p>\n<p>grafana 支持导入其他的 Dashboard，在 grafana 官方网站可以搜到大量与 k8s 相关的 dashboard。 </p>\n<h3 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h3><p>本文介绍了对 kubernetes 和容器监控比较成熟的两个方案，虽然目前开源的方案比较多，但是要形成采集、存储、展示、报警一个完成的体系还需要在使用过程中不断探索与完善。</p>\n"},{"title":"大规模场景下 kubernetes 集群的性能优化","date":"2019-10-12T08:00:30.000Z","type":"improvements","_content":"\n### 一、etcd 优化\n\n- 1、etcd 采用本地 ssd 盘作为后端存储存储\n\n- 2、etcd 独立部署在非 k8s node 上\n\n- 3、etcd 快照(snap)与预写式日志(wal)分盘存储\n\netcd 详细的优化操作可以参考上篇文章：[etcd 性能测试与调优](http://blog.tianfeiyu.com/2019/10/08/etcd_improvements/)。\n\n\n\n### 二、apiserver 的优化\n\n#### 1、参数调整\n\n- `--max-mutating-requests-inflight` ：在给定时间内的最大 mutating 请求数，调整 apiserver 的流控 qos，可以调整至 3000，默认为 200\n\n- `--max-requests-inflight`：在给定时间内的最大 non-mutating 请求数，默认 400，可以调整至 1000\n\n- `--watch-cache-sizes`：调大 resources 的 watch size，默认为 100，当集群中 node 以及 pod 数量非常多时可以稍微调大，比如： `--watch-cache-sizes=node#1000,pod#5000`\n\n#### 2、etcd 多实例支持\n\n对于不同 object 进行分库存储，首先应该将数据与状态分离，即将 events 放在单独的 etcd 实例中，在 apiserver 的配置中加上`--etcd-servers-overrides=/events#https://xxx:3379;https://xxx:3379;https://xxx:3379;https://xxxx:3379;https://xxx:3379`，后期可以将 pod、node  等 object 也分离在单独的 etcd 实例中。\n\n#### 3、apiserver 的负载均衡\n\n通常为了保证集群的高可用，集群中一般会有多个 master 节点，kubelet 的连接也会被均分到不同的 apiserver，在 k8s v1.10 以前的版本中，kubelet 使用 HTTP/2，HTTP/2 为了提高网络性能，一个主机只建立一个连接，所有的请求都通过该连接进行，默认情况下，即使网络异常，它还是重用这个连接，直到操作系统将连接关闭，而操作系统关闭僵尸连接的时间默认是十几分钟，所以在 v1.10 以前的版本中 kubelet 连接 apiserver 超时之后不会主动 reset 掉连接进行重试，除非主动重启 kubelet 或者等待十多分钟后其进行重试。\n\n此问题在 v1.10 版本中被修复过（[track/close kubelet->API connections on heartbeat failure #63492](https://github.com/kubernetes/kubernetes/pull/63492)），代码也被 merge 到了 v1.8 和 v1.9，但是该问题并没有完全修复，直到 v1.14 版本才被完全修复（ [kubelet: fix fail to close kubelet->API connections on heartbeat failure #78016](https://github.com/kubernetes/kubernetes/pull/78016)）。\n\n所以为了保证 apiserver 的连接数均衡，请使用 v1.14 及以上版本。\n\n#### 4、使用 pprof 进行性能分析\n\npprof 是 golang 的一大杀器，要想进行源码级别的性能分析，必须使用 pprof。\n\n```\n// 安装相关包\n$ brew install graphviz\n\n// 启动 pprof\n$ go tool pprof http://localhost:8001/debug/pprof/profile\nFile: kube-apiserver\nType: cpu\nTime: Oct 11, 2019 at 11:39am (CST)\nDuration: 30s, Total samples = 620ms ( 2.07%)\nEntering interactive mode (type \"help\" for commands, \"o\" for options)\n(pprof) web   // 使用 web 命令生成 svg 文件\n```\n\n然后打开 svg 文件：\n\n![apiserver](http://cdn.tianfeiyu.com/image-20191011115425979.png)\n\n\n\n可以通过 graph 以及交互式界面得到 cpu 耗时、goroutine 阻塞等信息，apiserver 中的对象比较多，序列化会消耗非常大的时间，golang 标准库的 json 也有很严重的性能问题，开源的 json-iter 相比标准库有不少性能上的提升，但 json-iter 有很多标准库不兼容的问题，此前也有相关的 [issue](https://github.com/kubernetes/kubernetes/pull/54289) 进行反馈但并没有合进主线。\n\n\n\n### 三、kube-controller-manager 的优化\n\n#### 1、参数优化\n\n- 调大 --kube-api-qps 值：可以调整至 100，默认值为 20\n\n- 调大 --kube-api-burst 值：可以调整至 100，默认值为 30\n\n- 禁用不需要的 controller：kubernetes v1.14 中已有 35 个 controller，默认启动为`--controllers`，即启动所有 controller，可以禁用不需要的 controller\n\n- 调整 controller 同步资源的周期：避免过多的资源同步导致集群资源的消耗，所有带有 `--concurrent` 前缀的参数\n\n\n\n#### 2、kube-controller-manager 升级过程 informer 预加载\n\n> 参考自 [阿里巴巴云原生实践 15 讲](https://zhuanlan.zhihu.com/p/73125817)\n\ncontroller-manager 中存储的对象非常多，每次升级过程中从 apiserver 获取这些对象并反序列化的开销是无法忽略的，重启  controller-manager 恢复时可能要花费几分钟才能完成。我们需要尽量的减小 controller-manager 单次升级对系统的中断时间，主要有以下两处改造：\n\n- 预启动备 controller informer，提前加载 controller 需要的数据\n\n- 主 controller 升级时，会主动释放 Leader Lease，触发备立即接管工作\n\n通过此方案 controller-manager 中断时间降低到秒级别(升级时 < 2s)，即使在异常宕机时，备仅需等待 leader lease 的过期(默认 15s)，无需要花费几分 钟重新同步数据。通过这个增强，显著的降低了 controller-manager MTTR（平均恢复时间），同时降低了 controller-manager 恢复时对 apiserver 的性能冲击。\n\n此方案需要对 controller-manager 上面两处的代码进行修改，controller-manager 默认的启动方式是先拿到锁然后 callback run 方法，在 run 方法中会启动 informers 然后同步对象，在停止时也要改为主动释放 leader release。\n\n\n\n### 四、kube-scheduler 优化\n\n在 k8s 核心组件中，调度器的功能做的比较通用，大部分公司都不会局限于当前调度器的功能而进行一系列的改造，例如美团就对 kube-scheduler 进行过一些优化，并将[**预选失败中断机制**](https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html)（详见[PR](https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html)）和[**将全局最优解改为局部最优解**](https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html)（详见[PR1](https://github.com/kubernetes/kubernetes/pull/66733)，[PR2](https://github.com/kubernetes/kubernetes/pull/67555)）等重要 feature 回馈给了社区。\n\n\n\n首先还是使用好调度器的基本功能：\n\n- [Pod/Node Affinity & Anti-affinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity)\n- [Taint & Toleration](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)\n- [Priority & Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/)\n- [Pod Disruption Budget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)\n\n然后再进行一些必要的优化。\n\n\n\n#### 1、参数优化\n\n调大`--kube-api-qps` 值：可以调整至 100，默认值为 50\n\n#### 2、调度器优化\n\n- 扩展调度器功能：目前可以通过 [scheduler_extender](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md) 很方便的扩展调度器，比如对于 GPU 的调度，可以通过  [scheduler_extender](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md)  + [device-plugins](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) 来支持。\n\n- [多调度器](https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/)支持：kubernetes 也支持在集群中运行多个调度器调度不同作业，例如可以在 pod 的 `spec.schedulerName` 指定对应的调度器，也可以在 job 的 `.spec.template.spec.schedulerName` 指定调度器。\n\n- 动态调度支持：由于 kubernetes 的默认调度器只在 pod 创建过程中进行一次性调度，后续不会重新去平衡 pod 在集群中的分布，导致实际的资源使用率不均衡，此时集群中会存在部分热点宿主，为了解决默认调度器的功能缺陷，kubernetes 孵化了一个工具 [Descheduler](https://github.com/kubernetes-incubator/descheduler) 来对默认调度器的功能进行一些补充，详细说明可以参考官方文档。\n\n#### 3、其他优化策略\n\n- 根据实际资源使用率进行调度：目前默认的调度仅根据 pod 的 request 值进行调度，对于一些资源使用率非常不均衡的场景可以考虑直接以实际的使用率进行调度。\n\n- 等价类划分（Equivalence classes）:典型的用户扩容请求为一次扩容多个容器，因此我 们通过将 pending 队列中的请求划分等价类的方式，实现批处理，显著的降 低 Predicates/Priorities 的次数，这是阿里在今年的 kubeCon 上提出的一个优化方式。\n\n\n\n### 五、kubelet 优化\n\n#### 1、使用 node release 减少心跳上报频率\n\n在大规模场景下，大量 node 的心跳汇报严重影响了 node 的 watch，apiserver 处理心跳请求也需要非常大的开销。而开启 nodeLease 之后，kubelet 会使用非常轻量的 nodeLease 对象 (0.1 KB) 更新请求替换老的 Update Node Status 方式，这会大大减轻 apiserver 的负担。\n\n#### 2、使用 bookmark 机制\n\nkubernetes v1.15 支持 bookmark 机制，bookmark 主要作用是只将特定的事件发送给客户端，从而避免增加 apiserver 的负载。bookmark 的核心思想概括起来就是在 client 与 server 之间保持一个“心跳”， 即使队列中无 client 需要感知的更新，reflector 内部的版本号也需要及时的更新。\n\n比如：每个节点上的 kubelet 仅关注 和自己节点相关的 pods，pod storage 队列是有限的(FIFO)，当 pods 的队列更新时，旧的变更就会从队列中淘汰，当队列中的更新与某个 kubelet client 无关时，kubelet client watch 的 resourceVersion 仍然保持不变，若此时 kubelet client 重连 apiserver 后，这时候 apiserver 无法判断当前队列的最小值与 kubelet client 之间是否存在需要感知的变更，因此返回 client too old version err 触发 kubelet client 重新 list 所有的数据。\n\n#### 3、限制驱逐\n\nkubelet 拥有节点自动修复的能力，例如在发现异常容器或不合规容器后，会对它们进行驱逐删除操作，这对于有些场景来说风险太大。例如当 kubelet 发现当前宿主机上容器个数比设置的最大容器个数大时，会挑选驱逐和删除某些容器，虽然正常情况下不会轻易发生这种问题，但是也需要对此进行控制，降低此类风险，配置 kubelet 的参数 `----eviction-hard=` 来确保在任何情况 kubelet 都不会驱逐容器。\n\n#### 4、原地升级\n\nkubernetes 默认只要 pod 的 spec 信息有改动，例如镜像信息，此时 pod 的 hash 值就会改变，然后会导致 pod 的销毁重建，一个Pod中可能包含了主业务容器，还有不可剥离的依赖业务容器，以及SideCar组件容器等，这在生产环境中代价是很大的，一方面 ip 和 hostname 可能会发生改变，pod 重启也需要一定的时间，另一方面频繁的重建也给集群管理带来了更多的压力，甚至还可能导致无法调度成功。为了解决该问题，就需要支持容器的原地升级。可以开发一个 operator 来实现相关的功能，这种方法需要重新实现一个 resource 对应于 k8s 中的应用，然后当 pod 中的 image 改变后只更新 pod 不重建，kubelet 会重启 container 的，可以参考阿里的 [cafeDeployment](https://github.com/openkruise/kruise)，或者对原生 deployment/statefulset 中的控制器直接进行修改。\n\n\n\n### 六、kube-proxy 优化\n\n#### 1、使用 ipvs 模式\n\n由于 iptables 匹配时延和规则更新时延在大规模集群中呈指数增长，增加以及删除规则非常耗时，所以需要转为 ipvs，ipvs 使用 hash 表，其增加或者删除一条规则几乎不受规则基数的影响。iptables 以及 ipvs 详细的介绍会在后面的文章中介绍。\n\n#### 2、独立部署\n\nkube-proxy 默认与 kubelet 同时部署在一台 node 上，可以将 kube-proxy 组件独立部署在非 k8s node 上，避免在所有 node 上都产生大量 iptables 规则。\n\n\n\n### 七、镜像优化\n\n一个容器的镜像平均 2G 左右，若频繁的拉取镜像可能会将宿主机的带宽打满，甚至影响镜像仓库的使用，\n\n- 1、限制镜像的大小\n\n- 2、镜像缓存\n\n- 3、使用 P2P 进行镜像分发，比如：dragonfly\n\n- 4、基础镜像预加载：一般镜像会分为三层，第一层基础镜像即 os，第二层环境镜像即带有 nginx、tomcat 等服务的镜像，第三层业务镜像也就是带有业务代码的镜像。基础镜像一般不会频繁更新，可在所有宿主机上预先加载，环境镜像可以定时进行加载，业务镜像则实时拉取。\n\n\n\n### 八、客户端优化\n\n在大规模场景下，集群中所有的 daemonset、webhook 以及 operator 等组件非常多，每个客户端都要从 apiserver 中获取资源，此时对 apiserver 的压力非常大，若客户端使用不当很可能导致 apiserver 或者 etcd 崩溃，此时对客户端的行为进行限制就非常有必要了。首先应确保所有客户端都使用 ListWatch 机制而不是只使用 List，并且在使用 ListWatch 机制时尽量不要覆盖 ListOption，即直接从 apiserver 的缓存中获取资源列表，避免请求直接命中 etcd。\n\n`k8s.io/apimachinery/pkg/apis/meta/v1/types.go`：\n\n```\n...\n\n// ListOptions is the query options to a standard REST list call.\ntype ListOptions struct {\n\t\t...\n\n    // When specified with a watch call, shows changes that occur after that particular version of a resource.\n    // Defaults to changes from the beginning of history.\n    // When specified for list:\n    // - if unset, then the result is returned from remote storage based on quorum-read flag;\n    // - if it's 0, then we simply return what we currently have in cache, no guarantee;\n    // - if set to non zero, then the result is at least as fresh as given rv.\n    // +optional\n    ResourceVersion string `json:\"resourceVersion,omitempty\" protobuf:\"bytes,4,opt,name=resourceVersion\"`\n\n    ...\n}\n\n...\n```\n\n\n\n### 九、资源使用率的提升\n\n在大规模场景中，提高资源使用率是非常有必要的，否则会存在严重的资源浪费，资源使用率高即宿主的 cpu 利用率，但是不可能一个宿主上所有容器的资源利用率都非常高，容器和物理机不同，一个服务下容器的平均 cpu idle 一般到 50% 时此服务就该扩容了，但物理机 idle 在 50% 时还是处于稳定运行状态的，而服务一般都会有潮汐现象，所以需要一些其他方法来提高整机的 cpu 使用率。\n\n- 1、pod 分配资源压缩：为 pod 设置 request 和 limit 值，对应的 pod qos 为 burstable。\n\n- 2、宿主资源超卖：比如将一个实际只有 48 核的宿主上报资源给 apiserver 时上报为 60 核，以此来对宿主进行资源超卖。第一种方法就是给宿主机打上特定的资源超卖标签，然后直接修改 kubelet 的代码上报时应用指定的超卖系数，或者使用 admission webhook 在 patch node status 时修改其资源中对应的值，这种方法需要对 kubelet 注册 apiserver 的原理有深入了解。\n\n- 3、在离线业务混部：大部分公司都会做在离线混部，在离线混部需要解决的问题有：\n  - 1、在线能及时抢占离线资源（目前内核不支持）\n  - 2、让离线高效的利用空闲 CPU\n\n​          腾讯云对在离线有深入研究，整机资源使用率已达 90%，可以借鉴其一些设计理念，参考：[腾讯成本优化黑科技：整机CPU利用率最高提升至90%](https://mp.weixin.qq.com/s/dC-G74XtlnRzFTQmZdC19A)。\n\n\n\n### 十、动态调整 Pod 资源限制\n\n> 参考：[超大规模商用 K8s 场景下，阿里巴巴如何动态解决容器资源的按需分配问题？](https://mp.weixin.qq.com/s/013J6iMJPI9ddICKdr485A)\n\n在大规模集群场景，服务可能会因高峰期资源不足导致响应慢等问题，对于某些应用时间内 HPA 或者 VPA 都不是件容易的事情。先说 HPA，我们或许可以秒级拉起了 Pod，创建新的容器，然而拉起的容器是否真的可用呢。从创建到可用，中间要经过调度、分配ip、拉取镜像、同步白名单等，可能需要比较久的时间，对于大促和抢购秒杀 这种访问量“洪峰”可能仅维持几分钟或者十几分钟的实际场景，如果我们等到 HPA 的副本全部可用，可能市场活动早已经结束了。至于社区目前的 VPA 场景，删掉旧 Pod，创建新 Pod，这样的逻辑更难接受。\n\n目前阿里的 policy engine 支持动态调整 pod 的资源限制，底层使用类似 cadvisor 的一个数据采集组件，直接采集 cgroup 数据，然后对容器做画像，当容器资源不足时会瞬时快速修改容器 cgroup 文件目录下的的参数，如果是 cpu 型的，直接调整低优先级容器的 cgroup 下 cpu quota 的值，首先抑制低优先级的容器对于 cpu 的争抢，然后再适当上调高优先级容器的相关资源值。\n\n\n\n### 十一、其他优化方法\n\n1、禁用 `kubectl` 的 `--all` 操作，避免误操作导致某一资源全部被删除\n\n\n### 十二、总结\n\n以上是笔者对 kubernetes 性能优化方法的一些思考及总结，部分方法参考社区的文档。kubernetes 拥有庞大而快速发展的生态系统，以上提及的优化方法仅是冰山一角，性能优化无终点，在生产环境中能发挥价值才是最有用的。\n\n\n\n参考：\n[eBay应用程序集群管理器TESS.IO在大规模集群下的性能优化](https://mp.weixin.qq.com/s/znfLbETcyof-y49Xd3sn9w)\n\n[Meet a Kubernetes Descheduler](https://akomljen.com/meet-a-kubernetes-descheduler/)\n\n[网易云基于Kubernetes的深度定制化实践](https://segmentfault.com/a/1190000011001864)\n\n[开放下载《阿里巴巴云原生实践 15 讲》揭秘九年云原生规模化落地](https://zhuanlan.zhihu.com/p/73125817)\n\n[使用 K8S 几年后，这些技术专家有话要说](https://www.infoq.cn/article/bjFPSbYHwlVrbJlZskRv)\n\n[Kubernetes API 分析](https://ggaaooppeenngg.github.io/zh-CN/2017/11/05/Kubernetes-API-分析/)\n\n[Kubernetes 调度优化--重平衡策略方案整理](https://gist.github.com/ykfq/614daf69702c41aff3c3fc6c1058c5e7)\n\n[探秘金融级云原生发布工作负载 CafeDeployment](https://zhuanlan.zhihu.com/p/69753427)\n\n[腾讯成本优化黑科技：整机CPU利用率最高提升至90%](https://mp.weixin.qq.com/s/dC-G74XtlnRzFTQmZdC19A)\n\n[华为云在 K8S 大规模场景下的 Service 性能优化实践](https://zhuanlan.zhihu.com/p/37230013)\n\n[优化Kubernetes集群负载的技术方案探讨](https://mp.weixin.qq.com/s/MdcSvX33bCi2xROe-NQskg)\n\n[记一次kubernetes集群异常: kubelet连接apiserver超时](https://www.cnblogs.com/gaorong/p/10925480.html)\n\n","source":"_posts/k8s_improvements.md","raw":"---\ntitle: 大规模场景下 kubernetes 集群的性能优化\ndate: 2019-10-12 16:00:30\ntags: [\"kubernetes\",\"improvements\"]\ntype: \"improvements\"\n\n---\n\n### 一、etcd 优化\n\n- 1、etcd 采用本地 ssd 盘作为后端存储存储\n\n- 2、etcd 独立部署在非 k8s node 上\n\n- 3、etcd 快照(snap)与预写式日志(wal)分盘存储\n\netcd 详细的优化操作可以参考上篇文章：[etcd 性能测试与调优](http://blog.tianfeiyu.com/2019/10/08/etcd_improvements/)。\n\n\n\n### 二、apiserver 的优化\n\n#### 1、参数调整\n\n- `--max-mutating-requests-inflight` ：在给定时间内的最大 mutating 请求数，调整 apiserver 的流控 qos，可以调整至 3000，默认为 200\n\n- `--max-requests-inflight`：在给定时间内的最大 non-mutating 请求数，默认 400，可以调整至 1000\n\n- `--watch-cache-sizes`：调大 resources 的 watch size，默认为 100，当集群中 node 以及 pod 数量非常多时可以稍微调大，比如： `--watch-cache-sizes=node#1000,pod#5000`\n\n#### 2、etcd 多实例支持\n\n对于不同 object 进行分库存储，首先应该将数据与状态分离，即将 events 放在单独的 etcd 实例中，在 apiserver 的配置中加上`--etcd-servers-overrides=/events#https://xxx:3379;https://xxx:3379;https://xxx:3379;https://xxxx:3379;https://xxx:3379`，后期可以将 pod、node  等 object 也分离在单独的 etcd 实例中。\n\n#### 3、apiserver 的负载均衡\n\n通常为了保证集群的高可用，集群中一般会有多个 master 节点，kubelet 的连接也会被均分到不同的 apiserver，在 k8s v1.10 以前的版本中，kubelet 使用 HTTP/2，HTTP/2 为了提高网络性能，一个主机只建立一个连接，所有的请求都通过该连接进行，默认情况下，即使网络异常，它还是重用这个连接，直到操作系统将连接关闭，而操作系统关闭僵尸连接的时间默认是十几分钟，所以在 v1.10 以前的版本中 kubelet 连接 apiserver 超时之后不会主动 reset 掉连接进行重试，除非主动重启 kubelet 或者等待十多分钟后其进行重试。\n\n此问题在 v1.10 版本中被修复过（[track/close kubelet->API connections on heartbeat failure #63492](https://github.com/kubernetes/kubernetes/pull/63492)），代码也被 merge 到了 v1.8 和 v1.9，但是该问题并没有完全修复，直到 v1.14 版本才被完全修复（ [kubelet: fix fail to close kubelet->API connections on heartbeat failure #78016](https://github.com/kubernetes/kubernetes/pull/78016)）。\n\n所以为了保证 apiserver 的连接数均衡，请使用 v1.14 及以上版本。\n\n#### 4、使用 pprof 进行性能分析\n\npprof 是 golang 的一大杀器，要想进行源码级别的性能分析，必须使用 pprof。\n\n```\n// 安装相关包\n$ brew install graphviz\n\n// 启动 pprof\n$ go tool pprof http://localhost:8001/debug/pprof/profile\nFile: kube-apiserver\nType: cpu\nTime: Oct 11, 2019 at 11:39am (CST)\nDuration: 30s, Total samples = 620ms ( 2.07%)\nEntering interactive mode (type \"help\" for commands, \"o\" for options)\n(pprof) web   // 使用 web 命令生成 svg 文件\n```\n\n然后打开 svg 文件：\n\n![apiserver](http://cdn.tianfeiyu.com/image-20191011115425979.png)\n\n\n\n可以通过 graph 以及交互式界面得到 cpu 耗时、goroutine 阻塞等信息，apiserver 中的对象比较多，序列化会消耗非常大的时间，golang 标准库的 json 也有很严重的性能问题，开源的 json-iter 相比标准库有不少性能上的提升，但 json-iter 有很多标准库不兼容的问题，此前也有相关的 [issue](https://github.com/kubernetes/kubernetes/pull/54289) 进行反馈但并没有合进主线。\n\n\n\n### 三、kube-controller-manager 的优化\n\n#### 1、参数优化\n\n- 调大 --kube-api-qps 值：可以调整至 100，默认值为 20\n\n- 调大 --kube-api-burst 值：可以调整至 100，默认值为 30\n\n- 禁用不需要的 controller：kubernetes v1.14 中已有 35 个 controller，默认启动为`--controllers`，即启动所有 controller，可以禁用不需要的 controller\n\n- 调整 controller 同步资源的周期：避免过多的资源同步导致集群资源的消耗，所有带有 `--concurrent` 前缀的参数\n\n\n\n#### 2、kube-controller-manager 升级过程 informer 预加载\n\n> 参考自 [阿里巴巴云原生实践 15 讲](https://zhuanlan.zhihu.com/p/73125817)\n\ncontroller-manager 中存储的对象非常多，每次升级过程中从 apiserver 获取这些对象并反序列化的开销是无法忽略的，重启  controller-manager 恢复时可能要花费几分钟才能完成。我们需要尽量的减小 controller-manager 单次升级对系统的中断时间，主要有以下两处改造：\n\n- 预启动备 controller informer，提前加载 controller 需要的数据\n\n- 主 controller 升级时，会主动释放 Leader Lease，触发备立即接管工作\n\n通过此方案 controller-manager 中断时间降低到秒级别(升级时 < 2s)，即使在异常宕机时，备仅需等待 leader lease 的过期(默认 15s)，无需要花费几分 钟重新同步数据。通过这个增强，显著的降低了 controller-manager MTTR（平均恢复时间），同时降低了 controller-manager 恢复时对 apiserver 的性能冲击。\n\n此方案需要对 controller-manager 上面两处的代码进行修改，controller-manager 默认的启动方式是先拿到锁然后 callback run 方法，在 run 方法中会启动 informers 然后同步对象，在停止时也要改为主动释放 leader release。\n\n\n\n### 四、kube-scheduler 优化\n\n在 k8s 核心组件中，调度器的功能做的比较通用，大部分公司都不会局限于当前调度器的功能而进行一系列的改造，例如美团就对 kube-scheduler 进行过一些优化，并将[**预选失败中断机制**](https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html)（详见[PR](https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html)）和[**将全局最优解改为局部最优解**](https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html)（详见[PR1](https://github.com/kubernetes/kubernetes/pull/66733)，[PR2](https://github.com/kubernetes/kubernetes/pull/67555)）等重要 feature 回馈给了社区。\n\n\n\n首先还是使用好调度器的基本功能：\n\n- [Pod/Node Affinity & Anti-affinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity)\n- [Taint & Toleration](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/)\n- [Priority & Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/)\n- [Pod Disruption Budget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)\n\n然后再进行一些必要的优化。\n\n\n\n#### 1、参数优化\n\n调大`--kube-api-qps` 值：可以调整至 100，默认值为 50\n\n#### 2、调度器优化\n\n- 扩展调度器功能：目前可以通过 [scheduler_extender](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md) 很方便的扩展调度器，比如对于 GPU 的调度，可以通过  [scheduler_extender](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md)  + [device-plugins](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/) 来支持。\n\n- [多调度器](https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/)支持：kubernetes 也支持在集群中运行多个调度器调度不同作业，例如可以在 pod 的 `spec.schedulerName` 指定对应的调度器，也可以在 job 的 `.spec.template.spec.schedulerName` 指定调度器。\n\n- 动态调度支持：由于 kubernetes 的默认调度器只在 pod 创建过程中进行一次性调度，后续不会重新去平衡 pod 在集群中的分布，导致实际的资源使用率不均衡，此时集群中会存在部分热点宿主，为了解决默认调度器的功能缺陷，kubernetes 孵化了一个工具 [Descheduler](https://github.com/kubernetes-incubator/descheduler) 来对默认调度器的功能进行一些补充，详细说明可以参考官方文档。\n\n#### 3、其他优化策略\n\n- 根据实际资源使用率进行调度：目前默认的调度仅根据 pod 的 request 值进行调度，对于一些资源使用率非常不均衡的场景可以考虑直接以实际的使用率进行调度。\n\n- 等价类划分（Equivalence classes）:典型的用户扩容请求为一次扩容多个容器，因此我 们通过将 pending 队列中的请求划分等价类的方式，实现批处理，显著的降 低 Predicates/Priorities 的次数，这是阿里在今年的 kubeCon 上提出的一个优化方式。\n\n\n\n### 五、kubelet 优化\n\n#### 1、使用 node release 减少心跳上报频率\n\n在大规模场景下，大量 node 的心跳汇报严重影响了 node 的 watch，apiserver 处理心跳请求也需要非常大的开销。而开启 nodeLease 之后，kubelet 会使用非常轻量的 nodeLease 对象 (0.1 KB) 更新请求替换老的 Update Node Status 方式，这会大大减轻 apiserver 的负担。\n\n#### 2、使用 bookmark 机制\n\nkubernetes v1.15 支持 bookmark 机制，bookmark 主要作用是只将特定的事件发送给客户端，从而避免增加 apiserver 的负载。bookmark 的核心思想概括起来就是在 client 与 server 之间保持一个“心跳”， 即使队列中无 client 需要感知的更新，reflector 内部的版本号也需要及时的更新。\n\n比如：每个节点上的 kubelet 仅关注 和自己节点相关的 pods，pod storage 队列是有限的(FIFO)，当 pods 的队列更新时，旧的变更就会从队列中淘汰，当队列中的更新与某个 kubelet client 无关时，kubelet client watch 的 resourceVersion 仍然保持不变，若此时 kubelet client 重连 apiserver 后，这时候 apiserver 无法判断当前队列的最小值与 kubelet client 之间是否存在需要感知的变更，因此返回 client too old version err 触发 kubelet client 重新 list 所有的数据。\n\n#### 3、限制驱逐\n\nkubelet 拥有节点自动修复的能力，例如在发现异常容器或不合规容器后，会对它们进行驱逐删除操作，这对于有些场景来说风险太大。例如当 kubelet 发现当前宿主机上容器个数比设置的最大容器个数大时，会挑选驱逐和删除某些容器，虽然正常情况下不会轻易发生这种问题，但是也需要对此进行控制，降低此类风险，配置 kubelet 的参数 `----eviction-hard=` 来确保在任何情况 kubelet 都不会驱逐容器。\n\n#### 4、原地升级\n\nkubernetes 默认只要 pod 的 spec 信息有改动，例如镜像信息，此时 pod 的 hash 值就会改变，然后会导致 pod 的销毁重建，一个Pod中可能包含了主业务容器，还有不可剥离的依赖业务容器，以及SideCar组件容器等，这在生产环境中代价是很大的，一方面 ip 和 hostname 可能会发生改变，pod 重启也需要一定的时间，另一方面频繁的重建也给集群管理带来了更多的压力，甚至还可能导致无法调度成功。为了解决该问题，就需要支持容器的原地升级。可以开发一个 operator 来实现相关的功能，这种方法需要重新实现一个 resource 对应于 k8s 中的应用，然后当 pod 中的 image 改变后只更新 pod 不重建，kubelet 会重启 container 的，可以参考阿里的 [cafeDeployment](https://github.com/openkruise/kruise)，或者对原生 deployment/statefulset 中的控制器直接进行修改。\n\n\n\n### 六、kube-proxy 优化\n\n#### 1、使用 ipvs 模式\n\n由于 iptables 匹配时延和规则更新时延在大规模集群中呈指数增长，增加以及删除规则非常耗时，所以需要转为 ipvs，ipvs 使用 hash 表，其增加或者删除一条规则几乎不受规则基数的影响。iptables 以及 ipvs 详细的介绍会在后面的文章中介绍。\n\n#### 2、独立部署\n\nkube-proxy 默认与 kubelet 同时部署在一台 node 上，可以将 kube-proxy 组件独立部署在非 k8s node 上，避免在所有 node 上都产生大量 iptables 规则。\n\n\n\n### 七、镜像优化\n\n一个容器的镜像平均 2G 左右，若频繁的拉取镜像可能会将宿主机的带宽打满，甚至影响镜像仓库的使用，\n\n- 1、限制镜像的大小\n\n- 2、镜像缓存\n\n- 3、使用 P2P 进行镜像分发，比如：dragonfly\n\n- 4、基础镜像预加载：一般镜像会分为三层，第一层基础镜像即 os，第二层环境镜像即带有 nginx、tomcat 等服务的镜像，第三层业务镜像也就是带有业务代码的镜像。基础镜像一般不会频繁更新，可在所有宿主机上预先加载，环境镜像可以定时进行加载，业务镜像则实时拉取。\n\n\n\n### 八、客户端优化\n\n在大规模场景下，集群中所有的 daemonset、webhook 以及 operator 等组件非常多，每个客户端都要从 apiserver 中获取资源，此时对 apiserver 的压力非常大，若客户端使用不当很可能导致 apiserver 或者 etcd 崩溃，此时对客户端的行为进行限制就非常有必要了。首先应确保所有客户端都使用 ListWatch 机制而不是只使用 List，并且在使用 ListWatch 机制时尽量不要覆盖 ListOption，即直接从 apiserver 的缓存中获取资源列表，避免请求直接命中 etcd。\n\n`k8s.io/apimachinery/pkg/apis/meta/v1/types.go`：\n\n```\n...\n\n// ListOptions is the query options to a standard REST list call.\ntype ListOptions struct {\n\t\t...\n\n    // When specified with a watch call, shows changes that occur after that particular version of a resource.\n    // Defaults to changes from the beginning of history.\n    // When specified for list:\n    // - if unset, then the result is returned from remote storage based on quorum-read flag;\n    // - if it's 0, then we simply return what we currently have in cache, no guarantee;\n    // - if set to non zero, then the result is at least as fresh as given rv.\n    // +optional\n    ResourceVersion string `json:\"resourceVersion,omitempty\" protobuf:\"bytes,4,opt,name=resourceVersion\"`\n\n    ...\n}\n\n...\n```\n\n\n\n### 九、资源使用率的提升\n\n在大规模场景中，提高资源使用率是非常有必要的，否则会存在严重的资源浪费，资源使用率高即宿主的 cpu 利用率，但是不可能一个宿主上所有容器的资源利用率都非常高，容器和物理机不同，一个服务下容器的平均 cpu idle 一般到 50% 时此服务就该扩容了，但物理机 idle 在 50% 时还是处于稳定运行状态的，而服务一般都会有潮汐现象，所以需要一些其他方法来提高整机的 cpu 使用率。\n\n- 1、pod 分配资源压缩：为 pod 设置 request 和 limit 值，对应的 pod qos 为 burstable。\n\n- 2、宿主资源超卖：比如将一个实际只有 48 核的宿主上报资源给 apiserver 时上报为 60 核，以此来对宿主进行资源超卖。第一种方法就是给宿主机打上特定的资源超卖标签，然后直接修改 kubelet 的代码上报时应用指定的超卖系数，或者使用 admission webhook 在 patch node status 时修改其资源中对应的值，这种方法需要对 kubelet 注册 apiserver 的原理有深入了解。\n\n- 3、在离线业务混部：大部分公司都会做在离线混部，在离线混部需要解决的问题有：\n  - 1、在线能及时抢占离线资源（目前内核不支持）\n  - 2、让离线高效的利用空闲 CPU\n\n​          腾讯云对在离线有深入研究，整机资源使用率已达 90%，可以借鉴其一些设计理念，参考：[腾讯成本优化黑科技：整机CPU利用率最高提升至90%](https://mp.weixin.qq.com/s/dC-G74XtlnRzFTQmZdC19A)。\n\n\n\n### 十、动态调整 Pod 资源限制\n\n> 参考：[超大规模商用 K8s 场景下，阿里巴巴如何动态解决容器资源的按需分配问题？](https://mp.weixin.qq.com/s/013J6iMJPI9ddICKdr485A)\n\n在大规模集群场景，服务可能会因高峰期资源不足导致响应慢等问题，对于某些应用时间内 HPA 或者 VPA 都不是件容易的事情。先说 HPA，我们或许可以秒级拉起了 Pod，创建新的容器，然而拉起的容器是否真的可用呢。从创建到可用，中间要经过调度、分配ip、拉取镜像、同步白名单等，可能需要比较久的时间，对于大促和抢购秒杀 这种访问量“洪峰”可能仅维持几分钟或者十几分钟的实际场景，如果我们等到 HPA 的副本全部可用，可能市场活动早已经结束了。至于社区目前的 VPA 场景，删掉旧 Pod，创建新 Pod，这样的逻辑更难接受。\n\n目前阿里的 policy engine 支持动态调整 pod 的资源限制，底层使用类似 cadvisor 的一个数据采集组件，直接采集 cgroup 数据，然后对容器做画像，当容器资源不足时会瞬时快速修改容器 cgroup 文件目录下的的参数，如果是 cpu 型的，直接调整低优先级容器的 cgroup 下 cpu quota 的值，首先抑制低优先级的容器对于 cpu 的争抢，然后再适当上调高优先级容器的相关资源值。\n\n\n\n### 十一、其他优化方法\n\n1、禁用 `kubectl` 的 `--all` 操作，避免误操作导致某一资源全部被删除\n\n\n### 十二、总结\n\n以上是笔者对 kubernetes 性能优化方法的一些思考及总结，部分方法参考社区的文档。kubernetes 拥有庞大而快速发展的生态系统，以上提及的优化方法仅是冰山一角，性能优化无终点，在生产环境中能发挥价值才是最有用的。\n\n\n\n参考：\n[eBay应用程序集群管理器TESS.IO在大规模集群下的性能优化](https://mp.weixin.qq.com/s/znfLbETcyof-y49Xd3sn9w)\n\n[Meet a Kubernetes Descheduler](https://akomljen.com/meet-a-kubernetes-descheduler/)\n\n[网易云基于Kubernetes的深度定制化实践](https://segmentfault.com/a/1190000011001864)\n\n[开放下载《阿里巴巴云原生实践 15 讲》揭秘九年云原生规模化落地](https://zhuanlan.zhihu.com/p/73125817)\n\n[使用 K8S 几年后，这些技术专家有话要说](https://www.infoq.cn/article/bjFPSbYHwlVrbJlZskRv)\n\n[Kubernetes API 分析](https://ggaaooppeenngg.github.io/zh-CN/2017/11/05/Kubernetes-API-分析/)\n\n[Kubernetes 调度优化--重平衡策略方案整理](https://gist.github.com/ykfq/614daf69702c41aff3c3fc6c1058c5e7)\n\n[探秘金融级云原生发布工作负载 CafeDeployment](https://zhuanlan.zhihu.com/p/69753427)\n\n[腾讯成本优化黑科技：整机CPU利用率最高提升至90%](https://mp.weixin.qq.com/s/dC-G74XtlnRzFTQmZdC19A)\n\n[华为云在 K8S 大规模场景下的 Service 性能优化实践](https://zhuanlan.zhihu.com/p/37230013)\n\n[优化Kubernetes集群负载的技术方案探讨](https://mp.weixin.qq.com/s/MdcSvX33bCi2xROe-NQskg)\n\n[记一次kubernetes集群异常: kubelet连接apiserver超时](https://www.cnblogs.com/gaorong/p/10925480.html)\n\n","slug":"k8s_improvements","published":1,"updated":"2019-10-12T09:00:53.369Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59e000kapwnp91sazgv","content":"<h3 id=\"一、etcd-优化\"><a href=\"#一、etcd-优化\" class=\"headerlink\" title=\"一、etcd 优化\"></a>一、etcd 优化</h3><ul>\n<li><p>1、etcd 采用本地 ssd 盘作为后端存储存储</p>\n</li>\n<li><p>2、etcd 独立部署在非 k8s node 上</p>\n</li>\n<li><p>3、etcd 快照(snap)与预写式日志(wal)分盘存储</p>\n</li>\n</ul>\n<p>etcd 详细的优化操作可以参考上篇文章：<a href=\"http://blog.tianfeiyu.com/2019/10/08/etcd_improvements/\" target=\"_blank\" rel=\"noopener\">etcd 性能测试与调优</a>。</p>\n<h3 id=\"二、apiserver-的优化\"><a href=\"#二、apiserver-的优化\" class=\"headerlink\" title=\"二、apiserver 的优化\"></a>二、apiserver 的优化</h3><h4 id=\"1、参数调整\"><a href=\"#1、参数调整\" class=\"headerlink\" title=\"1、参数调整\"></a>1、参数调整</h4><ul>\n<li><p><code>--max-mutating-requests-inflight</code> ：在给定时间内的最大 mutating 请求数，调整 apiserver 的流控 qos，可以调整至 3000，默认为 200</p>\n</li>\n<li><p><code>--max-requests-inflight</code>：在给定时间内的最大 non-mutating 请求数，默认 400，可以调整至 1000</p>\n</li>\n<li><p><code>--watch-cache-sizes</code>：调大 resources 的 watch size，默认为 100，当集群中 node 以及 pod 数量非常多时可以稍微调大，比如： <code>--watch-cache-sizes=node#1000,pod#5000</code></p>\n</li>\n</ul>\n<h4 id=\"2、etcd-多实例支持\"><a href=\"#2、etcd-多实例支持\" class=\"headerlink\" title=\"2、etcd 多实例支持\"></a>2、etcd 多实例支持</h4><p>对于不同 object 进行分库存储，首先应该将数据与状态分离，即将 events 放在单独的 etcd 实例中，在 apiserver 的配置中加上<code>--etcd-servers-overrides=/events#https://xxx:3379;https://xxx:3379;https://xxx:3379;https://xxxx:3379;https://xxx:3379</code>，后期可以将 pod、node  等 object 也分离在单独的 etcd 实例中。</p>\n<h4 id=\"3、apiserver-的负载均衡\"><a href=\"#3、apiserver-的负载均衡\" class=\"headerlink\" title=\"3、apiserver 的负载均衡\"></a>3、apiserver 的负载均衡</h4><p>通常为了保证集群的高可用，集群中一般会有多个 master 节点，kubelet 的连接也会被均分到不同的 apiserver，在 k8s v1.10 以前的版本中，kubelet 使用 HTTP/2，HTTP/2 为了提高网络性能，一个主机只建立一个连接，所有的请求都通过该连接进行，默认情况下，即使网络异常，它还是重用这个连接，直到操作系统将连接关闭，而操作系统关闭僵尸连接的时间默认是十几分钟，所以在 v1.10 以前的版本中 kubelet 连接 apiserver 超时之后不会主动 reset 掉连接进行重试，除非主动重启 kubelet 或者等待十多分钟后其进行重试。</p>\n<p>此问题在 v1.10 版本中被修复过（<a href=\"https://github.com/kubernetes/kubernetes/pull/63492\" target=\"_blank\" rel=\"noopener\">track/close kubelet-&gt;API connections on heartbeat failure #63492</a>），代码也被 merge 到了 v1.8 和 v1.9，但是该问题并没有完全修复，直到 v1.14 版本才被完全修复（ <a href=\"https://github.com/kubernetes/kubernetes/pull/78016\" target=\"_blank\" rel=\"noopener\">kubelet: fix fail to close kubelet-&gt;API connections on heartbeat failure #78016</a>）。</p>\n<p>所以为了保证 apiserver 的连接数均衡，请使用 v1.14 及以上版本。</p>\n<h4 id=\"4、使用-pprof-进行性能分析\"><a href=\"#4、使用-pprof-进行性能分析\" class=\"headerlink\" title=\"4、使用 pprof 进行性能分析\"></a>4、使用 pprof 进行性能分析</h4><p>pprof 是 golang 的一大杀器，要想进行源码级别的性能分析，必须使用 pprof。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 安装相关包</span><br><span class=\"line\">$ brew install graphviz</span><br><span class=\"line\"></span><br><span class=\"line\">// 启动 pprof</span><br><span class=\"line\">$ go tool pprof http://localhost:8001/debug/pprof/profile</span><br><span class=\"line\">File: kube-apiserver</span><br><span class=\"line\">Type: cpu</span><br><span class=\"line\">Time: Oct 11, 2019 at 11:39am (CST)</span><br><span class=\"line\">Duration: 30s, Total samples = 620ms ( 2.07%)</span><br><span class=\"line\">Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)</span><br><span class=\"line\">(pprof) web   // 使用 web 命令生成 svg 文件</span><br></pre></td></tr></table></figure>\n<p>然后打开 svg 文件：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20191011115425979.png\" alt=\"apiserver\"></p>\n<p>可以通过 graph 以及交互式界面得到 cpu 耗时、goroutine 阻塞等信息，apiserver 中的对象比较多，序列化会消耗非常大的时间，golang 标准库的 json 也有很严重的性能问题，开源的 json-iter 相比标准库有不少性能上的提升，但 json-iter 有很多标准库不兼容的问题，此前也有相关的 <a href=\"https://github.com/kubernetes/kubernetes/pull/54289\" target=\"_blank\" rel=\"noopener\">issue</a> 进行反馈但并没有合进主线。</p>\n<h3 id=\"三、kube-controller-manager-的优化\"><a href=\"#三、kube-controller-manager-的优化\" class=\"headerlink\" title=\"三、kube-controller-manager 的优化\"></a>三、kube-controller-manager 的优化</h3><h4 id=\"1、参数优化\"><a href=\"#1、参数优化\" class=\"headerlink\" title=\"1、参数优化\"></a>1、参数优化</h4><ul>\n<li><p>调大 –kube-api-qps 值：可以调整至 100，默认值为 20</p>\n</li>\n<li><p>调大 –kube-api-burst 值：可以调整至 100，默认值为 30</p>\n</li>\n<li><p>禁用不需要的 controller：kubernetes v1.14 中已有 35 个 controller，默认启动为<code>--controllers</code>，即启动所有 controller，可以禁用不需要的 controller</p>\n</li>\n<li><p>调整 controller 同步资源的周期：避免过多的资源同步导致集群资源的消耗，所有带有 <code>--concurrent</code> 前缀的参数</p>\n</li>\n</ul>\n<h4 id=\"2、kube-controller-manager-升级过程-informer-预加载\"><a href=\"#2、kube-controller-manager-升级过程-informer-预加载\" class=\"headerlink\" title=\"2、kube-controller-manager 升级过程 informer 预加载\"></a>2、kube-controller-manager 升级过程 informer 预加载</h4><blockquote>\n<p>参考自 <a href=\"https://zhuanlan.zhihu.com/p/73125817\" target=\"_blank\" rel=\"noopener\">阿里巴巴云原生实践 15 讲</a></p>\n</blockquote>\n<p>controller-manager 中存储的对象非常多，每次升级过程中从 apiserver 获取这些对象并反序列化的开销是无法忽略的，重启  controller-manager 恢复时可能要花费几分钟才能完成。我们需要尽量的减小 controller-manager 单次升级对系统的中断时间，主要有以下两处改造：</p>\n<ul>\n<li><p>预启动备 controller informer，提前加载 controller 需要的数据</p>\n</li>\n<li><p>主 controller 升级时，会主动释放 Leader Lease，触发备立即接管工作</p>\n</li>\n</ul>\n<p>通过此方案 controller-manager 中断时间降低到秒级别(升级时 &lt; 2s)，即使在异常宕机时，备仅需等待 leader lease 的过期(默认 15s)，无需要花费几分 钟重新同步数据。通过这个增强，显著的降低了 controller-manager MTTR（平均恢复时间），同时降低了 controller-manager 恢复时对 apiserver 的性能冲击。</p>\n<p>此方案需要对 controller-manager 上面两处的代码进行修改，controller-manager 默认的启动方式是先拿到锁然后 callback run 方法，在 run 方法中会启动 informers 然后同步对象，在停止时也要改为主动释放 leader release。</p>\n<h3 id=\"四、kube-scheduler-优化\"><a href=\"#四、kube-scheduler-优化\" class=\"headerlink\" title=\"四、kube-scheduler 优化\"></a>四、kube-scheduler 优化</h3><p>在 k8s 核心组件中，调度器的功能做的比较通用，大部分公司都不会局限于当前调度器的功能而进行一系列的改造，例如美团就对 kube-scheduler 进行过一些优化，并将<a href=\"https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html\" target=\"_blank\" rel=\"noopener\"><strong>预选失败中断机制</strong></a>（详见<a href=\"https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html\" target=\"_blank\" rel=\"noopener\">PR</a>）和<a href=\"https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html\" target=\"_blank\" rel=\"noopener\"><strong>将全局最优解改为局部最优解</strong></a>（详见<a href=\"https://github.com/kubernetes/kubernetes/pull/66733\" target=\"_blank\" rel=\"noopener\">PR1</a>，<a href=\"https://github.com/kubernetes/kubernetes/pull/67555\" target=\"_blank\" rel=\"noopener\">PR2</a>）等重要 feature 回馈给了社区。</p>\n<p>首先还是使用好调度器的基本功能：</p>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\" target=\"_blank\" rel=\"noopener\">Pod/Node Affinity &amp; Anti-affinity</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\" target=\"_blank\" rel=\"noopener\">Taint &amp; Toleration</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\" target=\"_blank\" rel=\"noopener\">Priority &amp; Preemption</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\" target=\"_blank\" rel=\"noopener\">Pod Disruption Budget</a></li>\n</ul>\n<p>然后再进行一些必要的优化。</p>\n<h4 id=\"1、参数优化-1\"><a href=\"#1、参数优化-1\" class=\"headerlink\" title=\"1、参数优化\"></a>1、参数优化</h4><p>调大<code>--kube-api-qps</code> 值：可以调整至 100，默认值为 50</p>\n<h4 id=\"2、调度器优化\"><a href=\"#2、调度器优化\" class=\"headerlink\" title=\"2、调度器优化\"></a>2、调度器优化</h4><ul>\n<li><p>扩展调度器功能：目前可以通过 <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md\" target=\"_blank\" rel=\"noopener\">scheduler_extender</a> 很方便的扩展调度器，比如对于 GPU 的调度，可以通过  <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md\" target=\"_blank\" rel=\"noopener\">scheduler_extender</a>  + <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\" target=\"_blank\" rel=\"noopener\">device-plugins</a> 来支持。</p>\n</li>\n<li><p><a href=\"https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\" target=\"_blank\" rel=\"noopener\">多调度器</a>支持：kubernetes 也支持在集群中运行多个调度器调度不同作业，例如可以在 pod 的 <code>spec.schedulerName</code> 指定对应的调度器，也可以在 job 的 <code>.spec.template.spec.schedulerName</code> 指定调度器。</p>\n</li>\n<li><p>动态调度支持：由于 kubernetes 的默认调度器只在 pod 创建过程中进行一次性调度，后续不会重新去平衡 pod 在集群中的分布，导致实际的资源使用率不均衡，此时集群中会存在部分热点宿主，为了解决默认调度器的功能缺陷，kubernetes 孵化了一个工具 <a href=\"https://github.com/kubernetes-incubator/descheduler\" target=\"_blank\" rel=\"noopener\">Descheduler</a> 来对默认调度器的功能进行一些补充，详细说明可以参考官方文档。</p>\n</li>\n</ul>\n<h4 id=\"3、其他优化策略\"><a href=\"#3、其他优化策略\" class=\"headerlink\" title=\"3、其他优化策略\"></a>3、其他优化策略</h4><ul>\n<li><p>根据实际资源使用率进行调度：目前默认的调度仅根据 pod 的 request 值进行调度，对于一些资源使用率非常不均衡的场景可以考虑直接以实际的使用率进行调度。</p>\n</li>\n<li><p>等价类划分（Equivalence classes）:典型的用户扩容请求为一次扩容多个容器，因此我 们通过将 pending 队列中的请求划分等价类的方式，实现批处理，显著的降 低 Predicates/Priorities 的次数，这是阿里在今年的 kubeCon 上提出的一个优化方式。</p>\n</li>\n</ul>\n<h3 id=\"五、kubelet-优化\"><a href=\"#五、kubelet-优化\" class=\"headerlink\" title=\"五、kubelet 优化\"></a>五、kubelet 优化</h3><h4 id=\"1、使用-node-release-减少心跳上报频率\"><a href=\"#1、使用-node-release-减少心跳上报频率\" class=\"headerlink\" title=\"1、使用 node release 减少心跳上报频率\"></a>1、使用 node release 减少心跳上报频率</h4><p>在大规模场景下，大量 node 的心跳汇报严重影响了 node 的 watch，apiserver 处理心跳请求也需要非常大的开销。而开启 nodeLease 之后，kubelet 会使用非常轻量的 nodeLease 对象 (0.1 KB) 更新请求替换老的 Update Node Status 方式，这会大大减轻 apiserver 的负担。</p>\n<h4 id=\"2、使用-bookmark-机制\"><a href=\"#2、使用-bookmark-机制\" class=\"headerlink\" title=\"2、使用 bookmark 机制\"></a>2、使用 bookmark 机制</h4><p>kubernetes v1.15 支持 bookmark 机制，bookmark 主要作用是只将特定的事件发送给客户端，从而避免增加 apiserver 的负载。bookmark 的核心思想概括起来就是在 client 与 server 之间保持一个“心跳”， 即使队列中无 client 需要感知的更新，reflector 内部的版本号也需要及时的更新。</p>\n<p>比如：每个节点上的 kubelet 仅关注 和自己节点相关的 pods，pod storage 队列是有限的(FIFO)，当 pods 的队列更新时，旧的变更就会从队列中淘汰，当队列中的更新与某个 kubelet client 无关时，kubelet client watch 的 resourceVersion 仍然保持不变，若此时 kubelet client 重连 apiserver 后，这时候 apiserver 无法判断当前队列的最小值与 kubelet client 之间是否存在需要感知的变更，因此返回 client too old version err 触发 kubelet client 重新 list 所有的数据。</p>\n<h4 id=\"3、限制驱逐\"><a href=\"#3、限制驱逐\" class=\"headerlink\" title=\"3、限制驱逐\"></a>3、限制驱逐</h4><p>kubelet 拥有节点自动修复的能力，例如在发现异常容器或不合规容器后，会对它们进行驱逐删除操作，这对于有些场景来说风险太大。例如当 kubelet 发现当前宿主机上容器个数比设置的最大容器个数大时，会挑选驱逐和删除某些容器，虽然正常情况下不会轻易发生这种问题，但是也需要对此进行控制，降低此类风险，配置 kubelet 的参数 <code>----eviction-hard=</code> 来确保在任何情况 kubelet 都不会驱逐容器。</p>\n<h4 id=\"4、原地升级\"><a href=\"#4、原地升级\" class=\"headerlink\" title=\"4、原地升级\"></a>4、原地升级</h4><p>kubernetes 默认只要 pod 的 spec 信息有改动，例如镜像信息，此时 pod 的 hash 值就会改变，然后会导致 pod 的销毁重建，一个Pod中可能包含了主业务容器，还有不可剥离的依赖业务容器，以及SideCar组件容器等，这在生产环境中代价是很大的，一方面 ip 和 hostname 可能会发生改变，pod 重启也需要一定的时间，另一方面频繁的重建也给集群管理带来了更多的压力，甚至还可能导致无法调度成功。为了解决该问题，就需要支持容器的原地升级。可以开发一个 operator 来实现相关的功能，这种方法需要重新实现一个 resource 对应于 k8s 中的应用，然后当 pod 中的 image 改变后只更新 pod 不重建，kubelet 会重启 container 的，可以参考阿里的 <a href=\"https://github.com/openkruise/kruise\" target=\"_blank\" rel=\"noopener\">cafeDeployment</a>，或者对原生 deployment/statefulset 中的控制器直接进行修改。</p>\n<h3 id=\"六、kube-proxy-优化\"><a href=\"#六、kube-proxy-优化\" class=\"headerlink\" title=\"六、kube-proxy 优化\"></a>六、kube-proxy 优化</h3><h4 id=\"1、使用-ipvs-模式\"><a href=\"#1、使用-ipvs-模式\" class=\"headerlink\" title=\"1、使用 ipvs 模式\"></a>1、使用 ipvs 模式</h4><p>由于 iptables 匹配时延和规则更新时延在大规模集群中呈指数增长，增加以及删除规则非常耗时，所以需要转为 ipvs，ipvs 使用 hash 表，其增加或者删除一条规则几乎不受规则基数的影响。iptables 以及 ipvs 详细的介绍会在后面的文章中介绍。</p>\n<h4 id=\"2、独立部署\"><a href=\"#2、独立部署\" class=\"headerlink\" title=\"2、独立部署\"></a>2、独立部署</h4><p>kube-proxy 默认与 kubelet 同时部署在一台 node 上，可以将 kube-proxy 组件独立部署在非 k8s node 上，避免在所有 node 上都产生大量 iptables 规则。</p>\n<h3 id=\"七、镜像优化\"><a href=\"#七、镜像优化\" class=\"headerlink\" title=\"七、镜像优化\"></a>七、镜像优化</h3><p>一个容器的镜像平均 2G 左右，若频繁的拉取镜像可能会将宿主机的带宽打满，甚至影响镜像仓库的使用，</p>\n<ul>\n<li><p>1、限制镜像的大小</p>\n</li>\n<li><p>2、镜像缓存</p>\n</li>\n<li><p>3、使用 P2P 进行镜像分发，比如：dragonfly</p>\n</li>\n<li><p>4、基础镜像预加载：一般镜像会分为三层，第一层基础镜像即 os，第二层环境镜像即带有 nginx、tomcat 等服务的镜像，第三层业务镜像也就是带有业务代码的镜像。基础镜像一般不会频繁更新，可在所有宿主机上预先加载，环境镜像可以定时进行加载，业务镜像则实时拉取。</p>\n</li>\n</ul>\n<h3 id=\"八、客户端优化\"><a href=\"#八、客户端优化\" class=\"headerlink\" title=\"八、客户端优化\"></a>八、客户端优化</h3><p>在大规模场景下，集群中所有的 daemonset、webhook 以及 operator 等组件非常多，每个客户端都要从 apiserver 中获取资源，此时对 apiserver 的压力非常大，若客户端使用不当很可能导致 apiserver 或者 etcd 崩溃，此时对客户端的行为进行限制就非常有必要了。首先应确保所有客户端都使用 ListWatch 机制而不是只使用 List，并且在使用 ListWatch 机制时尽量不要覆盖 ListOption，即直接从 apiserver 的缓存中获取资源列表，避免请求直接命中 etcd。</p>\n<p><code>k8s.io/apimachinery/pkg/apis/meta/v1/types.go</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">// ListOptions is the query options to a standard REST list call.</span><br><span class=\"line\">type ListOptions struct &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">    // When specified with a watch call, shows changes that occur after that particular version of a resource.</span><br><span class=\"line\">    // Defaults to changes from the beginning of history.</span><br><span class=\"line\">    // When specified for list:</span><br><span class=\"line\">    // - if unset, then the result is returned from remote storage based on quorum-read flag;</span><br><span class=\"line\">    // - if it&apos;s 0, then we simply return what we currently have in cache, no guarantee;</span><br><span class=\"line\">    // - if set to non zero, then the result is at least as fresh as given rv.</span><br><span class=\"line\">    // +optional</span><br><span class=\"line\">    ResourceVersion string `json:&quot;resourceVersion,omitempty&quot; protobuf:&quot;bytes,4,opt,name=resourceVersion&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<h3 id=\"九、资源使用率的提升\"><a href=\"#九、资源使用率的提升\" class=\"headerlink\" title=\"九、资源使用率的提升\"></a>九、资源使用率的提升</h3><p>在大规模场景中，提高资源使用率是非常有必要的，否则会存在严重的资源浪费，资源使用率高即宿主的 cpu 利用率，但是不可能一个宿主上所有容器的资源利用率都非常高，容器和物理机不同，一个服务下容器的平均 cpu idle 一般到 50% 时此服务就该扩容了，但物理机 idle 在 50% 时还是处于稳定运行状态的，而服务一般都会有潮汐现象，所以需要一些其他方法来提高整机的 cpu 使用率。</p>\n<ul>\n<li><p>1、pod 分配资源压缩：为 pod 设置 request 和 limit 值，对应的 pod qos 为 burstable。</p>\n</li>\n<li><p>2、宿主资源超卖：比如将一个实际只有 48 核的宿主上报资源给 apiserver 时上报为 60 核，以此来对宿主进行资源超卖。第一种方法就是给宿主机打上特定的资源超卖标签，然后直接修改 kubelet 的代码上报时应用指定的超卖系数，或者使用 admission webhook 在 patch node status 时修改其资源中对应的值，这种方法需要对 kubelet 注册 apiserver 的原理有深入了解。</p>\n</li>\n<li><p>3、在离线业务混部：大部分公司都会做在离线混部，在离线混部需要解决的问题有：</p>\n<ul>\n<li>1、在线能及时抢占离线资源（目前内核不支持）</li>\n<li>2、让离线高效的利用空闲 CPU</li>\n</ul>\n</li>\n</ul>\n<p>​          腾讯云对在离线有深入研究，整机资源使用率已达 90%，可以借鉴其一些设计理念，参考：<a href=\"https://mp.weixin.qq.com/s/dC-G74XtlnRzFTQmZdC19A\" target=\"_blank\" rel=\"noopener\">腾讯成本优化黑科技：整机CPU利用率最高提升至90%</a>。</p>\n<h3 id=\"十、动态调整-Pod-资源限制\"><a href=\"#十、动态调整-Pod-资源限制\" class=\"headerlink\" title=\"十、动态调整 Pod 资源限制\"></a>十、动态调整 Pod 资源限制</h3><blockquote>\n<p>参考：<a href=\"https://mp.weixin.qq.com/s/013J6iMJPI9ddICKdr485A\" target=\"_blank\" rel=\"noopener\">超大规模商用 K8s 场景下，阿里巴巴如何动态解决容器资源的按需分配问题？</a></p>\n</blockquote>\n<p>在大规模集群场景，服务可能会因高峰期资源不足导致响应慢等问题，对于某些应用时间内 HPA 或者 VPA 都不是件容易的事情。先说 HPA，我们或许可以秒级拉起了 Pod，创建新的容器，然而拉起的容器是否真的可用呢。从创建到可用，中间要经过调度、分配ip、拉取镜像、同步白名单等，可能需要比较久的时间，对于大促和抢购秒杀 这种访问量“洪峰”可能仅维持几分钟或者十几分钟的实际场景，如果我们等到 HPA 的副本全部可用，可能市场活动早已经结束了。至于社区目前的 VPA 场景，删掉旧 Pod，创建新 Pod，这样的逻辑更难接受。</p>\n<p>目前阿里的 policy engine 支持动态调整 pod 的资源限制，底层使用类似 cadvisor 的一个数据采集组件，直接采集 cgroup 数据，然后对容器做画像，当容器资源不足时会瞬时快速修改容器 cgroup 文件目录下的的参数，如果是 cpu 型的，直接调整低优先级容器的 cgroup 下 cpu quota 的值，首先抑制低优先级的容器对于 cpu 的争抢，然后再适当上调高优先级容器的相关资源值。</p>\n<h3 id=\"十一、其他优化方法\"><a href=\"#十一、其他优化方法\" class=\"headerlink\" title=\"十一、其他优化方法\"></a>十一、其他优化方法</h3><p>1、禁用 <code>kubectl</code> 的 <code>--all</code> 操作，避免误操作导致某一资源全部被删除</p>\n<h3 id=\"十二、总结\"><a href=\"#十二、总结\" class=\"headerlink\" title=\"十二、总结\"></a>十二、总结</h3><p>以上是笔者对 kubernetes 性能优化方法的一些思考及总结，部分方法参考社区的文档。kubernetes 拥有庞大而快速发展的生态系统，以上提及的优化方法仅是冰山一角，性能优化无终点，在生产环境中能发挥价值才是最有用的。</p>\n<p>参考：<br><a href=\"https://mp.weixin.qq.com/s/znfLbETcyof-y49Xd3sn9w\" target=\"_blank\" rel=\"noopener\">eBay应用程序集群管理器TESS.IO在大规模集群下的性能优化</a></p>\n<p><a href=\"https://akomljen.com/meet-a-kubernetes-descheduler/\" target=\"_blank\" rel=\"noopener\">Meet a Kubernetes Descheduler</a></p>\n<p><a href=\"https://segmentfault.com/a/1190000011001864\" target=\"_blank\" rel=\"noopener\">网易云基于Kubernetes的深度定制化实践</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/73125817\" target=\"_blank\" rel=\"noopener\">开放下载《阿里巴巴云原生实践 15 讲》揭秘九年云原生规模化落地</a></p>\n<p><a href=\"https://www.infoq.cn/article/bjFPSbYHwlVrbJlZskRv\" target=\"_blank\" rel=\"noopener\">使用 K8S 几年后，这些技术专家有话要说</a></p>\n<p><a href=\"https://ggaaooppeenngg.github.io/zh-CN/2017/11/05/Kubernetes-API-分析/\" target=\"_blank\" rel=\"noopener\">Kubernetes API 分析</a></p>\n<p><a href=\"https://gist.github.com/ykfq/614daf69702c41aff3c3fc6c1058c5e7\" target=\"_blank\" rel=\"noopener\">Kubernetes 调度优化–重平衡策略方案整理</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/69753427\" target=\"_blank\" rel=\"noopener\">探秘金融级云原生发布工作负载 CafeDeployment</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/dC-G74XtlnRzFTQmZdC19A\" target=\"_blank\" rel=\"noopener\">腾讯成本优化黑科技：整机CPU利用率最高提升至90%</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/37230013\" target=\"_blank\" rel=\"noopener\">华为云在 K8S 大规模场景下的 Service 性能优化实践</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/MdcSvX33bCi2xROe-NQskg\" target=\"_blank\" rel=\"noopener\">优化Kubernetes集群负载的技术方案探讨</a></p>\n<p><a href=\"https://www.cnblogs.com/gaorong/p/10925480.html\" target=\"_blank\" rel=\"noopener\">记一次kubernetes集群异常: kubelet连接apiserver超时</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<h3 id=\"一、etcd-优化\"><a href=\"#一、etcd-优化\" class=\"headerlink\" title=\"一、etcd 优化\"></a>一、etcd 优化</h3><ul>\n<li><p>1、etcd 采用本地 ssd 盘作为后端存储存储</p>\n</li>\n<li><p>2、etcd 独立部署在非 k8s node 上</p>\n</li>\n<li><p>3、etcd 快照(snap)与预写式日志(wal)分盘存储</p>\n</li>\n</ul>\n<p>etcd 详细的优化操作可以参考上篇文章：<a href=\"http://blog.tianfeiyu.com/2019/10/08/etcd_improvements/\" target=\"_blank\" rel=\"noopener\">etcd 性能测试与调优</a>。</p>\n<h3 id=\"二、apiserver-的优化\"><a href=\"#二、apiserver-的优化\" class=\"headerlink\" title=\"二、apiserver 的优化\"></a>二、apiserver 的优化</h3><h4 id=\"1、参数调整\"><a href=\"#1、参数调整\" class=\"headerlink\" title=\"1、参数调整\"></a>1、参数调整</h4><ul>\n<li><p><code>--max-mutating-requests-inflight</code> ：在给定时间内的最大 mutating 请求数，调整 apiserver 的流控 qos，可以调整至 3000，默认为 200</p>\n</li>\n<li><p><code>--max-requests-inflight</code>：在给定时间内的最大 non-mutating 请求数，默认 400，可以调整至 1000</p>\n</li>\n<li><p><code>--watch-cache-sizes</code>：调大 resources 的 watch size，默认为 100，当集群中 node 以及 pod 数量非常多时可以稍微调大，比如： <code>--watch-cache-sizes=node#1000,pod#5000</code></p>\n</li>\n</ul>\n<h4 id=\"2、etcd-多实例支持\"><a href=\"#2、etcd-多实例支持\" class=\"headerlink\" title=\"2、etcd 多实例支持\"></a>2、etcd 多实例支持</h4><p>对于不同 object 进行分库存储，首先应该将数据与状态分离，即将 events 放在单独的 etcd 实例中，在 apiserver 的配置中加上<code>--etcd-servers-overrides=/events#https://xxx:3379;https://xxx:3379;https://xxx:3379;https://xxxx:3379;https://xxx:3379</code>，后期可以将 pod、node  等 object 也分离在单独的 etcd 实例中。</p>\n<h4 id=\"3、apiserver-的负载均衡\"><a href=\"#3、apiserver-的负载均衡\" class=\"headerlink\" title=\"3、apiserver 的负载均衡\"></a>3、apiserver 的负载均衡</h4><p>通常为了保证集群的高可用，集群中一般会有多个 master 节点，kubelet 的连接也会被均分到不同的 apiserver，在 k8s v1.10 以前的版本中，kubelet 使用 HTTP/2，HTTP/2 为了提高网络性能，一个主机只建立一个连接，所有的请求都通过该连接进行，默认情况下，即使网络异常，它还是重用这个连接，直到操作系统将连接关闭，而操作系统关闭僵尸连接的时间默认是十几分钟，所以在 v1.10 以前的版本中 kubelet 连接 apiserver 超时之后不会主动 reset 掉连接进行重试，除非主动重启 kubelet 或者等待十多分钟后其进行重试。</p>\n<p>此问题在 v1.10 版本中被修复过（<a href=\"https://github.com/kubernetes/kubernetes/pull/63492\" target=\"_blank\" rel=\"noopener\">track/close kubelet-&gt;API connections on heartbeat failure #63492</a>），代码也被 merge 到了 v1.8 和 v1.9，但是该问题并没有完全修复，直到 v1.14 版本才被完全修复（ <a href=\"https://github.com/kubernetes/kubernetes/pull/78016\" target=\"_blank\" rel=\"noopener\">kubelet: fix fail to close kubelet-&gt;API connections on heartbeat failure #78016</a>）。</p>\n<p>所以为了保证 apiserver 的连接数均衡，请使用 v1.14 及以上版本。</p>\n<h4 id=\"4、使用-pprof-进行性能分析\"><a href=\"#4、使用-pprof-进行性能分析\" class=\"headerlink\" title=\"4、使用 pprof 进行性能分析\"></a>4、使用 pprof 进行性能分析</h4><p>pprof 是 golang 的一大杀器，要想进行源码级别的性能分析，必须使用 pprof。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 安装相关包</span><br><span class=\"line\">$ brew install graphviz</span><br><span class=\"line\"></span><br><span class=\"line\">// 启动 pprof</span><br><span class=\"line\">$ go tool pprof http://localhost:8001/debug/pprof/profile</span><br><span class=\"line\">File: kube-apiserver</span><br><span class=\"line\">Type: cpu</span><br><span class=\"line\">Time: Oct 11, 2019 at 11:39am (CST)</span><br><span class=\"line\">Duration: 30s, Total samples = 620ms ( 2.07%)</span><br><span class=\"line\">Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)</span><br><span class=\"line\">(pprof) web   // 使用 web 命令生成 svg 文件</span><br></pre></td></tr></table></figure>\n<p>然后打开 svg 文件：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20191011115425979.png\" alt=\"apiserver\"></p>\n<p>可以通过 graph 以及交互式界面得到 cpu 耗时、goroutine 阻塞等信息，apiserver 中的对象比较多，序列化会消耗非常大的时间，golang 标准库的 json 也有很严重的性能问题，开源的 json-iter 相比标准库有不少性能上的提升，但 json-iter 有很多标准库不兼容的问题，此前也有相关的 <a href=\"https://github.com/kubernetes/kubernetes/pull/54289\" target=\"_blank\" rel=\"noopener\">issue</a> 进行反馈但并没有合进主线。</p>\n<h3 id=\"三、kube-controller-manager-的优化\"><a href=\"#三、kube-controller-manager-的优化\" class=\"headerlink\" title=\"三、kube-controller-manager 的优化\"></a>三、kube-controller-manager 的优化</h3><h4 id=\"1、参数优化\"><a href=\"#1、参数优化\" class=\"headerlink\" title=\"1、参数优化\"></a>1、参数优化</h4><ul>\n<li><p>调大 –kube-api-qps 值：可以调整至 100，默认值为 20</p>\n</li>\n<li><p>调大 –kube-api-burst 值：可以调整至 100，默认值为 30</p>\n</li>\n<li><p>禁用不需要的 controller：kubernetes v1.14 中已有 35 个 controller，默认启动为<code>--controllers</code>，即启动所有 controller，可以禁用不需要的 controller</p>\n</li>\n<li><p>调整 controller 同步资源的周期：避免过多的资源同步导致集群资源的消耗，所有带有 <code>--concurrent</code> 前缀的参数</p>\n</li>\n</ul>\n<h4 id=\"2、kube-controller-manager-升级过程-informer-预加载\"><a href=\"#2、kube-controller-manager-升级过程-informer-预加载\" class=\"headerlink\" title=\"2、kube-controller-manager 升级过程 informer 预加载\"></a>2、kube-controller-manager 升级过程 informer 预加载</h4><blockquote>\n<p>参考自 <a href=\"https://zhuanlan.zhihu.com/p/73125817\" target=\"_blank\" rel=\"noopener\">阿里巴巴云原生实践 15 讲</a></p>\n</blockquote>\n<p>controller-manager 中存储的对象非常多，每次升级过程中从 apiserver 获取这些对象并反序列化的开销是无法忽略的，重启  controller-manager 恢复时可能要花费几分钟才能完成。我们需要尽量的减小 controller-manager 单次升级对系统的中断时间，主要有以下两处改造：</p>\n<ul>\n<li><p>预启动备 controller informer，提前加载 controller 需要的数据</p>\n</li>\n<li><p>主 controller 升级时，会主动释放 Leader Lease，触发备立即接管工作</p>\n</li>\n</ul>\n<p>通过此方案 controller-manager 中断时间降低到秒级别(升级时 &lt; 2s)，即使在异常宕机时，备仅需等待 leader lease 的过期(默认 15s)，无需要花费几分 钟重新同步数据。通过这个增强，显著的降低了 controller-manager MTTR（平均恢复时间），同时降低了 controller-manager 恢复时对 apiserver 的性能冲击。</p>\n<p>此方案需要对 controller-manager 上面两处的代码进行修改，controller-manager 默认的启动方式是先拿到锁然后 callback run 方法，在 run 方法中会启动 informers 然后同步对象，在停止时也要改为主动释放 leader release。</p>\n<h3 id=\"四、kube-scheduler-优化\"><a href=\"#四、kube-scheduler-优化\" class=\"headerlink\" title=\"四、kube-scheduler 优化\"></a>四、kube-scheduler 优化</h3><p>在 k8s 核心组件中，调度器的功能做的比较通用，大部分公司都不会局限于当前调度器的功能而进行一系列的改造，例如美团就对 kube-scheduler 进行过一些优化，并将<a href=\"https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html\" target=\"_blank\" rel=\"noopener\"><strong>预选失败中断机制</strong></a>（详见<a href=\"https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html\" target=\"_blank\" rel=\"noopener\">PR</a>）和<a href=\"https://tech.meituan.com/2019/08/22/kubernetes-cluster-management-practice.html\" target=\"_blank\" rel=\"noopener\"><strong>将全局最优解改为局部最优解</strong></a>（详见<a href=\"https://github.com/kubernetes/kubernetes/pull/66733\" target=\"_blank\" rel=\"noopener\">PR1</a>，<a href=\"https://github.com/kubernetes/kubernetes/pull/67555\" target=\"_blank\" rel=\"noopener\">PR2</a>）等重要 feature 回馈给了社区。</p>\n<p>首先还是使用好调度器的基本功能：</p>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\" target=\"_blank\" rel=\"noopener\">Pod/Node Affinity &amp; Anti-affinity</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\" target=\"_blank\" rel=\"noopener\">Taint &amp; Toleration</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\" target=\"_blank\" rel=\"noopener\">Priority &amp; Preemption</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\" target=\"_blank\" rel=\"noopener\">Pod Disruption Budget</a></li>\n</ul>\n<p>然后再进行一些必要的优化。</p>\n<h4 id=\"1、参数优化-1\"><a href=\"#1、参数优化-1\" class=\"headerlink\" title=\"1、参数优化\"></a>1、参数优化</h4><p>调大<code>--kube-api-qps</code> 值：可以调整至 100，默认值为 50</p>\n<h4 id=\"2、调度器优化\"><a href=\"#2、调度器优化\" class=\"headerlink\" title=\"2、调度器优化\"></a>2、调度器优化</h4><ul>\n<li><p>扩展调度器功能：目前可以通过 <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md\" target=\"_blank\" rel=\"noopener\">scheduler_extender</a> 很方便的扩展调度器，比如对于 GPU 的调度，可以通过  <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/scheduler_extender.md\" target=\"_blank\" rel=\"noopener\">scheduler_extender</a>  + <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/\" target=\"_blank\" rel=\"noopener\">device-plugins</a> 来支持。</p>\n</li>\n<li><p><a href=\"https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\" target=\"_blank\" rel=\"noopener\">多调度器</a>支持：kubernetes 也支持在集群中运行多个调度器调度不同作业，例如可以在 pod 的 <code>spec.schedulerName</code> 指定对应的调度器，也可以在 job 的 <code>.spec.template.spec.schedulerName</code> 指定调度器。</p>\n</li>\n<li><p>动态调度支持：由于 kubernetes 的默认调度器只在 pod 创建过程中进行一次性调度，后续不会重新去平衡 pod 在集群中的分布，导致实际的资源使用率不均衡，此时集群中会存在部分热点宿主，为了解决默认调度器的功能缺陷，kubernetes 孵化了一个工具 <a href=\"https://github.com/kubernetes-incubator/descheduler\" target=\"_blank\" rel=\"noopener\">Descheduler</a> 来对默认调度器的功能进行一些补充，详细说明可以参考官方文档。</p>\n</li>\n</ul>\n<h4 id=\"3、其他优化策略\"><a href=\"#3、其他优化策略\" class=\"headerlink\" title=\"3、其他优化策略\"></a>3、其他优化策略</h4><ul>\n<li><p>根据实际资源使用率进行调度：目前默认的调度仅根据 pod 的 request 值进行调度，对于一些资源使用率非常不均衡的场景可以考虑直接以实际的使用率进行调度。</p>\n</li>\n<li><p>等价类划分（Equivalence classes）:典型的用户扩容请求为一次扩容多个容器，因此我 们通过将 pending 队列中的请求划分等价类的方式，实现批处理，显著的降 低 Predicates/Priorities 的次数，这是阿里在今年的 kubeCon 上提出的一个优化方式。</p>\n</li>\n</ul>\n<h3 id=\"五、kubelet-优化\"><a href=\"#五、kubelet-优化\" class=\"headerlink\" title=\"五、kubelet 优化\"></a>五、kubelet 优化</h3><h4 id=\"1、使用-node-release-减少心跳上报频率\"><a href=\"#1、使用-node-release-减少心跳上报频率\" class=\"headerlink\" title=\"1、使用 node release 减少心跳上报频率\"></a>1、使用 node release 减少心跳上报频率</h4><p>在大规模场景下，大量 node 的心跳汇报严重影响了 node 的 watch，apiserver 处理心跳请求也需要非常大的开销。而开启 nodeLease 之后，kubelet 会使用非常轻量的 nodeLease 对象 (0.1 KB) 更新请求替换老的 Update Node Status 方式，这会大大减轻 apiserver 的负担。</p>\n<h4 id=\"2、使用-bookmark-机制\"><a href=\"#2、使用-bookmark-机制\" class=\"headerlink\" title=\"2、使用 bookmark 机制\"></a>2、使用 bookmark 机制</h4><p>kubernetes v1.15 支持 bookmark 机制，bookmark 主要作用是只将特定的事件发送给客户端，从而避免增加 apiserver 的负载。bookmark 的核心思想概括起来就是在 client 与 server 之间保持一个“心跳”， 即使队列中无 client 需要感知的更新，reflector 内部的版本号也需要及时的更新。</p>\n<p>比如：每个节点上的 kubelet 仅关注 和自己节点相关的 pods，pod storage 队列是有限的(FIFO)，当 pods 的队列更新时，旧的变更就会从队列中淘汰，当队列中的更新与某个 kubelet client 无关时，kubelet client watch 的 resourceVersion 仍然保持不变，若此时 kubelet client 重连 apiserver 后，这时候 apiserver 无法判断当前队列的最小值与 kubelet client 之间是否存在需要感知的变更，因此返回 client too old version err 触发 kubelet client 重新 list 所有的数据。</p>\n<h4 id=\"3、限制驱逐\"><a href=\"#3、限制驱逐\" class=\"headerlink\" title=\"3、限制驱逐\"></a>3、限制驱逐</h4><p>kubelet 拥有节点自动修复的能力，例如在发现异常容器或不合规容器后，会对它们进行驱逐删除操作，这对于有些场景来说风险太大。例如当 kubelet 发现当前宿主机上容器个数比设置的最大容器个数大时，会挑选驱逐和删除某些容器，虽然正常情况下不会轻易发生这种问题，但是也需要对此进行控制，降低此类风险，配置 kubelet 的参数 <code>----eviction-hard=</code> 来确保在任何情况 kubelet 都不会驱逐容器。</p>\n<h4 id=\"4、原地升级\"><a href=\"#4、原地升级\" class=\"headerlink\" title=\"4、原地升级\"></a>4、原地升级</h4><p>kubernetes 默认只要 pod 的 spec 信息有改动，例如镜像信息，此时 pod 的 hash 值就会改变，然后会导致 pod 的销毁重建，一个Pod中可能包含了主业务容器，还有不可剥离的依赖业务容器，以及SideCar组件容器等，这在生产环境中代价是很大的，一方面 ip 和 hostname 可能会发生改变，pod 重启也需要一定的时间，另一方面频繁的重建也给集群管理带来了更多的压力，甚至还可能导致无法调度成功。为了解决该问题，就需要支持容器的原地升级。可以开发一个 operator 来实现相关的功能，这种方法需要重新实现一个 resource 对应于 k8s 中的应用，然后当 pod 中的 image 改变后只更新 pod 不重建，kubelet 会重启 container 的，可以参考阿里的 <a href=\"https://github.com/openkruise/kruise\" target=\"_blank\" rel=\"noopener\">cafeDeployment</a>，或者对原生 deployment/statefulset 中的控制器直接进行修改。</p>\n<h3 id=\"六、kube-proxy-优化\"><a href=\"#六、kube-proxy-优化\" class=\"headerlink\" title=\"六、kube-proxy 优化\"></a>六、kube-proxy 优化</h3><h4 id=\"1、使用-ipvs-模式\"><a href=\"#1、使用-ipvs-模式\" class=\"headerlink\" title=\"1、使用 ipvs 模式\"></a>1、使用 ipvs 模式</h4><p>由于 iptables 匹配时延和规则更新时延在大规模集群中呈指数增长，增加以及删除规则非常耗时，所以需要转为 ipvs，ipvs 使用 hash 表，其增加或者删除一条规则几乎不受规则基数的影响。iptables 以及 ipvs 详细的介绍会在后面的文章中介绍。</p>\n<h4 id=\"2、独立部署\"><a href=\"#2、独立部署\" class=\"headerlink\" title=\"2、独立部署\"></a>2、独立部署</h4><p>kube-proxy 默认与 kubelet 同时部署在一台 node 上，可以将 kube-proxy 组件独立部署在非 k8s node 上，避免在所有 node 上都产生大量 iptables 规则。</p>\n<h3 id=\"七、镜像优化\"><a href=\"#七、镜像优化\" class=\"headerlink\" title=\"七、镜像优化\"></a>七、镜像优化</h3><p>一个容器的镜像平均 2G 左右，若频繁的拉取镜像可能会将宿主机的带宽打满，甚至影响镜像仓库的使用，</p>\n<ul>\n<li><p>1、限制镜像的大小</p>\n</li>\n<li><p>2、镜像缓存</p>\n</li>\n<li><p>3、使用 P2P 进行镜像分发，比如：dragonfly</p>\n</li>\n<li><p>4、基础镜像预加载：一般镜像会分为三层，第一层基础镜像即 os，第二层环境镜像即带有 nginx、tomcat 等服务的镜像，第三层业务镜像也就是带有业务代码的镜像。基础镜像一般不会频繁更新，可在所有宿主机上预先加载，环境镜像可以定时进行加载，业务镜像则实时拉取。</p>\n</li>\n</ul>\n<h3 id=\"八、客户端优化\"><a href=\"#八、客户端优化\" class=\"headerlink\" title=\"八、客户端优化\"></a>八、客户端优化</h3><p>在大规模场景下，集群中所有的 daemonset、webhook 以及 operator 等组件非常多，每个客户端都要从 apiserver 中获取资源，此时对 apiserver 的压力非常大，若客户端使用不当很可能导致 apiserver 或者 etcd 崩溃，此时对客户端的行为进行限制就非常有必要了。首先应确保所有客户端都使用 ListWatch 机制而不是只使用 List，并且在使用 ListWatch 机制时尽量不要覆盖 ListOption，即直接从 apiserver 的缓存中获取资源列表，避免请求直接命中 etcd。</p>\n<p><code>k8s.io/apimachinery/pkg/apis/meta/v1/types.go</code>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\">// ListOptions is the query options to a standard REST list call.</span><br><span class=\"line\">type ListOptions struct &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\"></span><br><span class=\"line\">    // When specified with a watch call, shows changes that occur after that particular version of a resource.</span><br><span class=\"line\">    // Defaults to changes from the beginning of history.</span><br><span class=\"line\">    // When specified for list:</span><br><span class=\"line\">    // - if unset, then the result is returned from remote storage based on quorum-read flag;</span><br><span class=\"line\">    // - if it&apos;s 0, then we simply return what we currently have in cache, no guarantee;</span><br><span class=\"line\">    // - if set to non zero, then the result is at least as fresh as given rv.</span><br><span class=\"line\">    // +optional</span><br><span class=\"line\">    ResourceVersion string `json:&quot;resourceVersion,omitempty&quot; protobuf:&quot;bytes,4,opt,name=resourceVersion&quot;`</span><br><span class=\"line\"></span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<h3 id=\"九、资源使用率的提升\"><a href=\"#九、资源使用率的提升\" class=\"headerlink\" title=\"九、资源使用率的提升\"></a>九、资源使用率的提升</h3><p>在大规模场景中，提高资源使用率是非常有必要的，否则会存在严重的资源浪费，资源使用率高即宿主的 cpu 利用率，但是不可能一个宿主上所有容器的资源利用率都非常高，容器和物理机不同，一个服务下容器的平均 cpu idle 一般到 50% 时此服务就该扩容了，但物理机 idle 在 50% 时还是处于稳定运行状态的，而服务一般都会有潮汐现象，所以需要一些其他方法来提高整机的 cpu 使用率。</p>\n<ul>\n<li><p>1、pod 分配资源压缩：为 pod 设置 request 和 limit 值，对应的 pod qos 为 burstable。</p>\n</li>\n<li><p>2、宿主资源超卖：比如将一个实际只有 48 核的宿主上报资源给 apiserver 时上报为 60 核，以此来对宿主进行资源超卖。第一种方法就是给宿主机打上特定的资源超卖标签，然后直接修改 kubelet 的代码上报时应用指定的超卖系数，或者使用 admission webhook 在 patch node status 时修改其资源中对应的值，这种方法需要对 kubelet 注册 apiserver 的原理有深入了解。</p>\n</li>\n<li><p>3、在离线业务混部：大部分公司都会做在离线混部，在离线混部需要解决的问题有：</p>\n<ul>\n<li>1、在线能及时抢占离线资源（目前内核不支持）</li>\n<li>2、让离线高效的利用空闲 CPU</li>\n</ul>\n</li>\n</ul>\n<p>​          腾讯云对在离线有深入研究，整机资源使用率已达 90%，可以借鉴其一些设计理念，参考：<a href=\"https://mp.weixin.qq.com/s/dC-G74XtlnRzFTQmZdC19A\" target=\"_blank\" rel=\"noopener\">腾讯成本优化黑科技：整机CPU利用率最高提升至90%</a>。</p>\n<h3 id=\"十、动态调整-Pod-资源限制\"><a href=\"#十、动态调整-Pod-资源限制\" class=\"headerlink\" title=\"十、动态调整 Pod 资源限制\"></a>十、动态调整 Pod 资源限制</h3><blockquote>\n<p>参考：<a href=\"https://mp.weixin.qq.com/s/013J6iMJPI9ddICKdr485A\" target=\"_blank\" rel=\"noopener\">超大规模商用 K8s 场景下，阿里巴巴如何动态解决容器资源的按需分配问题？</a></p>\n</blockquote>\n<p>在大规模集群场景，服务可能会因高峰期资源不足导致响应慢等问题，对于某些应用时间内 HPA 或者 VPA 都不是件容易的事情。先说 HPA，我们或许可以秒级拉起了 Pod，创建新的容器，然而拉起的容器是否真的可用呢。从创建到可用，中间要经过调度、分配ip、拉取镜像、同步白名单等，可能需要比较久的时间，对于大促和抢购秒杀 这种访问量“洪峰”可能仅维持几分钟或者十几分钟的实际场景，如果我们等到 HPA 的副本全部可用，可能市场活动早已经结束了。至于社区目前的 VPA 场景，删掉旧 Pod，创建新 Pod，这样的逻辑更难接受。</p>\n<p>目前阿里的 policy engine 支持动态调整 pod 的资源限制，底层使用类似 cadvisor 的一个数据采集组件，直接采集 cgroup 数据，然后对容器做画像，当容器资源不足时会瞬时快速修改容器 cgroup 文件目录下的的参数，如果是 cpu 型的，直接调整低优先级容器的 cgroup 下 cpu quota 的值，首先抑制低优先级的容器对于 cpu 的争抢，然后再适当上调高优先级容器的相关资源值。</p>\n<h3 id=\"十一、其他优化方法\"><a href=\"#十一、其他优化方法\" class=\"headerlink\" title=\"十一、其他优化方法\"></a>十一、其他优化方法</h3><p>1、禁用 <code>kubectl</code> 的 <code>--all</code> 操作，避免误操作导致某一资源全部被删除</p>\n<h3 id=\"十二、总结\"><a href=\"#十二、总结\" class=\"headerlink\" title=\"十二、总结\"></a>十二、总结</h3><p>以上是笔者对 kubernetes 性能优化方法的一些思考及总结，部分方法参考社区的文档。kubernetes 拥有庞大而快速发展的生态系统，以上提及的优化方法仅是冰山一角，性能优化无终点，在生产环境中能发挥价值才是最有用的。</p>\n<p>参考：<br><a href=\"https://mp.weixin.qq.com/s/znfLbETcyof-y49Xd3sn9w\" target=\"_blank\" rel=\"noopener\">eBay应用程序集群管理器TESS.IO在大规模集群下的性能优化</a></p>\n<p><a href=\"https://akomljen.com/meet-a-kubernetes-descheduler/\" target=\"_blank\" rel=\"noopener\">Meet a Kubernetes Descheduler</a></p>\n<p><a href=\"https://segmentfault.com/a/1190000011001864\" target=\"_blank\" rel=\"noopener\">网易云基于Kubernetes的深度定制化实践</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/73125817\" target=\"_blank\" rel=\"noopener\">开放下载《阿里巴巴云原生实践 15 讲》揭秘九年云原生规模化落地</a></p>\n<p><a href=\"https://www.infoq.cn/article/bjFPSbYHwlVrbJlZskRv\" target=\"_blank\" rel=\"noopener\">使用 K8S 几年后，这些技术专家有话要说</a></p>\n<p><a href=\"https://ggaaooppeenngg.github.io/zh-CN/2017/11/05/Kubernetes-API-分析/\" target=\"_blank\" rel=\"noopener\">Kubernetes API 分析</a></p>\n<p><a href=\"https://gist.github.com/ykfq/614daf69702c41aff3c3fc6c1058c5e7\" target=\"_blank\" rel=\"noopener\">Kubernetes 调度优化–重平衡策略方案整理</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/69753427\" target=\"_blank\" rel=\"noopener\">探秘金融级云原生发布工作负载 CafeDeployment</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/dC-G74XtlnRzFTQmZdC19A\" target=\"_blank\" rel=\"noopener\">腾讯成本优化黑科技：整机CPU利用率最高提升至90%</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/37230013\" target=\"_blank\" rel=\"noopener\">华为云在 K8S 大规模场景下的 Service 性能优化实践</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/MdcSvX33bCi2xROe-NQskg\" target=\"_blank\" rel=\"noopener\">优化Kubernetes集群负载的技术方案探讨</a></p>\n<p><a href=\"https://www.cnblogs.com/gaorong/p/10925480.html\" target=\"_blank\" rel=\"noopener\">记一次kubernetes集群异常: kubelet连接apiserver超时</a></p>\n"},{"title":"kubernets 中事件处理机制","date":"2019-02-26T12:49:30.000Z","type":"k8s-events","_content":"\n当集群中的 node 或 pod 异常时，大部分用户会使用 kubectl 查看对应的 events，那么 events 是从何而来的？其实 k8s 中的各个组件会将运行时产生的各种事件汇报到 apiserver，对于 k8s 中的可描述资源，使用 kubectl describe 都可以看到其相关的 events，那 k8s 中又有哪几个组件都上报 events 呢？ \n\n只要在 `k8s.io/kubernetes/cmd` 目录下暴力搜索一下就能知道哪些组件会产生 events：\n```\n$ grep -R -n -i \"EventRecorder\" .\n```\n\n可以看出，controller-manage、kube-proxy、kube-scheduler、kubelet 都使用了 EventRecorder，本文只讲述 kubelet 中对 Events 的使用。\n\n\n\n##### 1、Events 的定义\n\nevents 在 `k8s.io/api/core/v1/types.go` 中进行定义,结构体如下所示：\n\n```\ntype Event struct {\n    metav1.TypeMeta `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata\" protobuf:\"bytes,1,opt,name=metadata\"`\n    InvolvedObject ObjectReference `json:\"involvedObject\" protobuf:\"bytes,2,opt,name=involvedObject\"`\n    Reason string `json:\"reason,omitempty\" protobuf:\"bytes,3,opt,name=reason\"`\n    Message string `json:\"message,omitempty\" protobuf:\"bytes,4,opt,name=message\"`\n    Source EventSource `json:\"source,omitempty\" protobuf:\"bytes,5,opt,name=source\"`\n    FirstTimestamp metav1.Time `json:\"firstTimestamp,omitempty\" protobuf:\"bytes,6,opt,name=firstTimestamp\"`\n    LastTimestamp metav1.Time `json:\"lastTimestamp,omitempty\" protobuf:\"bytes,7,opt,name=lastTimestamp\"`\n    Count int32 `json:\"count,omitempty\" protobuf:\"varint,8,opt,name=count\"`\n    Type string `json:\"type,omitempty\" protobuf:\"bytes,9,opt,name=type\"`\n    EventTime metav1.MicroTime `json:\"eventTime,omitempty\" protobuf:\"bytes,10,opt,name=eventTime\"`\n    Series *EventSeries `json:\"series,omitempty\" protobuf:\"bytes,11,opt,name=series\"`\n    Action string `json:\"action,omitempty\" protobuf:\"bytes,12,opt,name=action\"`\n    Related *ObjectReference `json:\"related,omitempty\" protobuf:\"bytes,13,opt,name=related\"`\n    ReportingController string `json:\"reportingComponent\" protobuf:\"bytes,14,opt,name=reportingComponent\"`\n    ReportingInstance string `json:\"reportingInstance\" protobuf:\"bytes,15,opt,name=reportingInstance\"`\n    ReportingInstance string `json:\"reportingInstance\" protobuf:\"bytes,15,opt,name=reportingInstance\"`\n}\n```\n\n其中 InvolvedObject 代表和事件关联的对象，source 代表事件源，使用 kubectl 看到的事件一般包含 Type、Reason、Age、From、Message 几个字段。\n\nk8s 中 events 目前只有两种类型：\"Normal\" 和 \"Warning\"：\n\n![events 的两种类型](http://cdn.tianfeiyu.com/events.png)\n\n\n##### 2、EventBroadcaster 的初始化\n\nevents 的整个生命周期都与 EventBroadcaster 有关，kubelet 中对 EventBroadcaster 的初始化在`k8s.io/kubernetes/cmd/kubelet/app/server.go`中：\n\n\n```\nfunc RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error {\n  ...\n  // event 初始化\n  makeEventRecorder(kubeDeps, nodeName)\n  ...\n}\n\n\nfunc makeEventRecorder(kubeDeps *kubelet.Dependencies, nodeName types.NodeName) {\n  if kubeDeps.Recorder != nil {\n    return\n  }\n  // 初始化 EventBroadcaster \n  eventBroadcaster := record.NewBroadcaster()\n  // 初始化 EventRecorder\n  kubeDeps.Recorder = eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource{Component: componentKubelet, Host: string(nodeName)})\n  // 记录 events 到本地日志\n  eventBroadcaster.StartLogging(glog.V(3).Infof)\n  if kubeDeps.EventClient != nil {\n    glog.V(4).Infof(\"Sending events to api server.\")\n    // 上报 events 到 apiserver\n  eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: kubeDeps.EventClient.Events(\"\")})\n  } else {\n    glog.Warning(\"No api server defined - no events will be sent to API server.\")\n  }\n}\n```\n\nKubelet 在启动的时候会初始化一个 EventBroadcaster，它主要是对接收到的 events 做一些后续的处理(保存、上报等），EventBroadcaster 也会被 kubelet 中的其他模块使用，以下是相关的定义，对 events 生成和处理的函数都定义在 `k8s.io/client-go/tools/record/event.go` 中：\n\n```\ntype eventBroadcasterImpl struct {\n  *watch.Broadcaster\n  sleepDuration time.Duration\n}\n\n// EventBroadcaster knows how to receive events and send them to any EventSink, watcher, or log.\ntype EventBroadcaster interface {\n  StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface\n\n  StartRecordingToSink(sink EventSink) watch.Interface\n\n  StartLogging(logf func(format string, args ...interface{})) watch.Interface\n\n  NewRecorder(scheme *runtime.Scheme, source v1.EventSource) EventRecorder\n}\n```\n\nEventBroadcaster 是个接口类型，该接口有以下四个方法：\n- StartEventWatcher() ： EventBroadcaster 中的核心方法，接收各模块产生的 events，参数为一个处理 events 的函数，用户可以使用 StartEventWatcher() 接收 events 然后使用自定义的 handle 进行处理\n- StartRecordingToSink() ： 调用 StartEventWatcher() 接收 events，并将收到的 events 发送到 apiserver \n- StartLogging() ：也是调用 StartEventWatcher() 接收 events，然后保存 events 到日志\n- NewRecorder() ：会创建一个指定 EventSource 的 EventRecorder，EventSource 指明了哪个节点的哪个组件\n\n\neventBroadcasterImpl 是 eventBroadcaster 实际的对象，初始化 EventBroadcaster 对象的时候会初始化一个 Broadcaster，Broadcaster 会启动一个 goroutine 接收各组件产生的 events 并广播到每一个 watcher。\n\n```\nfunc NewBroadcaster() EventBroadcaster {\n  return &eventBroadcasterImpl{watch.NewBroadcaster(maxQueuedEvents, watch.DropIfChannelFull), defaultSleepDuration}\n}\n```\n\n可以看到，kubelet 在初始化完 EventBroadcaster 后会调用 StartRecordingToSink() 和 StartLogging() 两个方法，StartRecordingToSink() 处理函数会将收到的 events 进行缓存、过滤、聚合而后发送到 apiserver，StartLogging() 仅将 events 保存到 kubelet 的日志中。\n\n##### 3、Events 的生成\n\n从初始化 EventBroadcaster 的代码中可以看到 kubelet 在初始化完 EventBroadcaster 后紧接着初始化了 EventRecorder，并将已经初始化的 Broadcaster 对象作为参数传给了 EventRecorder，至此，EventBroadcaster、EventRecorder、Broadcaster 三个对象产生了关联。EventRecorder 的主要功能是生成指定格式的 events，以下是相关的定义：\n\n```\ntype recorderImpl struct {\n  scheme *runtime.Scheme\n  source v1.EventSource\n  *watch.Broadcaster\n  clock clock.Clock\n}\n\ntype EventRecorder interface {\n  Event(object runtime.Object, eventtype, reason, message string)\n\n  Eventf(object runtime.Object, eventtype, reason, messageFmt string, args ...interface{})\n\n  PastEventf(object runtime.Object, timestamp metav1.Time, eventtype, reason, messageFmt string, args ...interface{})\n\n  AnnotatedEventf(object runtime.Object, annotations map[string]string, eventtype, reason, messageFmt string, args ...interface{})\n}\n```\n\nEventRecorder 中包含的几个方法都是产生指定格式的 events，Event() 和 Eventf() 的功能类似 fmt.Println() 和 fmt.Printf()，kubelet 中的各个模块会调用 EventRecorder 生成 events。recorderImpl 是 EventRecorder 实际的对象。EventRecorder 的每个方法会调用 generateEvent，在 generateEvent 中初始化 events 。\n\n以下是生成 events 的函数：\n\n```\nfunc (recorder *recorderImpl) generateEvent(object runtime.Object, annotations map[string]string, timestamp metav1.Time, eventtype, reason, message string) {\n  ref, err := ref.GetReference(recorder.scheme, object)\n  if err != nil {\n    glog.Errorf(\"Could not construct reference to: '%#v' due to: '%v'. Will not report event: '%v' '%v' '%v'\", object, err, eventtype, reason, message)\n    return\n  }\n\n  if !validateEventType(eventtype) {\n    glog.Errorf(\"Unsupported event type: '%v'\", eventtype)\n    return\n  }\n\n  event := recorder.makeEvent(ref, annotations, eventtype, reason, message)\n  event.Source = recorder.source\n\n  go func() {\n    // NOTE: events should be a non-blocking operation\n    defer utilruntime.HandleCrash()\n    // 发送事件\n    recorder.Action(watch.Added, event)\n  }()\n}\n\nfunc (recorder *recorderImpl) makeEvent(ref *v1.ObjectReference, annotations map[string]string, eventtype, reason, message string) *v1.Event {\n  t := metav1.Time{Time: recorder.clock.Now()}\n  namespace := ref.Namespace\n  if namespace == \"\" {\n    namespace = metav1.NamespaceDefault\n  }\n  return &v1.Event{\n    ObjectMeta: metav1.ObjectMeta{\n      Name:        fmt.Sprintf(\"%v.%x\", ref.Name, t.UnixNano()),\n      Namespace:   namespace,\n      Annotations: annotations,\n    },\n    InvolvedObject: *ref,\n    Reason:         reason,\n    Message:        message,\n    FirstTimestamp: t,\n    LastTimestamp:  t,\n    Count:          1,\n    Type:           eventtype,\n  }\n}\n```\n初始化完 events 后会调用 recorder.Action() 将 events 发送到 Broadcaster 的事件接收队列中, Action() 是 Broadcaster 中的方法。\n\n以下是 Action() 方法的实现：\n\n```\nfunc (m *Broadcaster) Action(action EventType, obj runtime.Object) {\n  m.incoming <- Event{action, obj}\n}\n```\n\n##### 4、Events 的广播\n\n上面已经说了，EventBroadcaster 初始化时会初始化一个 Broadcaster，Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 的实现在 `k8s.io/apimachinery/pkg/watch/mux.go ` 中，Broadcaster 初始化完成后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发送过来的 events，Broadcaster 中有一个 map 会保存每一个注册的 watcher， 接着将 events 广播给所有的 watcher，每个 watcher 都有一个接收消息的 channel，watcher 可以通过它的 ResultChan() 方法从 channel 中读取数据进行消费。\n\n\n以下是 Broadcaster 广播 events 的实现：\n```\nfunc (m *Broadcaster) loop() {\n  for event := range m.incoming {\n    if event.Type == internalRunFunctionMarker {\n      event.Object.(functionFakeRuntimeObject)()\n      continue\n    }\n    m.distribute(event)\n  }\n  m.closeAll()\n  m.distributing.Done()\n}\n\n// distribute sends event to all watchers. Blocking.\nfunc (m *Broadcaster) distribute(event Event) {\n  m.lock.Lock()\n  defer m.lock.Unlock()\n  if m.fullChannelBehavior == DropIfChannelFull {\n    for _, w := range m.watchers {\n      select {\n      case w.result <- event:\n      case <-w.stopped:\n      default: // Don't block if the event can't be queued.\n      }\n    }\n  } else {\n    for _, w := range m.watchers {\n      select {\n      case w.result <- event:\n      case <-w.stopped:\n      }\n    }\n  }\n}\n```\n\n\n##### 5、Events 的处理\n\n那么 watcher 是从何而来呢？每一个要处理 events 的 client 都需要初始化一个 watcher，处理 events 的方法是在 EventBroadcaster 中定义的，以下是 EventBroadcaster 中对 events 处理的三个函数：\n\n```\nfunc (eventBroadcaster *eventBroadcasterImpl) StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface {\n  watcher := eventBroadcaster.Watch()\n  go func() {\n    defer utilruntime.HandleCrash()\n    for watchEvent := range watcher.ResultChan() {\n      event, ok := watchEvent.Object.(*v1.Event)\n      if !ok {\n        // This is all local, so there's no reason this should\n        // ever happen.\n        continue\n      }\n      eventHandler(event)\n    }\n  }()\n  return watcher\n}\n```\n\nStartEventWatcher() 首先实例化一个 watcher，每个 watcher 都会被塞入到 Broadcaster 的 watcher 列表中，watcher 从 Broadcaster 提供的 channel 中读取 events，然后再调用 eventHandler 进行处理，StartLogging() 和 StartRecordingToSink() 都是对 StartEventWatcher() 的封装，都会传入自己的处理函数。\n\n\n\n```\nfunc (eventBroadcaster *eventBroadcasterImpl) StartLogging(logf func(format string, args ...interface{})) watch.Interface {\n  return eventBroadcaster.StartEventWatcher(\n    func(e *v1.Event) {\n      logf(\"Event(%#v): type: '%v' reason: '%v' %v\", e.InvolvedObject, e.Type, e.Reason, e.Message)\n    })\n}\n```\n\nStartLogging() 传入的 eventHandler 仅将 events 保存到日志中。\n\n```\nfunc (eventBroadcaster *eventBroadcasterImpl) StartRecordingToSink(sink EventSink) watch.Interface {\n  // The default math/rand package functions aren't thread safe, so create a\n  // new Rand object for each StartRecording call.\n  randGen := rand.New(rand.NewSource(time.Now().UnixNano()))\n  eventCorrelator := NewEventCorrelator(clock.RealClock{})\n  return eventBroadcaster.StartEventWatcher(\n    func(event *v1.Event) {\n      recordToSink(sink, event, eventCorrelator, randGen, eventBroadcaster.sleepDuration)\n    })\n}\n\nfunc recordToSink(sink EventSink, event *v1.Event, eventCorrelator *EventCorrelator, randGen *rand.Rand, sleepDuration time.Duration) {\n  eventCopy := *event\n  event = &eventCopy\n  result, err := eventCorrelator.EventCorrelate(event)\n  if err != nil {\n    utilruntime.HandleError(err)\n  }\n  if result.Skip {\n    return\n  }\n  tries := 0\n  for {\n    if recordEvent(sink, result.Event, result.Patch, result.Event.Count > 1, eventCorrelator) {\n      break\n    }\n    tries++\n    if tries >= maxTriesPerEvent {\n      glog.Errorf(\"Unable to write event '%#v' (retry limit exceeded!)\", event)\n      break\n    }\n    // 第一次重试增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件\n    if tries == 1 {\n      time.Sleep(time.Duration(float64(sleepDuration) * randGen.Float64()))\n    } else {\n      time.Sleep(sleepDuration)\n    }\n  }\n}\n```\n\nStartRecordingToSink() 方法先根据当前时间生成一个随机数发生器 randGen，增加随机数是为了在重试时增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件，接着实例化一个EventCorrelator，EventCorrelator 会对事件做一些预处理的工作，其中包括过滤、聚合、缓存等操作，具体代码不做详细分析，最后将 recordToSink() 函数作为处理函数，recordToSink() 会将处理后的 events 发送到 apiserver，这是 StartEventWatcher() 的整个工作流程。\n\n\n##### 6、Events 简单实现\n\n了解完 events 的整个处理流程后，可以参考其实现方式写一个 demo，要实现一个完整的 events 需要包含以下几个功能：\n\n- 1、事件的产生\n- 2、事件的发送\n- 3、事件广播\n- 4、事件缓存\n- 5、事件过滤和聚合\n\n```\npackage main\n\nimport (\n  \"fmt\"\n  \"sync\"\n  \"time\"\n)\n\n// watcher queue\nconst queueLength = int64(1)\n\n// Events xxx\ntype Events struct {\n  Reason    string\n  Message   string\n  Source    string\n  Type      string\n  Count     int64\n  Timestamp time.Time\n}\n\n// EventBroadcaster xxx\ntype EventBroadcaster interface {\n  Event(etype, reason, message string)\n  StartLogging() Interface\n  Stop()\n}\n\n// eventBroadcaster xxx\ntype eventBroadcasterImpl struct {\n  *Broadcaster\n}\n\nfunc NewEventBroadcaster() EventBroadcaster {\n  return &eventBroadcasterImpl{NewBroadcaster(queueLength)}\n}\n\nfunc (eventBroadcaster *eventBroadcasterImpl) Stop() {\n  eventBroadcaster.Shutdown()\n}\n\n// generate event\nfunc (eventBroadcaster *eventBroadcasterImpl) Event(etype, reason, message string) {\n  events := &Events{Type: etype, Reason: reason, Message: message}\n  // send event to broadcast\n  eventBroadcaster.Action(events)\n}\n\n// 仅实现 StartLogging() 的功能，将日志打印\nfunc (eventBroadcaster *eventBroadcasterImpl) StartLogging() Interface {\n  // register a watcher\n  watcher := eventBroadcaster.Watch()\n  go func() {\n    for watchEvent := range watcher.ResultChan() {\n      fmt.Printf(\"%v\\n\", watchEvent)\n    }\n  }()\n\n  go func() {\n    time.Sleep(time.Second * 4)\n    watcher.Stop()\n  }()\n\n  return watcher\n}\n\n// --------------------\n// Broadcaster 定义与实现\n// 接收 events channel 的长度\nconst incomingQueuLength = 100\n\ntype Broadcaster struct {\n  lock             sync.Mutex\n  incoming         chan Events\n  watchers         map[int64]*broadcasterWatcher\n  watchersQueue    int64\n  watchQueueLength int64\n  distributing     sync.WaitGroup\n}\n\nfunc NewBroadcaster(queueLength int64) *Broadcaster {\n  m := &Broadcaster{\n    incoming:         make(chan Events, incomingQueuLength),\n    watchers:         map[int64]*broadcasterWatcher{},\n    watchQueueLength: queueLength,\n  }\n  m.distributing.Add(1)\n  // 后台启动一个 goroutine 广播 events\n  go m.loop()\n  return m\n}\n\n// Broadcaster 接收所产生的 events\nfunc (m *Broadcaster) Action(event *Events) {\n  m.incoming <- *event\n}\n\n// 广播 events 到每个 watcher\nfunc (m *Broadcaster) loop() {\n  // 从 incoming channel 中读取所接收到的 events\n  for event := range m.incoming {\n    // 发送 events 到每一个 watcher\n    for _, w := range m.watchers {\n      select {\n      case w.result <- event:\n      case <-w.stopped:\n      default:\n      }\n    }\n  }\n  m.closeAll()\n  m.distributing.Done()\n}\n\nfunc (m *Broadcaster) Shutdown() {\n  close(m.incoming)\n  m.distributing.Wait()\n}\n\nfunc (m *Broadcaster) closeAll() {\n  // TODO\n  m.lock.Lock()\n  defer m.lock.Unlock()\n  for _, w := range m.watchers {\n    close(w.result)\n  }\n  m.watchers = map[int64]*broadcasterWatcher{}\n}\n\nfunc (m *Broadcaster) stopWatching(id int64) {\n  m.lock.Lock()\n  defer m.lock.Unlock()\n  w, ok := m.watchers[id]\n  if !ok {\n    return\n  }\n  delete(m.watchers, id)\n  close(w.result)\n}\n\n// 调用 Watch(）方法注册一个 watcher\nfunc (m *Broadcaster) Watch() Interface {\n  watcher := &broadcasterWatcher{\n    result:  make(chan Events, incomingQueuLength),\n    stopped: make(chan struct{}),\n    id:      m.watchQueueLength,\n    m:       m,\n  }\n  m.watchers[m.watchersQueue] = watcher\n  m.watchQueueLength++\n  return watcher\n}\n\n// watcher 实现\ntype Interface interface {\n  Stop()\n  ResultChan() <-chan Events\n}\n\ntype broadcasterWatcher struct {\n  result  chan Events\n  stopped chan struct{}\n  stop    sync.Once\n  id      int64\n  m       *Broadcaster\n}\n\n// 每个 watcher 通过该方法读取 channel 中广播的 events\nfunc (b *broadcasterWatcher) ResultChan() <-chan Events {\n  return b.result\n}\n\nfunc (b *broadcasterWatcher) Stop() {\n  b.stop.Do(func() {\n    close(b.stopped)\n    b.m.stopWatching(b.id)\n  })\n}\n\n// --------------------\n\nfunc main() {\n  eventBroadcast := NewEventBroadcaster()\n\n  var wg sync.WaitGroup\n  wg.Add(1)\n  // producer event\n  go func() {\n    defer wg.Done()\n    time.Sleep(time.Second)\n    eventBroadcast.Event(\"add\", \"test\", \"1\")\n    time.Sleep(time.Second * 2)\n    eventBroadcast.Event(\"add\", \"test\", \"2\")\n    time.Sleep(time.Second * 3)\n    eventBroadcast.Event(\"add\", \"test\", \"3\")\n    //eventBroadcast.Stop()\n  }()\n\n  eventBroadcast.StartLogging()\n  wg.Wait()\n}\n```\n\n此处仅简单实现，将 EventRecorder 处理 events 的功能直接放在了 EventBroadcaster 中实现，对 events 的处理方法仅实现了 StartLogging()，Broadcaster 中的部分功能是直接复制 k8s 中的代码，有一定的精简，其实现值得学习，此处对 EventCorrelator 并没有进行实现。\n\n\n代码请参考：https://github.com/gosoon/k8s-learning-notes/tree/master/k8s-package/events\n\n##### 7、总结\n\n本文讲述了 k8s 中 events 从产生到展示的一个完整过程，最后也实现了一个简单的 demo，在此将 kubelet 对 events 的整个处理过程再梳理下，其中主要有三个对象 EventBroadcaster、EventRecorder、Broadcaster：\n\n- 1、kubelet 首先会初始化 EventBroadcaster 对象，同时会初始化一个 Broadcaster 对象。\n- 2、kubelet 通过 EventBroadcaster 对象的 NewRecorder() 方法初始化 EventRecorder 对象，EventRecorder 对象提供的几个方法会生成 events 并通过 Action() 方法发送 events 到 Broadcaster 的 channel 队列中。\n- 3、Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 初始化后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发来的 events。\n- 4、EventBroadcaster 对 events 有三个处理方法：StartEventWatcher()、StartRecordingToSink()、StartLogging()，StartEventWatcher() 是其中的核心方法，会初始化一个 watcher 注册到 Broadcaster，其余两个处理函数对 StartEventWatcher() 进行了封装，并实现了自己的处理函数。\n- 5、 Broadcaster 中有一个 map 会保存每一个注册的 watcher，其会将所有的 events 广播给每一个 watcher，每个 watcher 通过它的 ResultChan() 方法从 channel 接收 events。\n- 6、kubelet 会使用 StartRecordingToSink() 和 StartLogging() 对 events 进行处理，StartRecordingToSink() 处理函数收到 events 后会进行缓存、过滤、聚合而后发送到 apiserver，apiserver 会将 events 保存到 etcd 中，使用 kubectl 或其他客户端可以查看。StartLogging() 仅将 events 保存到 kubelet 的日志中。\n","source":"_posts/k8s_events.md","raw":"---\ntitle: kubernets 中事件处理机制\ndate: 2019-02-26 20:49:30\ntags: [\"events\",\"kubelet\"]\ntype: \"k8s-events\"\n\n---\n\n当集群中的 node 或 pod 异常时，大部分用户会使用 kubectl 查看对应的 events，那么 events 是从何而来的？其实 k8s 中的各个组件会将运行时产生的各种事件汇报到 apiserver，对于 k8s 中的可描述资源，使用 kubectl describe 都可以看到其相关的 events，那 k8s 中又有哪几个组件都上报 events 呢？ \n\n只要在 `k8s.io/kubernetes/cmd` 目录下暴力搜索一下就能知道哪些组件会产生 events：\n```\n$ grep -R -n -i \"EventRecorder\" .\n```\n\n可以看出，controller-manage、kube-proxy、kube-scheduler、kubelet 都使用了 EventRecorder，本文只讲述 kubelet 中对 Events 的使用。\n\n\n\n##### 1、Events 的定义\n\nevents 在 `k8s.io/api/core/v1/types.go` 中进行定义,结构体如下所示：\n\n```\ntype Event struct {\n    metav1.TypeMeta `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata\" protobuf:\"bytes,1,opt,name=metadata\"`\n    InvolvedObject ObjectReference `json:\"involvedObject\" protobuf:\"bytes,2,opt,name=involvedObject\"`\n    Reason string `json:\"reason,omitempty\" protobuf:\"bytes,3,opt,name=reason\"`\n    Message string `json:\"message,omitempty\" protobuf:\"bytes,4,opt,name=message\"`\n    Source EventSource `json:\"source,omitempty\" protobuf:\"bytes,5,opt,name=source\"`\n    FirstTimestamp metav1.Time `json:\"firstTimestamp,omitempty\" protobuf:\"bytes,6,opt,name=firstTimestamp\"`\n    LastTimestamp metav1.Time `json:\"lastTimestamp,omitempty\" protobuf:\"bytes,7,opt,name=lastTimestamp\"`\n    Count int32 `json:\"count,omitempty\" protobuf:\"varint,8,opt,name=count\"`\n    Type string `json:\"type,omitempty\" protobuf:\"bytes,9,opt,name=type\"`\n    EventTime metav1.MicroTime `json:\"eventTime,omitempty\" protobuf:\"bytes,10,opt,name=eventTime\"`\n    Series *EventSeries `json:\"series,omitempty\" protobuf:\"bytes,11,opt,name=series\"`\n    Action string `json:\"action,omitempty\" protobuf:\"bytes,12,opt,name=action\"`\n    Related *ObjectReference `json:\"related,omitempty\" protobuf:\"bytes,13,opt,name=related\"`\n    ReportingController string `json:\"reportingComponent\" protobuf:\"bytes,14,opt,name=reportingComponent\"`\n    ReportingInstance string `json:\"reportingInstance\" protobuf:\"bytes,15,opt,name=reportingInstance\"`\n    ReportingInstance string `json:\"reportingInstance\" protobuf:\"bytes,15,opt,name=reportingInstance\"`\n}\n```\n\n其中 InvolvedObject 代表和事件关联的对象，source 代表事件源，使用 kubectl 看到的事件一般包含 Type、Reason、Age、From、Message 几个字段。\n\nk8s 中 events 目前只有两种类型：\"Normal\" 和 \"Warning\"：\n\n![events 的两种类型](http://cdn.tianfeiyu.com/events.png)\n\n\n##### 2、EventBroadcaster 的初始化\n\nevents 的整个生命周期都与 EventBroadcaster 有关，kubelet 中对 EventBroadcaster 的初始化在`k8s.io/kubernetes/cmd/kubelet/app/server.go`中：\n\n\n```\nfunc RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error {\n  ...\n  // event 初始化\n  makeEventRecorder(kubeDeps, nodeName)\n  ...\n}\n\n\nfunc makeEventRecorder(kubeDeps *kubelet.Dependencies, nodeName types.NodeName) {\n  if kubeDeps.Recorder != nil {\n    return\n  }\n  // 初始化 EventBroadcaster \n  eventBroadcaster := record.NewBroadcaster()\n  // 初始化 EventRecorder\n  kubeDeps.Recorder = eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource{Component: componentKubelet, Host: string(nodeName)})\n  // 记录 events 到本地日志\n  eventBroadcaster.StartLogging(glog.V(3).Infof)\n  if kubeDeps.EventClient != nil {\n    glog.V(4).Infof(\"Sending events to api server.\")\n    // 上报 events 到 apiserver\n  eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: kubeDeps.EventClient.Events(\"\")})\n  } else {\n    glog.Warning(\"No api server defined - no events will be sent to API server.\")\n  }\n}\n```\n\nKubelet 在启动的时候会初始化一个 EventBroadcaster，它主要是对接收到的 events 做一些后续的处理(保存、上报等），EventBroadcaster 也会被 kubelet 中的其他模块使用，以下是相关的定义，对 events 生成和处理的函数都定义在 `k8s.io/client-go/tools/record/event.go` 中：\n\n```\ntype eventBroadcasterImpl struct {\n  *watch.Broadcaster\n  sleepDuration time.Duration\n}\n\n// EventBroadcaster knows how to receive events and send them to any EventSink, watcher, or log.\ntype EventBroadcaster interface {\n  StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface\n\n  StartRecordingToSink(sink EventSink) watch.Interface\n\n  StartLogging(logf func(format string, args ...interface{})) watch.Interface\n\n  NewRecorder(scheme *runtime.Scheme, source v1.EventSource) EventRecorder\n}\n```\n\nEventBroadcaster 是个接口类型，该接口有以下四个方法：\n- StartEventWatcher() ： EventBroadcaster 中的核心方法，接收各模块产生的 events，参数为一个处理 events 的函数，用户可以使用 StartEventWatcher() 接收 events 然后使用自定义的 handle 进行处理\n- StartRecordingToSink() ： 调用 StartEventWatcher() 接收 events，并将收到的 events 发送到 apiserver \n- StartLogging() ：也是调用 StartEventWatcher() 接收 events，然后保存 events 到日志\n- NewRecorder() ：会创建一个指定 EventSource 的 EventRecorder，EventSource 指明了哪个节点的哪个组件\n\n\neventBroadcasterImpl 是 eventBroadcaster 实际的对象，初始化 EventBroadcaster 对象的时候会初始化一个 Broadcaster，Broadcaster 会启动一个 goroutine 接收各组件产生的 events 并广播到每一个 watcher。\n\n```\nfunc NewBroadcaster() EventBroadcaster {\n  return &eventBroadcasterImpl{watch.NewBroadcaster(maxQueuedEvents, watch.DropIfChannelFull), defaultSleepDuration}\n}\n```\n\n可以看到，kubelet 在初始化完 EventBroadcaster 后会调用 StartRecordingToSink() 和 StartLogging() 两个方法，StartRecordingToSink() 处理函数会将收到的 events 进行缓存、过滤、聚合而后发送到 apiserver，StartLogging() 仅将 events 保存到 kubelet 的日志中。\n\n##### 3、Events 的生成\n\n从初始化 EventBroadcaster 的代码中可以看到 kubelet 在初始化完 EventBroadcaster 后紧接着初始化了 EventRecorder，并将已经初始化的 Broadcaster 对象作为参数传给了 EventRecorder，至此，EventBroadcaster、EventRecorder、Broadcaster 三个对象产生了关联。EventRecorder 的主要功能是生成指定格式的 events，以下是相关的定义：\n\n```\ntype recorderImpl struct {\n  scheme *runtime.Scheme\n  source v1.EventSource\n  *watch.Broadcaster\n  clock clock.Clock\n}\n\ntype EventRecorder interface {\n  Event(object runtime.Object, eventtype, reason, message string)\n\n  Eventf(object runtime.Object, eventtype, reason, messageFmt string, args ...interface{})\n\n  PastEventf(object runtime.Object, timestamp metav1.Time, eventtype, reason, messageFmt string, args ...interface{})\n\n  AnnotatedEventf(object runtime.Object, annotations map[string]string, eventtype, reason, messageFmt string, args ...interface{})\n}\n```\n\nEventRecorder 中包含的几个方法都是产生指定格式的 events，Event() 和 Eventf() 的功能类似 fmt.Println() 和 fmt.Printf()，kubelet 中的各个模块会调用 EventRecorder 生成 events。recorderImpl 是 EventRecorder 实际的对象。EventRecorder 的每个方法会调用 generateEvent，在 generateEvent 中初始化 events 。\n\n以下是生成 events 的函数：\n\n```\nfunc (recorder *recorderImpl) generateEvent(object runtime.Object, annotations map[string]string, timestamp metav1.Time, eventtype, reason, message string) {\n  ref, err := ref.GetReference(recorder.scheme, object)\n  if err != nil {\n    glog.Errorf(\"Could not construct reference to: '%#v' due to: '%v'. Will not report event: '%v' '%v' '%v'\", object, err, eventtype, reason, message)\n    return\n  }\n\n  if !validateEventType(eventtype) {\n    glog.Errorf(\"Unsupported event type: '%v'\", eventtype)\n    return\n  }\n\n  event := recorder.makeEvent(ref, annotations, eventtype, reason, message)\n  event.Source = recorder.source\n\n  go func() {\n    // NOTE: events should be a non-blocking operation\n    defer utilruntime.HandleCrash()\n    // 发送事件\n    recorder.Action(watch.Added, event)\n  }()\n}\n\nfunc (recorder *recorderImpl) makeEvent(ref *v1.ObjectReference, annotations map[string]string, eventtype, reason, message string) *v1.Event {\n  t := metav1.Time{Time: recorder.clock.Now()}\n  namespace := ref.Namespace\n  if namespace == \"\" {\n    namespace = metav1.NamespaceDefault\n  }\n  return &v1.Event{\n    ObjectMeta: metav1.ObjectMeta{\n      Name:        fmt.Sprintf(\"%v.%x\", ref.Name, t.UnixNano()),\n      Namespace:   namespace,\n      Annotations: annotations,\n    },\n    InvolvedObject: *ref,\n    Reason:         reason,\n    Message:        message,\n    FirstTimestamp: t,\n    LastTimestamp:  t,\n    Count:          1,\n    Type:           eventtype,\n  }\n}\n```\n初始化完 events 后会调用 recorder.Action() 将 events 发送到 Broadcaster 的事件接收队列中, Action() 是 Broadcaster 中的方法。\n\n以下是 Action() 方法的实现：\n\n```\nfunc (m *Broadcaster) Action(action EventType, obj runtime.Object) {\n  m.incoming <- Event{action, obj}\n}\n```\n\n##### 4、Events 的广播\n\n上面已经说了，EventBroadcaster 初始化时会初始化一个 Broadcaster，Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 的实现在 `k8s.io/apimachinery/pkg/watch/mux.go ` 中，Broadcaster 初始化完成后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发送过来的 events，Broadcaster 中有一个 map 会保存每一个注册的 watcher， 接着将 events 广播给所有的 watcher，每个 watcher 都有一个接收消息的 channel，watcher 可以通过它的 ResultChan() 方法从 channel 中读取数据进行消费。\n\n\n以下是 Broadcaster 广播 events 的实现：\n```\nfunc (m *Broadcaster) loop() {\n  for event := range m.incoming {\n    if event.Type == internalRunFunctionMarker {\n      event.Object.(functionFakeRuntimeObject)()\n      continue\n    }\n    m.distribute(event)\n  }\n  m.closeAll()\n  m.distributing.Done()\n}\n\n// distribute sends event to all watchers. Blocking.\nfunc (m *Broadcaster) distribute(event Event) {\n  m.lock.Lock()\n  defer m.lock.Unlock()\n  if m.fullChannelBehavior == DropIfChannelFull {\n    for _, w := range m.watchers {\n      select {\n      case w.result <- event:\n      case <-w.stopped:\n      default: // Don't block if the event can't be queued.\n      }\n    }\n  } else {\n    for _, w := range m.watchers {\n      select {\n      case w.result <- event:\n      case <-w.stopped:\n      }\n    }\n  }\n}\n```\n\n\n##### 5、Events 的处理\n\n那么 watcher 是从何而来呢？每一个要处理 events 的 client 都需要初始化一个 watcher，处理 events 的方法是在 EventBroadcaster 中定义的，以下是 EventBroadcaster 中对 events 处理的三个函数：\n\n```\nfunc (eventBroadcaster *eventBroadcasterImpl) StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface {\n  watcher := eventBroadcaster.Watch()\n  go func() {\n    defer utilruntime.HandleCrash()\n    for watchEvent := range watcher.ResultChan() {\n      event, ok := watchEvent.Object.(*v1.Event)\n      if !ok {\n        // This is all local, so there's no reason this should\n        // ever happen.\n        continue\n      }\n      eventHandler(event)\n    }\n  }()\n  return watcher\n}\n```\n\nStartEventWatcher() 首先实例化一个 watcher，每个 watcher 都会被塞入到 Broadcaster 的 watcher 列表中，watcher 从 Broadcaster 提供的 channel 中读取 events，然后再调用 eventHandler 进行处理，StartLogging() 和 StartRecordingToSink() 都是对 StartEventWatcher() 的封装，都会传入自己的处理函数。\n\n\n\n```\nfunc (eventBroadcaster *eventBroadcasterImpl) StartLogging(logf func(format string, args ...interface{})) watch.Interface {\n  return eventBroadcaster.StartEventWatcher(\n    func(e *v1.Event) {\n      logf(\"Event(%#v): type: '%v' reason: '%v' %v\", e.InvolvedObject, e.Type, e.Reason, e.Message)\n    })\n}\n```\n\nStartLogging() 传入的 eventHandler 仅将 events 保存到日志中。\n\n```\nfunc (eventBroadcaster *eventBroadcasterImpl) StartRecordingToSink(sink EventSink) watch.Interface {\n  // The default math/rand package functions aren't thread safe, so create a\n  // new Rand object for each StartRecording call.\n  randGen := rand.New(rand.NewSource(time.Now().UnixNano()))\n  eventCorrelator := NewEventCorrelator(clock.RealClock{})\n  return eventBroadcaster.StartEventWatcher(\n    func(event *v1.Event) {\n      recordToSink(sink, event, eventCorrelator, randGen, eventBroadcaster.sleepDuration)\n    })\n}\n\nfunc recordToSink(sink EventSink, event *v1.Event, eventCorrelator *EventCorrelator, randGen *rand.Rand, sleepDuration time.Duration) {\n  eventCopy := *event\n  event = &eventCopy\n  result, err := eventCorrelator.EventCorrelate(event)\n  if err != nil {\n    utilruntime.HandleError(err)\n  }\n  if result.Skip {\n    return\n  }\n  tries := 0\n  for {\n    if recordEvent(sink, result.Event, result.Patch, result.Event.Count > 1, eventCorrelator) {\n      break\n    }\n    tries++\n    if tries >= maxTriesPerEvent {\n      glog.Errorf(\"Unable to write event '%#v' (retry limit exceeded!)\", event)\n      break\n    }\n    // 第一次重试增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件\n    if tries == 1 {\n      time.Sleep(time.Duration(float64(sleepDuration) * randGen.Float64()))\n    } else {\n      time.Sleep(sleepDuration)\n    }\n  }\n}\n```\n\nStartRecordingToSink() 方法先根据当前时间生成一个随机数发生器 randGen，增加随机数是为了在重试时增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件，接着实例化一个EventCorrelator，EventCorrelator 会对事件做一些预处理的工作，其中包括过滤、聚合、缓存等操作，具体代码不做详细分析，最后将 recordToSink() 函数作为处理函数，recordToSink() 会将处理后的 events 发送到 apiserver，这是 StartEventWatcher() 的整个工作流程。\n\n\n##### 6、Events 简单实现\n\n了解完 events 的整个处理流程后，可以参考其实现方式写一个 demo，要实现一个完整的 events 需要包含以下几个功能：\n\n- 1、事件的产生\n- 2、事件的发送\n- 3、事件广播\n- 4、事件缓存\n- 5、事件过滤和聚合\n\n```\npackage main\n\nimport (\n  \"fmt\"\n  \"sync\"\n  \"time\"\n)\n\n// watcher queue\nconst queueLength = int64(1)\n\n// Events xxx\ntype Events struct {\n  Reason    string\n  Message   string\n  Source    string\n  Type      string\n  Count     int64\n  Timestamp time.Time\n}\n\n// EventBroadcaster xxx\ntype EventBroadcaster interface {\n  Event(etype, reason, message string)\n  StartLogging() Interface\n  Stop()\n}\n\n// eventBroadcaster xxx\ntype eventBroadcasterImpl struct {\n  *Broadcaster\n}\n\nfunc NewEventBroadcaster() EventBroadcaster {\n  return &eventBroadcasterImpl{NewBroadcaster(queueLength)}\n}\n\nfunc (eventBroadcaster *eventBroadcasterImpl) Stop() {\n  eventBroadcaster.Shutdown()\n}\n\n// generate event\nfunc (eventBroadcaster *eventBroadcasterImpl) Event(etype, reason, message string) {\n  events := &Events{Type: etype, Reason: reason, Message: message}\n  // send event to broadcast\n  eventBroadcaster.Action(events)\n}\n\n// 仅实现 StartLogging() 的功能，将日志打印\nfunc (eventBroadcaster *eventBroadcasterImpl) StartLogging() Interface {\n  // register a watcher\n  watcher := eventBroadcaster.Watch()\n  go func() {\n    for watchEvent := range watcher.ResultChan() {\n      fmt.Printf(\"%v\\n\", watchEvent)\n    }\n  }()\n\n  go func() {\n    time.Sleep(time.Second * 4)\n    watcher.Stop()\n  }()\n\n  return watcher\n}\n\n// --------------------\n// Broadcaster 定义与实现\n// 接收 events channel 的长度\nconst incomingQueuLength = 100\n\ntype Broadcaster struct {\n  lock             sync.Mutex\n  incoming         chan Events\n  watchers         map[int64]*broadcasterWatcher\n  watchersQueue    int64\n  watchQueueLength int64\n  distributing     sync.WaitGroup\n}\n\nfunc NewBroadcaster(queueLength int64) *Broadcaster {\n  m := &Broadcaster{\n    incoming:         make(chan Events, incomingQueuLength),\n    watchers:         map[int64]*broadcasterWatcher{},\n    watchQueueLength: queueLength,\n  }\n  m.distributing.Add(1)\n  // 后台启动一个 goroutine 广播 events\n  go m.loop()\n  return m\n}\n\n// Broadcaster 接收所产生的 events\nfunc (m *Broadcaster) Action(event *Events) {\n  m.incoming <- *event\n}\n\n// 广播 events 到每个 watcher\nfunc (m *Broadcaster) loop() {\n  // 从 incoming channel 中读取所接收到的 events\n  for event := range m.incoming {\n    // 发送 events 到每一个 watcher\n    for _, w := range m.watchers {\n      select {\n      case w.result <- event:\n      case <-w.stopped:\n      default:\n      }\n    }\n  }\n  m.closeAll()\n  m.distributing.Done()\n}\n\nfunc (m *Broadcaster) Shutdown() {\n  close(m.incoming)\n  m.distributing.Wait()\n}\n\nfunc (m *Broadcaster) closeAll() {\n  // TODO\n  m.lock.Lock()\n  defer m.lock.Unlock()\n  for _, w := range m.watchers {\n    close(w.result)\n  }\n  m.watchers = map[int64]*broadcasterWatcher{}\n}\n\nfunc (m *Broadcaster) stopWatching(id int64) {\n  m.lock.Lock()\n  defer m.lock.Unlock()\n  w, ok := m.watchers[id]\n  if !ok {\n    return\n  }\n  delete(m.watchers, id)\n  close(w.result)\n}\n\n// 调用 Watch(）方法注册一个 watcher\nfunc (m *Broadcaster) Watch() Interface {\n  watcher := &broadcasterWatcher{\n    result:  make(chan Events, incomingQueuLength),\n    stopped: make(chan struct{}),\n    id:      m.watchQueueLength,\n    m:       m,\n  }\n  m.watchers[m.watchersQueue] = watcher\n  m.watchQueueLength++\n  return watcher\n}\n\n// watcher 实现\ntype Interface interface {\n  Stop()\n  ResultChan() <-chan Events\n}\n\ntype broadcasterWatcher struct {\n  result  chan Events\n  stopped chan struct{}\n  stop    sync.Once\n  id      int64\n  m       *Broadcaster\n}\n\n// 每个 watcher 通过该方法读取 channel 中广播的 events\nfunc (b *broadcasterWatcher) ResultChan() <-chan Events {\n  return b.result\n}\n\nfunc (b *broadcasterWatcher) Stop() {\n  b.stop.Do(func() {\n    close(b.stopped)\n    b.m.stopWatching(b.id)\n  })\n}\n\n// --------------------\n\nfunc main() {\n  eventBroadcast := NewEventBroadcaster()\n\n  var wg sync.WaitGroup\n  wg.Add(1)\n  // producer event\n  go func() {\n    defer wg.Done()\n    time.Sleep(time.Second)\n    eventBroadcast.Event(\"add\", \"test\", \"1\")\n    time.Sleep(time.Second * 2)\n    eventBroadcast.Event(\"add\", \"test\", \"2\")\n    time.Sleep(time.Second * 3)\n    eventBroadcast.Event(\"add\", \"test\", \"3\")\n    //eventBroadcast.Stop()\n  }()\n\n  eventBroadcast.StartLogging()\n  wg.Wait()\n}\n```\n\n此处仅简单实现，将 EventRecorder 处理 events 的功能直接放在了 EventBroadcaster 中实现，对 events 的处理方法仅实现了 StartLogging()，Broadcaster 中的部分功能是直接复制 k8s 中的代码，有一定的精简，其实现值得学习，此处对 EventCorrelator 并没有进行实现。\n\n\n代码请参考：https://github.com/gosoon/k8s-learning-notes/tree/master/k8s-package/events\n\n##### 7、总结\n\n本文讲述了 k8s 中 events 从产生到展示的一个完整过程，最后也实现了一个简单的 demo，在此将 kubelet 对 events 的整个处理过程再梳理下，其中主要有三个对象 EventBroadcaster、EventRecorder、Broadcaster：\n\n- 1、kubelet 首先会初始化 EventBroadcaster 对象，同时会初始化一个 Broadcaster 对象。\n- 2、kubelet 通过 EventBroadcaster 对象的 NewRecorder() 方法初始化 EventRecorder 对象，EventRecorder 对象提供的几个方法会生成 events 并通过 Action() 方法发送 events 到 Broadcaster 的 channel 队列中。\n- 3、Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 初始化后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发来的 events。\n- 4、EventBroadcaster 对 events 有三个处理方法：StartEventWatcher()、StartRecordingToSink()、StartLogging()，StartEventWatcher() 是其中的核心方法，会初始化一个 watcher 注册到 Broadcaster，其余两个处理函数对 StartEventWatcher() 进行了封装，并实现了自己的处理函数。\n- 5、 Broadcaster 中有一个 map 会保存每一个注册的 watcher，其会将所有的 events 广播给每一个 watcher，每个 watcher 通过它的 ResultChan() 方法从 channel 接收 events。\n- 6、kubelet 会使用 StartRecordingToSink() 和 StartLogging() 对 events 进行处理，StartRecordingToSink() 处理函数收到 events 后会进行缓存、过滤、聚合而后发送到 apiserver，apiserver 会将 events 保存到 etcd 中，使用 kubectl 或其他客户端可以查看。StartLogging() 仅将 events 保存到 kubelet 的日志中。\n","slug":"k8s_events","published":1,"updated":"2019-07-21T09:46:28.879Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59f000mapwnd36ph6g8","content":"<p>当集群中的 node 或 pod 异常时，大部分用户会使用 kubectl 查看对应的 events，那么 events 是从何而来的？其实 k8s 中的各个组件会将运行时产生的各种事件汇报到 apiserver，对于 k8s 中的可描述资源，使用 kubectl describe 都可以看到其相关的 events，那 k8s 中又有哪几个组件都上报 events 呢？ </p>\n<p>只要在 <code>k8s.io/kubernetes/cmd</code> 目录下暴力搜索一下就能知道哪些组件会产生 events：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ grep -R -n -i &quot;EventRecorder&quot; .</span><br></pre></td></tr></table></figure></p>\n<p>可以看出，controller-manage、kube-proxy、kube-scheduler、kubelet 都使用了 EventRecorder，本文只讲述 kubelet 中对 Events 的使用。</p>\n<h5 id=\"1、Events-的定义\"><a href=\"#1、Events-的定义\" class=\"headerlink\" title=\"1、Events 的定义\"></a>1、Events 的定义</h5><p>events 在 <code>k8s.io/api/core/v1/types.go</code> 中进行定义,结构体如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Event struct &#123;</span><br><span class=\"line\">    metav1.TypeMeta `json:&quot;,inline&quot;`</span><br><span class=\"line\">    metav1.ObjectMeta `json:&quot;metadata&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`</span><br><span class=\"line\">    InvolvedObject ObjectReference `json:&quot;involvedObject&quot; protobuf:&quot;bytes,2,opt,name=involvedObject&quot;`</span><br><span class=\"line\">    Reason string `json:&quot;reason,omitempty&quot; protobuf:&quot;bytes,3,opt,name=reason&quot;`</span><br><span class=\"line\">    Message string `json:&quot;message,omitempty&quot; protobuf:&quot;bytes,4,opt,name=message&quot;`</span><br><span class=\"line\">    Source EventSource `json:&quot;source,omitempty&quot; protobuf:&quot;bytes,5,opt,name=source&quot;`</span><br><span class=\"line\">    FirstTimestamp metav1.Time `json:&quot;firstTimestamp,omitempty&quot; protobuf:&quot;bytes,6,opt,name=firstTimestamp&quot;`</span><br><span class=\"line\">    LastTimestamp metav1.Time `json:&quot;lastTimestamp,omitempty&quot; protobuf:&quot;bytes,7,opt,name=lastTimestamp&quot;`</span><br><span class=\"line\">    Count int32 `json:&quot;count,omitempty&quot; protobuf:&quot;varint,8,opt,name=count&quot;`</span><br><span class=\"line\">    Type string `json:&quot;type,omitempty&quot; protobuf:&quot;bytes,9,opt,name=type&quot;`</span><br><span class=\"line\">    EventTime metav1.MicroTime `json:&quot;eventTime,omitempty&quot; protobuf:&quot;bytes,10,opt,name=eventTime&quot;`</span><br><span class=\"line\">    Series *EventSeries `json:&quot;series,omitempty&quot; protobuf:&quot;bytes,11,opt,name=series&quot;`</span><br><span class=\"line\">    Action string `json:&quot;action,omitempty&quot; protobuf:&quot;bytes,12,opt,name=action&quot;`</span><br><span class=\"line\">    Related *ObjectReference `json:&quot;related,omitempty&quot; protobuf:&quot;bytes,13,opt,name=related&quot;`</span><br><span class=\"line\">    ReportingController string `json:&quot;reportingComponent&quot; protobuf:&quot;bytes,14,opt,name=reportingComponent&quot;`</span><br><span class=\"line\">    ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;`</span><br><span class=\"line\">    ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;`</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 InvolvedObject 代表和事件关联的对象，source 代表事件源，使用 kubectl 看到的事件一般包含 Type、Reason、Age、From、Message 几个字段。</p>\n<p>k8s 中 events 目前只有两种类型：”Normal” 和 “Warning”：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/events.png\" alt=\"events 的两种类型\"></p>\n<h5 id=\"2、EventBroadcaster-的初始化\"><a href=\"#2、EventBroadcaster-的初始化\" class=\"headerlink\" title=\"2、EventBroadcaster 的初始化\"></a>2、EventBroadcaster 的初始化</h5><p>events 的整个生命周期都与 EventBroadcaster 有关，kubelet 中对 EventBroadcaster 的初始化在<code>k8s.io/kubernetes/cmd/kubelet/app/server.go</code>中：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // event 初始化</span><br><span class=\"line\">  makeEventRecorder(kubeDeps, nodeName)</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">func makeEventRecorder(kubeDeps *kubelet.Dependencies, nodeName types.NodeName) &#123;</span><br><span class=\"line\">  if kubeDeps.Recorder != nil &#123;</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // 初始化 EventBroadcaster </span><br><span class=\"line\">  eventBroadcaster := record.NewBroadcaster()</span><br><span class=\"line\">  // 初始化 EventRecorder</span><br><span class=\"line\">  kubeDeps.Recorder = eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource&#123;Component: componentKubelet, Host: string(nodeName)&#125;)</span><br><span class=\"line\">  // 记录 events 到本地日志</span><br><span class=\"line\">  eventBroadcaster.StartLogging(glog.V(3).Infof)</span><br><span class=\"line\">  if kubeDeps.EventClient != nil &#123;</span><br><span class=\"line\">    glog.V(4).Infof(&quot;Sending events to api server.&quot;)</span><br><span class=\"line\">    // 上报 events 到 apiserver</span><br><span class=\"line\">  eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeDeps.EventClient.Events(&quot;&quot;)&#125;)</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    glog.Warning(&quot;No api server defined - no events will be sent to API server.&quot;)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Kubelet 在启动的时候会初始化一个 EventBroadcaster，它主要是对接收到的 events 做一些后续的处理(保存、上报等），EventBroadcaster 也会被 kubelet 中的其他模块使用，以下是相关的定义，对 events 生成和处理的函数都定义在 <code>k8s.io/client-go/tools/record/event.go</code> 中：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type eventBroadcasterImpl struct &#123;</span><br><span class=\"line\">  *watch.Broadcaster</span><br><span class=\"line\">  sleepDuration time.Duration</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// EventBroadcaster knows how to receive events and send them to any EventSink, watcher, or log.</span><br><span class=\"line\">type EventBroadcaster interface &#123;</span><br><span class=\"line\">  StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface</span><br><span class=\"line\"></span><br><span class=\"line\">  StartRecordingToSink(sink EventSink) watch.Interface</span><br><span class=\"line\"></span><br><span class=\"line\">  StartLogging(logf func(format string, args ...interface&#123;&#125;)) watch.Interface</span><br><span class=\"line\"></span><br><span class=\"line\">  NewRecorder(scheme *runtime.Scheme, source v1.EventSource) EventRecorder</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>EventBroadcaster 是个接口类型，该接口有以下四个方法：</p>\n<ul>\n<li>StartEventWatcher() ： EventBroadcaster 中的核心方法，接收各模块产生的 events，参数为一个处理 events 的函数，用户可以使用 StartEventWatcher() 接收 events 然后使用自定义的 handle 进行处理</li>\n<li>StartRecordingToSink() ： 调用 StartEventWatcher() 接收 events，并将收到的 events 发送到 apiserver </li>\n<li>StartLogging() ：也是调用 StartEventWatcher() 接收 events，然后保存 events 到日志</li>\n<li>NewRecorder() ：会创建一个指定 EventSource 的 EventRecorder，EventSource 指明了哪个节点的哪个组件</li>\n</ul>\n<p>eventBroadcasterImpl 是 eventBroadcaster 实际的对象，初始化 EventBroadcaster 对象的时候会初始化一个 Broadcaster，Broadcaster 会启动一个 goroutine 接收各组件产生的 events 并广播到每一个 watcher。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewBroadcaster() EventBroadcaster &#123;</span><br><span class=\"line\">  return &amp;eventBroadcasterImpl&#123;watch.NewBroadcaster(maxQueuedEvents, watch.DropIfChannelFull), defaultSleepDuration&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，kubelet 在初始化完 EventBroadcaster 后会调用 StartRecordingToSink() 和 StartLogging() 两个方法，StartRecordingToSink() 处理函数会将收到的 events 进行缓存、过滤、聚合而后发送到 apiserver，StartLogging() 仅将 events 保存到 kubelet 的日志中。</p>\n<h5 id=\"3、Events-的生成\"><a href=\"#3、Events-的生成\" class=\"headerlink\" title=\"3、Events 的生成\"></a>3、Events 的生成</h5><p>从初始化 EventBroadcaster 的代码中可以看到 kubelet 在初始化完 EventBroadcaster 后紧接着初始化了 EventRecorder，并将已经初始化的 Broadcaster 对象作为参数传给了 EventRecorder，至此，EventBroadcaster、EventRecorder、Broadcaster 三个对象产生了关联。EventRecorder 的主要功能是生成指定格式的 events，以下是相关的定义：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type recorderImpl struct &#123;</span><br><span class=\"line\">  scheme *runtime.Scheme</span><br><span class=\"line\">  source v1.EventSource</span><br><span class=\"line\">  *watch.Broadcaster</span><br><span class=\"line\">  clock clock.Clock</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type EventRecorder interface &#123;</span><br><span class=\"line\">  Event(object runtime.Object, eventtype, reason, message string)</span><br><span class=\"line\"></span><br><span class=\"line\">  Eventf(object runtime.Object, eventtype, reason, messageFmt string, args ...interface&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">  PastEventf(object runtime.Object, timestamp metav1.Time, eventtype, reason, messageFmt string, args ...interface&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">  AnnotatedEventf(object runtime.Object, annotations map[string]string, eventtype, reason, messageFmt string, args ...interface&#123;&#125;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>EventRecorder 中包含的几个方法都是产生指定格式的 events，Event() 和 Eventf() 的功能类似 fmt.Println() 和 fmt.Printf()，kubelet 中的各个模块会调用 EventRecorder 生成 events。recorderImpl 是 EventRecorder 实际的对象。EventRecorder 的每个方法会调用 generateEvent，在 generateEvent 中初始化 events 。</p>\n<p>以下是生成 events 的函数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (recorder *recorderImpl) generateEvent(object runtime.Object, annotations map[string]string, timestamp metav1.Time, eventtype, reason, message string) &#123;</span><br><span class=\"line\">  ref, err := ref.GetReference(recorder.scheme, object)</span><br><span class=\"line\">  if err != nil &#123;</span><br><span class=\"line\">    glog.Errorf(&quot;Could not construct reference to: &apos;%#v&apos; due to: &apos;%v&apos;. Will not report event: &apos;%v&apos; &apos;%v&apos; &apos;%v&apos;&quot;, object, err, eventtype, reason, message)</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  if !validateEventType(eventtype) &#123;</span><br><span class=\"line\">    glog.Errorf(&quot;Unsupported event type: &apos;%v&apos;&quot;, eventtype)</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  event := recorder.makeEvent(ref, annotations, eventtype, reason, message)</span><br><span class=\"line\">  event.Source = recorder.source</span><br><span class=\"line\"></span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    // NOTE: events should be a non-blocking operation</span><br><span class=\"line\">    defer utilruntime.HandleCrash()</span><br><span class=\"line\">    // 发送事件</span><br><span class=\"line\">    recorder.Action(watch.Added, event)</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (recorder *recorderImpl) makeEvent(ref *v1.ObjectReference, annotations map[string]string, eventtype, reason, message string) *v1.Event &#123;</span><br><span class=\"line\">  t := metav1.Time&#123;Time: recorder.clock.Now()&#125;</span><br><span class=\"line\">  namespace := ref.Namespace</span><br><span class=\"line\">  if namespace == &quot;&quot; &#123;</span><br><span class=\"line\">    namespace = metav1.NamespaceDefault</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  return &amp;v1.Event&#123;</span><br><span class=\"line\">    ObjectMeta: metav1.ObjectMeta&#123;</span><br><span class=\"line\">      Name:        fmt.Sprintf(&quot;%v.%x&quot;, ref.Name, t.UnixNano()),</span><br><span class=\"line\">      Namespace:   namespace,</span><br><span class=\"line\">      Annotations: annotations,</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    InvolvedObject: *ref,</span><br><span class=\"line\">    Reason:         reason,</span><br><span class=\"line\">    Message:        message,</span><br><span class=\"line\">    FirstTimestamp: t,</span><br><span class=\"line\">    LastTimestamp:  t,</span><br><span class=\"line\">    Count:          1,</span><br><span class=\"line\">    Type:           eventtype,</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>初始化完 events 后会调用 recorder.Action() 将 events 发送到 Broadcaster 的事件接收队列中, Action() 是 Broadcaster 中的方法。</p>\n<p>以下是 Action() 方法的实现：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *Broadcaster) Action(action EventType, obj runtime.Object) &#123;</span><br><span class=\"line\">  m.incoming &lt;- Event&#123;action, obj&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"4、Events-的广播\"><a href=\"#4、Events-的广播\" class=\"headerlink\" title=\"4、Events 的广播\"></a>4、Events 的广播</h5><p>上面已经说了，EventBroadcaster 初始化时会初始化一个 Broadcaster，Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 的实现在 <code>k8s.io/apimachinery/pkg/watch/mux.go</code> 中，Broadcaster 初始化完成后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发送过来的 events，Broadcaster 中有一个 map 会保存每一个注册的 watcher， 接着将 events 广播给所有的 watcher，每个 watcher 都有一个接收消息的 channel，watcher 可以通过它的 ResultChan() 方法从 channel 中读取数据进行消费。</p>\n<p>以下是 Broadcaster 广播 events 的实现：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *Broadcaster) loop() &#123;</span><br><span class=\"line\">  for event := range m.incoming &#123;</span><br><span class=\"line\">    if event.Type == internalRunFunctionMarker &#123;</span><br><span class=\"line\">      event.Object.(functionFakeRuntimeObject)()</span><br><span class=\"line\">      continue</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    m.distribute(event)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.closeAll()</span><br><span class=\"line\">  m.distributing.Done()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// distribute sends event to all watchers. Blocking.</span><br><span class=\"line\">func (m *Broadcaster) distribute(event Event) &#123;</span><br><span class=\"line\">  m.lock.Lock()</span><br><span class=\"line\">  defer m.lock.Unlock()</span><br><span class=\"line\">  if m.fullChannelBehavior == DropIfChannelFull &#123;</span><br><span class=\"line\">    for _, w := range m.watchers &#123;</span><br><span class=\"line\">      select &#123;</span><br><span class=\"line\">      case w.result &lt;- event:</span><br><span class=\"line\">      case &lt;-w.stopped:</span><br><span class=\"line\">      default: // Don&apos;t block if the event can&apos;t be queued.</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    for _, w := range m.watchers &#123;</span><br><span class=\"line\">      select &#123;</span><br><span class=\"line\">      case w.result &lt;- event:</span><br><span class=\"line\">      case &lt;-w.stopped:</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"5、Events-的处理\"><a href=\"#5、Events-的处理\" class=\"headerlink\" title=\"5、Events 的处理\"></a>5、Events 的处理</h5><p>那么 watcher 是从何而来呢？每一个要处理 events 的 client 都需要初始化一个 watcher，处理 events 的方法是在 EventBroadcaster 中定义的，以下是 EventBroadcaster 中对 events 处理的三个函数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface &#123;</span><br><span class=\"line\">  watcher := eventBroadcaster.Watch()</span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    defer utilruntime.HandleCrash()</span><br><span class=\"line\">    for watchEvent := range watcher.ResultChan() &#123;</span><br><span class=\"line\">      event, ok := watchEvent.Object.(*v1.Event)</span><br><span class=\"line\">      if !ok &#123;</span><br><span class=\"line\">        // This is all local, so there&apos;s no reason this should</span><br><span class=\"line\">        // ever happen.</span><br><span class=\"line\">        continue</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      eventHandler(event)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\">  return watcher</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>StartEventWatcher() 首先实例化一个 watcher，每个 watcher 都会被塞入到 Broadcaster 的 watcher 列表中，watcher 从 Broadcaster 提供的 channel 中读取 events，然后再调用 eventHandler 进行处理，StartLogging() 和 StartRecordingToSink() 都是对 StartEventWatcher() 的封装，都会传入自己的处理函数。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) StartLogging(logf func(format string, args ...interface&#123;&#125;)) watch.Interface &#123;</span><br><span class=\"line\">  return eventBroadcaster.StartEventWatcher(</span><br><span class=\"line\">    func(e *v1.Event) &#123;</span><br><span class=\"line\">      logf(&quot;Event(%#v): type: &apos;%v&apos; reason: &apos;%v&apos; %v&quot;, e.InvolvedObject, e.Type, e.Reason, e.Message)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>StartLogging() 传入的 eventHandler 仅将 events 保存到日志中。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) StartRecordingToSink(sink EventSink) watch.Interface &#123;</span><br><span class=\"line\">  // The default math/rand package functions aren&apos;t thread safe, so create a</span><br><span class=\"line\">  // new Rand object for each StartRecording call.</span><br><span class=\"line\">  randGen := rand.New(rand.NewSource(time.Now().UnixNano()))</span><br><span class=\"line\">  eventCorrelator := NewEventCorrelator(clock.RealClock&#123;&#125;)</span><br><span class=\"line\">  return eventBroadcaster.StartEventWatcher(</span><br><span class=\"line\">    func(event *v1.Event) &#123;</span><br><span class=\"line\">      recordToSink(sink, event, eventCorrelator, randGen, eventBroadcaster.sleepDuration)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func recordToSink(sink EventSink, event *v1.Event, eventCorrelator *EventCorrelator, randGen *rand.Rand, sleepDuration time.Duration) &#123;</span><br><span class=\"line\">  eventCopy := *event</span><br><span class=\"line\">  event = &amp;eventCopy</span><br><span class=\"line\">  result, err := eventCorrelator.EventCorrelate(event)</span><br><span class=\"line\">  if err != nil &#123;</span><br><span class=\"line\">    utilruntime.HandleError(err)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  if result.Skip &#123;</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  tries := 0</span><br><span class=\"line\">  for &#123;</span><br><span class=\"line\">    if recordEvent(sink, result.Event, result.Patch, result.Event.Count &gt; 1, eventCorrelator) &#123;</span><br><span class=\"line\">      break</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    tries++</span><br><span class=\"line\">    if tries &gt;= maxTriesPerEvent &#123;</span><br><span class=\"line\">      glog.Errorf(&quot;Unable to write event &apos;%#v&apos; (retry limit exceeded!)&quot;, event)</span><br><span class=\"line\">      break</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 第一次重试增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件</span><br><span class=\"line\">    if tries == 1 &#123;</span><br><span class=\"line\">      time.Sleep(time.Duration(float64(sleepDuration) * randGen.Float64()))</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      time.Sleep(sleepDuration)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>StartRecordingToSink() 方法先根据当前时间生成一个随机数发生器 randGen，增加随机数是为了在重试时增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件，接着实例化一个EventCorrelator，EventCorrelator 会对事件做一些预处理的工作，其中包括过滤、聚合、缓存等操作，具体代码不做详细分析，最后将 recordToSink() 函数作为处理函数，recordToSink() 会将处理后的 events 发送到 apiserver，这是 StartEventWatcher() 的整个工作流程。</p>\n<h5 id=\"6、Events-简单实现\"><a href=\"#6、Events-简单实现\" class=\"headerlink\" title=\"6、Events 简单实现\"></a>6、Events 简单实现</h5><p>了解完 events 的整个处理流程后，可以参考其实现方式写一个 demo，要实现一个完整的 events 需要包含以下几个功能：</p>\n<ul>\n<li>1、事件的产生</li>\n<li>2、事件的发送</li>\n<li>3、事件广播</li>\n<li>4、事件缓存</li>\n<li>5、事件过滤和聚合</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">  &quot;fmt&quot;</span><br><span class=\"line\">  &quot;sync&quot;</span><br><span class=\"line\">  &quot;time&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// watcher queue</span><br><span class=\"line\">const queueLength = int64(1)</span><br><span class=\"line\"></span><br><span class=\"line\">// Events xxx</span><br><span class=\"line\">type Events struct &#123;</span><br><span class=\"line\">  Reason    string</span><br><span class=\"line\">  Message   string</span><br><span class=\"line\">  Source    string</span><br><span class=\"line\">  Type      string</span><br><span class=\"line\">  Count     int64</span><br><span class=\"line\">  Timestamp time.Time</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// EventBroadcaster xxx</span><br><span class=\"line\">type EventBroadcaster interface &#123;</span><br><span class=\"line\">  Event(etype, reason, message string)</span><br><span class=\"line\">  StartLogging() Interface</span><br><span class=\"line\">  Stop()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// eventBroadcaster xxx</span><br><span class=\"line\">type eventBroadcasterImpl struct &#123;</span><br><span class=\"line\">  *Broadcaster</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func NewEventBroadcaster() EventBroadcaster &#123;</span><br><span class=\"line\">  return &amp;eventBroadcasterImpl&#123;NewBroadcaster(queueLength)&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) Stop() &#123;</span><br><span class=\"line\">  eventBroadcaster.Shutdown()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// generate event</span><br><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) Event(etype, reason, message string) &#123;</span><br><span class=\"line\">  events := &amp;Events&#123;Type: etype, Reason: reason, Message: message&#125;</span><br><span class=\"line\">  // send event to broadcast</span><br><span class=\"line\">  eventBroadcaster.Action(events)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 仅实现 StartLogging() 的功能，将日志打印</span><br><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) StartLogging() Interface &#123;</span><br><span class=\"line\">  // register a watcher</span><br><span class=\"line\">  watcher := eventBroadcaster.Watch()</span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    for watchEvent := range watcher.ResultChan() &#123;</span><br><span class=\"line\">      fmt.Printf(&quot;%v\\n&quot;, watchEvent)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    time.Sleep(time.Second * 4)</span><br><span class=\"line\">    watcher.Stop()</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">  return watcher</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// --------------------</span><br><span class=\"line\">// Broadcaster 定义与实现</span><br><span class=\"line\">// 接收 events channel 的长度</span><br><span class=\"line\">const incomingQueuLength = 100</span><br><span class=\"line\"></span><br><span class=\"line\">type Broadcaster struct &#123;</span><br><span class=\"line\">  lock             sync.Mutex</span><br><span class=\"line\">  incoming         chan Events</span><br><span class=\"line\">  watchers         map[int64]*broadcasterWatcher</span><br><span class=\"line\">  watchersQueue    int64</span><br><span class=\"line\">  watchQueueLength int64</span><br><span class=\"line\">  distributing     sync.WaitGroup</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func NewBroadcaster(queueLength int64) *Broadcaster &#123;</span><br><span class=\"line\">  m := &amp;Broadcaster&#123;</span><br><span class=\"line\">    incoming:         make(chan Events, incomingQueuLength),</span><br><span class=\"line\">    watchers:         map[int64]*broadcasterWatcher&#123;&#125;,</span><br><span class=\"line\">    watchQueueLength: queueLength,</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.distributing.Add(1)</span><br><span class=\"line\">  // 后台启动一个 goroutine 广播 events</span><br><span class=\"line\">  go m.loop()</span><br><span class=\"line\">  return m</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Broadcaster 接收所产生的 events</span><br><span class=\"line\">func (m *Broadcaster) Action(event *Events) &#123;</span><br><span class=\"line\">  m.incoming &lt;- *event</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 广播 events 到每个 watcher</span><br><span class=\"line\">func (m *Broadcaster) loop() &#123;</span><br><span class=\"line\">  // 从 incoming channel 中读取所接收到的 events</span><br><span class=\"line\">  for event := range m.incoming &#123;</span><br><span class=\"line\">    // 发送 events 到每一个 watcher</span><br><span class=\"line\">    for _, w := range m.watchers &#123;</span><br><span class=\"line\">      select &#123;</span><br><span class=\"line\">      case w.result &lt;- event:</span><br><span class=\"line\">      case &lt;-w.stopped:</span><br><span class=\"line\">      default:</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.closeAll()</span><br><span class=\"line\">  m.distributing.Done()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (m *Broadcaster) Shutdown() &#123;</span><br><span class=\"line\">  close(m.incoming)</span><br><span class=\"line\">  m.distributing.Wait()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (m *Broadcaster) closeAll() &#123;</span><br><span class=\"line\">  // TODO</span><br><span class=\"line\">  m.lock.Lock()</span><br><span class=\"line\">  defer m.lock.Unlock()</span><br><span class=\"line\">  for _, w := range m.watchers &#123;</span><br><span class=\"line\">    close(w.result)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.watchers = map[int64]*broadcasterWatcher&#123;&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (m *Broadcaster) stopWatching(id int64) &#123;</span><br><span class=\"line\">  m.lock.Lock()</span><br><span class=\"line\">  defer m.lock.Unlock()</span><br><span class=\"line\">  w, ok := m.watchers[id]</span><br><span class=\"line\">  if !ok &#123;</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  delete(m.watchers, id)</span><br><span class=\"line\">  close(w.result)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 调用 Watch(）方法注册一个 watcher</span><br><span class=\"line\">func (m *Broadcaster) Watch() Interface &#123;</span><br><span class=\"line\">  watcher := &amp;broadcasterWatcher&#123;</span><br><span class=\"line\">    result:  make(chan Events, incomingQueuLength),</span><br><span class=\"line\">    stopped: make(chan struct&#123;&#125;),</span><br><span class=\"line\">    id:      m.watchQueueLength,</span><br><span class=\"line\">    m:       m,</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.watchers[m.watchersQueue] = watcher</span><br><span class=\"line\">  m.watchQueueLength++</span><br><span class=\"line\">  return watcher</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// watcher 实现</span><br><span class=\"line\">type Interface interface &#123;</span><br><span class=\"line\">  Stop()</span><br><span class=\"line\">  ResultChan() &lt;-chan Events</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type broadcasterWatcher struct &#123;</span><br><span class=\"line\">  result  chan Events</span><br><span class=\"line\">  stopped chan struct&#123;&#125;</span><br><span class=\"line\">  stop    sync.Once</span><br><span class=\"line\">  id      int64</span><br><span class=\"line\">  m       *Broadcaster</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 每个 watcher 通过该方法读取 channel 中广播的 events</span><br><span class=\"line\">func (b *broadcasterWatcher) ResultChan() &lt;-chan Events &#123;</span><br><span class=\"line\">  return b.result</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (b *broadcasterWatcher) Stop() &#123;</span><br><span class=\"line\">  b.stop.Do(func() &#123;</span><br><span class=\"line\">    close(b.stopped)</span><br><span class=\"line\">    b.m.stopWatching(b.id)</span><br><span class=\"line\">  &#125;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// --------------------</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">  eventBroadcast := NewEventBroadcaster()</span><br><span class=\"line\"></span><br><span class=\"line\">  var wg sync.WaitGroup</span><br><span class=\"line\">  wg.Add(1)</span><br><span class=\"line\">  // producer event</span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    defer wg.Done()</span><br><span class=\"line\">    time.Sleep(time.Second)</span><br><span class=\"line\">    eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;1&quot;)</span><br><span class=\"line\">    time.Sleep(time.Second * 2)</span><br><span class=\"line\">    eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;2&quot;)</span><br><span class=\"line\">    time.Sleep(time.Second * 3)</span><br><span class=\"line\">    eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;3&quot;)</span><br><span class=\"line\">    //eventBroadcast.Stop()</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">  eventBroadcast.StartLogging()</span><br><span class=\"line\">  wg.Wait()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>此处仅简单实现，将 EventRecorder 处理 events 的功能直接放在了 EventBroadcaster 中实现，对 events 的处理方法仅实现了 StartLogging()，Broadcaster 中的部分功能是直接复制 k8s 中的代码，有一定的精简，其实现值得学习，此处对 EventCorrelator 并没有进行实现。</p>\n<p>代码请参考：<a href=\"https://github.com/gosoon/k8s-learning-notes/tree/master/k8s-package/events\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/k8s-learning-notes/tree/master/k8s-package/events</a></p>\n<h5 id=\"7、总结\"><a href=\"#7、总结\" class=\"headerlink\" title=\"7、总结\"></a>7、总结</h5><p>本文讲述了 k8s 中 events 从产生到展示的一个完整过程，最后也实现了一个简单的 demo，在此将 kubelet 对 events 的整个处理过程再梳理下，其中主要有三个对象 EventBroadcaster、EventRecorder、Broadcaster：</p>\n<ul>\n<li>1、kubelet 首先会初始化 EventBroadcaster 对象，同时会初始化一个 Broadcaster 对象。</li>\n<li>2、kubelet 通过 EventBroadcaster 对象的 NewRecorder() 方法初始化 EventRecorder 对象，EventRecorder 对象提供的几个方法会生成 events 并通过 Action() 方法发送 events 到 Broadcaster 的 channel 队列中。</li>\n<li>3、Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 初始化后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发来的 events。</li>\n<li>4、EventBroadcaster 对 events 有三个处理方法：StartEventWatcher()、StartRecordingToSink()、StartLogging()，StartEventWatcher() 是其中的核心方法，会初始化一个 watcher 注册到 Broadcaster，其余两个处理函数对 StartEventWatcher() 进行了封装，并实现了自己的处理函数。</li>\n<li>5、 Broadcaster 中有一个 map 会保存每一个注册的 watcher，其会将所有的 events 广播给每一个 watcher，每个 watcher 通过它的 ResultChan() 方法从 channel 接收 events。</li>\n<li>6、kubelet 会使用 StartRecordingToSink() 和 StartLogging() 对 events 进行处理，StartRecordingToSink() 处理函数收到 events 后会进行缓存、过滤、聚合而后发送到 apiserver，apiserver 会将 events 保存到 etcd 中，使用 kubectl 或其他客户端可以查看。StartLogging() 仅将 events 保存到 kubelet 的日志中。</li>\n</ul>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/06/09/node_status/\">kubelet 状态上报的方式</a></li><li><a href=\"http://yoursite.com/2019/01/03/kubelet_create_pod/\">kubelet 创建 pod 的流程</a></li><li><a href=\"http://yoursite.com/2018/12/23/kubelet_init/\">kubelet 启动流程分析</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>当集群中的 node 或 pod 异常时，大部分用户会使用 kubectl 查看对应的 events，那么 events 是从何而来的？其实 k8s 中的各个组件会将运行时产生的各种事件汇报到 apiserver，对于 k8s 中的可描述资源，使用 kubectl describe 都可以看到其相关的 events，那 k8s 中又有哪几个组件都上报 events 呢？ </p>\n<p>只要在 <code>k8s.io/kubernetes/cmd</code> 目录下暴力搜索一下就能知道哪些组件会产生 events：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ grep -R -n -i &quot;EventRecorder&quot; .</span><br></pre></td></tr></table></figure></p>\n<p>可以看出，controller-manage、kube-proxy、kube-scheduler、kubelet 都使用了 EventRecorder，本文只讲述 kubelet 中对 Events 的使用。</p>\n<h5 id=\"1、Events-的定义\"><a href=\"#1、Events-的定义\" class=\"headerlink\" title=\"1、Events 的定义\"></a>1、Events 的定义</h5><p>events 在 <code>k8s.io/api/core/v1/types.go</code> 中进行定义,结构体如下所示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type Event struct &#123;</span><br><span class=\"line\">    metav1.TypeMeta `json:&quot;,inline&quot;`</span><br><span class=\"line\">    metav1.ObjectMeta `json:&quot;metadata&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`</span><br><span class=\"line\">    InvolvedObject ObjectReference `json:&quot;involvedObject&quot; protobuf:&quot;bytes,2,opt,name=involvedObject&quot;`</span><br><span class=\"line\">    Reason string `json:&quot;reason,omitempty&quot; protobuf:&quot;bytes,3,opt,name=reason&quot;`</span><br><span class=\"line\">    Message string `json:&quot;message,omitempty&quot; protobuf:&quot;bytes,4,opt,name=message&quot;`</span><br><span class=\"line\">    Source EventSource `json:&quot;source,omitempty&quot; protobuf:&quot;bytes,5,opt,name=source&quot;`</span><br><span class=\"line\">    FirstTimestamp metav1.Time `json:&quot;firstTimestamp,omitempty&quot; protobuf:&quot;bytes,6,opt,name=firstTimestamp&quot;`</span><br><span class=\"line\">    LastTimestamp metav1.Time `json:&quot;lastTimestamp,omitempty&quot; protobuf:&quot;bytes,7,opt,name=lastTimestamp&quot;`</span><br><span class=\"line\">    Count int32 `json:&quot;count,omitempty&quot; protobuf:&quot;varint,8,opt,name=count&quot;`</span><br><span class=\"line\">    Type string `json:&quot;type,omitempty&quot; protobuf:&quot;bytes,9,opt,name=type&quot;`</span><br><span class=\"line\">    EventTime metav1.MicroTime `json:&quot;eventTime,omitempty&quot; protobuf:&quot;bytes,10,opt,name=eventTime&quot;`</span><br><span class=\"line\">    Series *EventSeries `json:&quot;series,omitempty&quot; protobuf:&quot;bytes,11,opt,name=series&quot;`</span><br><span class=\"line\">    Action string `json:&quot;action,omitempty&quot; protobuf:&quot;bytes,12,opt,name=action&quot;`</span><br><span class=\"line\">    Related *ObjectReference `json:&quot;related,omitempty&quot; protobuf:&quot;bytes,13,opt,name=related&quot;`</span><br><span class=\"line\">    ReportingController string `json:&quot;reportingComponent&quot; protobuf:&quot;bytes,14,opt,name=reportingComponent&quot;`</span><br><span class=\"line\">    ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;`</span><br><span class=\"line\">    ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;`</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>其中 InvolvedObject 代表和事件关联的对象，source 代表事件源，使用 kubectl 看到的事件一般包含 Type、Reason、Age、From、Message 几个字段。</p>\n<p>k8s 中 events 目前只有两种类型：”Normal” 和 “Warning”：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/events.png\" alt=\"events 的两种类型\"></p>\n<h5 id=\"2、EventBroadcaster-的初始化\"><a href=\"#2、EventBroadcaster-的初始化\" class=\"headerlink\" title=\"2、EventBroadcaster 的初始化\"></a>2、EventBroadcaster 的初始化</h5><p>events 的整个生命周期都与 EventBroadcaster 有关，kubelet 中对 EventBroadcaster 的初始化在<code>k8s.io/kubernetes/cmd/kubelet/app/server.go</code>中：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  // event 初始化</span><br><span class=\"line\">  makeEventRecorder(kubeDeps, nodeName)</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">func makeEventRecorder(kubeDeps *kubelet.Dependencies, nodeName types.NodeName) &#123;</span><br><span class=\"line\">  if kubeDeps.Recorder != nil &#123;</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  // 初始化 EventBroadcaster </span><br><span class=\"line\">  eventBroadcaster := record.NewBroadcaster()</span><br><span class=\"line\">  // 初始化 EventRecorder</span><br><span class=\"line\">  kubeDeps.Recorder = eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource&#123;Component: componentKubelet, Host: string(nodeName)&#125;)</span><br><span class=\"line\">  // 记录 events 到本地日志</span><br><span class=\"line\">  eventBroadcaster.StartLogging(glog.V(3).Infof)</span><br><span class=\"line\">  if kubeDeps.EventClient != nil &#123;</span><br><span class=\"line\">    glog.V(4).Infof(&quot;Sending events to api server.&quot;)</span><br><span class=\"line\">    // 上报 events 到 apiserver</span><br><span class=\"line\">  eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeDeps.EventClient.Events(&quot;&quot;)&#125;)</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    glog.Warning(&quot;No api server defined - no events will be sent to API server.&quot;)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Kubelet 在启动的时候会初始化一个 EventBroadcaster，它主要是对接收到的 events 做一些后续的处理(保存、上报等），EventBroadcaster 也会被 kubelet 中的其他模块使用，以下是相关的定义，对 events 生成和处理的函数都定义在 <code>k8s.io/client-go/tools/record/event.go</code> 中：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type eventBroadcasterImpl struct &#123;</span><br><span class=\"line\">  *watch.Broadcaster</span><br><span class=\"line\">  sleepDuration time.Duration</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// EventBroadcaster knows how to receive events and send them to any EventSink, watcher, or log.</span><br><span class=\"line\">type EventBroadcaster interface &#123;</span><br><span class=\"line\">  StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface</span><br><span class=\"line\"></span><br><span class=\"line\">  StartRecordingToSink(sink EventSink) watch.Interface</span><br><span class=\"line\"></span><br><span class=\"line\">  StartLogging(logf func(format string, args ...interface&#123;&#125;)) watch.Interface</span><br><span class=\"line\"></span><br><span class=\"line\">  NewRecorder(scheme *runtime.Scheme, source v1.EventSource) EventRecorder</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>EventBroadcaster 是个接口类型，该接口有以下四个方法：</p>\n<ul>\n<li>StartEventWatcher() ： EventBroadcaster 中的核心方法，接收各模块产生的 events，参数为一个处理 events 的函数，用户可以使用 StartEventWatcher() 接收 events 然后使用自定义的 handle 进行处理</li>\n<li>StartRecordingToSink() ： 调用 StartEventWatcher() 接收 events，并将收到的 events 发送到 apiserver </li>\n<li>StartLogging() ：也是调用 StartEventWatcher() 接收 events，然后保存 events 到日志</li>\n<li>NewRecorder() ：会创建一个指定 EventSource 的 EventRecorder，EventSource 指明了哪个节点的哪个组件</li>\n</ul>\n<p>eventBroadcasterImpl 是 eventBroadcaster 实际的对象，初始化 EventBroadcaster 对象的时候会初始化一个 Broadcaster，Broadcaster 会启动一个 goroutine 接收各组件产生的 events 并广播到每一个 watcher。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewBroadcaster() EventBroadcaster &#123;</span><br><span class=\"line\">  return &amp;eventBroadcasterImpl&#123;watch.NewBroadcaster(maxQueuedEvents, watch.DropIfChannelFull), defaultSleepDuration&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>可以看到，kubelet 在初始化完 EventBroadcaster 后会调用 StartRecordingToSink() 和 StartLogging() 两个方法，StartRecordingToSink() 处理函数会将收到的 events 进行缓存、过滤、聚合而后发送到 apiserver，StartLogging() 仅将 events 保存到 kubelet 的日志中。</p>\n<h5 id=\"3、Events-的生成\"><a href=\"#3、Events-的生成\" class=\"headerlink\" title=\"3、Events 的生成\"></a>3、Events 的生成</h5><p>从初始化 EventBroadcaster 的代码中可以看到 kubelet 在初始化完 EventBroadcaster 后紧接着初始化了 EventRecorder，并将已经初始化的 Broadcaster 对象作为参数传给了 EventRecorder，至此，EventBroadcaster、EventRecorder、Broadcaster 三个对象产生了关联。EventRecorder 的主要功能是生成指定格式的 events，以下是相关的定义：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type recorderImpl struct &#123;</span><br><span class=\"line\">  scheme *runtime.Scheme</span><br><span class=\"line\">  source v1.EventSource</span><br><span class=\"line\">  *watch.Broadcaster</span><br><span class=\"line\">  clock clock.Clock</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type EventRecorder interface &#123;</span><br><span class=\"line\">  Event(object runtime.Object, eventtype, reason, message string)</span><br><span class=\"line\"></span><br><span class=\"line\">  Eventf(object runtime.Object, eventtype, reason, messageFmt string, args ...interface&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">  PastEventf(object runtime.Object, timestamp metav1.Time, eventtype, reason, messageFmt string, args ...interface&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">  AnnotatedEventf(object runtime.Object, annotations map[string]string, eventtype, reason, messageFmt string, args ...interface&#123;&#125;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>EventRecorder 中包含的几个方法都是产生指定格式的 events，Event() 和 Eventf() 的功能类似 fmt.Println() 和 fmt.Printf()，kubelet 中的各个模块会调用 EventRecorder 生成 events。recorderImpl 是 EventRecorder 实际的对象。EventRecorder 的每个方法会调用 generateEvent，在 generateEvent 中初始化 events 。</p>\n<p>以下是生成 events 的函数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (recorder *recorderImpl) generateEvent(object runtime.Object, annotations map[string]string, timestamp metav1.Time, eventtype, reason, message string) &#123;</span><br><span class=\"line\">  ref, err := ref.GetReference(recorder.scheme, object)</span><br><span class=\"line\">  if err != nil &#123;</span><br><span class=\"line\">    glog.Errorf(&quot;Could not construct reference to: &apos;%#v&apos; due to: &apos;%v&apos;. Will not report event: &apos;%v&apos; &apos;%v&apos; &apos;%v&apos;&quot;, object, err, eventtype, reason, message)</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  if !validateEventType(eventtype) &#123;</span><br><span class=\"line\">    glog.Errorf(&quot;Unsupported event type: &apos;%v&apos;&quot;, eventtype)</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  event := recorder.makeEvent(ref, annotations, eventtype, reason, message)</span><br><span class=\"line\">  event.Source = recorder.source</span><br><span class=\"line\"></span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    // NOTE: events should be a non-blocking operation</span><br><span class=\"line\">    defer utilruntime.HandleCrash()</span><br><span class=\"line\">    // 发送事件</span><br><span class=\"line\">    recorder.Action(watch.Added, event)</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (recorder *recorderImpl) makeEvent(ref *v1.ObjectReference, annotations map[string]string, eventtype, reason, message string) *v1.Event &#123;</span><br><span class=\"line\">  t := metav1.Time&#123;Time: recorder.clock.Now()&#125;</span><br><span class=\"line\">  namespace := ref.Namespace</span><br><span class=\"line\">  if namespace == &quot;&quot; &#123;</span><br><span class=\"line\">    namespace = metav1.NamespaceDefault</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  return &amp;v1.Event&#123;</span><br><span class=\"line\">    ObjectMeta: metav1.ObjectMeta&#123;</span><br><span class=\"line\">      Name:        fmt.Sprintf(&quot;%v.%x&quot;, ref.Name, t.UnixNano()),</span><br><span class=\"line\">      Namespace:   namespace,</span><br><span class=\"line\">      Annotations: annotations,</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    InvolvedObject: *ref,</span><br><span class=\"line\">    Reason:         reason,</span><br><span class=\"line\">    Message:        message,</span><br><span class=\"line\">    FirstTimestamp: t,</span><br><span class=\"line\">    LastTimestamp:  t,</span><br><span class=\"line\">    Count:          1,</span><br><span class=\"line\">    Type:           eventtype,</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>初始化完 events 后会调用 recorder.Action() 将 events 发送到 Broadcaster 的事件接收队列中, Action() 是 Broadcaster 中的方法。</p>\n<p>以下是 Action() 方法的实现：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *Broadcaster) Action(action EventType, obj runtime.Object) &#123;</span><br><span class=\"line\">  m.incoming &lt;- Event&#123;action, obj&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"4、Events-的广播\"><a href=\"#4、Events-的广播\" class=\"headerlink\" title=\"4、Events 的广播\"></a>4、Events 的广播</h5><p>上面已经说了，EventBroadcaster 初始化时会初始化一个 Broadcaster，Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 的实现在 <code>k8s.io/apimachinery/pkg/watch/mux.go</code> 中，Broadcaster 初始化完成后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发送过来的 events，Broadcaster 中有一个 map 会保存每一个注册的 watcher， 接着将 events 广播给所有的 watcher，每个 watcher 都有一个接收消息的 channel，watcher 可以通过它的 ResultChan() 方法从 channel 中读取数据进行消费。</p>\n<p>以下是 Broadcaster 广播 events 的实现：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *Broadcaster) loop() &#123;</span><br><span class=\"line\">  for event := range m.incoming &#123;</span><br><span class=\"line\">    if event.Type == internalRunFunctionMarker &#123;</span><br><span class=\"line\">      event.Object.(functionFakeRuntimeObject)()</span><br><span class=\"line\">      continue</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    m.distribute(event)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.closeAll()</span><br><span class=\"line\">  m.distributing.Done()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// distribute sends event to all watchers. Blocking.</span><br><span class=\"line\">func (m *Broadcaster) distribute(event Event) &#123;</span><br><span class=\"line\">  m.lock.Lock()</span><br><span class=\"line\">  defer m.lock.Unlock()</span><br><span class=\"line\">  if m.fullChannelBehavior == DropIfChannelFull &#123;</span><br><span class=\"line\">    for _, w := range m.watchers &#123;</span><br><span class=\"line\">      select &#123;</span><br><span class=\"line\">      case w.result &lt;- event:</span><br><span class=\"line\">      case &lt;-w.stopped:</span><br><span class=\"line\">      default: // Don&apos;t block if the event can&apos;t be queued.</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125; else &#123;</span><br><span class=\"line\">    for _, w := range m.watchers &#123;</span><br><span class=\"line\">      select &#123;</span><br><span class=\"line\">      case w.result &lt;- event:</span><br><span class=\"line\">      case &lt;-w.stopped:</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h5 id=\"5、Events-的处理\"><a href=\"#5、Events-的处理\" class=\"headerlink\" title=\"5、Events 的处理\"></a>5、Events 的处理</h5><p>那么 watcher 是从何而来呢？每一个要处理 events 的 client 都需要初始化一个 watcher，处理 events 的方法是在 EventBroadcaster 中定义的，以下是 EventBroadcaster 中对 events 处理的三个函数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface &#123;</span><br><span class=\"line\">  watcher := eventBroadcaster.Watch()</span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    defer utilruntime.HandleCrash()</span><br><span class=\"line\">    for watchEvent := range watcher.ResultChan() &#123;</span><br><span class=\"line\">      event, ok := watchEvent.Object.(*v1.Event)</span><br><span class=\"line\">      if !ok &#123;</span><br><span class=\"line\">        // This is all local, so there&apos;s no reason this should</span><br><span class=\"line\">        // ever happen.</span><br><span class=\"line\">        continue</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      eventHandler(event)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\">  return watcher</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>StartEventWatcher() 首先实例化一个 watcher，每个 watcher 都会被塞入到 Broadcaster 的 watcher 列表中，watcher 从 Broadcaster 提供的 channel 中读取 events，然后再调用 eventHandler 进行处理，StartLogging() 和 StartRecordingToSink() 都是对 StartEventWatcher() 的封装，都会传入自己的处理函数。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) StartLogging(logf func(format string, args ...interface&#123;&#125;)) watch.Interface &#123;</span><br><span class=\"line\">  return eventBroadcaster.StartEventWatcher(</span><br><span class=\"line\">    func(e *v1.Event) &#123;</span><br><span class=\"line\">      logf(&quot;Event(%#v): type: &apos;%v&apos; reason: &apos;%v&apos; %v&quot;, e.InvolvedObject, e.Type, e.Reason, e.Message)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>StartLogging() 传入的 eventHandler 仅将 events 保存到日志中。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) StartRecordingToSink(sink EventSink) watch.Interface &#123;</span><br><span class=\"line\">  // The default math/rand package functions aren&apos;t thread safe, so create a</span><br><span class=\"line\">  // new Rand object for each StartRecording call.</span><br><span class=\"line\">  randGen := rand.New(rand.NewSource(time.Now().UnixNano()))</span><br><span class=\"line\">  eventCorrelator := NewEventCorrelator(clock.RealClock&#123;&#125;)</span><br><span class=\"line\">  return eventBroadcaster.StartEventWatcher(</span><br><span class=\"line\">    func(event *v1.Event) &#123;</span><br><span class=\"line\">      recordToSink(sink, event, eventCorrelator, randGen, eventBroadcaster.sleepDuration)</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func recordToSink(sink EventSink, event *v1.Event, eventCorrelator *EventCorrelator, randGen *rand.Rand, sleepDuration time.Duration) &#123;</span><br><span class=\"line\">  eventCopy := *event</span><br><span class=\"line\">  event = &amp;eventCopy</span><br><span class=\"line\">  result, err := eventCorrelator.EventCorrelate(event)</span><br><span class=\"line\">  if err != nil &#123;</span><br><span class=\"line\">    utilruntime.HandleError(err)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  if result.Skip &#123;</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  tries := 0</span><br><span class=\"line\">  for &#123;</span><br><span class=\"line\">    if recordEvent(sink, result.Event, result.Patch, result.Event.Count &gt; 1, eventCorrelator) &#123;</span><br><span class=\"line\">      break</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    tries++</span><br><span class=\"line\">    if tries &gt;= maxTriesPerEvent &#123;</span><br><span class=\"line\">      glog.Errorf(&quot;Unable to write event &apos;%#v&apos; (retry limit exceeded!)&quot;, event)</span><br><span class=\"line\">      break</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 第一次重试增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件</span><br><span class=\"line\">    if tries == 1 &#123;</span><br><span class=\"line\">      time.Sleep(time.Duration(float64(sleepDuration) * randGen.Float64()))</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      time.Sleep(sleepDuration)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>StartRecordingToSink() 方法先根据当前时间生成一个随机数发生器 randGen，增加随机数是为了在重试时增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件，接着实例化一个EventCorrelator，EventCorrelator 会对事件做一些预处理的工作，其中包括过滤、聚合、缓存等操作，具体代码不做详细分析，最后将 recordToSink() 函数作为处理函数，recordToSink() 会将处理后的 events 发送到 apiserver，这是 StartEventWatcher() 的整个工作流程。</p>\n<h5 id=\"6、Events-简单实现\"><a href=\"#6、Events-简单实现\" class=\"headerlink\" title=\"6、Events 简单实现\"></a>6、Events 简单实现</h5><p>了解完 events 的整个处理流程后，可以参考其实现方式写一个 demo，要实现一个完整的 events 需要包含以下几个功能：</p>\n<ul>\n<li>1、事件的产生</li>\n<li>2、事件的发送</li>\n<li>3、事件广播</li>\n<li>4、事件缓存</li>\n<li>5、事件过滤和聚合</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">  &quot;fmt&quot;</span><br><span class=\"line\">  &quot;sync&quot;</span><br><span class=\"line\">  &quot;time&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">// watcher queue</span><br><span class=\"line\">const queueLength = int64(1)</span><br><span class=\"line\"></span><br><span class=\"line\">// Events xxx</span><br><span class=\"line\">type Events struct &#123;</span><br><span class=\"line\">  Reason    string</span><br><span class=\"line\">  Message   string</span><br><span class=\"line\">  Source    string</span><br><span class=\"line\">  Type      string</span><br><span class=\"line\">  Count     int64</span><br><span class=\"line\">  Timestamp time.Time</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// EventBroadcaster xxx</span><br><span class=\"line\">type EventBroadcaster interface &#123;</span><br><span class=\"line\">  Event(etype, reason, message string)</span><br><span class=\"line\">  StartLogging() Interface</span><br><span class=\"line\">  Stop()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// eventBroadcaster xxx</span><br><span class=\"line\">type eventBroadcasterImpl struct &#123;</span><br><span class=\"line\">  *Broadcaster</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func NewEventBroadcaster() EventBroadcaster &#123;</span><br><span class=\"line\">  return &amp;eventBroadcasterImpl&#123;NewBroadcaster(queueLength)&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) Stop() &#123;</span><br><span class=\"line\">  eventBroadcaster.Shutdown()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// generate event</span><br><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) Event(etype, reason, message string) &#123;</span><br><span class=\"line\">  events := &amp;Events&#123;Type: etype, Reason: reason, Message: message&#125;</span><br><span class=\"line\">  // send event to broadcast</span><br><span class=\"line\">  eventBroadcaster.Action(events)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 仅实现 StartLogging() 的功能，将日志打印</span><br><span class=\"line\">func (eventBroadcaster *eventBroadcasterImpl) StartLogging() Interface &#123;</span><br><span class=\"line\">  // register a watcher</span><br><span class=\"line\">  watcher := eventBroadcaster.Watch()</span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    for watchEvent := range watcher.ResultChan() &#123;</span><br><span class=\"line\">      fmt.Printf(&quot;%v\\n&quot;, watchEvent)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    time.Sleep(time.Second * 4)</span><br><span class=\"line\">    watcher.Stop()</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">  return watcher</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// --------------------</span><br><span class=\"line\">// Broadcaster 定义与实现</span><br><span class=\"line\">// 接收 events channel 的长度</span><br><span class=\"line\">const incomingQueuLength = 100</span><br><span class=\"line\"></span><br><span class=\"line\">type Broadcaster struct &#123;</span><br><span class=\"line\">  lock             sync.Mutex</span><br><span class=\"line\">  incoming         chan Events</span><br><span class=\"line\">  watchers         map[int64]*broadcasterWatcher</span><br><span class=\"line\">  watchersQueue    int64</span><br><span class=\"line\">  watchQueueLength int64</span><br><span class=\"line\">  distributing     sync.WaitGroup</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func NewBroadcaster(queueLength int64) *Broadcaster &#123;</span><br><span class=\"line\">  m := &amp;Broadcaster&#123;</span><br><span class=\"line\">    incoming:         make(chan Events, incomingQueuLength),</span><br><span class=\"line\">    watchers:         map[int64]*broadcasterWatcher&#123;&#125;,</span><br><span class=\"line\">    watchQueueLength: queueLength,</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.distributing.Add(1)</span><br><span class=\"line\">  // 后台启动一个 goroutine 广播 events</span><br><span class=\"line\">  go m.loop()</span><br><span class=\"line\">  return m</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// Broadcaster 接收所产生的 events</span><br><span class=\"line\">func (m *Broadcaster) Action(event *Events) &#123;</span><br><span class=\"line\">  m.incoming &lt;- *event</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 广播 events 到每个 watcher</span><br><span class=\"line\">func (m *Broadcaster) loop() &#123;</span><br><span class=\"line\">  // 从 incoming channel 中读取所接收到的 events</span><br><span class=\"line\">  for event := range m.incoming &#123;</span><br><span class=\"line\">    // 发送 events 到每一个 watcher</span><br><span class=\"line\">    for _, w := range m.watchers &#123;</span><br><span class=\"line\">      select &#123;</span><br><span class=\"line\">      case w.result &lt;- event:</span><br><span class=\"line\">      case &lt;-w.stopped:</span><br><span class=\"line\">      default:</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.closeAll()</span><br><span class=\"line\">  m.distributing.Done()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (m *Broadcaster) Shutdown() &#123;</span><br><span class=\"line\">  close(m.incoming)</span><br><span class=\"line\">  m.distributing.Wait()</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (m *Broadcaster) closeAll() &#123;</span><br><span class=\"line\">  // TODO</span><br><span class=\"line\">  m.lock.Lock()</span><br><span class=\"line\">  defer m.lock.Unlock()</span><br><span class=\"line\">  for _, w := range m.watchers &#123;</span><br><span class=\"line\">    close(w.result)</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.watchers = map[int64]*broadcasterWatcher&#123;&#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (m *Broadcaster) stopWatching(id int64) &#123;</span><br><span class=\"line\">  m.lock.Lock()</span><br><span class=\"line\">  defer m.lock.Unlock()</span><br><span class=\"line\">  w, ok := m.watchers[id]</span><br><span class=\"line\">  if !ok &#123;</span><br><span class=\"line\">    return</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  delete(m.watchers, id)</span><br><span class=\"line\">  close(w.result)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 调用 Watch(）方法注册一个 watcher</span><br><span class=\"line\">func (m *Broadcaster) Watch() Interface &#123;</span><br><span class=\"line\">  watcher := &amp;broadcasterWatcher&#123;</span><br><span class=\"line\">    result:  make(chan Events, incomingQueuLength),</span><br><span class=\"line\">    stopped: make(chan struct&#123;&#125;),</span><br><span class=\"line\">    id:      m.watchQueueLength,</span><br><span class=\"line\">    m:       m,</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  m.watchers[m.watchersQueue] = watcher</span><br><span class=\"line\">  m.watchQueueLength++</span><br><span class=\"line\">  return watcher</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// watcher 实现</span><br><span class=\"line\">type Interface interface &#123;</span><br><span class=\"line\">  Stop()</span><br><span class=\"line\">  ResultChan() &lt;-chan Events</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">type broadcasterWatcher struct &#123;</span><br><span class=\"line\">  result  chan Events</span><br><span class=\"line\">  stopped chan struct&#123;&#125;</span><br><span class=\"line\">  stop    sync.Once</span><br><span class=\"line\">  id      int64</span><br><span class=\"line\">  m       *Broadcaster</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 每个 watcher 通过该方法读取 channel 中广播的 events</span><br><span class=\"line\">func (b *broadcasterWatcher) ResultChan() &lt;-chan Events &#123;</span><br><span class=\"line\">  return b.result</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func (b *broadcasterWatcher) Stop() &#123;</span><br><span class=\"line\">  b.stop.Do(func() &#123;</span><br><span class=\"line\">    close(b.stopped)</span><br><span class=\"line\">    b.m.stopWatching(b.id)</span><br><span class=\"line\">  &#125;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// --------------------</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">  eventBroadcast := NewEventBroadcaster()</span><br><span class=\"line\"></span><br><span class=\"line\">  var wg sync.WaitGroup</span><br><span class=\"line\">  wg.Add(1)</span><br><span class=\"line\">  // producer event</span><br><span class=\"line\">  go func() &#123;</span><br><span class=\"line\">    defer wg.Done()</span><br><span class=\"line\">    time.Sleep(time.Second)</span><br><span class=\"line\">    eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;1&quot;)</span><br><span class=\"line\">    time.Sleep(time.Second * 2)</span><br><span class=\"line\">    eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;2&quot;)</span><br><span class=\"line\">    time.Sleep(time.Second * 3)</span><br><span class=\"line\">    eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;3&quot;)</span><br><span class=\"line\">    //eventBroadcast.Stop()</span><br><span class=\"line\">  &#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">  eventBroadcast.StartLogging()</span><br><span class=\"line\">  wg.Wait()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>此处仅简单实现，将 EventRecorder 处理 events 的功能直接放在了 EventBroadcaster 中实现，对 events 的处理方法仅实现了 StartLogging()，Broadcaster 中的部分功能是直接复制 k8s 中的代码，有一定的精简，其实现值得学习，此处对 EventCorrelator 并没有进行实现。</p>\n<p>代码请参考：<a href=\"https://github.com/gosoon/k8s-learning-notes/tree/master/k8s-package/events\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/k8s-learning-notes/tree/master/k8s-package/events</a></p>\n<h5 id=\"7、总结\"><a href=\"#7、总结\" class=\"headerlink\" title=\"7、总结\"></a>7、总结</h5><p>本文讲述了 k8s 中 events 从产生到展示的一个完整过程，最后也实现了一个简单的 demo，在此将 kubelet 对 events 的整个处理过程再梳理下，其中主要有三个对象 EventBroadcaster、EventRecorder、Broadcaster：</p>\n<ul>\n<li>1、kubelet 首先会初始化 EventBroadcaster 对象，同时会初始化一个 Broadcaster 对象。</li>\n<li>2、kubelet 通过 EventBroadcaster 对象的 NewRecorder() 方法初始化 EventRecorder 对象，EventRecorder 对象提供的几个方法会生成 events 并通过 Action() 方法发送 events 到 Broadcaster 的 channel 队列中。</li>\n<li>3、Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 初始化后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发来的 events。</li>\n<li>4、EventBroadcaster 对 events 有三个处理方法：StartEventWatcher()、StartRecordingToSink()、StartLogging()，StartEventWatcher() 是其中的核心方法，会初始化一个 watcher 注册到 Broadcaster，其余两个处理函数对 StartEventWatcher() 进行了封装，并实现了自己的处理函数。</li>\n<li>5、 Broadcaster 中有一个 map 会保存每一个注册的 watcher，其会将所有的 events 广播给每一个 watcher，每个 watcher 通过它的 ResultChan() 方法从 channel 接收 events。</li>\n<li>6、kubelet 会使用 StartRecordingToSink() 和 StartLogging() 对 events 进行处理，StartRecordingToSink() 处理函数收到 events 后会进行缓存、过滤、聚合而后发送到 apiserver，apiserver 会将 events 保存到 etcd 中，使用 kubectl 或其他客户端可以查看。StartLogging() 仅将 events 保存到 kubelet 的日志中。</li>\n</ul>\n"},{"title":"kubernets 中组件高可用的实现方式","date":"2019-03-12T23:49:30.000Z","type":"component-HA","_content":"生产环境中为了保障业务的稳定性，集群都需要高可用部署，k8s 中 apiserver 是无状态的，可以横向扩容保证其高可用，kube-controller-manager 和 kube-scheduler 两个组件通过 leader 选举保障高可用，即正常情况下 kube-scheduler 或 kube-manager-controller 组件的多个副本只有一个是处于业务逻辑运行状态，其它副本则不断的尝试去获取锁，去竞争 leader，直到自己成为leader。如果正在运行的 leader 因某种原因导致当前进程退出，或者锁丢失，则由其它副本去竞争新的 leader，获取 leader 继而执行业务逻辑。\n\n> kubernetes 版本： v1.12 \n\n#### 组件高可用的使用\n\nk8s 中已经为 kube-controller-manager、kube-scheduler 组件实现了高可用，只需在每个组件的配置文件中添加 `--leader-elect=true` 参数即可启用。在每个组件的日志中可以看到 HA 相关参数的默认值：\n\n```\nI0306 19:17:14.109511  161798 flags.go:33] FLAG: --leader-elect=\"true\"\nI0306 19:17:14.109513  161798 flags.go:33] FLAG: --leader-elect-lease-duration=\"15s\"\nI0306 19:17:14.109516  161798 flags.go:33] FLAG: --leader-elect-renew-deadline=\"10s\"\nI0306 19:17:14.109518  161798 flags.go:33] FLAG: --leader-elect-resource-lock=\"endpoints\"\nI0306 19:17:14.109520  161798 flags.go:33] FLAG: --leader-elect-retry-period=\"2s\"\n```\n\nkubernetes 中查看组件 leader 的方法：\n\n\n```\n$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml && \n  kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml\n```\n\n当前组件 leader 的 hostname 会写在 annotation 的 control-plane.alpha.kubernetes.io/leader 字段里。\n\n\n#### Leader Election 的实现\n\nLeader Election 的过程本质上是一个竞争分布式锁的过程。在 Kubernetes 中，这个分布式锁是以创建 Endpoint 资源的形式进行，谁先创建了该资源，谁就先获得锁，之后会对该资源不断更新以保持锁的拥有权。\n\n\n\n下面开始讲述 kube-controller-manager 中 leader 的竞争过程，cm 在加载及配置完参数后就开始执行 run 方法了。代码在 `k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go` 中：\n\n```\n// Run runs the KubeControllerManagerOptions.  This should never exit.\nfunc Run(c *config.CompletedConfig, stopCh <-chan struct{}) error {\n\t\t...\n\t\t// kube-controller-manager 的核心\n\t\trun := func(ctx context.Context) {\n\t\trootClientBuilder := controller.SimpleControllerClientBuilder{\n\t\t\tClientConfig: c.Kubeconfig,\n\t\t}\n\t\tvar clientBuilder controller.ControllerClientBuilder\n\t\tif c.ComponentConfig.KubeCloudShared.UseServiceAccountCredentials {\n\t\t\tif len(c.ComponentConfig.SAController.ServiceAccountKeyFile) == 0 {\n\t\t\t\t// It'c possible another controller process is creating the tokens for us.\n\t\t\t\t// If one isn't, we'll timeout and exit when our client builder is unable to create the tokens.\n\t\t\t\tglog.Warningf(\"--use-service-account-credentials was specified without providing a --service-account-private-key-file\")\n\t\t\t}\n\t\t\tclientBuilder = controller.SAControllerClientBuilder{\n\t\t\t\tClientConfig:         restclient.AnonymousClientConfig(c.Kubeconfig),\n\t\t\t\tCoreClient:           c.Client.CoreV1(),\n\t\t\t\tAuthenticationClient: c.Client.AuthenticationV1(),\n\t\t\t\tNamespace:            \"kube-system\",\n\t\t\t}\n\t\t} else {\n\t\t\tclientBuilder = rootClientBuilder\n\t\t}\n\t\tcontrollerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done())\n\t\tif err != nil {\n\t\t\tglog.Fatalf(\"error building controller context: %v\", err)\n\t\t}\n\t\tsaTokenControllerInitFunc := serviceAccountTokenControllerStarter{rootClientBuilder: rootClientBuilder}.startServiceAccountTokenController\n\t\t// 初始化及启动所有的 controller\n\t\tif err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil {\n\t\t\tglog.Fatalf(\"error starting controllers: %v\", err)\n\t\t}\n\n\t\tcontrollerContext.InformerFactory.Start(controllerContext.Stop)\n\t\tclose(controllerContext.InformersStarted)\n\n\t\tselect {}\n\t}\n\n    // 如果 LeaderElect 参数未配置,说明 controller-manager 是单点启动的，\n    // 则直接调用 run 方法来启动需要被启动的控制器即可。\n    if !c.ComponentConfig.Generic.LeaderElection.LeaderElect {\n        run(context.TODO())\n        panic(\"unreachable\")\n    }\n\n    // 如果 LeaderElect 参数配置为 true,说明 controller-manager 是以 HA 方式启动的，\n    // 则执行下面的代码进行 leader 选举，选举出的 leader 会回调 run 方法。\n    id, err := os.Hostname()\n    if err != nil {\n        return err\n    }\n\n    // add a uniquifier so that two processes on the same host don't accidentally both become active\n    id = id + \"_\" + string(uuid.NewUUID())\n    \n    // 初始化资源锁\n    rl, err := resourcelock.New(c.ComponentConfig.Generic.LeaderElection.ResourceLock,\n        \"kube-system\",\n        \"kube-controller-manager\",\n        c.LeaderElectionClient.CoreV1(),\n        resourcelock.ResourceLockConfig{\n            Identity:      id,\n            EventRecorder: c.EventRecorder,\n        })\n    if err != nil {\n        glog.Fatalf(\"error creating lock: %v\", err)\n    }\n    // 进入到选举的流程\n    leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig{\n        Lock:          rl,\n        LeaseDuration: c.ComponentConfig.Generic.LeaderElection.LeaseDuration.Duration,\n        RenewDeadline: c.ComponentConfig.Generic.LeaderElection.RenewDeadline.Duration,\n        RetryPeriod:   c.ComponentConfig.Generic.LeaderElection.RetryPeriod.Duration,\n        Callbacks: leaderelection.LeaderCallbacks{\n            OnStartedLeading: run,\n            OnStoppedLeading: func() {\n                glog.Fatalf(\"leaderelection lost\")\n            },\n        },\n        WatchDog: electionChecker,\n        Name:     \"kube-controller-manager\",\n    })\n    panic(\"unreachable\")\n}\n```\n\n- 1、初始化资源锁，kubernetes 中默认的资源锁使用 `endpoints`，也就是 c.ComponentConfig.Generic.LeaderElection.ResourceLock 的值为 \"endpoints\"，在代码中我并没有找到对 ResourceLock 初始化的地方，只看到了对该参数的说明以及日志中配置的默认值：\n\n![](http://cdn.tianfeiyu.com/leader-1.png)\n\n​在初始化资源锁的时候还传入了 EventRecorder，其作用是当 leader 发生变化的时候会将对应的 events 发送到 apiserver。\n\n\n- 2、rl 资源锁被用于 controller-manager 进行 leader 的选举，RunOrDie 方法中就是 leader 的选举过程了。\n\n- 3、Callbacks 中定义了在切换状态后需要执行的操作，当成为 leader 后会执行 OnStartedLeading 中的 run 方法，run 方法是 controller-manager 的核心，run 方法中会初始化并启动所包含资源的 controller，以下是 kube-controller-manager 中所有的 controller：\n\n```\nfunc NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc {\n\tcontrollers := map[string]InitFunc{}\n\tcontrollers[\"endpoint\"] = startEndpointController\n\tcontrollers[\"replicationcontroller\"] = startReplicationController\n\tcontrollers[\"podgc\"] = startPodGCController\n\tcontrollers[\"resourcequota\"] = startResourceQuotaController\n\tcontrollers[\"namespace\"] = startNamespaceController\n\tcontrollers[\"serviceaccount\"] = startServiceAccountController\n\tcontrollers[\"garbagecollector\"] = startGarbageCollectorController\n\tcontrollers[\"daemonset\"] = startDaemonSetController\n\tcontrollers[\"job\"] = startJobController\n\tcontrollers[\"deployment\"] = startDeploymentController\n\tcontrollers[\"replicaset\"] = startReplicaSetController\n\tcontrollers[\"horizontalpodautoscaling\"] = startHPAController\n\tcontrollers[\"disruption\"] = startDisruptionController\n\tcontrollers[\"statefulset\"] = startStatefulSetController\n\tcontrollers[\"cronjob\"] = startCronJobController\n\tcontrollers[\"csrsigning\"] = startCSRSigningController\n\tcontrollers[\"csrapproving\"] = startCSRApprovingController\n\tcontrollers[\"csrcleaner\"] = startCSRCleanerController\n\tcontrollers[\"ttl\"] = startTTLController\n\tcontrollers[\"bootstrapsigner\"] = startBootstrapSignerController\n\tcontrollers[\"tokencleaner\"] = startTokenCleanerController\n\tcontrollers[\"nodeipam\"] = startNodeIpamController\n\tif loopMode == IncludeCloudLoops {\n\t\tcontrollers[\"service\"] = startServiceController\n\t\tcontrollers[\"route\"] = startRouteController\n\t}\n\tcontrollers[\"nodelifecycle\"] = startNodeLifecycleController\n\tcontrollers[\"persistentvolume-binder\"] = startPersistentVolumeBinderController\n\tcontrollers[\"attachdetach\"] = startAttachDetachController\n\tcontrollers[\"persistentvolume-expander\"] = startVolumeExpandController\n\tcontrollers[\"clusterrole-aggregation\"] = startClusterRoleAggregrationController\n\tcontrollers[\"pvc-protection\"] = startPVCProtectionController\n\tcontrollers[\"pv-protection\"] = startPVProtectionController\n\tcontrollers[\"ttl-after-finished\"] = startTTLAfterFinishedController\n\n\treturn controllers\n}\n```\n\nOnStoppedLeading 是从 leader 状态切换为 slave 要执行的操作，此方法仅打印了一条日志。\n\n\n\n```\nfunc RunOrDie(ctx context.Context, lec LeaderElectionConfig) {\n    le, err := NewLeaderElector(lec)\n    if err != nil {\n        panic(err)\n    }\n    if lec.WatchDog != nil {\n        lec.WatchDog.SetLeaderElection(le)\n    }\n    le.Run(ctx)\n}\n```\n\n在 RunOrDie 中首先调用 NewLeaderElector 初始化了一个 LeaderElector 对象，然后执行 LeaderElector 的 run 方法进行选举。\n\n\n```\nfunc (le *LeaderElector) Run(ctx context.Context) {\n\tdefer func() {\n\t\truntime.HandleCrash()\n\t\tle.config.Callbacks.OnStoppedLeading()\n\t}()\n\tif !le.acquire(ctx) {\n\t\treturn // ctx signalled done\n\t}\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tgo le.config.Callbacks.OnStartedLeading(ctx)\n\tle.renew(ctx)\n}\n```\n\nRun 中首先会执行 acquire 尝试获取锁，获取到锁之后会回调 OnStartedLeading 启动所需要的 controller，然后会执行 renew 方法定期更新锁，保持 leader 的状态。\n\n\n```\nfunc (le *LeaderElector) acquire(ctx context.Context) bool {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tsucceeded := false\n\tdesc := le.config.Lock.Describe()\n\tglog.Infof(\"attempting to acquire leader lease  %v...\", desc)\n\twait.JitterUntil(func() {\n\t\t// 尝试创建或者续约资源锁\n\t\tsucceeded = le.tryAcquireOrRenew()\n\t\t// leader 可能发生了改变，在 maybeReportTransition 方法中会\n\t\t// 执行相应的 OnNewLeader() 回调函数,代码中对 OnNewLeader() 并没有初始化\n\t\tle.maybeReportTransition()\n\t\tif !succeeded {\n\t\t\tglog.V(4).Infof(\"failed to acquire lease %v\", desc)\n\t\t\treturn\n\t\t}\n\t\tle.config.Lock.RecordEvent(\"became leader\")\n\t\tglog.Infof(\"successfully acquired lease %v\", desc)\n\t\tcancel()\n\t}, le.config.RetryPeriod, JitterFactor, true, ctx.Done())\n\treturn succeeded\n}\n```\n在 acquire 中首先初始化了一个 ctx，通过 wait.JitterUntil 周期性的去调用 le.tryAcquireOrRenew 方法来获取资源锁，直到获取为止。如果获取不到锁，则会以 RetryPeriod 为间隔不断尝试。如果获取到锁，就会关闭 ctx 通知 wait.JitterUntil 停止尝试，tryAcquireOrRenew 是最核心的方法。\n\n\n\n```\nfunc (le *LeaderElector) tryAcquireOrRenew() bool {\n\tnow := metav1.Now()\n\tleaderElectionRecord := rl.LeaderElectionRecord{\n\t\tHolderIdentity:       le.config.Lock.Identity(),\n\t\tLeaseDurationSeconds: int(le.config.LeaseDuration / time.Second),\n\t\tRenewTime:            now,\n\t\tAcquireTime:          now,\n\t}\n\n\t// 1、获取当前的资源锁\n\toldLeaderElectionRecord, err := le.config.Lock.Get()\n\tif err != nil {\n\t\tif !errors.IsNotFound(err) {\n\t\t\tglog.Errorf(\"error retrieving resource lock %v: %v\", le.config.Lock.Describe(), err)\n\t\t\treturn false\n\t\t}\n\t\t// 没有获取到资源锁，开始创建资源锁，若创建成功则成为 leader \n\t\tif err = le.config.Lock.Create(leaderElectionRecord); err != nil {\n\t\t\tglog.Errorf(\"error initially creating leader election record: %v\", err)\n\t\t\treturn false\n\t\t}\n\t\tle.observedRecord = leaderElectionRecord\n\t\tle.observedTime = le.clock.Now()\n\t\treturn true\n\t}\n\n\t// 2、获取资源锁后检查当前 id 是不是 leader\n\tif !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) {\n\t\tle.observedRecord = *oldLeaderElectionRecord\n\t\tle.observedTime = le.clock.Now()\n\t}\n\t// 如果资源锁没有过期且当前 id 不是 Leader，直接返回\n\tif le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &&\n\t\t!le.IsLeader() {\n\t\tglog.V(4).Infof(\"lock is held by %v and has not yet expired\", oldLeaderElectionRecord.HolderIdentity)\n\t\treturn false\n\t}\n\n\t// 3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，准备续租\n\t// 如果是非 Leader 节点则抢夺资源锁\n\tif le.IsLeader() {\n\t\tleaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime\n\t\tleaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions\n\t} else {\n\t\tleaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1\n\t}\n\n        // 更新资源\n        // 对于 Leader 来说，这是一个续租的过程\n        // 对于非 Leader 节点（仅在上一个资源锁已经过期），这是一个更新锁所有权的过程\n\tif err = le.config.Lock.Update(leaderElectionRecord); err != nil {\n\t\tglog.Errorf(\"Failed to update lock: %v\", err)\n\t\treturn false\n\t}\n\tle.observedRecord = leaderElectionRecord\n\tle.observedTime = le.clock.Now()\n\treturn true\n}\n```\n\n上面的这个函数的主要逻辑：\n- 1、获取 ElectionRecord 记录，如果没有则创建一条新的 ElectionRecord 记录，创建成功则表示获取到锁并成为 leader 了。\n- 2、当获取到资源锁后开始检查其中的信息，比较当前 id 是不是 leader 以及资源锁有没有过期，如果资源锁没有过期且当前 id 不是 Leader，则直接返回。\n- 3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，更新资源锁进行续租。\n- 4、如果当前 id 不是 Leader 但是资源锁已经过期了，则抢夺资源锁，抢夺成功则成为 leader 否则返回。\n\n\n最后是 renew 方法：\n\n```\nfunc (le *LeaderElector) renew(ctx context.Context) {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\twait.Until(func() {\n\t\ttimeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline)\n\t\tdefer timeoutCancel()\n                // 每间隔 RetryPeriod 就执行 tryAcquireOrRenew()\n                // 如果 tryAcquireOrRenew() 返回 false 说明续租失败\n\t\terr := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) {\n\t\t\tdone := make(chan bool, 1)\n\t\t\tgo func() {\n\t\t\t\tdefer close(done)\n\t\t\t\tdone <- le.tryAcquireOrRenew()\n\t\t\t}()\n\n\t\t\tselect {\n\t\t\tcase <-timeoutCtx.Done():\n\t\t\t\treturn false, fmt.Errorf(\"failed to tryAcquireOrRenew %s\", timeoutCtx.Err())\n\t\t\tcase result := <-done:\n\t\t\t\treturn result, nil\n\t\t\t}\n\t\t}, timeoutCtx.Done())\n\n\t\tle.maybeReportTransition()\n\t\tdesc := le.config.Lock.Describe()\n\t\tif err == nil {\n\t\t\tglog.V(4).Infof(\"successfully renewed lease %v\", desc)\n\t\t\treturn\n\t\t}\n\t\t// 续租失败，说明已经不是 Leader，然后程序 panic\n\t\tle.config.Lock.RecordEvent(\"stopped leading\")\n\t\tglog.Infof(\"failed to renew lease %v: %v\", desc, err)\n\t\tcancel()\n\t}, le.config.RetryPeriod, ctx.Done())\n}\n```\n获取到锁之后定期进行更新，renew 只有在获取锁之后才会调用，它会通过持续更新资源锁的数据，来确保继续持有已获得的锁，保持自己的 leader 状态。\n\n\n\n#### Leader Election 功能的使用\n\n以下是一个 demo，使用 k8s 中 `k8s.io/client-go/tools/leaderelection` 进行一个演示：\n\n\n```\npackage main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"os\"\n\t\"time\"\n\n\t\"github.com/golang/glog\"\n\t\"k8s.io/api/core/v1\"\n\t\"k8s.io/client-go/kubernetes\"\n\t\"k8s.io/client-go/kubernetes/scheme\"\n\tv1core \"k8s.io/client-go/kubernetes/typed/core/v1\"\n\t\"k8s.io/client-go/tools/clientcmd\"\n\t\"k8s.io/client-go/tools/leaderelection\"\n\t\"k8s.io/client-go/tools/leaderelection/resourcelock\"\n\t\"k8s.io/client-go/tools/record\"\n)\n\nvar (\n\tmasterURL  string\n\tkubeconfig string\n)\n\nfunc init() {\n\tflag.StringVar(&kubeconfig, \"kubeconfig\", \"\", \"Path to a kubeconfig. Only required if out-of-cluster.\")\n\tflag.StringVar(&masterURL, \"master\", \"\", \"The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.\")\n\n\tflag.Set(\"logtostderr\", \"true\")\n}\n\nfunc main() {\n\tflag.Parse()\n\tdefer glog.Flush()\n\n\tid, err := os.Hostname()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\t\n\t// 加载 kubeconfig 配置\n\tcfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)\n\tif err != nil {\n\t\tglog.Fatalf(\"Error building kubeconfig: %s\", err.Error())\n\t}\n\n\t// 创建 kubeclient\n\tkubeClient, err := kubernetes.NewForConfig(cfg)\n\tif err != nil {\n\t\tglog.Fatalf(\"Error building kubernetes clientset: %s\", err.Error())\n\t}\n\n\t// 初始化 eventRecorder\n\teventBroadcaster := record.NewBroadcaster()\n\teventRecorder := eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"test-1\"})\n\teventBroadcaster.StartLogging(glog.Infof)\n\teventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: kubeClient.CoreV1().Events(\"\")})\n\n\trun := func(ctx context.Context) {\n\t\tfmt.Println(\"run.........\")\n\t\tselect {}\n\t}\n\n\tid = id + \"_\" + \"1\"\n\trl, err := resourcelock.New(\"endpoints\",\n\t\t\"kube-system\",\n\t\t\"test\",\n\t\tkubeClient.CoreV1(),\n\t\tresourcelock.ResourceLockConfig{\n\t\t\tIdentity:      id,\n\t\t\tEventRecorder: eventRecorder,\n\t\t})\n\tif err != nil {\n\t\tglog.Fatalf(\"error creating lock: %v\", err)\n\t}\n\n\tleaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig{\n\t\tLock:          rl,\n\t\tLeaseDuration: 15 * time.Second,\n\t\tRenewDeadline: 10 * time.Second,\n\t\tRetryPeriod:   2 * time.Second,\n\t\tCallbacks: leaderelection.LeaderCallbacks{\n\t\t\tOnStartedLeading: run,\n\t\t\tOnStoppedLeading: func() {\n\t\t\t\tglog.Info(\"leaderelection lost\")\n\t\t\t},\n\t\t},\n\t\tName: \"test-1\",\n\t})\n}\n```\n\n分别使用多个 hostname 同时运行后并测试 leader 切换，可以在 events 中看到 leader 切换的记录：\n\n\n```\n# kubectl describe endpoints test  -n kube-system\nName:         test\nNamespace:    kube-system\nLabels:       <none>\nAnnotations:  control-plane.alpha.kubernetes.io/leader={\"holderIdentity\":\"localhost_2\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2019-03-10T08:47:42Z\",\"renewTime\":\"2019-03-10T08:47:44Z\",\"leaderTransitions\":2}\nSubsets:\nEvents:\n  Type    Reason          Age   From    Message\n  ----    ------          ----  ----    -------\n  Normal  LeaderElection  50s   test-1  localhost_1 became leader\n  Normal  LeaderElection  5s    test-2  localhost_2 became leader\n```\n\n\n#### 总结\n\n本文讲述了 kube-controller-manager 使用 HA 的方式启动后 leader 选举过程的实现说明，k8s 中通过创建 endpoints 资源以及对该资源的持续更新来实现资源锁轮转的过程。但是相对于其他分布式锁的实现，普遍是直接基于现有的中间件实现，比如 redis、zookeeper、etcd 等，其所有对锁的操作都是原子性的，那 k8s 选举过程中的原子操作是如何实现的？k8s 中的原子操作最终也是通过 etcd 实现的，其在做 update 更新锁的操作时采用的是乐观锁，通过对比 resourceVersion 实现的，详细的实现下节再讲。\n\n![api resource](http://cdn.tianfeiyu.com/api-resource-1.png)\n\n\n\n参考文档：\n[API OVERVIEW](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/)\n[Simple leader election with Kubernetes and Docker](https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/)\n\n\n","source":"_posts/k8s_leader_election.md","raw":"---\ntitle: kubernets 中组件高可用的实现方式\ndate: 2019-03-13 07:49:30\ntags: [\"leader-election\",\"component\"]\ntype: \"component-HA\"\n\n---\n生产环境中为了保障业务的稳定性，集群都需要高可用部署，k8s 中 apiserver 是无状态的，可以横向扩容保证其高可用，kube-controller-manager 和 kube-scheduler 两个组件通过 leader 选举保障高可用，即正常情况下 kube-scheduler 或 kube-manager-controller 组件的多个副本只有一个是处于业务逻辑运行状态，其它副本则不断的尝试去获取锁，去竞争 leader，直到自己成为leader。如果正在运行的 leader 因某种原因导致当前进程退出，或者锁丢失，则由其它副本去竞争新的 leader，获取 leader 继而执行业务逻辑。\n\n> kubernetes 版本： v1.12 \n\n#### 组件高可用的使用\n\nk8s 中已经为 kube-controller-manager、kube-scheduler 组件实现了高可用，只需在每个组件的配置文件中添加 `--leader-elect=true` 参数即可启用。在每个组件的日志中可以看到 HA 相关参数的默认值：\n\n```\nI0306 19:17:14.109511  161798 flags.go:33] FLAG: --leader-elect=\"true\"\nI0306 19:17:14.109513  161798 flags.go:33] FLAG: --leader-elect-lease-duration=\"15s\"\nI0306 19:17:14.109516  161798 flags.go:33] FLAG: --leader-elect-renew-deadline=\"10s\"\nI0306 19:17:14.109518  161798 flags.go:33] FLAG: --leader-elect-resource-lock=\"endpoints\"\nI0306 19:17:14.109520  161798 flags.go:33] FLAG: --leader-elect-retry-period=\"2s\"\n```\n\nkubernetes 中查看组件 leader 的方法：\n\n\n```\n$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml && \n  kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml\n```\n\n当前组件 leader 的 hostname 会写在 annotation 的 control-plane.alpha.kubernetes.io/leader 字段里。\n\n\n#### Leader Election 的实现\n\nLeader Election 的过程本质上是一个竞争分布式锁的过程。在 Kubernetes 中，这个分布式锁是以创建 Endpoint 资源的形式进行，谁先创建了该资源，谁就先获得锁，之后会对该资源不断更新以保持锁的拥有权。\n\n\n\n下面开始讲述 kube-controller-manager 中 leader 的竞争过程，cm 在加载及配置完参数后就开始执行 run 方法了。代码在 `k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go` 中：\n\n```\n// Run runs the KubeControllerManagerOptions.  This should never exit.\nfunc Run(c *config.CompletedConfig, stopCh <-chan struct{}) error {\n\t\t...\n\t\t// kube-controller-manager 的核心\n\t\trun := func(ctx context.Context) {\n\t\trootClientBuilder := controller.SimpleControllerClientBuilder{\n\t\t\tClientConfig: c.Kubeconfig,\n\t\t}\n\t\tvar clientBuilder controller.ControllerClientBuilder\n\t\tif c.ComponentConfig.KubeCloudShared.UseServiceAccountCredentials {\n\t\t\tif len(c.ComponentConfig.SAController.ServiceAccountKeyFile) == 0 {\n\t\t\t\t// It'c possible another controller process is creating the tokens for us.\n\t\t\t\t// If one isn't, we'll timeout and exit when our client builder is unable to create the tokens.\n\t\t\t\tglog.Warningf(\"--use-service-account-credentials was specified without providing a --service-account-private-key-file\")\n\t\t\t}\n\t\t\tclientBuilder = controller.SAControllerClientBuilder{\n\t\t\t\tClientConfig:         restclient.AnonymousClientConfig(c.Kubeconfig),\n\t\t\t\tCoreClient:           c.Client.CoreV1(),\n\t\t\t\tAuthenticationClient: c.Client.AuthenticationV1(),\n\t\t\t\tNamespace:            \"kube-system\",\n\t\t\t}\n\t\t} else {\n\t\t\tclientBuilder = rootClientBuilder\n\t\t}\n\t\tcontrollerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done())\n\t\tif err != nil {\n\t\t\tglog.Fatalf(\"error building controller context: %v\", err)\n\t\t}\n\t\tsaTokenControllerInitFunc := serviceAccountTokenControllerStarter{rootClientBuilder: rootClientBuilder}.startServiceAccountTokenController\n\t\t// 初始化及启动所有的 controller\n\t\tif err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil {\n\t\t\tglog.Fatalf(\"error starting controllers: %v\", err)\n\t\t}\n\n\t\tcontrollerContext.InformerFactory.Start(controllerContext.Stop)\n\t\tclose(controllerContext.InformersStarted)\n\n\t\tselect {}\n\t}\n\n    // 如果 LeaderElect 参数未配置,说明 controller-manager 是单点启动的，\n    // 则直接调用 run 方法来启动需要被启动的控制器即可。\n    if !c.ComponentConfig.Generic.LeaderElection.LeaderElect {\n        run(context.TODO())\n        panic(\"unreachable\")\n    }\n\n    // 如果 LeaderElect 参数配置为 true,说明 controller-manager 是以 HA 方式启动的，\n    // 则执行下面的代码进行 leader 选举，选举出的 leader 会回调 run 方法。\n    id, err := os.Hostname()\n    if err != nil {\n        return err\n    }\n\n    // add a uniquifier so that two processes on the same host don't accidentally both become active\n    id = id + \"_\" + string(uuid.NewUUID())\n    \n    // 初始化资源锁\n    rl, err := resourcelock.New(c.ComponentConfig.Generic.LeaderElection.ResourceLock,\n        \"kube-system\",\n        \"kube-controller-manager\",\n        c.LeaderElectionClient.CoreV1(),\n        resourcelock.ResourceLockConfig{\n            Identity:      id,\n            EventRecorder: c.EventRecorder,\n        })\n    if err != nil {\n        glog.Fatalf(\"error creating lock: %v\", err)\n    }\n    // 进入到选举的流程\n    leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig{\n        Lock:          rl,\n        LeaseDuration: c.ComponentConfig.Generic.LeaderElection.LeaseDuration.Duration,\n        RenewDeadline: c.ComponentConfig.Generic.LeaderElection.RenewDeadline.Duration,\n        RetryPeriod:   c.ComponentConfig.Generic.LeaderElection.RetryPeriod.Duration,\n        Callbacks: leaderelection.LeaderCallbacks{\n            OnStartedLeading: run,\n            OnStoppedLeading: func() {\n                glog.Fatalf(\"leaderelection lost\")\n            },\n        },\n        WatchDog: electionChecker,\n        Name:     \"kube-controller-manager\",\n    })\n    panic(\"unreachable\")\n}\n```\n\n- 1、初始化资源锁，kubernetes 中默认的资源锁使用 `endpoints`，也就是 c.ComponentConfig.Generic.LeaderElection.ResourceLock 的值为 \"endpoints\"，在代码中我并没有找到对 ResourceLock 初始化的地方，只看到了对该参数的说明以及日志中配置的默认值：\n\n![](http://cdn.tianfeiyu.com/leader-1.png)\n\n​在初始化资源锁的时候还传入了 EventRecorder，其作用是当 leader 发生变化的时候会将对应的 events 发送到 apiserver。\n\n\n- 2、rl 资源锁被用于 controller-manager 进行 leader 的选举，RunOrDie 方法中就是 leader 的选举过程了。\n\n- 3、Callbacks 中定义了在切换状态后需要执行的操作，当成为 leader 后会执行 OnStartedLeading 中的 run 方法，run 方法是 controller-manager 的核心，run 方法中会初始化并启动所包含资源的 controller，以下是 kube-controller-manager 中所有的 controller：\n\n```\nfunc NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc {\n\tcontrollers := map[string]InitFunc{}\n\tcontrollers[\"endpoint\"] = startEndpointController\n\tcontrollers[\"replicationcontroller\"] = startReplicationController\n\tcontrollers[\"podgc\"] = startPodGCController\n\tcontrollers[\"resourcequota\"] = startResourceQuotaController\n\tcontrollers[\"namespace\"] = startNamespaceController\n\tcontrollers[\"serviceaccount\"] = startServiceAccountController\n\tcontrollers[\"garbagecollector\"] = startGarbageCollectorController\n\tcontrollers[\"daemonset\"] = startDaemonSetController\n\tcontrollers[\"job\"] = startJobController\n\tcontrollers[\"deployment\"] = startDeploymentController\n\tcontrollers[\"replicaset\"] = startReplicaSetController\n\tcontrollers[\"horizontalpodautoscaling\"] = startHPAController\n\tcontrollers[\"disruption\"] = startDisruptionController\n\tcontrollers[\"statefulset\"] = startStatefulSetController\n\tcontrollers[\"cronjob\"] = startCronJobController\n\tcontrollers[\"csrsigning\"] = startCSRSigningController\n\tcontrollers[\"csrapproving\"] = startCSRApprovingController\n\tcontrollers[\"csrcleaner\"] = startCSRCleanerController\n\tcontrollers[\"ttl\"] = startTTLController\n\tcontrollers[\"bootstrapsigner\"] = startBootstrapSignerController\n\tcontrollers[\"tokencleaner\"] = startTokenCleanerController\n\tcontrollers[\"nodeipam\"] = startNodeIpamController\n\tif loopMode == IncludeCloudLoops {\n\t\tcontrollers[\"service\"] = startServiceController\n\t\tcontrollers[\"route\"] = startRouteController\n\t}\n\tcontrollers[\"nodelifecycle\"] = startNodeLifecycleController\n\tcontrollers[\"persistentvolume-binder\"] = startPersistentVolumeBinderController\n\tcontrollers[\"attachdetach\"] = startAttachDetachController\n\tcontrollers[\"persistentvolume-expander\"] = startVolumeExpandController\n\tcontrollers[\"clusterrole-aggregation\"] = startClusterRoleAggregrationController\n\tcontrollers[\"pvc-protection\"] = startPVCProtectionController\n\tcontrollers[\"pv-protection\"] = startPVProtectionController\n\tcontrollers[\"ttl-after-finished\"] = startTTLAfterFinishedController\n\n\treturn controllers\n}\n```\n\nOnStoppedLeading 是从 leader 状态切换为 slave 要执行的操作，此方法仅打印了一条日志。\n\n\n\n```\nfunc RunOrDie(ctx context.Context, lec LeaderElectionConfig) {\n    le, err := NewLeaderElector(lec)\n    if err != nil {\n        panic(err)\n    }\n    if lec.WatchDog != nil {\n        lec.WatchDog.SetLeaderElection(le)\n    }\n    le.Run(ctx)\n}\n```\n\n在 RunOrDie 中首先调用 NewLeaderElector 初始化了一个 LeaderElector 对象，然后执行 LeaderElector 的 run 方法进行选举。\n\n\n```\nfunc (le *LeaderElector) Run(ctx context.Context) {\n\tdefer func() {\n\t\truntime.HandleCrash()\n\t\tle.config.Callbacks.OnStoppedLeading()\n\t}()\n\tif !le.acquire(ctx) {\n\t\treturn // ctx signalled done\n\t}\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tgo le.config.Callbacks.OnStartedLeading(ctx)\n\tle.renew(ctx)\n}\n```\n\nRun 中首先会执行 acquire 尝试获取锁，获取到锁之后会回调 OnStartedLeading 启动所需要的 controller，然后会执行 renew 方法定期更新锁，保持 leader 的状态。\n\n\n```\nfunc (le *LeaderElector) acquire(ctx context.Context) bool {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tsucceeded := false\n\tdesc := le.config.Lock.Describe()\n\tglog.Infof(\"attempting to acquire leader lease  %v...\", desc)\n\twait.JitterUntil(func() {\n\t\t// 尝试创建或者续约资源锁\n\t\tsucceeded = le.tryAcquireOrRenew()\n\t\t// leader 可能发生了改变，在 maybeReportTransition 方法中会\n\t\t// 执行相应的 OnNewLeader() 回调函数,代码中对 OnNewLeader() 并没有初始化\n\t\tle.maybeReportTransition()\n\t\tif !succeeded {\n\t\t\tglog.V(4).Infof(\"failed to acquire lease %v\", desc)\n\t\t\treturn\n\t\t}\n\t\tle.config.Lock.RecordEvent(\"became leader\")\n\t\tglog.Infof(\"successfully acquired lease %v\", desc)\n\t\tcancel()\n\t}, le.config.RetryPeriod, JitterFactor, true, ctx.Done())\n\treturn succeeded\n}\n```\n在 acquire 中首先初始化了一个 ctx，通过 wait.JitterUntil 周期性的去调用 le.tryAcquireOrRenew 方法来获取资源锁，直到获取为止。如果获取不到锁，则会以 RetryPeriod 为间隔不断尝试。如果获取到锁，就会关闭 ctx 通知 wait.JitterUntil 停止尝试，tryAcquireOrRenew 是最核心的方法。\n\n\n\n```\nfunc (le *LeaderElector) tryAcquireOrRenew() bool {\n\tnow := metav1.Now()\n\tleaderElectionRecord := rl.LeaderElectionRecord{\n\t\tHolderIdentity:       le.config.Lock.Identity(),\n\t\tLeaseDurationSeconds: int(le.config.LeaseDuration / time.Second),\n\t\tRenewTime:            now,\n\t\tAcquireTime:          now,\n\t}\n\n\t// 1、获取当前的资源锁\n\toldLeaderElectionRecord, err := le.config.Lock.Get()\n\tif err != nil {\n\t\tif !errors.IsNotFound(err) {\n\t\t\tglog.Errorf(\"error retrieving resource lock %v: %v\", le.config.Lock.Describe(), err)\n\t\t\treturn false\n\t\t}\n\t\t// 没有获取到资源锁，开始创建资源锁，若创建成功则成为 leader \n\t\tif err = le.config.Lock.Create(leaderElectionRecord); err != nil {\n\t\t\tglog.Errorf(\"error initially creating leader election record: %v\", err)\n\t\t\treturn false\n\t\t}\n\t\tle.observedRecord = leaderElectionRecord\n\t\tle.observedTime = le.clock.Now()\n\t\treturn true\n\t}\n\n\t// 2、获取资源锁后检查当前 id 是不是 leader\n\tif !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) {\n\t\tle.observedRecord = *oldLeaderElectionRecord\n\t\tle.observedTime = le.clock.Now()\n\t}\n\t// 如果资源锁没有过期且当前 id 不是 Leader，直接返回\n\tif le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &&\n\t\t!le.IsLeader() {\n\t\tglog.V(4).Infof(\"lock is held by %v and has not yet expired\", oldLeaderElectionRecord.HolderIdentity)\n\t\treturn false\n\t}\n\n\t// 3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，准备续租\n\t// 如果是非 Leader 节点则抢夺资源锁\n\tif le.IsLeader() {\n\t\tleaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime\n\t\tleaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions\n\t} else {\n\t\tleaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1\n\t}\n\n        // 更新资源\n        // 对于 Leader 来说，这是一个续租的过程\n        // 对于非 Leader 节点（仅在上一个资源锁已经过期），这是一个更新锁所有权的过程\n\tif err = le.config.Lock.Update(leaderElectionRecord); err != nil {\n\t\tglog.Errorf(\"Failed to update lock: %v\", err)\n\t\treturn false\n\t}\n\tle.observedRecord = leaderElectionRecord\n\tle.observedTime = le.clock.Now()\n\treturn true\n}\n```\n\n上面的这个函数的主要逻辑：\n- 1、获取 ElectionRecord 记录，如果没有则创建一条新的 ElectionRecord 记录，创建成功则表示获取到锁并成为 leader 了。\n- 2、当获取到资源锁后开始检查其中的信息，比较当前 id 是不是 leader 以及资源锁有没有过期，如果资源锁没有过期且当前 id 不是 Leader，则直接返回。\n- 3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，更新资源锁进行续租。\n- 4、如果当前 id 不是 Leader 但是资源锁已经过期了，则抢夺资源锁，抢夺成功则成为 leader 否则返回。\n\n\n最后是 renew 方法：\n\n```\nfunc (le *LeaderElector) renew(ctx context.Context) {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\twait.Until(func() {\n\t\ttimeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline)\n\t\tdefer timeoutCancel()\n                // 每间隔 RetryPeriod 就执行 tryAcquireOrRenew()\n                // 如果 tryAcquireOrRenew() 返回 false 说明续租失败\n\t\terr := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) {\n\t\t\tdone := make(chan bool, 1)\n\t\t\tgo func() {\n\t\t\t\tdefer close(done)\n\t\t\t\tdone <- le.tryAcquireOrRenew()\n\t\t\t}()\n\n\t\t\tselect {\n\t\t\tcase <-timeoutCtx.Done():\n\t\t\t\treturn false, fmt.Errorf(\"failed to tryAcquireOrRenew %s\", timeoutCtx.Err())\n\t\t\tcase result := <-done:\n\t\t\t\treturn result, nil\n\t\t\t}\n\t\t}, timeoutCtx.Done())\n\n\t\tle.maybeReportTransition()\n\t\tdesc := le.config.Lock.Describe()\n\t\tif err == nil {\n\t\t\tglog.V(4).Infof(\"successfully renewed lease %v\", desc)\n\t\t\treturn\n\t\t}\n\t\t// 续租失败，说明已经不是 Leader，然后程序 panic\n\t\tle.config.Lock.RecordEvent(\"stopped leading\")\n\t\tglog.Infof(\"failed to renew lease %v: %v\", desc, err)\n\t\tcancel()\n\t}, le.config.RetryPeriod, ctx.Done())\n}\n```\n获取到锁之后定期进行更新，renew 只有在获取锁之后才会调用，它会通过持续更新资源锁的数据，来确保继续持有已获得的锁，保持自己的 leader 状态。\n\n\n\n#### Leader Election 功能的使用\n\n以下是一个 demo，使用 k8s 中 `k8s.io/client-go/tools/leaderelection` 进行一个演示：\n\n\n```\npackage main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"os\"\n\t\"time\"\n\n\t\"github.com/golang/glog\"\n\t\"k8s.io/api/core/v1\"\n\t\"k8s.io/client-go/kubernetes\"\n\t\"k8s.io/client-go/kubernetes/scheme\"\n\tv1core \"k8s.io/client-go/kubernetes/typed/core/v1\"\n\t\"k8s.io/client-go/tools/clientcmd\"\n\t\"k8s.io/client-go/tools/leaderelection\"\n\t\"k8s.io/client-go/tools/leaderelection/resourcelock\"\n\t\"k8s.io/client-go/tools/record\"\n)\n\nvar (\n\tmasterURL  string\n\tkubeconfig string\n)\n\nfunc init() {\n\tflag.StringVar(&kubeconfig, \"kubeconfig\", \"\", \"Path to a kubeconfig. Only required if out-of-cluster.\")\n\tflag.StringVar(&masterURL, \"master\", \"\", \"The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.\")\n\n\tflag.Set(\"logtostderr\", \"true\")\n}\n\nfunc main() {\n\tflag.Parse()\n\tdefer glog.Flush()\n\n\tid, err := os.Hostname()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\t\n\t// 加载 kubeconfig 配置\n\tcfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)\n\tif err != nil {\n\t\tglog.Fatalf(\"Error building kubeconfig: %s\", err.Error())\n\t}\n\n\t// 创建 kubeclient\n\tkubeClient, err := kubernetes.NewForConfig(cfg)\n\tif err != nil {\n\t\tglog.Fatalf(\"Error building kubernetes clientset: %s\", err.Error())\n\t}\n\n\t// 初始化 eventRecorder\n\teventBroadcaster := record.NewBroadcaster()\n\teventRecorder := eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"test-1\"})\n\teventBroadcaster.StartLogging(glog.Infof)\n\teventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: kubeClient.CoreV1().Events(\"\")})\n\n\trun := func(ctx context.Context) {\n\t\tfmt.Println(\"run.........\")\n\t\tselect {}\n\t}\n\n\tid = id + \"_\" + \"1\"\n\trl, err := resourcelock.New(\"endpoints\",\n\t\t\"kube-system\",\n\t\t\"test\",\n\t\tkubeClient.CoreV1(),\n\t\tresourcelock.ResourceLockConfig{\n\t\t\tIdentity:      id,\n\t\t\tEventRecorder: eventRecorder,\n\t\t})\n\tif err != nil {\n\t\tglog.Fatalf(\"error creating lock: %v\", err)\n\t}\n\n\tleaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig{\n\t\tLock:          rl,\n\t\tLeaseDuration: 15 * time.Second,\n\t\tRenewDeadline: 10 * time.Second,\n\t\tRetryPeriod:   2 * time.Second,\n\t\tCallbacks: leaderelection.LeaderCallbacks{\n\t\t\tOnStartedLeading: run,\n\t\t\tOnStoppedLeading: func() {\n\t\t\t\tglog.Info(\"leaderelection lost\")\n\t\t\t},\n\t\t},\n\t\tName: \"test-1\",\n\t})\n}\n```\n\n分别使用多个 hostname 同时运行后并测试 leader 切换，可以在 events 中看到 leader 切换的记录：\n\n\n```\n# kubectl describe endpoints test  -n kube-system\nName:         test\nNamespace:    kube-system\nLabels:       <none>\nAnnotations:  control-plane.alpha.kubernetes.io/leader={\"holderIdentity\":\"localhost_2\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2019-03-10T08:47:42Z\",\"renewTime\":\"2019-03-10T08:47:44Z\",\"leaderTransitions\":2}\nSubsets:\nEvents:\n  Type    Reason          Age   From    Message\n  ----    ------          ----  ----    -------\n  Normal  LeaderElection  50s   test-1  localhost_1 became leader\n  Normal  LeaderElection  5s    test-2  localhost_2 became leader\n```\n\n\n#### 总结\n\n本文讲述了 kube-controller-manager 使用 HA 的方式启动后 leader 选举过程的实现说明，k8s 中通过创建 endpoints 资源以及对该资源的持续更新来实现资源锁轮转的过程。但是相对于其他分布式锁的实现，普遍是直接基于现有的中间件实现，比如 redis、zookeeper、etcd 等，其所有对锁的操作都是原子性的，那 k8s 选举过程中的原子操作是如何实现的？k8s 中的原子操作最终也是通过 etcd 实现的，其在做 update 更新锁的操作时采用的是乐观锁，通过对比 resourceVersion 实现的，详细的实现下节再讲。\n\n![api resource](http://cdn.tianfeiyu.com/api-resource-1.png)\n\n\n\n参考文档：\n[API OVERVIEW](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/)\n[Simple leader election with Kubernetes and Docker](https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/)\n\n\n","slug":"k8s_leader_election","published":1,"updated":"2019-07-21T10:01:02.574Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59g000oapwn9m1mzxxy","content":"<p>生产环境中为了保障业务的稳定性，集群都需要高可用部署，k8s 中 apiserver 是无状态的，可以横向扩容保证其高可用，kube-controller-manager 和 kube-scheduler 两个组件通过 leader 选举保障高可用，即正常情况下 kube-scheduler 或 kube-manager-controller 组件的多个副本只有一个是处于业务逻辑运行状态，其它副本则不断的尝试去获取锁，去竞争 leader，直到自己成为leader。如果正在运行的 leader 因某种原因导致当前进程退出，或者锁丢失，则由其它副本去竞争新的 leader，获取 leader 继而执行业务逻辑。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<h4 id=\"组件高可用的使用\"><a href=\"#组件高可用的使用\" class=\"headerlink\" title=\"组件高可用的使用\"></a>组件高可用的使用</h4><p>k8s 中已经为 kube-controller-manager、kube-scheduler 组件实现了高可用，只需在每个组件的配置文件中添加 <code>--leader-elect=true</code> 参数即可启用。在每个组件的日志中可以看到 HA 相关参数的默认值：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I0306 19:17:14.109511  161798 flags.go:33] FLAG: --leader-elect=&quot;true&quot;</span><br><span class=\"line\">I0306 19:17:14.109513  161798 flags.go:33] FLAG: --leader-elect-lease-duration=&quot;15s&quot;</span><br><span class=\"line\">I0306 19:17:14.109516  161798 flags.go:33] FLAG: --leader-elect-renew-deadline=&quot;10s&quot;</span><br><span class=\"line\">I0306 19:17:14.109518  161798 flags.go:33] FLAG: --leader-elect-resource-lock=&quot;endpoints&quot;</span><br><span class=\"line\">I0306 19:17:14.109520  161798 flags.go:33] FLAG: --leader-elect-retry-period=&quot;2s&quot;</span><br></pre></td></tr></table></figure>\n<p>kubernetes 中查看组件 leader 的方法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml &amp;&amp; </span><br><span class=\"line\">  kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml</span><br></pre></td></tr></table></figure>\n<p>当前组件 leader 的 hostname 会写在 annotation 的 control-plane.alpha.kubernetes.io/leader 字段里。</p>\n<h4 id=\"Leader-Election-的实现\"><a href=\"#Leader-Election-的实现\" class=\"headerlink\" title=\"Leader Election 的实现\"></a>Leader Election 的实现</h4><p>Leader Election 的过程本质上是一个竞争分布式锁的过程。在 Kubernetes 中，这个分布式锁是以创建 Endpoint 资源的形式进行，谁先创建了该资源，谁就先获得锁，之后会对该资源不断更新以保持锁的拥有权。</p>\n<p>下面开始讲述 kube-controller-manager 中 leader 的竞争过程，cm 在加载及配置完参数后就开始执行 run 方法了。代码在 <code>k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go</code> 中：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Run runs the KubeControllerManagerOptions.  This should never exit.</span><br><span class=\"line\">func Run(c *config.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;) error &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// kube-controller-manager 的核心</span><br><span class=\"line\">\t\trun := func(ctx context.Context) &#123;</span><br><span class=\"line\">\t\trootClientBuilder := controller.SimpleControllerClientBuilder&#123;</span><br><span class=\"line\">\t\t\tClientConfig: c.Kubeconfig,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tvar clientBuilder controller.ControllerClientBuilder</span><br><span class=\"line\">\t\tif c.ComponentConfig.KubeCloudShared.UseServiceAccountCredentials &#123;</span><br><span class=\"line\">\t\t\tif len(c.ComponentConfig.SAController.ServiceAccountKeyFile) == 0 &#123;</span><br><span class=\"line\">\t\t\t\t// It&apos;c possible another controller process is creating the tokens for us.</span><br><span class=\"line\">\t\t\t\t// If one isn&apos;t, we&apos;ll timeout and exit when our client builder is unable to create the tokens.</span><br><span class=\"line\">\t\t\t\tglog.Warningf(&quot;--use-service-account-credentials was specified without providing a --service-account-private-key-file&quot;)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tclientBuilder = controller.SAControllerClientBuilder&#123;</span><br><span class=\"line\">\t\t\t\tClientConfig:         restclient.AnonymousClientConfig(c.Kubeconfig),</span><br><span class=\"line\">\t\t\t\tCoreClient:           c.Client.CoreV1(),</span><br><span class=\"line\">\t\t\t\tAuthenticationClient: c.Client.AuthenticationV1(),</span><br><span class=\"line\">\t\t\t\tNamespace:            &quot;kube-system&quot;,</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\tclientBuilder = rootClientBuilder</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tcontrollerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done())</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Fatalf(&quot;error building controller context: %v&quot;, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tsaTokenControllerInitFunc := serviceAccountTokenControllerStarter&#123;rootClientBuilder: rootClientBuilder&#125;.startServiceAccountTokenController</span><br><span class=\"line\">\t\t// 初始化及启动所有的 controller</span><br><span class=\"line\">\t\tif err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Fatalf(&quot;error starting controllers: %v&quot;, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tcontrollerContext.InformerFactory.Start(controllerContext.Stop)</span><br><span class=\"line\">\t\tclose(controllerContext.InformersStarted)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tselect &#123;&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 如果 LeaderElect 参数未配置,说明 controller-manager 是单点启动的，</span><br><span class=\"line\">    // 则直接调用 run 方法来启动需要被启动的控制器即可。</span><br><span class=\"line\">    if !c.ComponentConfig.Generic.LeaderElection.LeaderElect &#123;</span><br><span class=\"line\">        run(context.TODO())</span><br><span class=\"line\">        panic(&quot;unreachable&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 如果 LeaderElect 参数配置为 true,说明 controller-manager 是以 HA 方式启动的，</span><br><span class=\"line\">    // 则执行下面的代码进行 leader 选举，选举出的 leader 会回调 run 方法。</span><br><span class=\"line\">    id, err := os.Hostname()</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // add a uniquifier so that two processes on the same host don&apos;t accidentally both become active</span><br><span class=\"line\">    id = id + &quot;_&quot; + string(uuid.NewUUID())</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 初始化资源锁</span><br><span class=\"line\">    rl, err := resourcelock.New(c.ComponentConfig.Generic.LeaderElection.ResourceLock,</span><br><span class=\"line\">        &quot;kube-system&quot;,</span><br><span class=\"line\">        &quot;kube-controller-manager&quot;,</span><br><span class=\"line\">        c.LeaderElectionClient.CoreV1(),</span><br><span class=\"line\">        resourcelock.ResourceLockConfig&#123;</span><br><span class=\"line\">            Identity:      id,</span><br><span class=\"line\">            EventRecorder: c.EventRecorder,</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        glog.Fatalf(&quot;error creating lock: %v&quot;, err)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 进入到选举的流程</span><br><span class=\"line\">    leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig&#123;</span><br><span class=\"line\">        Lock:          rl,</span><br><span class=\"line\">        LeaseDuration: c.ComponentConfig.Generic.LeaderElection.LeaseDuration.Duration,</span><br><span class=\"line\">        RenewDeadline: c.ComponentConfig.Generic.LeaderElection.RenewDeadline.Duration,</span><br><span class=\"line\">        RetryPeriod:   c.ComponentConfig.Generic.LeaderElection.RetryPeriod.Duration,</span><br><span class=\"line\">        Callbacks: leaderelection.LeaderCallbacks&#123;</span><br><span class=\"line\">            OnStartedLeading: run,</span><br><span class=\"line\">            OnStoppedLeading: func() &#123;</span><br><span class=\"line\">                glog.Fatalf(&quot;leaderelection lost&quot;)</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        WatchDog: electionChecker,</span><br><span class=\"line\">        Name:     &quot;kube-controller-manager&quot;,</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    panic(&quot;unreachable&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>1、初始化资源锁，kubernetes 中默认的资源锁使用 <code>endpoints</code>，也就是 c.ComponentConfig.Generic.LeaderElection.ResourceLock 的值为 “endpoints”，在代码中我并没有找到对 ResourceLock 初始化的地方，只看到了对该参数的说明以及日志中配置的默认值：</li>\n</ul>\n<p><img src=\"http://cdn.tianfeiyu.com/leader-1.png\" alt=\"\"></p>\n<p>​在初始化资源锁的时候还传入了 EventRecorder，其作用是当 leader 发生变化的时候会将对应的 events 发送到 apiserver。</p>\n<ul>\n<li><p>2、rl 资源锁被用于 controller-manager 进行 leader 的选举，RunOrDie 方法中就是 leader 的选举过程了。</p>\n</li>\n<li><p>3、Callbacks 中定义了在切换状态后需要执行的操作，当成为 leader 后会执行 OnStartedLeading 中的 run 方法，run 方法是 controller-manager 的核心，run 方法中会初始化并启动所包含资源的 controller，以下是 kube-controller-manager 中所有的 controller：</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc &#123;</span><br><span class=\"line\">\tcontrollers := map[string]InitFunc&#123;&#125;</span><br><span class=\"line\">\tcontrollers[&quot;endpoint&quot;] = startEndpointController</span><br><span class=\"line\">\tcontrollers[&quot;replicationcontroller&quot;] = startReplicationController</span><br><span class=\"line\">\tcontrollers[&quot;podgc&quot;] = startPodGCController</span><br><span class=\"line\">\tcontrollers[&quot;resourcequota&quot;] = startResourceQuotaController</span><br><span class=\"line\">\tcontrollers[&quot;namespace&quot;] = startNamespaceController</span><br><span class=\"line\">\tcontrollers[&quot;serviceaccount&quot;] = startServiceAccountController</span><br><span class=\"line\">\tcontrollers[&quot;garbagecollector&quot;] = startGarbageCollectorController</span><br><span class=\"line\">\tcontrollers[&quot;daemonset&quot;] = startDaemonSetController</span><br><span class=\"line\">\tcontrollers[&quot;job&quot;] = startJobController</span><br><span class=\"line\">\tcontrollers[&quot;deployment&quot;] = startDeploymentController</span><br><span class=\"line\">\tcontrollers[&quot;replicaset&quot;] = startReplicaSetController</span><br><span class=\"line\">\tcontrollers[&quot;horizontalpodautoscaling&quot;] = startHPAController</span><br><span class=\"line\">\tcontrollers[&quot;disruption&quot;] = startDisruptionController</span><br><span class=\"line\">\tcontrollers[&quot;statefulset&quot;] = startStatefulSetController</span><br><span class=\"line\">\tcontrollers[&quot;cronjob&quot;] = startCronJobController</span><br><span class=\"line\">\tcontrollers[&quot;csrsigning&quot;] = startCSRSigningController</span><br><span class=\"line\">\tcontrollers[&quot;csrapproving&quot;] = startCSRApprovingController</span><br><span class=\"line\">\tcontrollers[&quot;csrcleaner&quot;] = startCSRCleanerController</span><br><span class=\"line\">\tcontrollers[&quot;ttl&quot;] = startTTLController</span><br><span class=\"line\">\tcontrollers[&quot;bootstrapsigner&quot;] = startBootstrapSignerController</span><br><span class=\"line\">\tcontrollers[&quot;tokencleaner&quot;] = startTokenCleanerController</span><br><span class=\"line\">\tcontrollers[&quot;nodeipam&quot;] = startNodeIpamController</span><br><span class=\"line\">\tif loopMode == IncludeCloudLoops &#123;</span><br><span class=\"line\">\t\tcontrollers[&quot;service&quot;] = startServiceController</span><br><span class=\"line\">\t\tcontrollers[&quot;route&quot;] = startRouteController</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tcontrollers[&quot;nodelifecycle&quot;] = startNodeLifecycleController</span><br><span class=\"line\">\tcontrollers[&quot;persistentvolume-binder&quot;] = startPersistentVolumeBinderController</span><br><span class=\"line\">\tcontrollers[&quot;attachdetach&quot;] = startAttachDetachController</span><br><span class=\"line\">\tcontrollers[&quot;persistentvolume-expander&quot;] = startVolumeExpandController</span><br><span class=\"line\">\tcontrollers[&quot;clusterrole-aggregation&quot;] = startClusterRoleAggregrationController</span><br><span class=\"line\">\tcontrollers[&quot;pvc-protection&quot;] = startPVCProtectionController</span><br><span class=\"line\">\tcontrollers[&quot;pv-protection&quot;] = startPVProtectionController</span><br><span class=\"line\">\tcontrollers[&quot;ttl-after-finished&quot;] = startTTLAfterFinishedController</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn controllers</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>OnStoppedLeading 是从 leader 状态切换为 slave 要执行的操作，此方法仅打印了一条日志。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func RunOrDie(ctx context.Context, lec LeaderElectionConfig) &#123;</span><br><span class=\"line\">    le, err := NewLeaderElector(lec)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        panic(err)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if lec.WatchDog != nil &#123;</span><br><span class=\"line\">        lec.WatchDog.SetLeaderElection(le)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    le.Run(ctx)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 RunOrDie 中首先调用 NewLeaderElector 初始化了一个 LeaderElector 对象，然后执行 LeaderElector 的 run 方法进行选举。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (le *LeaderElector) Run(ctx context.Context) &#123;</span><br><span class=\"line\">\tdefer func() &#123;</span><br><span class=\"line\">\t\truntime.HandleCrash()</span><br><span class=\"line\">\t\tle.config.Callbacks.OnStoppedLeading()</span><br><span class=\"line\">\t&#125;()</span><br><span class=\"line\">\tif !le.acquire(ctx) &#123;</span><br><span class=\"line\">\t\treturn // ctx signalled done</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tctx, cancel := context.WithCancel(ctx)</span><br><span class=\"line\">\tdefer cancel()</span><br><span class=\"line\">\tgo le.config.Callbacks.OnStartedLeading(ctx)</span><br><span class=\"line\">\tle.renew(ctx)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Run 中首先会执行 acquire 尝试获取锁，获取到锁之后会回调 OnStartedLeading 启动所需要的 controller，然后会执行 renew 方法定期更新锁，保持 leader 的状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (le *LeaderElector) acquire(ctx context.Context) bool &#123;</span><br><span class=\"line\">\tctx, cancel := context.WithCancel(ctx)</span><br><span class=\"line\">\tdefer cancel()</span><br><span class=\"line\">\tsucceeded := false</span><br><span class=\"line\">\tdesc := le.config.Lock.Describe()</span><br><span class=\"line\">\tglog.Infof(&quot;attempting to acquire leader lease  %v...&quot;, desc)</span><br><span class=\"line\">\twait.JitterUntil(func() &#123;</span><br><span class=\"line\">\t\t// 尝试创建或者续约资源锁</span><br><span class=\"line\">\t\tsucceeded = le.tryAcquireOrRenew()</span><br><span class=\"line\">\t\t// leader 可能发生了改变，在 maybeReportTransition 方法中会</span><br><span class=\"line\">\t\t// 执行相应的 OnNewLeader() 回调函数,代码中对 OnNewLeader() 并没有初始化</span><br><span class=\"line\">\t\tle.maybeReportTransition()</span><br><span class=\"line\">\t\tif !succeeded &#123;</span><br><span class=\"line\">\t\t\tglog.V(4).Infof(&quot;failed to acquire lease %v&quot;, desc)</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tle.config.Lock.RecordEvent(&quot;became leader&quot;)</span><br><span class=\"line\">\t\tglog.Infof(&quot;successfully acquired lease %v&quot;, desc)</span><br><span class=\"line\">\t\tcancel()</span><br><span class=\"line\">\t&#125;, le.config.RetryPeriod, JitterFactor, true, ctx.Done())</span><br><span class=\"line\">\treturn succeeded</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 acquire 中首先初始化了一个 ctx，通过 wait.JitterUntil 周期性的去调用 le.tryAcquireOrRenew 方法来获取资源锁，直到获取为止。如果获取不到锁，则会以 RetryPeriod 为间隔不断尝试。如果获取到锁，就会关闭 ctx 通知 wait.JitterUntil 停止尝试，tryAcquireOrRenew 是最核心的方法。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (le *LeaderElector) tryAcquireOrRenew() bool &#123;</span><br><span class=\"line\">\tnow := metav1.Now()</span><br><span class=\"line\">\tleaderElectionRecord := rl.LeaderElectionRecord&#123;</span><br><span class=\"line\">\t\tHolderIdentity:       le.config.Lock.Identity(),</span><br><span class=\"line\">\t\tLeaseDurationSeconds: int(le.config.LeaseDuration / time.Second),</span><br><span class=\"line\">\t\tRenewTime:            now,</span><br><span class=\"line\">\t\tAcquireTime:          now,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 1、获取当前的资源锁</span><br><span class=\"line\">\toldLeaderElectionRecord, err := le.config.Lock.Get()</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tif !errors.IsNotFound(err) &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;error retrieving resource lock %v: %v&quot;, le.config.Lock.Describe(), err)</span><br><span class=\"line\">\t\t\treturn false</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 没有获取到资源锁，开始创建资源锁，若创建成功则成为 leader </span><br><span class=\"line\">\t\tif err = le.config.Lock.Create(leaderElectionRecord); err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;error initially creating leader election record: %v&quot;, err)</span><br><span class=\"line\">\t\t\treturn false</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tle.observedRecord = leaderElectionRecord</span><br><span class=\"line\">\t\tle.observedTime = le.clock.Now()</span><br><span class=\"line\">\t\treturn true</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、获取资源锁后检查当前 id 是不是 leader</span><br><span class=\"line\">\tif !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) &#123;</span><br><span class=\"line\">\t\tle.observedRecord = *oldLeaderElectionRecord</span><br><span class=\"line\">\t\tle.observedTime = le.clock.Now()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 如果资源锁没有过期且当前 id 不是 Leader，直接返回</span><br><span class=\"line\">\tif le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp;</span><br><span class=\"line\">\t\t!le.IsLeader() &#123;</span><br><span class=\"line\">\t\tglog.V(4).Infof(&quot;lock is held by %v and has not yet expired&quot;, oldLeaderElectionRecord.HolderIdentity)</span><br><span class=\"line\">\t\treturn false</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，准备续租</span><br><span class=\"line\">\t// 如果是非 Leader 节点则抢夺资源锁</span><br><span class=\"line\">\tif le.IsLeader() &#123;</span><br><span class=\"line\">\t\tleaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime</span><br><span class=\"line\">\t\tleaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\tleaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        // 更新资源</span><br><span class=\"line\">        // 对于 Leader 来说，这是一个续租的过程</span><br><span class=\"line\">        // 对于非 Leader 节点（仅在上一个资源锁已经过期），这是一个更新锁所有权的过程</span><br><span class=\"line\">\tif err = le.config.Lock.Update(leaderElectionRecord); err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;Failed to update lock: %v&quot;, err)</span><br><span class=\"line\">\t\treturn false</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tle.observedRecord = leaderElectionRecord</span><br><span class=\"line\">\tle.observedTime = le.clock.Now()</span><br><span class=\"line\">\treturn true</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的这个函数的主要逻辑：</p>\n<ul>\n<li>1、获取 ElectionRecord 记录，如果没有则创建一条新的 ElectionRecord 记录，创建成功则表示获取到锁并成为 leader 了。</li>\n<li>2、当获取到资源锁后开始检查其中的信息，比较当前 id 是不是 leader 以及资源锁有没有过期，如果资源锁没有过期且当前 id 不是 Leader，则直接返回。</li>\n<li>3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，更新资源锁进行续租。</li>\n<li>4、如果当前 id 不是 Leader 但是资源锁已经过期了，则抢夺资源锁，抢夺成功则成为 leader 否则返回。</li>\n</ul>\n<p>最后是 renew 方法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (le *LeaderElector) renew(ctx context.Context) &#123;</span><br><span class=\"line\">\tctx, cancel := context.WithCancel(ctx)</span><br><span class=\"line\">\tdefer cancel()</span><br><span class=\"line\">\twait.Until(func() &#123;</span><br><span class=\"line\">\t\ttimeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline)</span><br><span class=\"line\">\t\tdefer timeoutCancel()</span><br><span class=\"line\">                // 每间隔 RetryPeriod 就执行 tryAcquireOrRenew()</span><br><span class=\"line\">                // 如果 tryAcquireOrRenew() 返回 false 说明续租失败</span><br><span class=\"line\">\t\terr := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) &#123;</span><br><span class=\"line\">\t\t\tdone := make(chan bool, 1)</span><br><span class=\"line\">\t\t\tgo func() &#123;</span><br><span class=\"line\">\t\t\t\tdefer close(done)</span><br><span class=\"line\">\t\t\t\tdone &lt;- le.tryAcquireOrRenew()</span><br><span class=\"line\">\t\t\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tselect &#123;</span><br><span class=\"line\">\t\t\tcase &lt;-timeoutCtx.Done():</span><br><span class=\"line\">\t\t\t\treturn false, fmt.Errorf(&quot;failed to tryAcquireOrRenew %s&quot;, timeoutCtx.Err())</span><br><span class=\"line\">\t\t\tcase result := &lt;-done:</span><br><span class=\"line\">\t\t\t\treturn result, nil</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;, timeoutCtx.Done())</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tle.maybeReportTransition()</span><br><span class=\"line\">\t\tdesc := le.config.Lock.Describe()</span><br><span class=\"line\">\t\tif err == nil &#123;</span><br><span class=\"line\">\t\t\tglog.V(4).Infof(&quot;successfully renewed lease %v&quot;, desc)</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 续租失败，说明已经不是 Leader，然后程序 panic</span><br><span class=\"line\">\t\tle.config.Lock.RecordEvent(&quot;stopped leading&quot;)</span><br><span class=\"line\">\t\tglog.Infof(&quot;failed to renew lease %v: %v&quot;, desc, err)</span><br><span class=\"line\">\t\tcancel()</span><br><span class=\"line\">\t&#125;, le.config.RetryPeriod, ctx.Done())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>获取到锁之后定期进行更新，renew 只有在获取锁之后才会调用，它会通过持续更新资源锁的数据，来确保继续持有已获得的锁，保持自己的 leader 状态。</p>\n<h4 id=\"Leader-Election-功能的使用\"><a href=\"#Leader-Election-功能的使用\" class=\"headerlink\" title=\"Leader Election 功能的使用\"></a>Leader Election 功能的使用</h4><p>以下是一个 demo，使用 k8s 中 <code>k8s.io/client-go/tools/leaderelection</code> 进行一个演示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;context&quot;</span><br><span class=\"line\">\t&quot;flag&quot;</span><br><span class=\"line\">\t&quot;fmt&quot;</span><br><span class=\"line\">\t&quot;os&quot;</span><br><span class=\"line\">\t&quot;time&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&quot;github.com/golang/glog&quot;</span><br><span class=\"line\">\t&quot;k8s.io/api/core/v1&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/kubernetes&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/kubernetes/scheme&quot;</span><br><span class=\"line\">\tv1core &quot;k8s.io/client-go/kubernetes/typed/core/v1&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/clientcmd&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/leaderelection&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/leaderelection/resourcelock&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/record&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">var (</span><br><span class=\"line\">\tmasterURL  string</span><br><span class=\"line\">\tkubeconfig string</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func init() &#123;</span><br><span class=\"line\">\tflag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;Path to a kubeconfig. Only required if out-of-cluster.&quot;)</span><br><span class=\"line\">\tflag.StringVar(&amp;masterURL, &quot;master&quot;, &quot;&quot;, &quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\tflag.Set(&quot;logtostderr&quot;, &quot;true&quot;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tflag.Parse()</span><br><span class=\"line\">\tdefer glog.Flush()</span><br><span class=\"line\"></span><br><span class=\"line\">\tid, err := os.Hostname()</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tpanic(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 加载 kubeconfig 配置</span><br><span class=\"line\">\tcfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Fatalf(&quot;Error building kubeconfig: %s&quot;, err.Error())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 kubeclient</span><br><span class=\"line\">\tkubeClient, err := kubernetes.NewForConfig(cfg)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Fatalf(&quot;Error building kubernetes clientset: %s&quot;, err.Error())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 初始化 eventRecorder</span><br><span class=\"line\">\teventBroadcaster := record.NewBroadcaster()</span><br><span class=\"line\">\teventRecorder := eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: &quot;test-1&quot;&#125;)</span><br><span class=\"line\">\teventBroadcaster.StartLogging(glog.Infof)</span><br><span class=\"line\">\teventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeClient.CoreV1().Events(&quot;&quot;)&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\trun := func(ctx context.Context) &#123;</span><br><span class=\"line\">\t\tfmt.Println(&quot;run.........&quot;)</span><br><span class=\"line\">\t\tselect &#123;&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tid = id + &quot;_&quot; + &quot;1&quot;</span><br><span class=\"line\">\trl, err := resourcelock.New(&quot;endpoints&quot;,</span><br><span class=\"line\">\t\t&quot;kube-system&quot;,</span><br><span class=\"line\">\t\t&quot;test&quot;,</span><br><span class=\"line\">\t\tkubeClient.CoreV1(),</span><br><span class=\"line\">\t\tresourcelock.ResourceLockConfig&#123;</span><br><span class=\"line\">\t\t\tIdentity:      id,</span><br><span class=\"line\">\t\t\tEventRecorder: eventRecorder,</span><br><span class=\"line\">\t\t&#125;)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Fatalf(&quot;error creating lock: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tleaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig&#123;</span><br><span class=\"line\">\t\tLock:          rl,</span><br><span class=\"line\">\t\tLeaseDuration: 15 * time.Second,</span><br><span class=\"line\">\t\tRenewDeadline: 10 * time.Second,</span><br><span class=\"line\">\t\tRetryPeriod:   2 * time.Second,</span><br><span class=\"line\">\t\tCallbacks: leaderelection.LeaderCallbacks&#123;</span><br><span class=\"line\">\t\t\tOnStartedLeading: run,</span><br><span class=\"line\">\t\t\tOnStoppedLeading: func() &#123;</span><br><span class=\"line\">\t\t\t\tglog.Info(&quot;leaderelection lost&quot;)</span><br><span class=\"line\">\t\t\t&#125;,</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t\tName: &quot;test-1&quot;,</span><br><span class=\"line\">\t&#125;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>分别使用多个 hostname 同时运行后并测试 leader 切换，可以在 events 中看到 leader 切换的记录：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># kubectl describe endpoints test  -n kube-system</span><br><span class=\"line\">Name:         test</span><br><span class=\"line\">Namespace:    kube-system</span><br><span class=\"line\">Labels:       &lt;none&gt;</span><br><span class=\"line\">Annotations:  control-plane.alpha.kubernetes.io/leader=&#123;&quot;holderIdentity&quot;:&quot;localhost_2&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-03-10T08:47:42Z&quot;,&quot;renewTime&quot;:&quot;2019-03-10T08:47:44Z&quot;,&quot;leaderTransitions&quot;:2&#125;</span><br><span class=\"line\">Subsets:</span><br><span class=\"line\">Events:</span><br><span class=\"line\">  Type    Reason          Age   From    Message</span><br><span class=\"line\">  ----    ------          ----  ----    -------</span><br><span class=\"line\">  Normal  LeaderElection  50s   test-1  localhost_1 became leader</span><br><span class=\"line\">  Normal  LeaderElection  5s    test-2  localhost_2 became leader</span><br></pre></td></tr></table></figure>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>本文讲述了 kube-controller-manager 使用 HA 的方式启动后 leader 选举过程的实现说明，k8s 中通过创建 endpoints 资源以及对该资源的持续更新来实现资源锁轮转的过程。但是相对于其他分布式锁的实现，普遍是直接基于现有的中间件实现，比如 redis、zookeeper、etcd 等，其所有对锁的操作都是原子性的，那 k8s 选举过程中的原子操作是如何实现的？k8s 中的原子操作最终也是通过 etcd 实现的，其在做 update 更新锁的操作时采用的是乐观锁，通过对比 resourceVersion 实现的，详细的实现下节再讲。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/api-resource-1.png\" alt=\"api resource\"></p>\n<p>参考文档：<br><a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/\" target=\"_blank\" rel=\"noopener\">API OVERVIEW</a><br><a href=\"https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/\" target=\"_blank\" rel=\"noopener\">Simple leader election with Kubernetes and Docker</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>生产环境中为了保障业务的稳定性，集群都需要高可用部署，k8s 中 apiserver 是无状态的，可以横向扩容保证其高可用，kube-controller-manager 和 kube-scheduler 两个组件通过 leader 选举保障高可用，即正常情况下 kube-scheduler 或 kube-manager-controller 组件的多个副本只有一个是处于业务逻辑运行状态，其它副本则不断的尝试去获取锁，去竞争 leader，直到自己成为leader。如果正在运行的 leader 因某种原因导致当前进程退出，或者锁丢失，则由其它副本去竞争新的 leader，获取 leader 继而执行业务逻辑。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<h4 id=\"组件高可用的使用\"><a href=\"#组件高可用的使用\" class=\"headerlink\" title=\"组件高可用的使用\"></a>组件高可用的使用</h4><p>k8s 中已经为 kube-controller-manager、kube-scheduler 组件实现了高可用，只需在每个组件的配置文件中添加 <code>--leader-elect=true</code> 参数即可启用。在每个组件的日志中可以看到 HA 相关参数的默认值：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I0306 19:17:14.109511  161798 flags.go:33] FLAG: --leader-elect=&quot;true&quot;</span><br><span class=\"line\">I0306 19:17:14.109513  161798 flags.go:33] FLAG: --leader-elect-lease-duration=&quot;15s&quot;</span><br><span class=\"line\">I0306 19:17:14.109516  161798 flags.go:33] FLAG: --leader-elect-renew-deadline=&quot;10s&quot;</span><br><span class=\"line\">I0306 19:17:14.109518  161798 flags.go:33] FLAG: --leader-elect-resource-lock=&quot;endpoints&quot;</span><br><span class=\"line\">I0306 19:17:14.109520  161798 flags.go:33] FLAG: --leader-elect-retry-period=&quot;2s&quot;</span><br></pre></td></tr></table></figure>\n<p>kubernetes 中查看组件 leader 的方法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml &amp;&amp; </span><br><span class=\"line\">  kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml</span><br></pre></td></tr></table></figure>\n<p>当前组件 leader 的 hostname 会写在 annotation 的 control-plane.alpha.kubernetes.io/leader 字段里。</p>\n<h4 id=\"Leader-Election-的实现\"><a href=\"#Leader-Election-的实现\" class=\"headerlink\" title=\"Leader Election 的实现\"></a>Leader Election 的实现</h4><p>Leader Election 的过程本质上是一个竞争分布式锁的过程。在 Kubernetes 中，这个分布式锁是以创建 Endpoint 资源的形式进行，谁先创建了该资源，谁就先获得锁，之后会对该资源不断更新以保持锁的拥有权。</p>\n<p>下面开始讲述 kube-controller-manager 中 leader 的竞争过程，cm 在加载及配置完参数后就开始执行 run 方法了。代码在 <code>k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go</code> 中：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Run runs the KubeControllerManagerOptions.  This should never exit.</span><br><span class=\"line\">func Run(c *config.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;) error &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// kube-controller-manager 的核心</span><br><span class=\"line\">\t\trun := func(ctx context.Context) &#123;</span><br><span class=\"line\">\t\trootClientBuilder := controller.SimpleControllerClientBuilder&#123;</span><br><span class=\"line\">\t\t\tClientConfig: c.Kubeconfig,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tvar clientBuilder controller.ControllerClientBuilder</span><br><span class=\"line\">\t\tif c.ComponentConfig.KubeCloudShared.UseServiceAccountCredentials &#123;</span><br><span class=\"line\">\t\t\tif len(c.ComponentConfig.SAController.ServiceAccountKeyFile) == 0 &#123;</span><br><span class=\"line\">\t\t\t\t// It&apos;c possible another controller process is creating the tokens for us.</span><br><span class=\"line\">\t\t\t\t// If one isn&apos;t, we&apos;ll timeout and exit when our client builder is unable to create the tokens.</span><br><span class=\"line\">\t\t\t\tglog.Warningf(&quot;--use-service-account-credentials was specified without providing a --service-account-private-key-file&quot;)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tclientBuilder = controller.SAControllerClientBuilder&#123;</span><br><span class=\"line\">\t\t\t\tClientConfig:         restclient.AnonymousClientConfig(c.Kubeconfig),</span><br><span class=\"line\">\t\t\t\tCoreClient:           c.Client.CoreV1(),</span><br><span class=\"line\">\t\t\t\tAuthenticationClient: c.Client.AuthenticationV1(),</span><br><span class=\"line\">\t\t\t\tNamespace:            &quot;kube-system&quot;,</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125; else &#123;</span><br><span class=\"line\">\t\t\tclientBuilder = rootClientBuilder</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tcontrollerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done())</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Fatalf(&quot;error building controller context: %v&quot;, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tsaTokenControllerInitFunc := serviceAccountTokenControllerStarter&#123;rootClientBuilder: rootClientBuilder&#125;.startServiceAccountTokenController</span><br><span class=\"line\">\t\t// 初始化及启动所有的 controller</span><br><span class=\"line\">\t\tif err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Fatalf(&quot;error starting controllers: %v&quot;, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tcontrollerContext.InformerFactory.Start(controllerContext.Stop)</span><br><span class=\"line\">\t\tclose(controllerContext.InformersStarted)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tselect &#123;&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 如果 LeaderElect 参数未配置,说明 controller-manager 是单点启动的，</span><br><span class=\"line\">    // 则直接调用 run 方法来启动需要被启动的控制器即可。</span><br><span class=\"line\">    if !c.ComponentConfig.Generic.LeaderElection.LeaderElect &#123;</span><br><span class=\"line\">        run(context.TODO())</span><br><span class=\"line\">        panic(&quot;unreachable&quot;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 如果 LeaderElect 参数配置为 true,说明 controller-manager 是以 HA 方式启动的，</span><br><span class=\"line\">    // 则执行下面的代码进行 leader 选举，选举出的 leader 会回调 run 方法。</span><br><span class=\"line\">    id, err := os.Hostname()</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // add a uniquifier so that two processes on the same host don&apos;t accidentally both become active</span><br><span class=\"line\">    id = id + &quot;_&quot; + string(uuid.NewUUID())</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 初始化资源锁</span><br><span class=\"line\">    rl, err := resourcelock.New(c.ComponentConfig.Generic.LeaderElection.ResourceLock,</span><br><span class=\"line\">        &quot;kube-system&quot;,</span><br><span class=\"line\">        &quot;kube-controller-manager&quot;,</span><br><span class=\"line\">        c.LeaderElectionClient.CoreV1(),</span><br><span class=\"line\">        resourcelock.ResourceLockConfig&#123;</span><br><span class=\"line\">            Identity:      id,</span><br><span class=\"line\">            EventRecorder: c.EventRecorder,</span><br><span class=\"line\">        &#125;)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        glog.Fatalf(&quot;error creating lock: %v&quot;, err)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 进入到选举的流程</span><br><span class=\"line\">    leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig&#123;</span><br><span class=\"line\">        Lock:          rl,</span><br><span class=\"line\">        LeaseDuration: c.ComponentConfig.Generic.LeaderElection.LeaseDuration.Duration,</span><br><span class=\"line\">        RenewDeadline: c.ComponentConfig.Generic.LeaderElection.RenewDeadline.Duration,</span><br><span class=\"line\">        RetryPeriod:   c.ComponentConfig.Generic.LeaderElection.RetryPeriod.Duration,</span><br><span class=\"line\">        Callbacks: leaderelection.LeaderCallbacks&#123;</span><br><span class=\"line\">            OnStartedLeading: run,</span><br><span class=\"line\">            OnStoppedLeading: func() &#123;</span><br><span class=\"line\">                glog.Fatalf(&quot;leaderelection lost&quot;)</span><br><span class=\"line\">            &#125;,</span><br><span class=\"line\">        &#125;,</span><br><span class=\"line\">        WatchDog: electionChecker,</span><br><span class=\"line\">        Name:     &quot;kube-controller-manager&quot;,</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    panic(&quot;unreachable&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>1、初始化资源锁，kubernetes 中默认的资源锁使用 <code>endpoints</code>，也就是 c.ComponentConfig.Generic.LeaderElection.ResourceLock 的值为 “endpoints”，在代码中我并没有找到对 ResourceLock 初始化的地方，只看到了对该参数的说明以及日志中配置的默认值：</li>\n</ul>\n<p><img src=\"http://cdn.tianfeiyu.com/leader-1.png\" alt=\"\"></p>\n<p>​在初始化资源锁的时候还传入了 EventRecorder，其作用是当 leader 发生变化的时候会将对应的 events 发送到 apiserver。</p>\n<ul>\n<li><p>2、rl 资源锁被用于 controller-manager 进行 leader 的选举，RunOrDie 方法中就是 leader 的选举过程了。</p>\n</li>\n<li><p>3、Callbacks 中定义了在切换状态后需要执行的操作，当成为 leader 后会执行 OnStartedLeading 中的 run 方法，run 方法是 controller-manager 的核心，run 方法中会初始化并启动所包含资源的 controller，以下是 kube-controller-manager 中所有的 controller：</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc &#123;</span><br><span class=\"line\">\tcontrollers := map[string]InitFunc&#123;&#125;</span><br><span class=\"line\">\tcontrollers[&quot;endpoint&quot;] = startEndpointController</span><br><span class=\"line\">\tcontrollers[&quot;replicationcontroller&quot;] = startReplicationController</span><br><span class=\"line\">\tcontrollers[&quot;podgc&quot;] = startPodGCController</span><br><span class=\"line\">\tcontrollers[&quot;resourcequota&quot;] = startResourceQuotaController</span><br><span class=\"line\">\tcontrollers[&quot;namespace&quot;] = startNamespaceController</span><br><span class=\"line\">\tcontrollers[&quot;serviceaccount&quot;] = startServiceAccountController</span><br><span class=\"line\">\tcontrollers[&quot;garbagecollector&quot;] = startGarbageCollectorController</span><br><span class=\"line\">\tcontrollers[&quot;daemonset&quot;] = startDaemonSetController</span><br><span class=\"line\">\tcontrollers[&quot;job&quot;] = startJobController</span><br><span class=\"line\">\tcontrollers[&quot;deployment&quot;] = startDeploymentController</span><br><span class=\"line\">\tcontrollers[&quot;replicaset&quot;] = startReplicaSetController</span><br><span class=\"line\">\tcontrollers[&quot;horizontalpodautoscaling&quot;] = startHPAController</span><br><span class=\"line\">\tcontrollers[&quot;disruption&quot;] = startDisruptionController</span><br><span class=\"line\">\tcontrollers[&quot;statefulset&quot;] = startStatefulSetController</span><br><span class=\"line\">\tcontrollers[&quot;cronjob&quot;] = startCronJobController</span><br><span class=\"line\">\tcontrollers[&quot;csrsigning&quot;] = startCSRSigningController</span><br><span class=\"line\">\tcontrollers[&quot;csrapproving&quot;] = startCSRApprovingController</span><br><span class=\"line\">\tcontrollers[&quot;csrcleaner&quot;] = startCSRCleanerController</span><br><span class=\"line\">\tcontrollers[&quot;ttl&quot;] = startTTLController</span><br><span class=\"line\">\tcontrollers[&quot;bootstrapsigner&quot;] = startBootstrapSignerController</span><br><span class=\"line\">\tcontrollers[&quot;tokencleaner&quot;] = startTokenCleanerController</span><br><span class=\"line\">\tcontrollers[&quot;nodeipam&quot;] = startNodeIpamController</span><br><span class=\"line\">\tif loopMode == IncludeCloudLoops &#123;</span><br><span class=\"line\">\t\tcontrollers[&quot;service&quot;] = startServiceController</span><br><span class=\"line\">\t\tcontrollers[&quot;route&quot;] = startRouteController</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tcontrollers[&quot;nodelifecycle&quot;] = startNodeLifecycleController</span><br><span class=\"line\">\tcontrollers[&quot;persistentvolume-binder&quot;] = startPersistentVolumeBinderController</span><br><span class=\"line\">\tcontrollers[&quot;attachdetach&quot;] = startAttachDetachController</span><br><span class=\"line\">\tcontrollers[&quot;persistentvolume-expander&quot;] = startVolumeExpandController</span><br><span class=\"line\">\tcontrollers[&quot;clusterrole-aggregation&quot;] = startClusterRoleAggregrationController</span><br><span class=\"line\">\tcontrollers[&quot;pvc-protection&quot;] = startPVCProtectionController</span><br><span class=\"line\">\tcontrollers[&quot;pv-protection&quot;] = startPVProtectionController</span><br><span class=\"line\">\tcontrollers[&quot;ttl-after-finished&quot;] = startTTLAfterFinishedController</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn controllers</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>OnStoppedLeading 是从 leader 状态切换为 slave 要执行的操作，此方法仅打印了一条日志。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func RunOrDie(ctx context.Context, lec LeaderElectionConfig) &#123;</span><br><span class=\"line\">    le, err := NewLeaderElector(lec)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        panic(err)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if lec.WatchDog != nil &#123;</span><br><span class=\"line\">        lec.WatchDog.SetLeaderElection(le)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    le.Run(ctx)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 RunOrDie 中首先调用 NewLeaderElector 初始化了一个 LeaderElector 对象，然后执行 LeaderElector 的 run 方法进行选举。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (le *LeaderElector) Run(ctx context.Context) &#123;</span><br><span class=\"line\">\tdefer func() &#123;</span><br><span class=\"line\">\t\truntime.HandleCrash()</span><br><span class=\"line\">\t\tle.config.Callbacks.OnStoppedLeading()</span><br><span class=\"line\">\t&#125;()</span><br><span class=\"line\">\tif !le.acquire(ctx) &#123;</span><br><span class=\"line\">\t\treturn // ctx signalled done</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tctx, cancel := context.WithCancel(ctx)</span><br><span class=\"line\">\tdefer cancel()</span><br><span class=\"line\">\tgo le.config.Callbacks.OnStartedLeading(ctx)</span><br><span class=\"line\">\tle.renew(ctx)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Run 中首先会执行 acquire 尝试获取锁，获取到锁之后会回调 OnStartedLeading 启动所需要的 controller，然后会执行 renew 方法定期更新锁，保持 leader 的状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (le *LeaderElector) acquire(ctx context.Context) bool &#123;</span><br><span class=\"line\">\tctx, cancel := context.WithCancel(ctx)</span><br><span class=\"line\">\tdefer cancel()</span><br><span class=\"line\">\tsucceeded := false</span><br><span class=\"line\">\tdesc := le.config.Lock.Describe()</span><br><span class=\"line\">\tglog.Infof(&quot;attempting to acquire leader lease  %v...&quot;, desc)</span><br><span class=\"line\">\twait.JitterUntil(func() &#123;</span><br><span class=\"line\">\t\t// 尝试创建或者续约资源锁</span><br><span class=\"line\">\t\tsucceeded = le.tryAcquireOrRenew()</span><br><span class=\"line\">\t\t// leader 可能发生了改变，在 maybeReportTransition 方法中会</span><br><span class=\"line\">\t\t// 执行相应的 OnNewLeader() 回调函数,代码中对 OnNewLeader() 并没有初始化</span><br><span class=\"line\">\t\tle.maybeReportTransition()</span><br><span class=\"line\">\t\tif !succeeded &#123;</span><br><span class=\"line\">\t\t\tglog.V(4).Infof(&quot;failed to acquire lease %v&quot;, desc)</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tle.config.Lock.RecordEvent(&quot;became leader&quot;)</span><br><span class=\"line\">\t\tglog.Infof(&quot;successfully acquired lease %v&quot;, desc)</span><br><span class=\"line\">\t\tcancel()</span><br><span class=\"line\">\t&#125;, le.config.RetryPeriod, JitterFactor, true, ctx.Done())</span><br><span class=\"line\">\treturn succeeded</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 acquire 中首先初始化了一个 ctx，通过 wait.JitterUntil 周期性的去调用 le.tryAcquireOrRenew 方法来获取资源锁，直到获取为止。如果获取不到锁，则会以 RetryPeriod 为间隔不断尝试。如果获取到锁，就会关闭 ctx 通知 wait.JitterUntil 停止尝试，tryAcquireOrRenew 是最核心的方法。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (le *LeaderElector) tryAcquireOrRenew() bool &#123;</span><br><span class=\"line\">\tnow := metav1.Now()</span><br><span class=\"line\">\tleaderElectionRecord := rl.LeaderElectionRecord&#123;</span><br><span class=\"line\">\t\tHolderIdentity:       le.config.Lock.Identity(),</span><br><span class=\"line\">\t\tLeaseDurationSeconds: int(le.config.LeaseDuration / time.Second),</span><br><span class=\"line\">\t\tRenewTime:            now,</span><br><span class=\"line\">\t\tAcquireTime:          now,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 1、获取当前的资源锁</span><br><span class=\"line\">\toldLeaderElectionRecord, err := le.config.Lock.Get()</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tif !errors.IsNotFound(err) &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;error retrieving resource lock %v: %v&quot;, le.config.Lock.Describe(), err)</span><br><span class=\"line\">\t\t\treturn false</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 没有获取到资源锁，开始创建资源锁，若创建成功则成为 leader </span><br><span class=\"line\">\t\tif err = le.config.Lock.Create(leaderElectionRecord); err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;error initially creating leader election record: %v&quot;, err)</span><br><span class=\"line\">\t\t\treturn false</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tle.observedRecord = leaderElectionRecord</span><br><span class=\"line\">\t\tle.observedTime = le.clock.Now()</span><br><span class=\"line\">\t\treturn true</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、获取资源锁后检查当前 id 是不是 leader</span><br><span class=\"line\">\tif !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) &#123;</span><br><span class=\"line\">\t\tle.observedRecord = *oldLeaderElectionRecord</span><br><span class=\"line\">\t\tle.observedTime = le.clock.Now()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 如果资源锁没有过期且当前 id 不是 Leader，直接返回</span><br><span class=\"line\">\tif le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp;</span><br><span class=\"line\">\t\t!le.IsLeader() &#123;</span><br><span class=\"line\">\t\tglog.V(4).Infof(&quot;lock is held by %v and has not yet expired&quot;, oldLeaderElectionRecord.HolderIdentity)</span><br><span class=\"line\">\t\treturn false</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，准备续租</span><br><span class=\"line\">\t// 如果是非 Leader 节点则抢夺资源锁</span><br><span class=\"line\">\tif le.IsLeader() &#123;</span><br><span class=\"line\">\t\tleaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime</span><br><span class=\"line\">\t\tleaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\tleaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        // 更新资源</span><br><span class=\"line\">        // 对于 Leader 来说，这是一个续租的过程</span><br><span class=\"line\">        // 对于非 Leader 节点（仅在上一个资源锁已经过期），这是一个更新锁所有权的过程</span><br><span class=\"line\">\tif err = le.config.Lock.Update(leaderElectionRecord); err != nil &#123;</span><br><span class=\"line\">\t\tglog.Errorf(&quot;Failed to update lock: %v&quot;, err)</span><br><span class=\"line\">\t\treturn false</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tle.observedRecord = leaderElectionRecord</span><br><span class=\"line\">\tle.observedTime = le.clock.Now()</span><br><span class=\"line\">\treturn true</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的这个函数的主要逻辑：</p>\n<ul>\n<li>1、获取 ElectionRecord 记录，如果没有则创建一条新的 ElectionRecord 记录，创建成功则表示获取到锁并成为 leader 了。</li>\n<li>2、当获取到资源锁后开始检查其中的信息，比较当前 id 是不是 leader 以及资源锁有没有过期，如果资源锁没有过期且当前 id 不是 Leader，则直接返回。</li>\n<li>3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，更新资源锁进行续租。</li>\n<li>4、如果当前 id 不是 Leader 但是资源锁已经过期了，则抢夺资源锁，抢夺成功则成为 leader 否则返回。</li>\n</ul>\n<p>最后是 renew 方法：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (le *LeaderElector) renew(ctx context.Context) &#123;</span><br><span class=\"line\">\tctx, cancel := context.WithCancel(ctx)</span><br><span class=\"line\">\tdefer cancel()</span><br><span class=\"line\">\twait.Until(func() &#123;</span><br><span class=\"line\">\t\ttimeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline)</span><br><span class=\"line\">\t\tdefer timeoutCancel()</span><br><span class=\"line\">                // 每间隔 RetryPeriod 就执行 tryAcquireOrRenew()</span><br><span class=\"line\">                // 如果 tryAcquireOrRenew() 返回 false 说明续租失败</span><br><span class=\"line\">\t\terr := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) &#123;</span><br><span class=\"line\">\t\t\tdone := make(chan bool, 1)</span><br><span class=\"line\">\t\t\tgo func() &#123;</span><br><span class=\"line\">\t\t\t\tdefer close(done)</span><br><span class=\"line\">\t\t\t\tdone &lt;- le.tryAcquireOrRenew()</span><br><span class=\"line\">\t\t\t&#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\tselect &#123;</span><br><span class=\"line\">\t\t\tcase &lt;-timeoutCtx.Done():</span><br><span class=\"line\">\t\t\t\treturn false, fmt.Errorf(&quot;failed to tryAcquireOrRenew %s&quot;, timeoutCtx.Err())</span><br><span class=\"line\">\t\t\tcase result := &lt;-done:</span><br><span class=\"line\">\t\t\t\treturn result, nil</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;, timeoutCtx.Done())</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tle.maybeReportTransition()</span><br><span class=\"line\">\t\tdesc := le.config.Lock.Describe()</span><br><span class=\"line\">\t\tif err == nil &#123;</span><br><span class=\"line\">\t\t\tglog.V(4).Infof(&quot;successfully renewed lease %v&quot;, desc)</span><br><span class=\"line\">\t\t\treturn</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 续租失败，说明已经不是 Leader，然后程序 panic</span><br><span class=\"line\">\t\tle.config.Lock.RecordEvent(&quot;stopped leading&quot;)</span><br><span class=\"line\">\t\tglog.Infof(&quot;failed to renew lease %v: %v&quot;, desc, err)</span><br><span class=\"line\">\t\tcancel()</span><br><span class=\"line\">\t&#125;, le.config.RetryPeriod, ctx.Done())</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>获取到锁之后定期进行更新，renew 只有在获取锁之后才会调用，它会通过持续更新资源锁的数据，来确保继续持有已获得的锁，保持自己的 leader 状态。</p>\n<h4 id=\"Leader-Election-功能的使用\"><a href=\"#Leader-Election-功能的使用\" class=\"headerlink\" title=\"Leader Election 功能的使用\"></a>Leader Election 功能的使用</h4><p>以下是一个 demo，使用 k8s 中 <code>k8s.io/client-go/tools/leaderelection</code> 进行一个演示：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package main</span><br><span class=\"line\"></span><br><span class=\"line\">import (</span><br><span class=\"line\">\t&quot;context&quot;</span><br><span class=\"line\">\t&quot;flag&quot;</span><br><span class=\"line\">\t&quot;fmt&quot;</span><br><span class=\"line\">\t&quot;os&quot;</span><br><span class=\"line\">\t&quot;time&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">\t&quot;github.com/golang/glog&quot;</span><br><span class=\"line\">\t&quot;k8s.io/api/core/v1&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/kubernetes&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/kubernetes/scheme&quot;</span><br><span class=\"line\">\tv1core &quot;k8s.io/client-go/kubernetes/typed/core/v1&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/clientcmd&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/leaderelection&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/leaderelection/resourcelock&quot;</span><br><span class=\"line\">\t&quot;k8s.io/client-go/tools/record&quot;</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">var (</span><br><span class=\"line\">\tmasterURL  string</span><br><span class=\"line\">\tkubeconfig string</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">func init() &#123;</span><br><span class=\"line\">\tflag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;Path to a kubeconfig. Only required if out-of-cluster.&quot;)</span><br><span class=\"line\">\tflag.StringVar(&amp;masterURL, &quot;master&quot;, &quot;&quot;, &quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\tflag.Set(&quot;logtostderr&quot;, &quot;true&quot;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\tflag.Parse()</span><br><span class=\"line\">\tdefer glog.Flush()</span><br><span class=\"line\"></span><br><span class=\"line\">\tid, err := os.Hostname()</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tpanic(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 加载 kubeconfig 配置</span><br><span class=\"line\">\tcfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Fatalf(&quot;Error building kubeconfig: %s&quot;, err.Error())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 kubeclient</span><br><span class=\"line\">\tkubeClient, err := kubernetes.NewForConfig(cfg)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Fatalf(&quot;Error building kubernetes clientset: %s&quot;, err.Error())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 初始化 eventRecorder</span><br><span class=\"line\">\teventBroadcaster := record.NewBroadcaster()</span><br><span class=\"line\">\teventRecorder := eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: &quot;test-1&quot;&#125;)</span><br><span class=\"line\">\teventBroadcaster.StartLogging(glog.Infof)</span><br><span class=\"line\">\teventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeClient.CoreV1().Events(&quot;&quot;)&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\trun := func(ctx context.Context) &#123;</span><br><span class=\"line\">\t\tfmt.Println(&quot;run.........&quot;)</span><br><span class=\"line\">\t\tselect &#123;&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tid = id + &quot;_&quot; + &quot;1&quot;</span><br><span class=\"line\">\trl, err := resourcelock.New(&quot;endpoints&quot;,</span><br><span class=\"line\">\t\t&quot;kube-system&quot;,</span><br><span class=\"line\">\t\t&quot;test&quot;,</span><br><span class=\"line\">\t\tkubeClient.CoreV1(),</span><br><span class=\"line\">\t\tresourcelock.ResourceLockConfig&#123;</span><br><span class=\"line\">\t\t\tIdentity:      id,</span><br><span class=\"line\">\t\t\tEventRecorder: eventRecorder,</span><br><span class=\"line\">\t\t&#125;)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\tglog.Fatalf(&quot;error creating lock: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tleaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig&#123;</span><br><span class=\"line\">\t\tLock:          rl,</span><br><span class=\"line\">\t\tLeaseDuration: 15 * time.Second,</span><br><span class=\"line\">\t\tRenewDeadline: 10 * time.Second,</span><br><span class=\"line\">\t\tRetryPeriod:   2 * time.Second,</span><br><span class=\"line\">\t\tCallbacks: leaderelection.LeaderCallbacks&#123;</span><br><span class=\"line\">\t\t\tOnStartedLeading: run,</span><br><span class=\"line\">\t\t\tOnStoppedLeading: func() &#123;</span><br><span class=\"line\">\t\t\t\tglog.Info(&quot;leaderelection lost&quot;)</span><br><span class=\"line\">\t\t\t&#125;,</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t\tName: &quot;test-1&quot;,</span><br><span class=\"line\">\t&#125;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>分别使用多个 hostname 同时运行后并测试 leader 切换，可以在 events 中看到 leader 切换的记录：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># kubectl describe endpoints test  -n kube-system</span><br><span class=\"line\">Name:         test</span><br><span class=\"line\">Namespace:    kube-system</span><br><span class=\"line\">Labels:       &lt;none&gt;</span><br><span class=\"line\">Annotations:  control-plane.alpha.kubernetes.io/leader=&#123;&quot;holderIdentity&quot;:&quot;localhost_2&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-03-10T08:47:42Z&quot;,&quot;renewTime&quot;:&quot;2019-03-10T08:47:44Z&quot;,&quot;leaderTransitions&quot;:2&#125;</span><br><span class=\"line\">Subsets:</span><br><span class=\"line\">Events:</span><br><span class=\"line\">  Type    Reason          Age   From    Message</span><br><span class=\"line\">  ----    ------          ----  ----    -------</span><br><span class=\"line\">  Normal  LeaderElection  50s   test-1  localhost_1 became leader</span><br><span class=\"line\">  Normal  LeaderElection  5s    test-2  localhost_2 became leader</span><br></pre></td></tr></table></figure>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>本文讲述了 kube-controller-manager 使用 HA 的方式启动后 leader 选举过程的实现说明，k8s 中通过创建 endpoints 资源以及对该资源的持续更新来实现资源锁轮转的过程。但是相对于其他分布式锁的实现，普遍是直接基于现有的中间件实现，比如 redis、zookeeper、etcd 等，其所有对锁的操作都是原子性的，那 k8s 选举过程中的原子操作是如何实现的？k8s 中的原子操作最终也是通过 etcd 实现的，其在做 update 更新锁的操作时采用的是乐观锁，通过对比 resourceVersion 实现的，详细的实现下节再讲。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/api-resource-1.png\" alt=\"api resource\"></p>\n<p>参考文档：<br><a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/\" target=\"_blank\" rel=\"noopener\">API OVERVIEW</a><br><a href=\"https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/\" target=\"_blank\" rel=\"noopener\">Simple leader election with Kubernetes and Docker</a></p>\n"},{"title":"kubernetes 指标采集组件 metrics-server 的部署","date":"2019-04-14T13:27:30.000Z","type":"metrics-server","_content":"\n[metrics-server](https://github.com/kubernetes-incubator/metrics-server) 是一个采集集群中指标的组件，类似于 cadvisor，在 v1.8 版本中引入，官方将其作为 heapster 的替代者，metric-server 属于 core metrics(核心指标)，提供 API metrics.k8s.io，仅可以查看 node、pod 当前 CPU/Memory/Storage 的资源使用情况，也支持通过 Metrics API 的形式获取，以此数据提供给 Dashboard、HPA、scheduler 等使用。\n\n#### 一、开启 API Aggregation\n\n由于 metrics-server 需要暴露 API，但 k8s 的 API 要统一管理，如何将 apiserver 的请求转发给 metrics-server ，解决方案就是使用 [kube-aggregator](https://github.com/kubernetes/kube-aggregator) ，所以在部署 metrics-server 之前，需要在 kube-apiserver 中开启 API Aggregation，即增加以下配置：\n\n```\n--proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt\n--proxy-client-key-file=/etc/kubernetes/certs/proxy.key\n--requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt\n--requestheader-allowed-names=aggregator\n--requestheader-extra-headers-prefix=X-Remote-Extra-\n--requestheader-group-headers=X-Remote-Group\n--requestheader-username-headers=X-Remote-User\n```\n\n如果kube-proxy没有在Master上面运行，还需要配置\n\n```\n--enable-aggregator-routing=true\n```\n\n[kube-aggregator](https://github.com/kubernetes/kube-aggregator)  的详细设计文档请参考：[configure-aggregation-layer](https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/)\n\n#### 二、部署 metrics-server\n\n##### 1、获取配置文件\n\n```\n$ git clone  https://github.com/kubernetes/kubernetes\n$ cd  kubernetes/cluster/addons/metrics-server/\n```\n\n##### 2、修改 metrics-server 配置参数\n\n修改 `resource-reader.yaml` 文件：\n\n```\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  - nodes\n  - nodes/stats    #新增这一行\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n```\n\n修改 `metrics-server-deployment.yaml`文件:\n\n```\n\n      ......\n      # metrics-server containers 启动参数作如下修改：\n      containers:\n      - name: metrics-server\n        image: k8s.gcr.io/metrics-server-amd64:v0.3.1\n        command:\n        - /metrics-server\n        - --metric-resolution=30s\n        - --kubelet-insecure-tls\n        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n        # These are needed for GKE, which doesn't support secure communication yet.\n        # Remove these lines for non-GKE clusters, and when GKE supports token-based auth.\n        #- --kubelet-port=10255\n        #- --deprecated-kubelet-completely-insecure=true\n\t\t\t\t\n\t......           \n\t# 修改启动参数：\n        command:\n          - /pod_nanny\n          - --config-dir=/etc/config\n          - --cpu=80m\n          - --extra-cpu=0.5m\n          - --memory=80Mi\n          - --extra-memory=8Mi\n          - --threshold=5\n          - --deployment=metrics-server-v0.3.1\n          - --container=metrics-server\n          - --poll-period=300000\n          - --estimator=exponential\n          # Specifies the smallest cluster (defined in number of nodes)\n          # resources will be scaled to.\n          #- --minClusterSize={{ metrics_server_min_cluster_size }}\n```\n\n##### 3、部署\n\n```\nkubectl apply -f .  \n```\n\nmetrics-server 的资源占用量会随着集群中的 Pod 数量的不断增长而不断上升，因此需要 addon-resizer 垂直扩缩 metrics-server。addon-resizer 依据集群中节点的数量线性地扩展 metrics-server，以保证其能够有能力提供完整的metrics API 服务，具体参考：[addon-resizer](https://github.com/kubernetes/autoscaler/tree/master/addon-resizer)。\n\n>  所需要的镜像可以在 [k8s-system-images](https://github.com/gosoon/k8s-system-images.git)  中下载。\n\n\n\n检查是否部署成功：\n\n```\n$ kubectl get apiservices | grep metrics\nv1beta1.metrics.k8s.io     kube-system/metrics-server   True        2m\n\n$ kubectl get pod -n kube-system\nmetrics-server-v0.3.1-65b6db6945-rpqwf   2/2     Running   0          20h\n```\n\n\n\n#### 三、metrics-server 的使用\n\n由于采集数据间隔为1分钟，等待数分钟后查看数据：\n\n```\n$ kubectl top node\nNAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nnode1            108m         2%     1532Mi          40%\n\n$ kubectl top pod -n kube-system\nNAME                                     CPU(cores)   MEMORY(bytes)\ncoredns-576cbf47c7-8v6n8                 2m           14Mi\ncoredns-576cbf47c7-qk7rk                 2m           10Mi\netcd-node1                               11m          80Mi\nkube-apiserver-node1                     17m          566Mi\nkube-controller-manager-node1            17m          67Mi\nkube-flannel-ds-amd64-8lvs2              2m           13Mi\nkube-proxy-85lhl                         3m           19Mi\nkube-scheduler-node1                     5m           16Mi\nmetrics-server-v0.3.1-65b6db6945-rpqwf   2m           19Mi\n```\n\nMetrics-server 可用 [API](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md) 列表如下：\n\n- `http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes`\n- `http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/<node-name>`\n- `http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods`\n- `http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespace/<namespace-name>/pods/<pod-name>`\n\n由于 k8s 在 v1.10 后废弃了 8080 端口，可以通过代理或者使用认证的方式访问这些 API：\n```\n$ kubectl proxy\n$ curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes\n```\n\n也可以直接通过 kubectl 命令来访问这些 API，比如：\n```\n$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes\n$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods\n$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<node-name>\n$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/<namespace-name>/pods/<pod-name>\n```\n\n","source":"_posts/k8s_metrics_server.md","raw":"---\ntitle: kubernetes 指标采集组件 metrics-server 的部署\ndate: 2019-04-14 21:27:30\ntags: [\"metrics-server\"]\ntype: \"metrics-server\"\n\n---\n\n[metrics-server](https://github.com/kubernetes-incubator/metrics-server) 是一个采集集群中指标的组件，类似于 cadvisor，在 v1.8 版本中引入，官方将其作为 heapster 的替代者，metric-server 属于 core metrics(核心指标)，提供 API metrics.k8s.io，仅可以查看 node、pod 当前 CPU/Memory/Storage 的资源使用情况，也支持通过 Metrics API 的形式获取，以此数据提供给 Dashboard、HPA、scheduler 等使用。\n\n#### 一、开启 API Aggregation\n\n由于 metrics-server 需要暴露 API，但 k8s 的 API 要统一管理，如何将 apiserver 的请求转发给 metrics-server ，解决方案就是使用 [kube-aggregator](https://github.com/kubernetes/kube-aggregator) ，所以在部署 metrics-server 之前，需要在 kube-apiserver 中开启 API Aggregation，即增加以下配置：\n\n```\n--proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt\n--proxy-client-key-file=/etc/kubernetes/certs/proxy.key\n--requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt\n--requestheader-allowed-names=aggregator\n--requestheader-extra-headers-prefix=X-Remote-Extra-\n--requestheader-group-headers=X-Remote-Group\n--requestheader-username-headers=X-Remote-User\n```\n\n如果kube-proxy没有在Master上面运行，还需要配置\n\n```\n--enable-aggregator-routing=true\n```\n\n[kube-aggregator](https://github.com/kubernetes/kube-aggregator)  的详细设计文档请参考：[configure-aggregation-layer](https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/)\n\n#### 二、部署 metrics-server\n\n##### 1、获取配置文件\n\n```\n$ git clone  https://github.com/kubernetes/kubernetes\n$ cd  kubernetes/cluster/addons/metrics-server/\n```\n\n##### 2、修改 metrics-server 配置参数\n\n修改 `resource-reader.yaml` 文件：\n\n```\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  - nodes\n  - nodes/stats    #新增这一行\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n```\n\n修改 `metrics-server-deployment.yaml`文件:\n\n```\n\n      ......\n      # metrics-server containers 启动参数作如下修改：\n      containers:\n      - name: metrics-server\n        image: k8s.gcr.io/metrics-server-amd64:v0.3.1\n        command:\n        - /metrics-server\n        - --metric-resolution=30s\n        - --kubelet-insecure-tls\n        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n        # These are needed for GKE, which doesn't support secure communication yet.\n        # Remove these lines for non-GKE clusters, and when GKE supports token-based auth.\n        #- --kubelet-port=10255\n        #- --deprecated-kubelet-completely-insecure=true\n\t\t\t\t\n\t......           \n\t# 修改启动参数：\n        command:\n          - /pod_nanny\n          - --config-dir=/etc/config\n          - --cpu=80m\n          - --extra-cpu=0.5m\n          - --memory=80Mi\n          - --extra-memory=8Mi\n          - --threshold=5\n          - --deployment=metrics-server-v0.3.1\n          - --container=metrics-server\n          - --poll-period=300000\n          - --estimator=exponential\n          # Specifies the smallest cluster (defined in number of nodes)\n          # resources will be scaled to.\n          #- --minClusterSize={{ metrics_server_min_cluster_size }}\n```\n\n##### 3、部署\n\n```\nkubectl apply -f .  \n```\n\nmetrics-server 的资源占用量会随着集群中的 Pod 数量的不断增长而不断上升，因此需要 addon-resizer 垂直扩缩 metrics-server。addon-resizer 依据集群中节点的数量线性地扩展 metrics-server，以保证其能够有能力提供完整的metrics API 服务，具体参考：[addon-resizer](https://github.com/kubernetes/autoscaler/tree/master/addon-resizer)。\n\n>  所需要的镜像可以在 [k8s-system-images](https://github.com/gosoon/k8s-system-images.git)  中下载。\n\n\n\n检查是否部署成功：\n\n```\n$ kubectl get apiservices | grep metrics\nv1beta1.metrics.k8s.io     kube-system/metrics-server   True        2m\n\n$ kubectl get pod -n kube-system\nmetrics-server-v0.3.1-65b6db6945-rpqwf   2/2     Running   0          20h\n```\n\n\n\n#### 三、metrics-server 的使用\n\n由于采集数据间隔为1分钟，等待数分钟后查看数据：\n\n```\n$ kubectl top node\nNAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nnode1            108m         2%     1532Mi          40%\n\n$ kubectl top pod -n kube-system\nNAME                                     CPU(cores)   MEMORY(bytes)\ncoredns-576cbf47c7-8v6n8                 2m           14Mi\ncoredns-576cbf47c7-qk7rk                 2m           10Mi\netcd-node1                               11m          80Mi\nkube-apiserver-node1                     17m          566Mi\nkube-controller-manager-node1            17m          67Mi\nkube-flannel-ds-amd64-8lvs2              2m           13Mi\nkube-proxy-85lhl                         3m           19Mi\nkube-scheduler-node1                     5m           16Mi\nmetrics-server-v0.3.1-65b6db6945-rpqwf   2m           19Mi\n```\n\nMetrics-server 可用 [API](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md) 列表如下：\n\n- `http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes`\n- `http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/<node-name>`\n- `http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods`\n- `http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespace/<namespace-name>/pods/<pod-name>`\n\n由于 k8s 在 v1.10 后废弃了 8080 端口，可以通过代理或者使用认证的方式访问这些 API：\n```\n$ kubectl proxy\n$ curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes\n```\n\n也可以直接通过 kubectl 命令来访问这些 API，比如：\n```\n$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes\n$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods\n$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<node-name>\n$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/<namespace-name>/pods/<pod-name>\n```\n\n","slug":"k8s_metrics_server","published":1,"updated":"2019-06-01T14:26:16.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59h000papwnwstrw469","content":"<p><a href=\"https://github.com/kubernetes-incubator/metrics-server\" target=\"_blank\" rel=\"noopener\">metrics-server</a> 是一个采集集群中指标的组件，类似于 cadvisor，在 v1.8 版本中引入，官方将其作为 heapster 的替代者，metric-server 属于 core metrics(核心指标)，提供 API metrics.k8s.io，仅可以查看 node、pod 当前 CPU/Memory/Storage 的资源使用情况，也支持通过 Metrics API 的形式获取，以此数据提供给 Dashboard、HPA、scheduler 等使用。</p>\n<h4 id=\"一、开启-API-Aggregation\"><a href=\"#一、开启-API-Aggregation\" class=\"headerlink\" title=\"一、开启 API Aggregation\"></a>一、开启 API Aggregation</h4><p>由于 metrics-server 需要暴露 API，但 k8s 的 API 要统一管理，如何将 apiserver 的请求转发给 metrics-server ，解决方案就是使用 <a href=\"https://github.com/kubernetes/kube-aggregator\" target=\"_blank\" rel=\"noopener\">kube-aggregator</a> ，所以在部署 metrics-server 之前，需要在 kube-apiserver 中开启 API Aggregation，即增加以下配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt</span><br><span class=\"line\">--proxy-client-key-file=/etc/kubernetes/certs/proxy.key</span><br><span class=\"line\">--requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt</span><br><span class=\"line\">--requestheader-allowed-names=aggregator</span><br><span class=\"line\">--requestheader-extra-headers-prefix=X-Remote-Extra-</span><br><span class=\"line\">--requestheader-group-headers=X-Remote-Group</span><br><span class=\"line\">--requestheader-username-headers=X-Remote-User</span><br></pre></td></tr></table></figure>\n<p>如果kube-proxy没有在Master上面运行，还需要配置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--enable-aggregator-routing=true</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/kubernetes/kube-aggregator\" target=\"_blank\" rel=\"noopener\">kube-aggregator</a>  的详细设计文档请参考：<a href=\"https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/\" target=\"_blank\" rel=\"noopener\">configure-aggregation-layer</a></p>\n<h4 id=\"二、部署-metrics-server\"><a href=\"#二、部署-metrics-server\" class=\"headerlink\" title=\"二、部署 metrics-server\"></a>二、部署 metrics-server</h4><h5 id=\"1、获取配置文件\"><a href=\"#1、获取配置文件\" class=\"headerlink\" title=\"1、获取配置文件\"></a>1、获取配置文件</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone  https://github.com/kubernetes/kubernetes</span><br><span class=\"line\">$ cd  kubernetes/cluster/addons/metrics-server/</span><br></pre></td></tr></table></figure>\n<h5 id=\"2、修改-metrics-server-配置参数\"><a href=\"#2、修改-metrics-server-配置参数\" class=\"headerlink\" title=\"2、修改 metrics-server 配置参数\"></a>2、修改 metrics-server 配置参数</h5><p>修改 <code>resource-reader.yaml</code> 文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rules:</span><br><span class=\"line\">- apiGroups:</span><br><span class=\"line\">  - &quot;&quot;</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">  - pods</span><br><span class=\"line\">  - nodes</span><br><span class=\"line\">  - nodes/stats    #新增这一行</span><br><span class=\"line\">  - namespaces</span><br><span class=\"line\">  verbs:</span><br><span class=\"line\">  - get</span><br><span class=\"line\">  - list</span><br><span class=\"line\">  - watch</span><br></pre></td></tr></table></figure>\n<p>修改 <code>metrics-server-deployment.yaml</code>文件:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">     ......</span><br><span class=\"line\">     # metrics-server containers 启动参数作如下修改：</span><br><span class=\"line\">     containers:</span><br><span class=\"line\">     - name: metrics-server</span><br><span class=\"line\">       image: k8s.gcr.io/metrics-server-amd64:v0.3.1</span><br><span class=\"line\">       command:</span><br><span class=\"line\">       - /metrics-server</span><br><span class=\"line\">       - --metric-resolution=30s</span><br><span class=\"line\">       - --kubelet-insecure-tls</span><br><span class=\"line\">       - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class=\"line\">       # These are needed for GKE, which doesn&apos;t support secure communication yet.</span><br><span class=\"line\">       # Remove these lines for non-GKE clusters, and when GKE supports token-based auth.</span><br><span class=\"line\">       #- --kubelet-port=10255</span><br><span class=\"line\">       #- --deprecated-kubelet-completely-insecure=true</span><br><span class=\"line\">\t\t\t</span><br><span class=\"line\">......           </span><br><span class=\"line\"># 修改启动参数：</span><br><span class=\"line\">       command:</span><br><span class=\"line\">         - /pod_nanny</span><br><span class=\"line\">         - --config-dir=/etc/config</span><br><span class=\"line\">         - --cpu=80m</span><br><span class=\"line\">         - --extra-cpu=0.5m</span><br><span class=\"line\">         - --memory=80Mi</span><br><span class=\"line\">         - --extra-memory=8Mi</span><br><span class=\"line\">         - --threshold=5</span><br><span class=\"line\">         - --deployment=metrics-server-v0.3.1</span><br><span class=\"line\">         - --container=metrics-server</span><br><span class=\"line\">         - --poll-period=300000</span><br><span class=\"line\">         - --estimator=exponential</span><br><span class=\"line\">         # Specifies the smallest cluster (defined in number of nodes)</span><br><span class=\"line\">         # resources will be scaled to.</span><br><span class=\"line\">         #- --minClusterSize=&#123;&#123; metrics_server_min_cluster_size &#125;&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3、部署\"><a href=\"#3、部署\" class=\"headerlink\" title=\"3、部署\"></a>3、部署</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl apply -f .</span><br></pre></td></tr></table></figure>\n<p>metrics-server 的资源占用量会随着集群中的 Pod 数量的不断增长而不断上升，因此需要 addon-resizer 垂直扩缩 metrics-server。addon-resizer 依据集群中节点的数量线性地扩展 metrics-server，以保证其能够有能力提供完整的metrics API 服务，具体参考：<a href=\"https://github.com/kubernetes/autoscaler/tree/master/addon-resizer\" target=\"_blank\" rel=\"noopener\">addon-resizer</a>。</p>\n<blockquote>\n<p> 所需要的镜像可以在 <a href=\"https://github.com/gosoon/k8s-system-images.git\" target=\"_blank\" rel=\"noopener\">k8s-system-images</a>  中下载。</p>\n</blockquote>\n<p>检查是否部署成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get apiservices | grep metrics</span><br><span class=\"line\">v1beta1.metrics.k8s.io     kube-system/metrics-server   True        2m</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get pod -n kube-system</span><br><span class=\"line\">metrics-server-v0.3.1-65b6db6945-rpqwf   2/2     Running   0          20h</span><br></pre></td></tr></table></figure>\n<h4 id=\"三、metrics-server-的使用\"><a href=\"#三、metrics-server-的使用\" class=\"headerlink\" title=\"三、metrics-server 的使用\"></a>三、metrics-server 的使用</h4><p>由于采集数据间隔为1分钟，等待数分钟后查看数据：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl top node</span><br><span class=\"line\">NAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class=\"line\">node1            108m         2%     1532Mi          40%</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl top pod -n kube-system</span><br><span class=\"line\">NAME                                     CPU(cores)   MEMORY(bytes)</span><br><span class=\"line\">coredns-576cbf47c7-8v6n8                 2m           14Mi</span><br><span class=\"line\">coredns-576cbf47c7-qk7rk                 2m           10Mi</span><br><span class=\"line\">etcd-node1                               11m          80Mi</span><br><span class=\"line\">kube-apiserver-node1                     17m          566Mi</span><br><span class=\"line\">kube-controller-manager-node1            17m          67Mi</span><br><span class=\"line\">kube-flannel-ds-amd64-8lvs2              2m           13Mi</span><br><span class=\"line\">kube-proxy-85lhl                         3m           19Mi</span><br><span class=\"line\">kube-scheduler-node1                     5m           16Mi</span><br><span class=\"line\">metrics-server-v0.3.1-65b6db6945-rpqwf   2m           19Mi</span><br></pre></td></tr></table></figure>\n<p>Metrics-server 可用 <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md\" target=\"_blank\" rel=\"noopener\">API</a> 列表如下：</p>\n<ul>\n<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes</code></li>\n<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/&lt;node-name&gt;</code></li>\n<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods</code></li>\n<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespace/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;</code></li>\n</ul>\n<p>由于 k8s 在 v1.10 后废弃了 8080 端口，可以通过代理或者使用认证的方式访问这些 API：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl proxy</span><br><span class=\"line\">$ curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes</span><br></pre></td></tr></table></figure></p>\n<p>也可以直接通过 kubectl 命令来访问这些 API，比如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes</span><br><span class=\"line\">$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods</span><br><span class=\"line\">$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/&lt;node-name&gt;</span><br><span class=\"line\">$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p><a href=\"https://github.com/kubernetes-incubator/metrics-server\" target=\"_blank\" rel=\"noopener\">metrics-server</a> 是一个采集集群中指标的组件，类似于 cadvisor，在 v1.8 版本中引入，官方将其作为 heapster 的替代者，metric-server 属于 core metrics(核心指标)，提供 API metrics.k8s.io，仅可以查看 node、pod 当前 CPU/Memory/Storage 的资源使用情况，也支持通过 Metrics API 的形式获取，以此数据提供给 Dashboard、HPA、scheduler 等使用。</p>\n<h4 id=\"一、开启-API-Aggregation\"><a href=\"#一、开启-API-Aggregation\" class=\"headerlink\" title=\"一、开启 API Aggregation\"></a>一、开启 API Aggregation</h4><p>由于 metrics-server 需要暴露 API，但 k8s 的 API 要统一管理，如何将 apiserver 的请求转发给 metrics-server ，解决方案就是使用 <a href=\"https://github.com/kubernetes/kube-aggregator\" target=\"_blank\" rel=\"noopener\">kube-aggregator</a> ，所以在部署 metrics-server 之前，需要在 kube-apiserver 中开启 API Aggregation，即增加以下配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt</span><br><span class=\"line\">--proxy-client-key-file=/etc/kubernetes/certs/proxy.key</span><br><span class=\"line\">--requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt</span><br><span class=\"line\">--requestheader-allowed-names=aggregator</span><br><span class=\"line\">--requestheader-extra-headers-prefix=X-Remote-Extra-</span><br><span class=\"line\">--requestheader-group-headers=X-Remote-Group</span><br><span class=\"line\">--requestheader-username-headers=X-Remote-User</span><br></pre></td></tr></table></figure>\n<p>如果kube-proxy没有在Master上面运行，还需要配置</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--enable-aggregator-routing=true</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://github.com/kubernetes/kube-aggregator\" target=\"_blank\" rel=\"noopener\">kube-aggregator</a>  的详细设计文档请参考：<a href=\"https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/\" target=\"_blank\" rel=\"noopener\">configure-aggregation-layer</a></p>\n<h4 id=\"二、部署-metrics-server\"><a href=\"#二、部署-metrics-server\" class=\"headerlink\" title=\"二、部署 metrics-server\"></a>二、部署 metrics-server</h4><h5 id=\"1、获取配置文件\"><a href=\"#1、获取配置文件\" class=\"headerlink\" title=\"1、获取配置文件\"></a>1、获取配置文件</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone  https://github.com/kubernetes/kubernetes</span><br><span class=\"line\">$ cd  kubernetes/cluster/addons/metrics-server/</span><br></pre></td></tr></table></figure>\n<h5 id=\"2、修改-metrics-server-配置参数\"><a href=\"#2、修改-metrics-server-配置参数\" class=\"headerlink\" title=\"2、修改 metrics-server 配置参数\"></a>2、修改 metrics-server 配置参数</h5><p>修改 <code>resource-reader.yaml</code> 文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rules:</span><br><span class=\"line\">- apiGroups:</span><br><span class=\"line\">  - &quot;&quot;</span><br><span class=\"line\">  resources:</span><br><span class=\"line\">  - pods</span><br><span class=\"line\">  - nodes</span><br><span class=\"line\">  - nodes/stats    #新增这一行</span><br><span class=\"line\">  - namespaces</span><br><span class=\"line\">  verbs:</span><br><span class=\"line\">  - get</span><br><span class=\"line\">  - list</span><br><span class=\"line\">  - watch</span><br></pre></td></tr></table></figure>\n<p>修改 <code>metrics-server-deployment.yaml</code>文件:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">     ......</span><br><span class=\"line\">     # metrics-server containers 启动参数作如下修改：</span><br><span class=\"line\">     containers:</span><br><span class=\"line\">     - name: metrics-server</span><br><span class=\"line\">       image: k8s.gcr.io/metrics-server-amd64:v0.3.1</span><br><span class=\"line\">       command:</span><br><span class=\"line\">       - /metrics-server</span><br><span class=\"line\">       - --metric-resolution=30s</span><br><span class=\"line\">       - --kubelet-insecure-tls</span><br><span class=\"line\">       - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class=\"line\">       # These are needed for GKE, which doesn&apos;t support secure communication yet.</span><br><span class=\"line\">       # Remove these lines for non-GKE clusters, and when GKE supports token-based auth.</span><br><span class=\"line\">       #- --kubelet-port=10255</span><br><span class=\"line\">       #- --deprecated-kubelet-completely-insecure=true</span><br><span class=\"line\">\t\t\t</span><br><span class=\"line\">......           </span><br><span class=\"line\"># 修改启动参数：</span><br><span class=\"line\">       command:</span><br><span class=\"line\">         - /pod_nanny</span><br><span class=\"line\">         - --config-dir=/etc/config</span><br><span class=\"line\">         - --cpu=80m</span><br><span class=\"line\">         - --extra-cpu=0.5m</span><br><span class=\"line\">         - --memory=80Mi</span><br><span class=\"line\">         - --extra-memory=8Mi</span><br><span class=\"line\">         - --threshold=5</span><br><span class=\"line\">         - --deployment=metrics-server-v0.3.1</span><br><span class=\"line\">         - --container=metrics-server</span><br><span class=\"line\">         - --poll-period=300000</span><br><span class=\"line\">         - --estimator=exponential</span><br><span class=\"line\">         # Specifies the smallest cluster (defined in number of nodes)</span><br><span class=\"line\">         # resources will be scaled to.</span><br><span class=\"line\">         #- --minClusterSize=&#123;&#123; metrics_server_min_cluster_size &#125;&#125;</span><br></pre></td></tr></table></figure>\n<h5 id=\"3、部署\"><a href=\"#3、部署\" class=\"headerlink\" title=\"3、部署\"></a>3、部署</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl apply -f .</span><br></pre></td></tr></table></figure>\n<p>metrics-server 的资源占用量会随着集群中的 Pod 数量的不断增长而不断上升，因此需要 addon-resizer 垂直扩缩 metrics-server。addon-resizer 依据集群中节点的数量线性地扩展 metrics-server，以保证其能够有能力提供完整的metrics API 服务，具体参考：<a href=\"https://github.com/kubernetes/autoscaler/tree/master/addon-resizer\" target=\"_blank\" rel=\"noopener\">addon-resizer</a>。</p>\n<blockquote>\n<p> 所需要的镜像可以在 <a href=\"https://github.com/gosoon/k8s-system-images.git\" target=\"_blank\" rel=\"noopener\">k8s-system-images</a>  中下载。</p>\n</blockquote>\n<p>检查是否部署成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get apiservices | grep metrics</span><br><span class=\"line\">v1beta1.metrics.k8s.io     kube-system/metrics-server   True        2m</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get pod -n kube-system</span><br><span class=\"line\">metrics-server-v0.3.1-65b6db6945-rpqwf   2/2     Running   0          20h</span><br></pre></td></tr></table></figure>\n<h4 id=\"三、metrics-server-的使用\"><a href=\"#三、metrics-server-的使用\" class=\"headerlink\" title=\"三、metrics-server 的使用\"></a>三、metrics-server 的使用</h4><p>由于采集数据间隔为1分钟，等待数分钟后查看数据：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl top node</span><br><span class=\"line\">NAME             CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class=\"line\">node1            108m         2%     1532Mi          40%</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl top pod -n kube-system</span><br><span class=\"line\">NAME                                     CPU(cores)   MEMORY(bytes)</span><br><span class=\"line\">coredns-576cbf47c7-8v6n8                 2m           14Mi</span><br><span class=\"line\">coredns-576cbf47c7-qk7rk                 2m           10Mi</span><br><span class=\"line\">etcd-node1                               11m          80Mi</span><br><span class=\"line\">kube-apiserver-node1                     17m          566Mi</span><br><span class=\"line\">kube-controller-manager-node1            17m          67Mi</span><br><span class=\"line\">kube-flannel-ds-amd64-8lvs2              2m           13Mi</span><br><span class=\"line\">kube-proxy-85lhl                         3m           19Mi</span><br><span class=\"line\">kube-scheduler-node1                     5m           16Mi</span><br><span class=\"line\">metrics-server-v0.3.1-65b6db6945-rpqwf   2m           19Mi</span><br></pre></td></tr></table></figure>\n<p>Metrics-server 可用 <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md\" target=\"_blank\" rel=\"noopener\">API</a> 列表如下：</p>\n<ul>\n<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes</code></li>\n<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/&lt;node-name&gt;</code></li>\n<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods</code></li>\n<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespace/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;</code></li>\n</ul>\n<p>由于 k8s 在 v1.10 后废弃了 8080 端口，可以通过代理或者使用认证的方式访问这些 API：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl proxy</span><br><span class=\"line\">$ curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes</span><br></pre></td></tr></table></figure></p>\n<p>也可以直接通过 kubectl 命令来访问这些 API，比如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes</span><br><span class=\"line\">$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods</span><br><span class=\"line\">$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/&lt;node-name&gt;</span><br><span class=\"line\">$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;</span><br></pre></td></tr></table></figure></p>\n"},{"title":"kubernetes 集群升级至 v1.12 需要注意的几个问题","date":"2019-03-05T10:30:30.000Z","type":"v1.12","_content":"\n最近我们生产环境的集群开始升级至 v1.12 版本了，之前的版本是 v1.8，由于跨了多个版本，风险还是比较大的，官方的建议也是一个一个版本升级，k8s 每三个月出一个版本，集群上了规模后升级太麻烦，鉴于我们真正使用 k8s 中的功能还是比较少的，耦合性没有那么大，所以风险还是相对可控，测试环境运行 v1.12 一段时间后发现问题不大，于是开始升级。此处记录几个升级过程要注意的问题：\n\n### 1、注意 k8s 中 resource version 的变化\n\nk8s 中许多 resouce 都是随着 k8s 的版本变化而变化的，例如，statefulset 在 v1.8 版本中 apiVersion 是 apps/v1beta1，在 v1.12 中变为了 apps/v1。k8s 有接口可以获取到当前版本所有的 OpenAPI ：\n\n![OpenAPI](http://cdn.tianfeiyu.com/openapi-1.png)\n\n参考文档：[The Kubernetes API](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)\n\n虽然 k8s 中 resource version 都是向下兼容的，但是在升级完成后尽量使用当前版本的 resource version 避免不必要的麻烦。\n\n### 2、kubelet 配置文件格式\n\nv1.8 中 kubelet 的配置是在 /etc/kubernetes/kubelet 文件中的 KUBELET_ARGS 后面指定，但是在 v1.12 中开始使用 config.yaml 文件，即所有的配置都可以放在 yaml 文件中，由于配置是兼容的，所以暂时也可以继续用以前的方式，其中有些参数仅支持在 config.yaml 文件中指定。\n\nconfig.yaml 文件的官方说明：[Set Kubelet parameters via a config file](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/)\n\n一个例子：\n\n```\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\n- pods\neventBurst: 10\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionPressureTransitionPeriod: 5m0s\nfailSwapOn: true\nfileCheckFrequency: 20s\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 20s\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nimageMinimumGCAge: 2m0s\nmaxOpenFiles: 1000000\nmaxPods: 110\nnodeLeaseDurationSeconds: 40\nnodeStatusUpdateFrequency: 10s\noomScoreAdj: -999\npodPidsLimit: -1\nport: 10250\nstaticPodPath: /etc/kubernetes/manifests\n```\n\n将 kubelet 配置文件中的 LOG_LEVEL 参数改为大于等于 5 可以看到 config.yaml 中配置的定义，以方便排查问题：\n\n![config](http://cdn.tianfeiyu.com/iterm-1.png)\n\n对应的日志输出：\n\n```\nI0228 16:14:14.064292  191819 server.go:260] KubeletConfiguration: \nconfig.KubeletConfiguration{TypeMeta:v1.TypeMeta{Kind:\"\", \nAPIVersion:\"\"}, StaticPodPath:\"\", Sync    \nFrequency:v1.Duration{Duration:60000000000}, \nFileCheckFrequency:v1.Duration{Duration:20000000000},\nHTTPCheckFrequency:v1.Duration{Duration:20000000000}, \nStaticPodURL:\"\", StaticPodURLHeader:map[string][]string(nil),\nAddress:\"0.0.0.0\", Port:10250, \n...\n```\n\n> 注意：kubelet 配置文件中 ARGS 中定义的参数会覆盖 config.yaml 中的定义。\n\n\n### 3、feature-gates 中功能的使用\n\nv1.12 中 feature-gates 中许多功能默认为开启状态，需要根据实际场景选择，不必要的功能在配置文件中将其关闭。\n\nk8s 各版本中的 Feature 列表以及是否启用状态可以在 [Feature Gates](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/) 中查看。\n\n\n### 4、cadvisor 的使用 \n\nk8s 在 1.12 中将 cadvisor 从 kubelet 中移除了，若要使用 cadvisor，官方建议使用 DaemonSet 进行部署。由于我们一直从 cadvisor 获取容器的监控数据然后推送到自有的监控系统中进行展示，所以 cadvisor 还得继续使用。\n\n这是官方推荐的 cadvisor 部署方法，[cAdvisor Kubernetes Daemonset](\nhttps://github.com/google/cadvisor/blob/master/deploy/kubernetes/README.md)，其中用了 `k8s.gcr.io/cadvisor:v0.30.2` 镜像，在我们的测试环境中，该镜像无法启动，报错 `/sys/fs/cgroup/cpuacct,cpu: no such file or directory`, 经查 cadvisor v0.30.2 版本的镜像使用 cgroup v2，v2 版本中已经没有了 cpuacct subsystem，而 linux kernel 4.5 以上的版本才支持 cgroup v2，与我们的实际场景不太相符，最后测试发现 v0.28.0 的镜像可以正常使用。\n\n由于要兼容之前的使用方式，cadvisor 在宿主机上需要启动 4194 端口，但是创建容器又要结合自身的网络方案，最终我们使用 hostnetwork 的方式部署。\n\n```\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: kube-system\n  labels:\n    app: cadvisor\nspec:\n  selector:\n    matchLabels:\n      name: cadvisor\n  template:\n    metadata:\n      labels:\n        name: cadvisor\n    spec:\n      hostNetwork: true\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        key: enabledDiskSchedule\n        value: \"true\"\n        effect: NoSchedule\n      containers:\n      - name: cadvisor\n        image: k8s.gcr.io/cadvisor:v0.28.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n          - name: http\n            containerPort: 4194\n            protocol: TCP\n        readinessProbe:\n          tcpSocket:\n            port: 4194\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        args:\n          - --housekeeping_interval=10s\n          - --port=4194\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n```\n\n官方建议使用 kustomize 进行部署，kustomize 是 k8s 的一个配置管理工具，此处暂不详细解释。\n\n> 注意：若集群中有打 taint 的宿主，需要在 yaml 文件中加上对应的 tolerations。\n\n\n","source":"_posts/k8s_v1.12.md","raw":"---\ntitle: kubernetes 集群升级至 v1.12 需要注意的几个问题\ndate: 2019-03-05 18:30:30\ntags: \"kubernetes v1.12\"\ntype: \"v1.12\"\n\n---\n\n最近我们生产环境的集群开始升级至 v1.12 版本了，之前的版本是 v1.8，由于跨了多个版本，风险还是比较大的，官方的建议也是一个一个版本升级，k8s 每三个月出一个版本，集群上了规模后升级太麻烦，鉴于我们真正使用 k8s 中的功能还是比较少的，耦合性没有那么大，所以风险还是相对可控，测试环境运行 v1.12 一段时间后发现问题不大，于是开始升级。此处记录几个升级过程要注意的问题：\n\n### 1、注意 k8s 中 resource version 的变化\n\nk8s 中许多 resouce 都是随着 k8s 的版本变化而变化的，例如，statefulset 在 v1.8 版本中 apiVersion 是 apps/v1beta1，在 v1.12 中变为了 apps/v1。k8s 有接口可以获取到当前版本所有的 OpenAPI ：\n\n![OpenAPI](http://cdn.tianfeiyu.com/openapi-1.png)\n\n参考文档：[The Kubernetes API](https://kubernetes.io/docs/concepts/overview/kubernetes-api/)\n\n虽然 k8s 中 resource version 都是向下兼容的，但是在升级完成后尽量使用当前版本的 resource version 避免不必要的麻烦。\n\n### 2、kubelet 配置文件格式\n\nv1.8 中 kubelet 的配置是在 /etc/kubernetes/kubelet 文件中的 KUBELET_ARGS 后面指定，但是在 v1.12 中开始使用 config.yaml 文件，即所有的配置都可以放在 yaml 文件中，由于配置是兼容的，所以暂时也可以继续用以前的方式，其中有些参数仅支持在 config.yaml 文件中指定。\n\nconfig.yaml 文件的官方说明：[Set Kubelet parameters via a config file](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/)\n\n一个例子：\n\n```\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\n- pods\neventBurst: 10\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionPressureTransitionPeriod: 5m0s\nfailSwapOn: true\nfileCheckFrequency: 20s\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nhttpCheckFrequency: 20s\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nimageMinimumGCAge: 2m0s\nmaxOpenFiles: 1000000\nmaxPods: 110\nnodeLeaseDurationSeconds: 40\nnodeStatusUpdateFrequency: 10s\noomScoreAdj: -999\npodPidsLimit: -1\nport: 10250\nstaticPodPath: /etc/kubernetes/manifests\n```\n\n将 kubelet 配置文件中的 LOG_LEVEL 参数改为大于等于 5 可以看到 config.yaml 中配置的定义，以方便排查问题：\n\n![config](http://cdn.tianfeiyu.com/iterm-1.png)\n\n对应的日志输出：\n\n```\nI0228 16:14:14.064292  191819 server.go:260] KubeletConfiguration: \nconfig.KubeletConfiguration{TypeMeta:v1.TypeMeta{Kind:\"\", \nAPIVersion:\"\"}, StaticPodPath:\"\", Sync    \nFrequency:v1.Duration{Duration:60000000000}, \nFileCheckFrequency:v1.Duration{Duration:20000000000},\nHTTPCheckFrequency:v1.Duration{Duration:20000000000}, \nStaticPodURL:\"\", StaticPodURLHeader:map[string][]string(nil),\nAddress:\"0.0.0.0\", Port:10250, \n...\n```\n\n> 注意：kubelet 配置文件中 ARGS 中定义的参数会覆盖 config.yaml 中的定义。\n\n\n### 3、feature-gates 中功能的使用\n\nv1.12 中 feature-gates 中许多功能默认为开启状态，需要根据实际场景选择，不必要的功能在配置文件中将其关闭。\n\nk8s 各版本中的 Feature 列表以及是否启用状态可以在 [Feature Gates](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/) 中查看。\n\n\n### 4、cadvisor 的使用 \n\nk8s 在 1.12 中将 cadvisor 从 kubelet 中移除了，若要使用 cadvisor，官方建议使用 DaemonSet 进行部署。由于我们一直从 cadvisor 获取容器的监控数据然后推送到自有的监控系统中进行展示，所以 cadvisor 还得继续使用。\n\n这是官方推荐的 cadvisor 部署方法，[cAdvisor Kubernetes Daemonset](\nhttps://github.com/google/cadvisor/blob/master/deploy/kubernetes/README.md)，其中用了 `k8s.gcr.io/cadvisor:v0.30.2` 镜像，在我们的测试环境中，该镜像无法启动，报错 `/sys/fs/cgroup/cpuacct,cpu: no such file or directory`, 经查 cadvisor v0.30.2 版本的镜像使用 cgroup v2，v2 版本中已经没有了 cpuacct subsystem，而 linux kernel 4.5 以上的版本才支持 cgroup v2，与我们的实际场景不太相符，最后测试发现 v0.28.0 的镜像可以正常使用。\n\n由于要兼容之前的使用方式，cadvisor 在宿主机上需要启动 4194 端口，但是创建容器又要结合自身的网络方案，最终我们使用 hostnetwork 的方式部署。\n\n```\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cadvisor\n  namespace: kube-system\n  labels:\n    app: cadvisor\nspec:\n  selector:\n    matchLabels:\n      name: cadvisor\n  template:\n    metadata:\n      labels:\n        name: cadvisor\n    spec:\n      hostNetwork: true\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        key: enabledDiskSchedule\n        value: \"true\"\n        effect: NoSchedule\n      containers:\n      - name: cadvisor\n        image: k8s.gcr.io/cadvisor:v0.28.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n          - name: http\n            containerPort: 4194\n            protocol: TCP\n        readinessProbe:\n          tcpSocket:\n            port: 4194\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        args:\n          - --housekeeping_interval=10s\n          - --port=4194\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n```\n\n官方建议使用 kustomize 进行部署，kustomize 是 k8s 的一个配置管理工具，此处暂不详细解释。\n\n> 注意：若集群中有打 taint 的宿主，需要在 yaml 文件中加上对应的 tolerations。\n\n\n","slug":"k8s_v1.12","published":1,"updated":"2019-07-21T09:58:04.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59i000rapwn32b2jula","content":"<p>最近我们生产环境的集群开始升级至 v1.12 版本了，之前的版本是 v1.8，由于跨了多个版本，风险还是比较大的，官方的建议也是一个一个版本升级，k8s 每三个月出一个版本，集群上了规模后升级太麻烦，鉴于我们真正使用 k8s 中的功能还是比较少的，耦合性没有那么大，所以风险还是相对可控，测试环境运行 v1.12 一段时间后发现问题不大，于是开始升级。此处记录几个升级过程要注意的问题：</p>\n<h3 id=\"1、注意-k8s-中-resource-version-的变化\"><a href=\"#1、注意-k8s-中-resource-version-的变化\" class=\"headerlink\" title=\"1、注意 k8s 中 resource version 的变化\"></a>1、注意 k8s 中 resource version 的变化</h3><p>k8s 中许多 resouce 都是随着 k8s 的版本变化而变化的，例如，statefulset 在 v1.8 版本中 apiVersion 是 apps/v1beta1，在 v1.12 中变为了 apps/v1。k8s 有接口可以获取到当前版本所有的 OpenAPI ：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/openapi-1.png\" alt=\"OpenAPI\"></p>\n<p>参考文档：<a href=\"https://kubernetes.io/docs/concepts/overview/kubernetes-api/\" target=\"_blank\" rel=\"noopener\">The Kubernetes API</a></p>\n<p>虽然 k8s 中 resource version 都是向下兼容的，但是在升级完成后尽量使用当前版本的 resource version 避免不必要的麻烦。</p>\n<h3 id=\"2、kubelet-配置文件格式\"><a href=\"#2、kubelet-配置文件格式\" class=\"headerlink\" title=\"2、kubelet 配置文件格式\"></a>2、kubelet 配置文件格式</h3><p>v1.8 中 kubelet 的配置是在 /etc/kubernetes/kubelet 文件中的 KUBELET_ARGS 后面指定，但是在 v1.12 中开始使用 config.yaml 文件，即所有的配置都可以放在 yaml 文件中，由于配置是兼容的，所以暂时也可以继续用以前的方式，其中有些参数仅支持在 config.yaml 文件中指定。</p>\n<p>config.yaml 文件的官方说明：<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/\" target=\"_blank\" rel=\"noopener\">Set Kubelet parameters via a config file</a></p>\n<p>一个例子：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class=\"line\">kind: KubeletConfiguration</span><br><span class=\"line\">address: 0.0.0.0</span><br><span class=\"line\">- pods</span><br><span class=\"line\">eventBurst: 10</span><br><span class=\"line\">eventRecordQPS: 5</span><br><span class=\"line\">evictionHard:</span><br><span class=\"line\">  imagefs.available: 15%</span><br><span class=\"line\">  memory.available: 100Mi</span><br><span class=\"line\">  nodefs.available: 10%</span><br><span class=\"line\">  nodefs.inodesFree: 5%</span><br><span class=\"line\">evictionPressureTransitionPeriod: 5m0s</span><br><span class=\"line\">failSwapOn: true</span><br><span class=\"line\">fileCheckFrequency: 20s</span><br><span class=\"line\">healthzBindAddress: 127.0.0.1</span><br><span class=\"line\">healthzPort: 10248</span><br><span class=\"line\">httpCheckFrequency: 20s</span><br><span class=\"line\">imageGCHighThresholdPercent: 85</span><br><span class=\"line\">imageGCLowThresholdPercent: 80</span><br><span class=\"line\">imageMinimumGCAge: 2m0s</span><br><span class=\"line\">maxOpenFiles: 1000000</span><br><span class=\"line\">maxPods: 110</span><br><span class=\"line\">nodeLeaseDurationSeconds: 40</span><br><span class=\"line\">nodeStatusUpdateFrequency: 10s</span><br><span class=\"line\">oomScoreAdj: -999</span><br><span class=\"line\">podPidsLimit: -1</span><br><span class=\"line\">port: 10250</span><br><span class=\"line\">staticPodPath: /etc/kubernetes/manifests</span><br></pre></td></tr></table></figure>\n<p>将 kubelet 配置文件中的 LOG_LEVEL 参数改为大于等于 5 可以看到 config.yaml 中配置的定义，以方便排查问题：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/iterm-1.png\" alt=\"config\"></p>\n<p>对应的日志输出：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I0228 16:14:14.064292  191819 server.go:260] KubeletConfiguration: </span><br><span class=\"line\">config.KubeletConfiguration&#123;TypeMeta:v1.TypeMeta&#123;Kind:&quot;&quot;, </span><br><span class=\"line\">APIVersion:&quot;&quot;&#125;, StaticPodPath:&quot;&quot;, Sync    </span><br><span class=\"line\">Frequency:v1.Duration&#123;Duration:60000000000&#125;, </span><br><span class=\"line\">FileCheckFrequency:v1.Duration&#123;Duration:20000000000&#125;,</span><br><span class=\"line\">HTTPCheckFrequency:v1.Duration&#123;Duration:20000000000&#125;, </span><br><span class=\"line\">StaticPodURL:&quot;&quot;, StaticPodURLHeader:map[string][]string(nil),</span><br><span class=\"line\">Address:&quot;0.0.0.0&quot;, Port:10250, </span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>注意：kubelet 配置文件中 ARGS 中定义的参数会覆盖 config.yaml 中的定义。</p>\n</blockquote>\n<h3 id=\"3、feature-gates-中功能的使用\"><a href=\"#3、feature-gates-中功能的使用\" class=\"headerlink\" title=\"3、feature-gates 中功能的使用\"></a>3、feature-gates 中功能的使用</h3><p>v1.12 中 feature-gates 中许多功能默认为开启状态，需要根据实际场景选择，不必要的功能在配置文件中将其关闭。</p>\n<p>k8s 各版本中的 Feature 列表以及是否启用状态可以在 <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\" target=\"_blank\" rel=\"noopener\">Feature Gates</a> 中查看。</p>\n<h3 id=\"4、cadvisor-的使用\"><a href=\"#4、cadvisor-的使用\" class=\"headerlink\" title=\"4、cadvisor 的使用\"></a>4、cadvisor 的使用</h3><p>k8s 在 1.12 中将 cadvisor 从 kubelet 中移除了，若要使用 cadvisor，官方建议使用 DaemonSet 进行部署。由于我们一直从 cadvisor 获取容器的监控数据然后推送到自有的监控系统中进行展示，所以 cadvisor 还得继续使用。</p>\n<p>这是官方推荐的 cadvisor 部署方法，<a href=\"https://github.com/google/cadvisor/blob/master/deploy/kubernetes/README.md\" target=\"_blank\" rel=\"noopener\">cAdvisor Kubernetes Daemonset</a>，其中用了 <code>k8s.gcr.io/cadvisor:v0.30.2</code> 镜像，在我们的测试环境中，该镜像无法启动，报错 <code>/sys/fs/cgroup/cpuacct,cpu: no such file or directory</code>, 经查 cadvisor v0.30.2 版本的镜像使用 cgroup v2，v2 版本中已经没有了 cpuacct subsystem，而 linux kernel 4.5 以上的版本才支持 cgroup v2，与我们的实际场景不太相符，最后测试发现 v0.28.0 的镜像可以正常使用。</p>\n<p>由于要兼容之前的使用方式，cadvisor 在宿主机上需要启动 4194 端口，但是创建容器又要结合自身的网络方案，最终我们使用 hostnetwork 的方式部署。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apps/v1</span><br><span class=\"line\">kind: DaemonSet</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: cadvisor</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    app: cadvisor</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      name: cadvisor</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        name: cadvisor</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      hostNetwork: true</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">      - key: node-role.kubernetes.io/master</span><br><span class=\"line\">        effect: NoSchedule</span><br><span class=\"line\">        key: enabledDiskSchedule</span><br><span class=\"line\">        value: &quot;true&quot;</span><br><span class=\"line\">        effect: NoSchedule</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: cadvisor</span><br><span class=\"line\">        image: k8s.gcr.io/cadvisor:v0.28.0</span><br><span class=\"line\">        imagePullPolicy: IfNotPresent</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - name: rootfs</span><br><span class=\"line\">          mountPath: /rootfs</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        - name: var-run</span><br><span class=\"line\">          mountPath: /var/run</span><br><span class=\"line\">          readOnly: false</span><br><span class=\"line\">        - name: sys</span><br><span class=\"line\">          mountPath: /sys</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        - name: docker</span><br><span class=\"line\">          mountPath: /var/lib/docker</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">          - name: http</span><br><span class=\"line\">            containerPort: 4194</span><br><span class=\"line\">            protocol: TCP</span><br><span class=\"line\">        readinessProbe:</span><br><span class=\"line\">          tcpSocket:</span><br><span class=\"line\">            port: 4194</span><br><span class=\"line\">          initialDelaySeconds: 5</span><br><span class=\"line\">          periodSeconds: 10</span><br><span class=\"line\">        args:</span><br><span class=\"line\">          - --housekeeping_interval=10s</span><br><span class=\"line\">          - --port=4194</span><br><span class=\"line\">      terminationGracePeriodSeconds: 30</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: rootfs</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /</span><br><span class=\"line\">      - name: var-run</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /var/run</span><br><span class=\"line\">      - name: sys</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /sys</span><br><span class=\"line\">      - name: docker</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /var/lib/docker</span><br></pre></td></tr></table></figure>\n<p>官方建议使用 kustomize 进行部署，kustomize 是 k8s 的一个配置管理工具，此处暂不详细解释。</p>\n<blockquote>\n<p>注意：若集群中有打 taint 的宿主，需要在 yaml 文件中加上对应的 tolerations。</p>\n</blockquote>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>最近我们生产环境的集群开始升级至 v1.12 版本了，之前的版本是 v1.8，由于跨了多个版本，风险还是比较大的，官方的建议也是一个一个版本升级，k8s 每三个月出一个版本，集群上了规模后升级太麻烦，鉴于我们真正使用 k8s 中的功能还是比较少的，耦合性没有那么大，所以风险还是相对可控，测试环境运行 v1.12 一段时间后发现问题不大，于是开始升级。此处记录几个升级过程要注意的问题：</p>\n<h3 id=\"1、注意-k8s-中-resource-version-的变化\"><a href=\"#1、注意-k8s-中-resource-version-的变化\" class=\"headerlink\" title=\"1、注意 k8s 中 resource version 的变化\"></a>1、注意 k8s 中 resource version 的变化</h3><p>k8s 中许多 resouce 都是随着 k8s 的版本变化而变化的，例如，statefulset 在 v1.8 版本中 apiVersion 是 apps/v1beta1，在 v1.12 中变为了 apps/v1。k8s 有接口可以获取到当前版本所有的 OpenAPI ：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/openapi-1.png\" alt=\"OpenAPI\"></p>\n<p>参考文档：<a href=\"https://kubernetes.io/docs/concepts/overview/kubernetes-api/\" target=\"_blank\" rel=\"noopener\">The Kubernetes API</a></p>\n<p>虽然 k8s 中 resource version 都是向下兼容的，但是在升级完成后尽量使用当前版本的 resource version 避免不必要的麻烦。</p>\n<h3 id=\"2、kubelet-配置文件格式\"><a href=\"#2、kubelet-配置文件格式\" class=\"headerlink\" title=\"2、kubelet 配置文件格式\"></a>2、kubelet 配置文件格式</h3><p>v1.8 中 kubelet 的配置是在 /etc/kubernetes/kubelet 文件中的 KUBELET_ARGS 后面指定，但是在 v1.12 中开始使用 config.yaml 文件，即所有的配置都可以放在 yaml 文件中，由于配置是兼容的，所以暂时也可以继续用以前的方式，其中有些参数仅支持在 config.yaml 文件中指定。</p>\n<p>config.yaml 文件的官方说明：<a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/\" target=\"_blank\" rel=\"noopener\">Set Kubelet parameters via a config file</a></p>\n<p>一个例子：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class=\"line\">kind: KubeletConfiguration</span><br><span class=\"line\">address: 0.0.0.0</span><br><span class=\"line\">- pods</span><br><span class=\"line\">eventBurst: 10</span><br><span class=\"line\">eventRecordQPS: 5</span><br><span class=\"line\">evictionHard:</span><br><span class=\"line\">  imagefs.available: 15%</span><br><span class=\"line\">  memory.available: 100Mi</span><br><span class=\"line\">  nodefs.available: 10%</span><br><span class=\"line\">  nodefs.inodesFree: 5%</span><br><span class=\"line\">evictionPressureTransitionPeriod: 5m0s</span><br><span class=\"line\">failSwapOn: true</span><br><span class=\"line\">fileCheckFrequency: 20s</span><br><span class=\"line\">healthzBindAddress: 127.0.0.1</span><br><span class=\"line\">healthzPort: 10248</span><br><span class=\"line\">httpCheckFrequency: 20s</span><br><span class=\"line\">imageGCHighThresholdPercent: 85</span><br><span class=\"line\">imageGCLowThresholdPercent: 80</span><br><span class=\"line\">imageMinimumGCAge: 2m0s</span><br><span class=\"line\">maxOpenFiles: 1000000</span><br><span class=\"line\">maxPods: 110</span><br><span class=\"line\">nodeLeaseDurationSeconds: 40</span><br><span class=\"line\">nodeStatusUpdateFrequency: 10s</span><br><span class=\"line\">oomScoreAdj: -999</span><br><span class=\"line\">podPidsLimit: -1</span><br><span class=\"line\">port: 10250</span><br><span class=\"line\">staticPodPath: /etc/kubernetes/manifests</span><br></pre></td></tr></table></figure>\n<p>将 kubelet 配置文件中的 LOG_LEVEL 参数改为大于等于 5 可以看到 config.yaml 中配置的定义，以方便排查问题：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/iterm-1.png\" alt=\"config\"></p>\n<p>对应的日志输出：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">I0228 16:14:14.064292  191819 server.go:260] KubeletConfiguration: </span><br><span class=\"line\">config.KubeletConfiguration&#123;TypeMeta:v1.TypeMeta&#123;Kind:&quot;&quot;, </span><br><span class=\"line\">APIVersion:&quot;&quot;&#125;, StaticPodPath:&quot;&quot;, Sync    </span><br><span class=\"line\">Frequency:v1.Duration&#123;Duration:60000000000&#125;, </span><br><span class=\"line\">FileCheckFrequency:v1.Duration&#123;Duration:20000000000&#125;,</span><br><span class=\"line\">HTTPCheckFrequency:v1.Duration&#123;Duration:20000000000&#125;, </span><br><span class=\"line\">StaticPodURL:&quot;&quot;, StaticPodURLHeader:map[string][]string(nil),</span><br><span class=\"line\">Address:&quot;0.0.0.0&quot;, Port:10250, </span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>注意：kubelet 配置文件中 ARGS 中定义的参数会覆盖 config.yaml 中的定义。</p>\n</blockquote>\n<h3 id=\"3、feature-gates-中功能的使用\"><a href=\"#3、feature-gates-中功能的使用\" class=\"headerlink\" title=\"3、feature-gates 中功能的使用\"></a>3、feature-gates 中功能的使用</h3><p>v1.12 中 feature-gates 中许多功能默认为开启状态，需要根据实际场景选择，不必要的功能在配置文件中将其关闭。</p>\n<p>k8s 各版本中的 Feature 列表以及是否启用状态可以在 <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\" target=\"_blank\" rel=\"noopener\">Feature Gates</a> 中查看。</p>\n<h3 id=\"4、cadvisor-的使用\"><a href=\"#4、cadvisor-的使用\" class=\"headerlink\" title=\"4、cadvisor 的使用\"></a>4、cadvisor 的使用</h3><p>k8s 在 1.12 中将 cadvisor 从 kubelet 中移除了，若要使用 cadvisor，官方建议使用 DaemonSet 进行部署。由于我们一直从 cadvisor 获取容器的监控数据然后推送到自有的监控系统中进行展示，所以 cadvisor 还得继续使用。</p>\n<p>这是官方推荐的 cadvisor 部署方法，<a href=\"https://github.com/google/cadvisor/blob/master/deploy/kubernetes/README.md\" target=\"_blank\" rel=\"noopener\">cAdvisor Kubernetes Daemonset</a>，其中用了 <code>k8s.gcr.io/cadvisor:v0.30.2</code> 镜像，在我们的测试环境中，该镜像无法启动，报错 <code>/sys/fs/cgroup/cpuacct,cpu: no such file or directory</code>, 经查 cadvisor v0.30.2 版本的镜像使用 cgroup v2，v2 版本中已经没有了 cpuacct subsystem，而 linux kernel 4.5 以上的版本才支持 cgroup v2，与我们的实际场景不太相符，最后测试发现 v0.28.0 的镜像可以正常使用。</p>\n<p>由于要兼容之前的使用方式，cadvisor 在宿主机上需要启动 4194 端口，但是创建容器又要结合自身的网络方案，最终我们使用 hostnetwork 的方式部署。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apps/v1</span><br><span class=\"line\">kind: DaemonSet</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: cadvisor</span><br><span class=\"line\">  namespace: kube-system</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    app: cadvisor</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  selector:</span><br><span class=\"line\">    matchLabels:</span><br><span class=\"line\">      name: cadvisor</span><br><span class=\"line\">  template:</span><br><span class=\"line\">    metadata:</span><br><span class=\"line\">      labels:</span><br><span class=\"line\">        name: cadvisor</span><br><span class=\"line\">    spec:</span><br><span class=\"line\">      hostNetwork: true</span><br><span class=\"line\">      tolerations:</span><br><span class=\"line\">      - key: node-role.kubernetes.io/master</span><br><span class=\"line\">        effect: NoSchedule</span><br><span class=\"line\">        key: enabledDiskSchedule</span><br><span class=\"line\">        value: &quot;true&quot;</span><br><span class=\"line\">        effect: NoSchedule</span><br><span class=\"line\">      containers:</span><br><span class=\"line\">      - name: cadvisor</span><br><span class=\"line\">        image: k8s.gcr.io/cadvisor:v0.28.0</span><br><span class=\"line\">        imagePullPolicy: IfNotPresent</span><br><span class=\"line\">        volumeMounts:</span><br><span class=\"line\">        - name: rootfs</span><br><span class=\"line\">          mountPath: /rootfs</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        - name: var-run</span><br><span class=\"line\">          mountPath: /var/run</span><br><span class=\"line\">          readOnly: false</span><br><span class=\"line\">        - name: sys</span><br><span class=\"line\">          mountPath: /sys</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        - name: docker</span><br><span class=\"line\">          mountPath: /var/lib/docker</span><br><span class=\"line\">          readOnly: true</span><br><span class=\"line\">        ports:</span><br><span class=\"line\">          - name: http</span><br><span class=\"line\">            containerPort: 4194</span><br><span class=\"line\">            protocol: TCP</span><br><span class=\"line\">        readinessProbe:</span><br><span class=\"line\">          tcpSocket:</span><br><span class=\"line\">            port: 4194</span><br><span class=\"line\">          initialDelaySeconds: 5</span><br><span class=\"line\">          periodSeconds: 10</span><br><span class=\"line\">        args:</span><br><span class=\"line\">          - --housekeeping_interval=10s</span><br><span class=\"line\">          - --port=4194</span><br><span class=\"line\">      terminationGracePeriodSeconds: 30</span><br><span class=\"line\">      volumes:</span><br><span class=\"line\">      - name: rootfs</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /</span><br><span class=\"line\">      - name: var-run</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /var/run</span><br><span class=\"line\">      - name: sys</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /sys</span><br><span class=\"line\">      - name: docker</span><br><span class=\"line\">        hostPath:</span><br><span class=\"line\">          path: /var/lib/docker</span><br></pre></td></tr></table></figure>\n<p>官方建议使用 kustomize 进行部署，kustomize 是 k8s 的一个配置管理工具，此处暂不详细解释。</p>\n<blockquote>\n<p>注意：若集群中有打 taint 的宿主，需要在 yaml 文件中加上对应的 tolerations。</p>\n</blockquote>\n"},{"title":"部署高可用 kubernetes 集群","date":"2019-07-12T08:30:00.000Z","type":"HA","_content":"\n\nkubernetes 虽然具有故障自愈和容错能力，但某些组件的异常会导致整个集群不可用，生产环境中将其部署为高可用还是非常有必要的，本文会介绍如何构建一个高可用的 Kubernetes 集群。kuber-controller-manager 和 kube-scheduler 的高可用官方已经实现了，都是通过 etcd 全局锁进行选举实现的，etcd 是一个分布式，强一致的（满足 CAP 的 CP）KV 存储系统，其天然具备高可用。而 apiserver 作为整个系统的核心，所有对数据的修改操作都是通过 apiserver 间接操作 etcd 的，所以 apiserver 的高可用实现是比较关键的。 \n\n\n#### kube-apiserver 的高可用配置\n\napiserver 本身是无状态的，可以横向扩展，其借助外部负载均衡软件配置高可用也相对容易，实现方案比较多，但一般会采用外部组件 LVS 或 HAProxy 的方式实现，我们生产环境是通过 LVS 实现的。apiserver 的高可用可以分为集群外高可用和集群内高可用。集群外高可用指对于直接调用 k8s API 的外部用户（例如 kubectl 、kubelet），客户端需要调用 apiserver 的 VIP 以达到高可用，此处 LVS 的部署以及 VIP 的配置不再详细说明。\n\n集群内的高可用配置是指对于部署到集群中的 pod 访问 kubernetes，kubernetes 集群创建完成后默认会启动一个`kubernetes`的 service 供集群内的 pod 访问，service 的 ClusterIP 默认值为 `172.0.0.1` ，每一个 service 对象生成时，都会生成一个用于暴露该对象后端对应 pod 的对象 endpoints，endpoints 中可以看到 apiserver 的实例。访问 kubernetes 的 service，service 会将请求转发到 endpoints 中的 ip 上，此时若 service 中的 endpoints 中没有 IP，则表示 apiserver 无法访问。 \n\n```\n$ kubectl get svc kubernetes\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   172.0.0.1    <none>        443/TCP   21d\n\n$ kubectl get endpoints kubernetes\nNAME         ENDPOINTS                       AGE\nkubernetes   10.0.2.15:6443, 10.0.2.16:6443  21d\n```\n\nkubernetes v1.9 之前 kube-apiserver service 的高可用也就是 master ip 要加入到 kubernetes service 的 endpoints 中必须要在参数中指定 `--apiserver-count` 的值，v1.9 出现了另外一个参数 `--endpoint-reconciler-type` 要取代以前的 `--apiserver-count`，但是此时该参数默认是禁用的（Alpha 版本），v1.10 也是默认禁用的。v1.11 中 `--endpoint-reconciler-type` 参数默认开启了，默认值是 `lease`。`--apiserver-count` 参数会在 v1.13 中被移除。v1.11 和 v1.12 中还可以使用 `--apiserver-count`，但前提是需要设置 `--endpoint-reconciler-type=master-count`。也就是说在 v1.11 以及之后的版本中 apiserver 中不需要进行配置了，启用了几个 apiserver 实例默认都会加到 对应的 endpoints 中。\n\n\n\n#### kube-controller-manager 和 kube-scheduler 的高可用配置\n\nkube-controller-manager 和 kube-scheduler 是由 leader election 实现高可用的，通过向 apiserver 中的 endpoint 加锁的方式来进行 leader election， 启用 leader election 需要在组件的配置中加入以下几个参数：\n\n```\n --leader-elect=true\n --leader-elect-lease-duration=15s\n --leader-elect-renew-deadline=10s\n --leader-elect-resource-lock=endpoints\n --leader-elect-retry-period=2s \n```\n\n组件当前的 leader 会写在 endpoints 的 holderIdentity 字段中， 使用以下命令查看组件当前的 leader:\n\n```\n$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml \n\n$ kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml\n```\n\n关于 kube-controller-manager 和 kube-scheduler 高可用的实现细节可以参考之前写的一篇文章：[kubernets 中组件高可用的实现方式](http://blog.tianfeiyu.com/2019/03/13/k8s_leader_election/)。\n\n\n#### etcd 的高可用配置\n\netcd 是一个分布式集群，也是一个有状态的服务，其天生就是高可用的架构。为了防止 etcd 脑裂，其组成 etcd 集群的个数一般为奇数个(3 或 5 个节点) 。若使用物理机搭建 k8s 集群，理论上集群的规模也会比较大，此时 etcd 也应该使用 3 个或者5 个节点部署一套独立运行的集群。若想要对 etcd 做到自动化运维，可以考虑使用 [etcd-operator](https://github.com/coreos/etcd-operator) 将 etcd 集群部署在 k8s 中。\n\n\n\n **kubernetes 中组件高可用部署的一个架构图**：\n\n![kubernetes 组件高可用部署](http://cdn.tianfeiyu.com/image-1.png)\n\n\n#### 总结\n\n本文主要介绍如何配置一个高可用 kubernetes 集群，kubernetes 新版本已经越来越趋近全面 TLS + RBAC 配置，若 kubernetes 集群还在使用 8080 端口，此时每个 master 节点上的 kube-controller-manager 和 kube-scheduler 都是通过 8080 端口连接 apiserver，若节点上的 apiserver 挂掉，则 kube-controller-manager 和 kube-scheduler 也会随之挂掉。apiserver 作为集群的核心组件，其必须高可用部署，其他组件实现高可用相对容易。\n\n\n参考：\n\nhttps://k8smeetup.github.io/docs/admin/high-availability/\n\n","source":"_posts/k8s_components_ha.md","raw":"---\ntitle: 部署高可用 kubernetes 集群\ndate: 2019-07-12 16:30:00\ntags: [\"kubernetes\",\"HA\"]\ntype: \"HA\"\n\n---\n\n\nkubernetes 虽然具有故障自愈和容错能力，但某些组件的异常会导致整个集群不可用，生产环境中将其部署为高可用还是非常有必要的，本文会介绍如何构建一个高可用的 Kubernetes 集群。kuber-controller-manager 和 kube-scheduler 的高可用官方已经实现了，都是通过 etcd 全局锁进行选举实现的，etcd 是一个分布式，强一致的（满足 CAP 的 CP）KV 存储系统，其天然具备高可用。而 apiserver 作为整个系统的核心，所有对数据的修改操作都是通过 apiserver 间接操作 etcd 的，所以 apiserver 的高可用实现是比较关键的。 \n\n\n#### kube-apiserver 的高可用配置\n\napiserver 本身是无状态的，可以横向扩展，其借助外部负载均衡软件配置高可用也相对容易，实现方案比较多，但一般会采用外部组件 LVS 或 HAProxy 的方式实现，我们生产环境是通过 LVS 实现的。apiserver 的高可用可以分为集群外高可用和集群内高可用。集群外高可用指对于直接调用 k8s API 的外部用户（例如 kubectl 、kubelet），客户端需要调用 apiserver 的 VIP 以达到高可用，此处 LVS 的部署以及 VIP 的配置不再详细说明。\n\n集群内的高可用配置是指对于部署到集群中的 pod 访问 kubernetes，kubernetes 集群创建完成后默认会启动一个`kubernetes`的 service 供集群内的 pod 访问，service 的 ClusterIP 默认值为 `172.0.0.1` ，每一个 service 对象生成时，都会生成一个用于暴露该对象后端对应 pod 的对象 endpoints，endpoints 中可以看到 apiserver 的实例。访问 kubernetes 的 service，service 会将请求转发到 endpoints 中的 ip 上，此时若 service 中的 endpoints 中没有 IP，则表示 apiserver 无法访问。 \n\n```\n$ kubectl get svc kubernetes\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   172.0.0.1    <none>        443/TCP   21d\n\n$ kubectl get endpoints kubernetes\nNAME         ENDPOINTS                       AGE\nkubernetes   10.0.2.15:6443, 10.0.2.16:6443  21d\n```\n\nkubernetes v1.9 之前 kube-apiserver service 的高可用也就是 master ip 要加入到 kubernetes service 的 endpoints 中必须要在参数中指定 `--apiserver-count` 的值，v1.9 出现了另外一个参数 `--endpoint-reconciler-type` 要取代以前的 `--apiserver-count`，但是此时该参数默认是禁用的（Alpha 版本），v1.10 也是默认禁用的。v1.11 中 `--endpoint-reconciler-type` 参数默认开启了，默认值是 `lease`。`--apiserver-count` 参数会在 v1.13 中被移除。v1.11 和 v1.12 中还可以使用 `--apiserver-count`，但前提是需要设置 `--endpoint-reconciler-type=master-count`。也就是说在 v1.11 以及之后的版本中 apiserver 中不需要进行配置了，启用了几个 apiserver 实例默认都会加到 对应的 endpoints 中。\n\n\n\n#### kube-controller-manager 和 kube-scheduler 的高可用配置\n\nkube-controller-manager 和 kube-scheduler 是由 leader election 实现高可用的，通过向 apiserver 中的 endpoint 加锁的方式来进行 leader election， 启用 leader election 需要在组件的配置中加入以下几个参数：\n\n```\n --leader-elect=true\n --leader-elect-lease-duration=15s\n --leader-elect-renew-deadline=10s\n --leader-elect-resource-lock=endpoints\n --leader-elect-retry-period=2s \n```\n\n组件当前的 leader 会写在 endpoints 的 holderIdentity 字段中， 使用以下命令查看组件当前的 leader:\n\n```\n$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml \n\n$ kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml\n```\n\n关于 kube-controller-manager 和 kube-scheduler 高可用的实现细节可以参考之前写的一篇文章：[kubernets 中组件高可用的实现方式](http://blog.tianfeiyu.com/2019/03/13/k8s_leader_election/)。\n\n\n#### etcd 的高可用配置\n\netcd 是一个分布式集群，也是一个有状态的服务，其天生就是高可用的架构。为了防止 etcd 脑裂，其组成 etcd 集群的个数一般为奇数个(3 或 5 个节点) 。若使用物理机搭建 k8s 集群，理论上集群的规模也会比较大，此时 etcd 也应该使用 3 个或者5 个节点部署一套独立运行的集群。若想要对 etcd 做到自动化运维，可以考虑使用 [etcd-operator](https://github.com/coreos/etcd-operator) 将 etcd 集群部署在 k8s 中。\n\n\n\n **kubernetes 中组件高可用部署的一个架构图**：\n\n![kubernetes 组件高可用部署](http://cdn.tianfeiyu.com/image-1.png)\n\n\n#### 总结\n\n本文主要介绍如何配置一个高可用 kubernetes 集群，kubernetes 新版本已经越来越趋近全面 TLS + RBAC 配置，若 kubernetes 集群还在使用 8080 端口，此时每个 master 节点上的 kube-controller-manager 和 kube-scheduler 都是通过 8080 端口连接 apiserver，若节点上的 apiserver 挂掉，则 kube-controller-manager 和 kube-scheduler 也会随之挂掉。apiserver 作为集群的核心组件，其必须高可用部署，其他组件实现高可用相对容易。\n\n\n参考：\n\nhttps://k8smeetup.github.io/docs/admin/high-availability/\n\n","slug":"k8s_components_ha","published":1,"updated":"2019-07-12T08:56:21.988Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59i000tapwnroqcz5fp","content":"<p>kubernetes 虽然具有故障自愈和容错能力，但某些组件的异常会导致整个集群不可用，生产环境中将其部署为高可用还是非常有必要的，本文会介绍如何构建一个高可用的 Kubernetes 集群。kuber-controller-manager 和 kube-scheduler 的高可用官方已经实现了，都是通过 etcd 全局锁进行选举实现的，etcd 是一个分布式，强一致的（满足 CAP 的 CP）KV 存储系统，其天然具备高可用。而 apiserver 作为整个系统的核心，所有对数据的修改操作都是通过 apiserver 间接操作 etcd 的，所以 apiserver 的高可用实现是比较关键的。 </p>\n<h4 id=\"kube-apiserver-的高可用配置\"><a href=\"#kube-apiserver-的高可用配置\" class=\"headerlink\" title=\"kube-apiserver 的高可用配置\"></a>kube-apiserver 的高可用配置</h4><p>apiserver 本身是无状态的，可以横向扩展，其借助外部负载均衡软件配置高可用也相对容易，实现方案比较多，但一般会采用外部组件 LVS 或 HAProxy 的方式实现，我们生产环境是通过 LVS 实现的。apiserver 的高可用可以分为集群外高可用和集群内高可用。集群外高可用指对于直接调用 k8s API 的外部用户（例如 kubectl 、kubelet），客户端需要调用 apiserver 的 VIP 以达到高可用，此处 LVS 的部署以及 VIP 的配置不再详细说明。</p>\n<p>集群内的高可用配置是指对于部署到集群中的 pod 访问 kubernetes，kubernetes 集群创建完成后默认会启动一个<code>kubernetes</code>的 service 供集群内的 pod 访问，service 的 ClusterIP 默认值为 <code>172.0.0.1</code> ，每一个 service 对象生成时，都会生成一个用于暴露该对象后端对应 pod 的对象 endpoints，endpoints 中可以看到 apiserver 的实例。访问 kubernetes 的 service，service 会将请求转发到 endpoints 中的 ip 上，此时若 service 中的 endpoints 中没有 IP，则表示 apiserver 无法访问。 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get svc kubernetes</span><br><span class=\"line\">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class=\"line\">kubernetes   ClusterIP   172.0.0.1    &lt;none&gt;        443/TCP   21d</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get endpoints kubernetes</span><br><span class=\"line\">NAME         ENDPOINTS                       AGE</span><br><span class=\"line\">kubernetes   10.0.2.15:6443, 10.0.2.16:6443  21d</span><br></pre></td></tr></table></figure>\n<p>kubernetes v1.9 之前 kube-apiserver service 的高可用也就是 master ip 要加入到 kubernetes service 的 endpoints 中必须要在参数中指定 <code>--apiserver-count</code> 的值，v1.9 出现了另外一个参数 <code>--endpoint-reconciler-type</code> 要取代以前的 <code>--apiserver-count</code>，但是此时该参数默认是禁用的（Alpha 版本），v1.10 也是默认禁用的。v1.11 中 <code>--endpoint-reconciler-type</code> 参数默认开启了，默认值是 <code>lease</code>。<code>--apiserver-count</code> 参数会在 v1.13 中被移除。v1.11 和 v1.12 中还可以使用 <code>--apiserver-count</code>，但前提是需要设置 <code>--endpoint-reconciler-type=master-count</code>。也就是说在 v1.11 以及之后的版本中 apiserver 中不需要进行配置了，启用了几个 apiserver 实例默认都会加到 对应的 endpoints 中。</p>\n<h4 id=\"kube-controller-manager-和-kube-scheduler-的高可用配置\"><a href=\"#kube-controller-manager-和-kube-scheduler-的高可用配置\" class=\"headerlink\" title=\"kube-controller-manager 和 kube-scheduler 的高可用配置\"></a>kube-controller-manager 和 kube-scheduler 的高可用配置</h4><p>kube-controller-manager 和 kube-scheduler 是由 leader election 实现高可用的，通过向 apiserver 中的 endpoint 加锁的方式来进行 leader election， 启用 leader election 需要在组件的配置中加入以下几个参数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--leader-elect=true</span><br><span class=\"line\">--leader-elect-lease-duration=15s</span><br><span class=\"line\">--leader-elect-renew-deadline=10s</span><br><span class=\"line\">--leader-elect-resource-lock=endpoints</span><br><span class=\"line\">--leader-elect-retry-period=2s</span><br></pre></td></tr></table></figure>\n<p>组件当前的 leader 会写在 endpoints 的 holderIdentity 字段中， 使用以下命令查看组件当前的 leader:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml </span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml</span><br></pre></td></tr></table></figure>\n<p>关于 kube-controller-manager 和 kube-scheduler 高可用的实现细节可以参考之前写的一篇文章：<a href=\"http://blog.tianfeiyu.com/2019/03/13/k8s_leader_election/\" target=\"_blank\" rel=\"noopener\">kubernets 中组件高可用的实现方式</a>。</p>\n<h4 id=\"etcd-的高可用配置\"><a href=\"#etcd-的高可用配置\" class=\"headerlink\" title=\"etcd 的高可用配置\"></a>etcd 的高可用配置</h4><p>etcd 是一个分布式集群，也是一个有状态的服务，其天生就是高可用的架构。为了防止 etcd 脑裂，其组成 etcd 集群的个数一般为奇数个(3 或 5 个节点) 。若使用物理机搭建 k8s 集群，理论上集群的规模也会比较大，此时 etcd 也应该使用 3 个或者5 个节点部署一套独立运行的集群。若想要对 etcd 做到自动化运维，可以考虑使用 <a href=\"https://github.com/coreos/etcd-operator\" target=\"_blank\" rel=\"noopener\">etcd-operator</a> 将 etcd 集群部署在 k8s 中。</p>\n<p> <strong>kubernetes 中组件高可用部署的一个架构图</strong>：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-1.png\" alt=\"kubernetes 组件高可用部署\"></p>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>本文主要介绍如何配置一个高可用 kubernetes 集群，kubernetes 新版本已经越来越趋近全面 TLS + RBAC 配置，若 kubernetes 集群还在使用 8080 端口，此时每个 master 节点上的 kube-controller-manager 和 kube-scheduler 都是通过 8080 端口连接 apiserver，若节点上的 apiserver 挂掉，则 kube-controller-manager 和 kube-scheduler 也会随之挂掉。apiserver 作为集群的核心组件，其必须高可用部署，其他组件实现高可用相对容易。</p>\n<p>参考：</p>\n<p><a href=\"https://k8smeetup.github.io/docs/admin/high-availability/\" target=\"_blank\" rel=\"noopener\">https://k8smeetup.github.io/docs/admin/high-availability/</a></p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"https://hanyajun.com/kubernetes/kubenetes_concept/\">kubernetes从入门到放弃--k8s基本概念</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>kubernetes 虽然具有故障自愈和容错能力，但某些组件的异常会导致整个集群不可用，生产环境中将其部署为高可用还是非常有必要的，本文会介绍如何构建一个高可用的 Kubernetes 集群。kuber-controller-manager 和 kube-scheduler 的高可用官方已经实现了，都是通过 etcd 全局锁进行选举实现的，etcd 是一个分布式，强一致的（满足 CAP 的 CP）KV 存储系统，其天然具备高可用。而 apiserver 作为整个系统的核心，所有对数据的修改操作都是通过 apiserver 间接操作 etcd 的，所以 apiserver 的高可用实现是比较关键的。 </p>\n<h4 id=\"kube-apiserver-的高可用配置\"><a href=\"#kube-apiserver-的高可用配置\" class=\"headerlink\" title=\"kube-apiserver 的高可用配置\"></a>kube-apiserver 的高可用配置</h4><p>apiserver 本身是无状态的，可以横向扩展，其借助外部负载均衡软件配置高可用也相对容易，实现方案比较多，但一般会采用外部组件 LVS 或 HAProxy 的方式实现，我们生产环境是通过 LVS 实现的。apiserver 的高可用可以分为集群外高可用和集群内高可用。集群外高可用指对于直接调用 k8s API 的外部用户（例如 kubectl 、kubelet），客户端需要调用 apiserver 的 VIP 以达到高可用，此处 LVS 的部署以及 VIP 的配置不再详细说明。</p>\n<p>集群内的高可用配置是指对于部署到集群中的 pod 访问 kubernetes，kubernetes 集群创建完成后默认会启动一个<code>kubernetes</code>的 service 供集群内的 pod 访问，service 的 ClusterIP 默认值为 <code>172.0.0.1</code> ，每一个 service 对象生成时，都会生成一个用于暴露该对象后端对应 pod 的对象 endpoints，endpoints 中可以看到 apiserver 的实例。访问 kubernetes 的 service，service 会将请求转发到 endpoints 中的 ip 上，此时若 service 中的 endpoints 中没有 IP，则表示 apiserver 无法访问。 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get svc kubernetes</span><br><span class=\"line\">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class=\"line\">kubernetes   ClusterIP   172.0.0.1    &lt;none&gt;        443/TCP   21d</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get endpoints kubernetes</span><br><span class=\"line\">NAME         ENDPOINTS                       AGE</span><br><span class=\"line\">kubernetes   10.0.2.15:6443, 10.0.2.16:6443  21d</span><br></pre></td></tr></table></figure>\n<p>kubernetes v1.9 之前 kube-apiserver service 的高可用也就是 master ip 要加入到 kubernetes service 的 endpoints 中必须要在参数中指定 <code>--apiserver-count</code> 的值，v1.9 出现了另外一个参数 <code>--endpoint-reconciler-type</code> 要取代以前的 <code>--apiserver-count</code>，但是此时该参数默认是禁用的（Alpha 版本），v1.10 也是默认禁用的。v1.11 中 <code>--endpoint-reconciler-type</code> 参数默认开启了，默认值是 <code>lease</code>。<code>--apiserver-count</code> 参数会在 v1.13 中被移除。v1.11 和 v1.12 中还可以使用 <code>--apiserver-count</code>，但前提是需要设置 <code>--endpoint-reconciler-type=master-count</code>。也就是说在 v1.11 以及之后的版本中 apiserver 中不需要进行配置了，启用了几个 apiserver 实例默认都会加到 对应的 endpoints 中。</p>\n<h4 id=\"kube-controller-manager-和-kube-scheduler-的高可用配置\"><a href=\"#kube-controller-manager-和-kube-scheduler-的高可用配置\" class=\"headerlink\" title=\"kube-controller-manager 和 kube-scheduler 的高可用配置\"></a>kube-controller-manager 和 kube-scheduler 的高可用配置</h4><p>kube-controller-manager 和 kube-scheduler 是由 leader election 实现高可用的，通过向 apiserver 中的 endpoint 加锁的方式来进行 leader election， 启用 leader election 需要在组件的配置中加入以下几个参数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">--leader-elect=true</span><br><span class=\"line\">--leader-elect-lease-duration=15s</span><br><span class=\"line\">--leader-elect-renew-deadline=10s</span><br><span class=\"line\">--leader-elect-resource-lock=endpoints</span><br><span class=\"line\">--leader-elect-retry-period=2s</span><br></pre></td></tr></table></figure>\n<p>组件当前的 leader 会写在 endpoints 的 holderIdentity 字段中， 使用以下命令查看组件当前的 leader:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml </span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml</span><br></pre></td></tr></table></figure>\n<p>关于 kube-controller-manager 和 kube-scheduler 高可用的实现细节可以参考之前写的一篇文章：<a href=\"http://blog.tianfeiyu.com/2019/03/13/k8s_leader_election/\" target=\"_blank\" rel=\"noopener\">kubernets 中组件高可用的实现方式</a>。</p>\n<h4 id=\"etcd-的高可用配置\"><a href=\"#etcd-的高可用配置\" class=\"headerlink\" title=\"etcd 的高可用配置\"></a>etcd 的高可用配置</h4><p>etcd 是一个分布式集群，也是一个有状态的服务，其天生就是高可用的架构。为了防止 etcd 脑裂，其组成 etcd 集群的个数一般为奇数个(3 或 5 个节点) 。若使用物理机搭建 k8s 集群，理论上集群的规模也会比较大，此时 etcd 也应该使用 3 个或者5 个节点部署一套独立运行的集群。若想要对 etcd 做到自动化运维，可以考虑使用 <a href=\"https://github.com/coreos/etcd-operator\" target=\"_blank\" rel=\"noopener\">etcd-operator</a> 将 etcd 集群部署在 k8s 中。</p>\n<p> <strong>kubernetes 中组件高可用部署的一个架构图</strong>：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-1.png\" alt=\"kubernetes 组件高可用部署\"></p>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>本文主要介绍如何配置一个高可用 kubernetes 集群，kubernetes 新版本已经越来越趋近全面 TLS + RBAC 配置，若 kubernetes 集群还在使用 8080 端口，此时每个 master 节点上的 kube-controller-manager 和 kube-scheduler 都是通过 8080 端口连接 apiserver，若节点上的 apiserver 挂掉，则 kube-controller-manager 和 kube-scheduler 也会随之挂掉。apiserver 作为集群的核心组件，其必须高可用部署，其他组件实现高可用相对容易。</p>\n<p>参考：</p>\n<p><a href=\"https://k8smeetup.github.io/docs/admin/high-availability/\" target=\"_blank\" rel=\"noopener\">https://k8smeetup.github.io/docs/admin/high-availability/</a></p>\n"},{"title":"kube-on-kube-operator 开发(三)","date":"2019-09-01T09:05:30.000Z","type":"kubernetes-operator","_content":"\n\n- [kube-on-kube-operator 开发(一)](http://blog.tianfeiyu.com/2019/08/05/kube_on_kube_operator_1/)\n\n- [kube-on-kube-operator 开发(二)](http://blog.tianfeiyu.com/2019/08/07/kube_on_kube_operator_2/)\n\n\n\n本文是介绍 kubernetes-operator 开发的第三篇，前几篇已经提到过 kubernetes-operator 的主要目标是实现以下三种场景中的集群管理：\n\n- kube-on-kube\n- kube-to-kube\n- kube-to-cloud-kube\n\n目前笔者主要在开发 kube-to-kube，这一节会介绍 kube-to-kube 中如何使用二进制方式部署一个集群，问什么要先支持部署二进制集群呢，可以参考之前的文章。目前 kubernetes-operator 中部署集群是通过 ansible 调用笔者写的一些脚本部署的，由于 kubernetes 二进制文件比较大，暂时仅支持离线部署，部署前请下载好所需的二进制文件，笔者也提供了部署 v1.14 需要的所有二进制文件、镜像、yaml 等。\n\n\n\n二进制安装 kubernetes 最困难的地方就在于其复杂的认证(Authentication)及鉴权(Authorization)机制，上篇文章已经介绍了 kubernetes 中的认证与鉴权机制以及其中的证书链，若安装过程中有疑问请参考  [浅析 kubernetes 的认证与鉴权机制](http://blog.tianfeiyu.com/2019/08/18/k8s_auth_rbac/)。\n\n\n\n使用 kubernetes-operator 管理集群时首选需要有一个元集群，元集群可以使用 minkube 或者 kind 部署一个单机版集群，然后将 kubernetes-operator 部署到该集群中再通过创建 CR 来部署一个业务集群，最后使用该业务集群作为元集群即可，或者也可以使用 kubernetes-operator 中部署业务集群的方式来部署元集群。\n\n\n\n> 部署集群前请先克隆 https://github.com/gosoon/kubernetes-operator 和 https://github.com/gosoon/kubernetes-utils 项目，部署集群所需要的一些工具、配置以及 bin 文件都存放在这两个项目中，你也可以使用自己的配置。\n\n### 准备环境\n\n禁用防火墙：\n\n```\n$ systemctl stop firewalld\n$ systemctl disable firewalld\n```\n\n禁用 SELinux：\n\n```\n$ setenforce 0\n$ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n```\n\n关闭 swap：\n\n```\nswapoff -a\n```\n\n修改内核参数：\n\n```\ncat <<EOF > /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsysctl --system\n```\n\n\n\n### 配置 CA 及创建 TLS 证书\n\n安装证书生成工具，本文使用 cfssl\n\n```\n$ cp kubernetes-utils/scripts/bin/certs/* /usr/bin/\n```\n\n#### etcd\n\n```\n$ cat << EOF > etcd-root-ca-csr.json\n{\n    \"CN\": \"etcd-root-ca\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 4096\n    },\n    \"names\": [\n        {\n            \"O\": \"etcd\",\n            \"OU\": \"etcd Security\",\n            \"L\": \"Beijing\",\n            \"ST\": \"Beijing\",\n            \"C\": \"CN\"\n        }\n    ],\n    \"ca\": {\n        \"expiry\": \"87600h\"\n    }\n}\nEOF\n\n$ cat << EOF > etcd-gencert.json\n{\n  \"signing\": {\n    \"default\": {\n        \"usages\": [\n          \"signing\",\n          \"key encipherment\",\n          \"server auth\",\n          \"client auth\"\n        ],\n        \"expiry\": \"87600h\"\n    }\n  }\n}\nEOF\n\n$ cat << EOF > etcd-csr.json\n{\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"O\": \"etcd\",\n            \"OU\": \"etcd Security\",\n            \"L\": \"Beijing\",\n            \"ST\": \"Beijing\",\n            \"C\": \"CN\"\n        }\n    ],\n    \"CN\": \"etcd\"\n}\nEOF\n\n$ cat << EOF > config-etcd-peer.json\n{\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"O\": \"etcd\",\n            \"OU\": \"etcd Security\",\n            \"L\": \"Beijing\",\n            \"ST\": \"Beijing\",\n            \"C\": \"CN\"\n        }\n    ],\n    \"CN\": \"etcd\"\n}\nEOF\n```\n\n\n\n```\n$ cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson -bare output/ca\n\n// 指定 etcd hosts，etcd server 和 etcd peer 中必须包含所有 etcd 的 hosts，eg：ETCD_HOSTS=\"10.0.4.15，10.0.2.15\"\n# etcd server\n$ cfssl gencert \\\n  -ca=output/ca.pem \\\n  -ca-key=output/ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=127.0.0.1,${ETCD_HOSTS} \\\n  -profile=server \\\n  server.json | cfssljson -bare output/etcd-server\n  \n# etcd peer\n$ cfssl gencert \\\n  -ca=output/ca.pem \\\n  -ca-key=output/ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=127.0.0.1,${ETCD_HOSTS} \\\n  -profile=peer \\\n  server.json | cfssljson -bare output/etcd-peer  \n```\n\n生成证书后反解 etcd server 和 peer 证书校验 ip 是否正确：\n\n```\n$ cfssl certinfo -cert etcd-peer.pem\n```\n\n#### master\n\n由于 master 组件的 CSR 配置与 kubernetes 中的认证与鉴权相关联，需要严格按照 kubernetes 中默认的 RBAC 进行配置，每个组件都有默认的 user 或者 group。\n\n```\n$ cat << EOF > ca-csr.json\n{\n  \"CN\": \"Kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Shanghai\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n$ cat << EOF > ca-config.json\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"87600h\"\n    },\n    \"profiles\": {\n      \"kubernetes\": {\n        \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"],\n        \"expiry\": \"87600h\"\n      }\n    }\n  }\n}\nEOF\n\n\n// kube-apiserver csr\n$ cat << EOF > kube-apiserver-csr.json\n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kube-controller-manager csr\n$ cat << EOF > kube-controller-manager-csr.json\n{\n  \"CN\": \"system:kube-controller-manager\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:kube-controller-manager\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kube-scheduler csr\n$ cat << EOF > kube-scheduler-csr.json\n{\n  \"CN\": \"system:kube-scheduler\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:kube-scheduler\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kubelet csr，请替换 nodeName\n$ cat << EOF > kubelet-csr.json\n{\n  \"CN\": \"system:node:<nodeName>\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:nodes\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// apiserver client csr\n$ cat << EOF > apiserver-kubelet-client-csr.json\n{\n  \"CN\": \"system:kubelet-api-admin\",\n  \"key\": {\n      \"algo\": \"rsa\",\n      \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:masters\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kube-proxy csr\n$ cat << EOF >  kube-proxy-csr.json\n{\n  \"CN\": \"system:kube-proxy\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:node-proxier\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kubectl csr\n$ cat << EOF >  admin-csr.json\n{\n  \"CN\": \"admin\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:masters\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n\n$ cfssl gencert -initca ca-csr.json | cfssljson -bare output/ca\n\n为了保证客户端与 Kubernetes API 的认证，Kubernetes API Server 凭证中必需包含 master 的静态 IP 地址,在 hostname 中指定\n# apiserver\n$ cfssl gencert \\\n  -ca=output/ca.pem \\\n  -ca-key=output/ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=10.250.0.1,${MASTER_HOSTS},${MASTER_VIP},127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc \\\n  -profile=kubernetes \\\n  kube-apiserver-csr.json | cfssljson -bare output/kube-apiserver\n\n# kubelet\nfor node in `echo ${NODE_HOSTS} | tr ',' ' '`;do\n    cfssl gencert \\\n      -ca=output/ca.pem \\\n      -ca-key=output/ca-key.pem \\\n      -config=ca-config.json \\\n      -hostname=${NODE_HOSTS} \\\n      -profile=kubernetes \\\n      kubelet-csr.json | cfssljson -bare output/kubelet\ndone\n\n# other component\nfor component in kube-controller-manager kube-scheduler kube-proxy apiserver-kubelet-client admin service-account;do\n    cfssl gencert \\\n      -ca=output/ca.pem \\\n      -ca-key=output/ca-key.pem \\\n      -config=ca-config.json \\\n      -profile=kubernetes \\\n      ${component}-csr.json | cfssljson -bare output/${component}\ndone\n```\n\n### 生成 kubeconfig \n\n```\n// 替换 apiserver \nKUBE_APISERVER=\"https://10.0.4.15:6443\"\nCERTS_DIR=\"/etc/kubernetes/ssl\"\n\n# 生成 kubectl 配置文件\necho \"Create kubectl kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kubectl.kubeconfig\nkubectl config set-credentials \"system:masters\" \\\n  --client-certificate=${CERTS_DIR}/admin.pem \\\n  --client-key=${CERTS_DIR}/admin-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kubectl.kubeconfig\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:masters \\\n  --kubeconfig=output/kubectl.kubeconfig\nkubectl config use-context default --kubeconfig=output/kubectl.kubeconfig\n\n# 生成 kube-controller-manager 配置文件\necho \"Create kube-controller-manager kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kube-controller-manager.kubeconfig\nkubectl config set-credentials \"system:kube-controller-manager\" \\\n  --client-certificate=${CERTS_DIR}/kube-controller-manager.pem \\\n  --client-key=${CERTS_DIR}/kube-controller-manager-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kube-controller-manager.kubeconfig\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-controller-manager \\\n  --kubeconfig=output/kube-controller-manager.kubeconfig\nkubectl config use-context default --kubeconfig=output/kube-controller-manager.kubeconfig\n\n# 生成 kube-scheduler 配置文件\necho \"Create kube-scheduler kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kube-scheduler.kubeconfig\nkubectl config set-credentials \"system:kube-scheduler\" \\\n  --client-certificate=${CERTS_DIR}/kube-scheduler.pem \\\n  --client-key=${CERTS_DIR}/kube-scheduler-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kube-scheduler.kubeconfig\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-scheduler \\\n  --kubeconfig=output/kube-scheduler.kubeconfig\nkubectl config use-context default --kubeconfig=output/kube-scheduler.kubeconfig\n\n\n# 生成 kubelet 配置文件,需要添加对应的 nodeName\necho \"Create kubelet kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kubelet-${node}.kubeconfig\n\nkubectl config set-credentials system:node:${node} \\\n  --client-certificate=${CERTS_DIR}/kubelet.pem \\\n  --client-key=${CERTS_DIR}/kubelet-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kubelet-${node}.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:node:${node} \\\n  --kubeconfig=${CERTS_DIR}/kubelet-${node}.kubeconfig\nkubectl config use-context default --kubeconfig=output/kubelet-${node}.kubeconfig\n\n\n# 生成 kube-proxy 配置文件\necho \"Create kube-proxy kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kube-proxy.kubeconfig\nkubectl config set-credentials \"system:kube-proxy\" \\\n  --client-certificate=${CERTS_DIR}/kube-proxy.pem \\\n  --client-key=${CERTS_DIR}/kube-proxy-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kube-proxy.kubeconfig\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-proxy \\\n  --kubeconfig=output/kube-proxy.kubeconfig\nkubectl config use-context default --kubeconfig=output/kube-proxy.kubeconfig\n```\n\n\n\n### 部署 \n\n#### 部署 etcd\n\n拷贝证书文件：\n\n```\n$ cp output/* /etc/etcd/ssl/\n```\n\n拷贝 bin 文件：\n\n```\n$ cp kubernetes-utils/scripts/bin/etcd_v3.3.13/* /usr/bin/\n```\n\n拷贝配置文件，配置文件中的 ip 需要手动替换掉：\n\n```\n$ cp kubernetes-operator/scripts/config/etcd/etcd.conf /etc/etcd/\n```\n\n#### 部署 k8s master 组件\n\n拷贝证书文件：\n\n```\n$ cp output/* /etc/kubernetes/ssl/\n```\n\n拷贝 bin 文件：\n\n```\n$ cp kubernetes-utils/scripts/bin/kubernetes_v1.14.0/* /usr/bin/\n```\n\n拷贝配置文件，配置文件中的 ip 需要手动替换掉：\n\n```\n$ cp kubernetes-operator/scripts/config/master/* /etc/kubernetes/\n```\n\n#### 部署 k8s node 组件\n\n部署 docker，拷贝 bin 文件：\n\n```\n$ cp kubernetes-utils/scripts/bin/docker-ce-18.06.1.ce/*  /usr/bin/\n```\n\n拷贝证书文件：\n\n```\n$ cp output/* /etc/kubernetes/ssl/\n```\n\n拷贝配置文件，配置文件中的 ip 需要手动替换掉：\n\n```\n$ cp kubernetes-operator/scripts/config/node/* /etc/kubernetes/\n```\n\n#### 创建 systemd 文件\n\n拷贝所有服务的 systemd 文件：\n\n拷贝配置文件，配置文件中的 ip 需要手动替换掉：\n\n```\n$ cp kubernetes-operator/scripts/systemd/* /usr/lib/systemd/system/\n```\n\n\n\n### 启动服务 \n\n首先启动 etcd 服务，etcd 所部署的几个节点需要同时启动，否则服务会启动失败。\n\n然后依次启动 master 上的组件和 node 上的组件。\n\n### 总结\n\n本文主要讲述了 kubernetes-operator 中 kube-to-kube 部署集群的方式，介绍了主要的部署步骤，文中部署集群所有的操作都提供了脚本的方式：https://github.com/gosoon/kubernetes-operator/tree/master/scripts。 kube-to-kube 的部署方式暂时是以 ansible + 自定义脚本的方式部署，部署方式也在持续更新与完善中。接下来会继续开发 kube-on-kube 的部署方式，kube-on-kube 会将业务集群的 master 组件部署在元集群中，kube-on-kube 方式暂时会采用对 kubeadm 封装的形式进行部署。\n\n\n\n","source":"_posts/kube_on_kube_operator_3.md","raw":"---\ntitle: kube-on-kube-operator 开发(三)\ndate: 2019-09-01 17:05:30\ntags: [\"operator\",\"kube-on-kube\"]\ntype: \"kubernetes-operator\"\n\n---\n\n\n- [kube-on-kube-operator 开发(一)](http://blog.tianfeiyu.com/2019/08/05/kube_on_kube_operator_1/)\n\n- [kube-on-kube-operator 开发(二)](http://blog.tianfeiyu.com/2019/08/07/kube_on_kube_operator_2/)\n\n\n\n本文是介绍 kubernetes-operator 开发的第三篇，前几篇已经提到过 kubernetes-operator 的主要目标是实现以下三种场景中的集群管理：\n\n- kube-on-kube\n- kube-to-kube\n- kube-to-cloud-kube\n\n目前笔者主要在开发 kube-to-kube，这一节会介绍 kube-to-kube 中如何使用二进制方式部署一个集群，问什么要先支持部署二进制集群呢，可以参考之前的文章。目前 kubernetes-operator 中部署集群是通过 ansible 调用笔者写的一些脚本部署的，由于 kubernetes 二进制文件比较大，暂时仅支持离线部署，部署前请下载好所需的二进制文件，笔者也提供了部署 v1.14 需要的所有二进制文件、镜像、yaml 等。\n\n\n\n二进制安装 kubernetes 最困难的地方就在于其复杂的认证(Authentication)及鉴权(Authorization)机制，上篇文章已经介绍了 kubernetes 中的认证与鉴权机制以及其中的证书链，若安装过程中有疑问请参考  [浅析 kubernetes 的认证与鉴权机制](http://blog.tianfeiyu.com/2019/08/18/k8s_auth_rbac/)。\n\n\n\n使用 kubernetes-operator 管理集群时首选需要有一个元集群，元集群可以使用 minkube 或者 kind 部署一个单机版集群，然后将 kubernetes-operator 部署到该集群中再通过创建 CR 来部署一个业务集群，最后使用该业务集群作为元集群即可，或者也可以使用 kubernetes-operator 中部署业务集群的方式来部署元集群。\n\n\n\n> 部署集群前请先克隆 https://github.com/gosoon/kubernetes-operator 和 https://github.com/gosoon/kubernetes-utils 项目，部署集群所需要的一些工具、配置以及 bin 文件都存放在这两个项目中，你也可以使用自己的配置。\n\n### 准备环境\n\n禁用防火墙：\n\n```\n$ systemctl stop firewalld\n$ systemctl disable firewalld\n```\n\n禁用 SELinux：\n\n```\n$ setenforce 0\n$ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n```\n\n关闭 swap：\n\n```\nswapoff -a\n```\n\n修改内核参数：\n\n```\ncat <<EOF > /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsysctl --system\n```\n\n\n\n### 配置 CA 及创建 TLS 证书\n\n安装证书生成工具，本文使用 cfssl\n\n```\n$ cp kubernetes-utils/scripts/bin/certs/* /usr/bin/\n```\n\n#### etcd\n\n```\n$ cat << EOF > etcd-root-ca-csr.json\n{\n    \"CN\": \"etcd-root-ca\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 4096\n    },\n    \"names\": [\n        {\n            \"O\": \"etcd\",\n            \"OU\": \"etcd Security\",\n            \"L\": \"Beijing\",\n            \"ST\": \"Beijing\",\n            \"C\": \"CN\"\n        }\n    ],\n    \"ca\": {\n        \"expiry\": \"87600h\"\n    }\n}\nEOF\n\n$ cat << EOF > etcd-gencert.json\n{\n  \"signing\": {\n    \"default\": {\n        \"usages\": [\n          \"signing\",\n          \"key encipherment\",\n          \"server auth\",\n          \"client auth\"\n        ],\n        \"expiry\": \"87600h\"\n    }\n  }\n}\nEOF\n\n$ cat << EOF > etcd-csr.json\n{\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"O\": \"etcd\",\n            \"OU\": \"etcd Security\",\n            \"L\": \"Beijing\",\n            \"ST\": \"Beijing\",\n            \"C\": \"CN\"\n        }\n    ],\n    \"CN\": \"etcd\"\n}\nEOF\n\n$ cat << EOF > config-etcd-peer.json\n{\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"O\": \"etcd\",\n            \"OU\": \"etcd Security\",\n            \"L\": \"Beijing\",\n            \"ST\": \"Beijing\",\n            \"C\": \"CN\"\n        }\n    ],\n    \"CN\": \"etcd\"\n}\nEOF\n```\n\n\n\n```\n$ cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson -bare output/ca\n\n// 指定 etcd hosts，etcd server 和 etcd peer 中必须包含所有 etcd 的 hosts，eg：ETCD_HOSTS=\"10.0.4.15，10.0.2.15\"\n# etcd server\n$ cfssl gencert \\\n  -ca=output/ca.pem \\\n  -ca-key=output/ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=127.0.0.1,${ETCD_HOSTS} \\\n  -profile=server \\\n  server.json | cfssljson -bare output/etcd-server\n  \n# etcd peer\n$ cfssl gencert \\\n  -ca=output/ca.pem \\\n  -ca-key=output/ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=127.0.0.1,${ETCD_HOSTS} \\\n  -profile=peer \\\n  server.json | cfssljson -bare output/etcd-peer  \n```\n\n生成证书后反解 etcd server 和 peer 证书校验 ip 是否正确：\n\n```\n$ cfssl certinfo -cert etcd-peer.pem\n```\n\n#### master\n\n由于 master 组件的 CSR 配置与 kubernetes 中的认证与鉴权相关联，需要严格按照 kubernetes 中默认的 RBAC 进行配置，每个组件都有默认的 user 或者 group。\n\n```\n$ cat << EOF > ca-csr.json\n{\n  \"CN\": \"Kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Shanghai\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n$ cat << EOF > ca-config.json\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"87600h\"\n    },\n    \"profiles\": {\n      \"kubernetes\": {\n        \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"],\n        \"expiry\": \"87600h\"\n      }\n    }\n  }\n}\nEOF\n\n\n// kube-apiserver csr\n$ cat << EOF > kube-apiserver-csr.json\n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kube-controller-manager csr\n$ cat << EOF > kube-controller-manager-csr.json\n{\n  \"CN\": \"system:kube-controller-manager\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:kube-controller-manager\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kube-scheduler csr\n$ cat << EOF > kube-scheduler-csr.json\n{\n  \"CN\": \"system:kube-scheduler\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:kube-scheduler\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kubelet csr，请替换 nodeName\n$ cat << EOF > kubelet-csr.json\n{\n  \"CN\": \"system:node:<nodeName>\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:nodes\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// apiserver client csr\n$ cat << EOF > apiserver-kubelet-client-csr.json\n{\n  \"CN\": \"system:kubelet-api-admin\",\n  \"key\": {\n      \"algo\": \"rsa\",\n      \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:masters\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kube-proxy csr\n$ cat << EOF >  kube-proxy-csr.json\n{\n  \"CN\": \"system:kube-proxy\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:node-proxier\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n// kubectl csr\n$ cat << EOF >  admin-csr.json\n{\n  \"CN\": \"admin\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"China\",\n      \"L\": \"Shanghai\",\n      \"O\": \"system:masters\",\n      \"OU\": \"Kubernetes\",\n      \"ST\": \"Shanghai\"\n    }\n  ]\n}\nEOF\n\n\n$ cfssl gencert -initca ca-csr.json | cfssljson -bare output/ca\n\n为了保证客户端与 Kubernetes API 的认证，Kubernetes API Server 凭证中必需包含 master 的静态 IP 地址,在 hostname 中指定\n# apiserver\n$ cfssl gencert \\\n  -ca=output/ca.pem \\\n  -ca-key=output/ca-key.pem \\\n  -config=ca-config.json \\\n  -hostname=10.250.0.1,${MASTER_HOSTS},${MASTER_VIP},127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc \\\n  -profile=kubernetes \\\n  kube-apiserver-csr.json | cfssljson -bare output/kube-apiserver\n\n# kubelet\nfor node in `echo ${NODE_HOSTS} | tr ',' ' '`;do\n    cfssl gencert \\\n      -ca=output/ca.pem \\\n      -ca-key=output/ca-key.pem \\\n      -config=ca-config.json \\\n      -hostname=${NODE_HOSTS} \\\n      -profile=kubernetes \\\n      kubelet-csr.json | cfssljson -bare output/kubelet\ndone\n\n# other component\nfor component in kube-controller-manager kube-scheduler kube-proxy apiserver-kubelet-client admin service-account;do\n    cfssl gencert \\\n      -ca=output/ca.pem \\\n      -ca-key=output/ca-key.pem \\\n      -config=ca-config.json \\\n      -profile=kubernetes \\\n      ${component}-csr.json | cfssljson -bare output/${component}\ndone\n```\n\n### 生成 kubeconfig \n\n```\n// 替换 apiserver \nKUBE_APISERVER=\"https://10.0.4.15:6443\"\nCERTS_DIR=\"/etc/kubernetes/ssl\"\n\n# 生成 kubectl 配置文件\necho \"Create kubectl kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kubectl.kubeconfig\nkubectl config set-credentials \"system:masters\" \\\n  --client-certificate=${CERTS_DIR}/admin.pem \\\n  --client-key=${CERTS_DIR}/admin-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kubectl.kubeconfig\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:masters \\\n  --kubeconfig=output/kubectl.kubeconfig\nkubectl config use-context default --kubeconfig=output/kubectl.kubeconfig\n\n# 生成 kube-controller-manager 配置文件\necho \"Create kube-controller-manager kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kube-controller-manager.kubeconfig\nkubectl config set-credentials \"system:kube-controller-manager\" \\\n  --client-certificate=${CERTS_DIR}/kube-controller-manager.pem \\\n  --client-key=${CERTS_DIR}/kube-controller-manager-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kube-controller-manager.kubeconfig\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-controller-manager \\\n  --kubeconfig=output/kube-controller-manager.kubeconfig\nkubectl config use-context default --kubeconfig=output/kube-controller-manager.kubeconfig\n\n# 生成 kube-scheduler 配置文件\necho \"Create kube-scheduler kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kube-scheduler.kubeconfig\nkubectl config set-credentials \"system:kube-scheduler\" \\\n  --client-certificate=${CERTS_DIR}/kube-scheduler.pem \\\n  --client-key=${CERTS_DIR}/kube-scheduler-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kube-scheduler.kubeconfig\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-scheduler \\\n  --kubeconfig=output/kube-scheduler.kubeconfig\nkubectl config use-context default --kubeconfig=output/kube-scheduler.kubeconfig\n\n\n# 生成 kubelet 配置文件,需要添加对应的 nodeName\necho \"Create kubelet kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kubelet-${node}.kubeconfig\n\nkubectl config set-credentials system:node:${node} \\\n  --client-certificate=${CERTS_DIR}/kubelet.pem \\\n  --client-key=${CERTS_DIR}/kubelet-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kubelet-${node}.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:node:${node} \\\n  --kubeconfig=${CERTS_DIR}/kubelet-${node}.kubeconfig\nkubectl config use-context default --kubeconfig=output/kubelet-${node}.kubeconfig\n\n\n# 生成 kube-proxy 配置文件\necho \"Create kube-proxy kubeconfig...\"\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=${CERTS_DIR}/ca.pem \\\n  --embed-certs=true \\\n  --server=${KUBE_APISERVER} \\\n  --kubeconfig=output/kube-proxy.kubeconfig\nkubectl config set-credentials \"system:kube-proxy\" \\\n  --client-certificate=${CERTS_DIR}/kube-proxy.pem \\\n  --client-key=${CERTS_DIR}/kube-proxy-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=output/kube-proxy.kubeconfig\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-proxy \\\n  --kubeconfig=output/kube-proxy.kubeconfig\nkubectl config use-context default --kubeconfig=output/kube-proxy.kubeconfig\n```\n\n\n\n### 部署 \n\n#### 部署 etcd\n\n拷贝证书文件：\n\n```\n$ cp output/* /etc/etcd/ssl/\n```\n\n拷贝 bin 文件：\n\n```\n$ cp kubernetes-utils/scripts/bin/etcd_v3.3.13/* /usr/bin/\n```\n\n拷贝配置文件，配置文件中的 ip 需要手动替换掉：\n\n```\n$ cp kubernetes-operator/scripts/config/etcd/etcd.conf /etc/etcd/\n```\n\n#### 部署 k8s master 组件\n\n拷贝证书文件：\n\n```\n$ cp output/* /etc/kubernetes/ssl/\n```\n\n拷贝 bin 文件：\n\n```\n$ cp kubernetes-utils/scripts/bin/kubernetes_v1.14.0/* /usr/bin/\n```\n\n拷贝配置文件，配置文件中的 ip 需要手动替换掉：\n\n```\n$ cp kubernetes-operator/scripts/config/master/* /etc/kubernetes/\n```\n\n#### 部署 k8s node 组件\n\n部署 docker，拷贝 bin 文件：\n\n```\n$ cp kubernetes-utils/scripts/bin/docker-ce-18.06.1.ce/*  /usr/bin/\n```\n\n拷贝证书文件：\n\n```\n$ cp output/* /etc/kubernetes/ssl/\n```\n\n拷贝配置文件，配置文件中的 ip 需要手动替换掉：\n\n```\n$ cp kubernetes-operator/scripts/config/node/* /etc/kubernetes/\n```\n\n#### 创建 systemd 文件\n\n拷贝所有服务的 systemd 文件：\n\n拷贝配置文件，配置文件中的 ip 需要手动替换掉：\n\n```\n$ cp kubernetes-operator/scripts/systemd/* /usr/lib/systemd/system/\n```\n\n\n\n### 启动服务 \n\n首先启动 etcd 服务，etcd 所部署的几个节点需要同时启动，否则服务会启动失败。\n\n然后依次启动 master 上的组件和 node 上的组件。\n\n### 总结\n\n本文主要讲述了 kubernetes-operator 中 kube-to-kube 部署集群的方式，介绍了主要的部署步骤，文中部署集群所有的操作都提供了脚本的方式：https://github.com/gosoon/kubernetes-operator/tree/master/scripts。 kube-to-kube 的部署方式暂时是以 ansible + 自定义脚本的方式部署，部署方式也在持续更新与完善中。接下来会继续开发 kube-on-kube 的部署方式，kube-on-kube 会将业务集群的 master 组件部署在元集群中，kube-on-kube 方式暂时会采用对 kubeadm 封装的形式进行部署。\n\n\n\n","slug":"kube_on_kube_operator_3","published":1,"updated":"2019-09-01T09:32:09.529Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59j000wapwnb44cupgf","content":"<ul>\n<li><p><a href=\"http://blog.tianfeiyu.com/2019/08/05/kube_on_kube_operator_1/\" target=\"_blank\" rel=\"noopener\">kube-on-kube-operator 开发(一)</a></p>\n</li>\n<li><p><a href=\"http://blog.tianfeiyu.com/2019/08/07/kube_on_kube_operator_2/\" target=\"_blank\" rel=\"noopener\">kube-on-kube-operator 开发(二)</a></p>\n</li>\n</ul>\n<p>本文是介绍 kubernetes-operator 开发的第三篇，前几篇已经提到过 kubernetes-operator 的主要目标是实现以下三种场景中的集群管理：</p>\n<ul>\n<li>kube-on-kube</li>\n<li>kube-to-kube</li>\n<li>kube-to-cloud-kube</li>\n</ul>\n<p>目前笔者主要在开发 kube-to-kube，这一节会介绍 kube-to-kube 中如何使用二进制方式部署一个集群，问什么要先支持部署二进制集群呢，可以参考之前的文章。目前 kubernetes-operator 中部署集群是通过 ansible 调用笔者写的一些脚本部署的，由于 kubernetes 二进制文件比较大，暂时仅支持离线部署，部署前请下载好所需的二进制文件，笔者也提供了部署 v1.14 需要的所有二进制文件、镜像、yaml 等。</p>\n<p>二进制安装 kubernetes 最困难的地方就在于其复杂的认证(Authentication)及鉴权(Authorization)机制，上篇文章已经介绍了 kubernetes 中的认证与鉴权机制以及其中的证书链，若安装过程中有疑问请参考  <a href=\"http://blog.tianfeiyu.com/2019/08/18/k8s_auth_rbac/\" target=\"_blank\" rel=\"noopener\">浅析 kubernetes 的认证与鉴权机制</a>。</p>\n<p>使用 kubernetes-operator 管理集群时首选需要有一个元集群，元集群可以使用 minkube 或者 kind 部署一个单机版集群，然后将 kubernetes-operator 部署到该集群中再通过创建 CR 来部署一个业务集群，最后使用该业务集群作为元集群即可，或者也可以使用 kubernetes-operator 中部署业务集群的方式来部署元集群。</p>\n<blockquote>\n<p>部署集群前请先克隆 <a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-operator</a> 和 <a href=\"https://github.com/gosoon/kubernetes-utils\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-utils</a> 项目，部署集群所需要的一些工具、配置以及 bin 文件都存放在这两个项目中，你也可以使用自己的配置。</p>\n</blockquote>\n<h3 id=\"准备环境\"><a href=\"#准备环境\" class=\"headerlink\" title=\"准备环境\"></a>准备环境</h3><p>禁用防火墙：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl stop firewalld</span><br><span class=\"line\">$ systemctl disable firewalld</span><br></pre></td></tr></table></figure>\n<p>禁用 SELinux：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ setenforce 0</span><br><span class=\"line\">$ sed -i &apos;s/^SELINUX=enforcing$/SELINUX=permissive/&apos; /etc/selinux/config</span><br></pre></td></tr></table></figure>\n<p>关闭 swap：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">swapoff -a</span><br></pre></td></tr></table></figure>\n<p>修改内核参数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf</span><br><span class=\"line\">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class=\"line\">net.bridge.bridge-nf-call-iptables = 1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">sysctl --system</span><br></pre></td></tr></table></figure>\n<h3 id=\"配置-CA-及创建-TLS-证书\"><a href=\"#配置-CA-及创建-TLS-证书\" class=\"headerlink\" title=\"配置 CA 及创建 TLS 证书\"></a>配置 CA 及创建 TLS 证书</h3><p>安装证书生成工具，本文使用 cfssl</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-utils/scripts/bin/certs/* /usr/bin/</span><br></pre></td></tr></table></figure>\n<h4 id=\"etcd\"><a href=\"#etcd\" class=\"headerlink\" title=\"etcd\"></a>etcd</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; etcd-root-ca-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;CN&quot;: &quot;etcd-root-ca&quot;,</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 4096</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;O&quot;: &quot;etcd&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;etcd Security&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;C&quot;: &quot;CN&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;ca&quot;: &#123;</span><br><span class=\"line\">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; etcd-gencert.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;signing&quot;: &#123;</span><br><span class=\"line\">    &quot;default&quot;: &#123;</span><br><span class=\"line\">        &quot;usages&quot;: [</span><br><span class=\"line\">          &quot;signing&quot;,</span><br><span class=\"line\">          &quot;key encipherment&quot;,</span><br><span class=\"line\">          &quot;server auth&quot;,</span><br><span class=\"line\">          &quot;client auth&quot;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; etcd-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 2048</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;O&quot;: &quot;etcd&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;etcd Security&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;C&quot;: &quot;CN&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;CN&quot;: &quot;etcd&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; config-etcd-peer.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 2048</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;O&quot;: &quot;etcd&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;etcd Security&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;C&quot;: &quot;CN&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;CN&quot;: &quot;etcd&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson -bare output/ca</span><br><span class=\"line\"></span><br><span class=\"line\">// 指定 etcd hosts，etcd server 和 etcd peer 中必须包含所有 etcd 的 hosts，eg：ETCD_HOSTS=&quot;10.0.4.15，10.0.2.15&quot;</span><br><span class=\"line\"># etcd server</span><br><span class=\"line\">$ cfssl gencert \\</span><br><span class=\"line\">  -ca=output/ca.pem \\</span><br><span class=\"line\">  -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">  -config=ca-config.json \\</span><br><span class=\"line\">  -hostname=127.0.0.1,$&#123;ETCD_HOSTS&#125; \\</span><br><span class=\"line\">  -profile=server \\</span><br><span class=\"line\">  server.json | cfssljson -bare output/etcd-server</span><br><span class=\"line\">  </span><br><span class=\"line\"># etcd peer</span><br><span class=\"line\">$ cfssl gencert \\</span><br><span class=\"line\">  -ca=output/ca.pem \\</span><br><span class=\"line\">  -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">  -config=ca-config.json \\</span><br><span class=\"line\">  -hostname=127.0.0.1,$&#123;ETCD_HOSTS&#125; \\</span><br><span class=\"line\">  -profile=peer \\</span><br><span class=\"line\">  server.json | cfssljson -bare output/etcd-peer</span><br></pre></td></tr></table></figure>\n<p>生成证书后反解 etcd server 和 peer 证书校验 ip 是否正确：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cfssl certinfo -cert etcd-peer.pem</span><br></pre></td></tr></table></figure>\n<h4 id=\"master\"><a href=\"#master\" class=\"headerlink\" title=\"master\"></a>master</h4><p>由于 master 组件的 CSR 配置与 kubernetes 中的认证与鉴权相关联，需要严格按照 kubernetes 中默认的 RBAC 进行配置，每个组件都有默认的 user 或者 group。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-config.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;signing&quot;: &#123;</span><br><span class=\"line\">    &quot;default&quot;: &#123;</span><br><span class=\"line\">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;profiles&quot;: &#123;</span><br><span class=\"line\">      &quot;kubernetes&quot;: &#123;</span><br><span class=\"line\">        &quot;usages&quot;: [&quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot;],</span><br><span class=\"line\">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// kube-apiserver csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; kube-apiserver-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kube-controller-manager csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; kube-controller-manager-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kube-scheduler csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; kube-scheduler-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kubelet csr，请替换 nodeName</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; kubelet-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:node:&lt;nodeName&gt;&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:nodes&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// apiserver client csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; apiserver-kubelet-client-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:kubelet-api-admin&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">      &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">      &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kube-proxy csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt;  kube-proxy-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:kube-proxy&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:node-proxier&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kubectl csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt;  admin-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;admin&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ cfssl gencert -initca ca-csr.json | cfssljson -bare output/ca</span><br><span class=\"line\"></span><br><span class=\"line\">为了保证客户端与 Kubernetes API 的认证，Kubernetes API Server 凭证中必需包含 master 的静态 IP 地址,在 hostname 中指定</span><br><span class=\"line\"># apiserver</span><br><span class=\"line\">$ cfssl gencert \\</span><br><span class=\"line\">  -ca=output/ca.pem \\</span><br><span class=\"line\">  -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">  -config=ca-config.json \\</span><br><span class=\"line\">  -hostname=10.250.0.1,$&#123;MASTER_HOSTS&#125;,$&#123;MASTER_VIP&#125;,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc \\</span><br><span class=\"line\">  -profile=kubernetes \\</span><br><span class=\"line\">  kube-apiserver-csr.json | cfssljson -bare output/kube-apiserver</span><br><span class=\"line\"></span><br><span class=\"line\"># kubelet</span><br><span class=\"line\">for node in `echo $&#123;NODE_HOSTS&#125; | tr &apos;,&apos; &apos; &apos;`;do</span><br><span class=\"line\">    cfssl gencert \\</span><br><span class=\"line\">      -ca=output/ca.pem \\</span><br><span class=\"line\">      -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">      -config=ca-config.json \\</span><br><span class=\"line\">      -hostname=$&#123;NODE_HOSTS&#125; \\</span><br><span class=\"line\">      -profile=kubernetes \\</span><br><span class=\"line\">      kubelet-csr.json | cfssljson -bare output/kubelet</span><br><span class=\"line\">done</span><br><span class=\"line\"></span><br><span class=\"line\"># other component</span><br><span class=\"line\">for component in kube-controller-manager kube-scheduler kube-proxy apiserver-kubelet-client admin service-account;do</span><br><span class=\"line\">    cfssl gencert \\</span><br><span class=\"line\">      -ca=output/ca.pem \\</span><br><span class=\"line\">      -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">      -config=ca-config.json \\</span><br><span class=\"line\">      -profile=kubernetes \\</span><br><span class=\"line\">      $&#123;component&#125;-csr.json | cfssljson -bare output/$&#123;component&#125;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<h3 id=\"生成-kubeconfig\"><a href=\"#生成-kubeconfig\" class=\"headerlink\" title=\"生成 kubeconfig\"></a>生成 kubeconfig</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 替换 apiserver </span><br><span class=\"line\">KUBE_APISERVER=&quot;https://10.0.4.15:6443&quot;</span><br><span class=\"line\">CERTS_DIR=&quot;/etc/kubernetes/ssl&quot;</span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kubectl 配置文件</span><br><span class=\"line\">echo &quot;Create kubectl kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kubectl.kubeconfig</span><br><span class=\"line\">kubectl config set-credentials &quot;system:masters&quot; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/admin.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/admin-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kubectl.kubeconfig</span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:masters \\</span><br><span class=\"line\">  --kubeconfig=output/kubectl.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kubectl.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kube-controller-manager 配置文件</span><br><span class=\"line\">echo &quot;Create kube-controller-manager kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kube-controller-manager.kubeconfig</span><br><span class=\"line\">kubectl config set-credentials &quot;system:kube-controller-manager&quot; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/kube-controller-manager.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/kube-controller-manager-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kube-controller-manager.kubeconfig</span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:kube-controller-manager \\</span><br><span class=\"line\">  --kubeconfig=output/kube-controller-manager.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kube-controller-manager.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kube-scheduler 配置文件</span><br><span class=\"line\">echo &quot;Create kube-scheduler kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kube-scheduler.kubeconfig</span><br><span class=\"line\">kubectl config set-credentials &quot;system:kube-scheduler&quot; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/kube-scheduler.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/kube-scheduler-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kube-scheduler.kubeconfig</span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:kube-scheduler \\</span><br><span class=\"line\">  --kubeconfig=output/kube-scheduler.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kube-scheduler.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kubelet 配置文件,需要添加对应的 nodeName</span><br><span class=\"line\">echo &quot;Create kubelet kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\">kubectl config set-credentials system:node:$&#123;node&#125; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/kubelet.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/kubelet-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:node:$&#123;node&#125; \\</span><br><span class=\"line\">  --kubeconfig=$&#123;CERTS_DIR&#125;/kubelet-$&#123;node&#125;.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kube-proxy 配置文件</span><br><span class=\"line\">echo &quot;Create kube-proxy kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kube-proxy.kubeconfig</span><br><span class=\"line\">kubectl config set-credentials &quot;system:kube-proxy&quot; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/kube-proxy.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/kube-proxy-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kube-proxy.kubeconfig</span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:kube-proxy \\</span><br><span class=\"line\">  --kubeconfig=output/kube-proxy.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h3><h4 id=\"部署-etcd\"><a href=\"#部署-etcd\" class=\"headerlink\" title=\"部署 etcd\"></a>部署 etcd</h4><p>拷贝证书文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp output/* /etc/etcd/ssl/</span><br></pre></td></tr></table></figure>\n<p>拷贝 bin 文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-utils/scripts/bin/etcd_v3.3.13/* /usr/bin/</span><br></pre></td></tr></table></figure>\n<p>拷贝配置文件，配置文件中的 ip 需要手动替换掉：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-operator/scripts/config/etcd/etcd.conf /etc/etcd/</span><br></pre></td></tr></table></figure>\n<h4 id=\"部署-k8s-master-组件\"><a href=\"#部署-k8s-master-组件\" class=\"headerlink\" title=\"部署 k8s master 组件\"></a>部署 k8s master 组件</h4><p>拷贝证书文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp output/* /etc/kubernetes/ssl/</span><br></pre></td></tr></table></figure>\n<p>拷贝 bin 文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-utils/scripts/bin/kubernetes_v1.14.0/* /usr/bin/</span><br></pre></td></tr></table></figure>\n<p>拷贝配置文件，配置文件中的 ip 需要手动替换掉：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-operator/scripts/config/master/* /etc/kubernetes/</span><br></pre></td></tr></table></figure>\n<h4 id=\"部署-k8s-node-组件\"><a href=\"#部署-k8s-node-组件\" class=\"headerlink\" title=\"部署 k8s node 组件\"></a>部署 k8s node 组件</h4><p>部署 docker，拷贝 bin 文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-utils/scripts/bin/docker-ce-18.06.1.ce/*  /usr/bin/</span><br></pre></td></tr></table></figure>\n<p>拷贝证书文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp output/* /etc/kubernetes/ssl/</span><br></pre></td></tr></table></figure>\n<p>拷贝配置文件，配置文件中的 ip 需要手动替换掉：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-operator/scripts/config/node/* /etc/kubernetes/</span><br></pre></td></tr></table></figure>\n<h4 id=\"创建-systemd-文件\"><a href=\"#创建-systemd-文件\" class=\"headerlink\" title=\"创建 systemd 文件\"></a>创建 systemd 文件</h4><p>拷贝所有服务的 systemd 文件：</p>\n<p>拷贝配置文件，配置文件中的 ip 需要手动替换掉：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-operator/scripts/systemd/* /usr/lib/systemd/system/</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动服务\"><a href=\"#启动服务\" class=\"headerlink\" title=\"启动服务\"></a>启动服务</h3><p>首先启动 etcd 服务，etcd 所部署的几个节点需要同时启动，否则服务会启动失败。</p>\n<p>然后依次启动 master 上的组件和 node 上的组件。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要讲述了 kubernetes-operator 中 kube-to-kube 部署集群的方式，介绍了主要的部署步骤，文中部署集群所有的操作都提供了脚本的方式：<a href=\"https://github.com/gosoon/kubernetes-operator/tree/master/scripts。\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-operator/tree/master/scripts。</a> kube-to-kube 的部署方式暂时是以 ansible + 自定义脚本的方式部署，部署方式也在持续更新与完善中。接下来会继续开发 kube-on-kube 的部署方式，kube-on-kube 会将业务集群的 master 组件部署在元集群中，kube-on-kube 方式暂时会采用对 kubeadm 封装的形式进行部署。</p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/\">kube-on-kube-operator 开发(二)</a></li><li><a href=\"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/\">kube-on-kube-operator 开发(一)</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<ul>\n<li><p><a href=\"http://blog.tianfeiyu.com/2019/08/05/kube_on_kube_operator_1/\" target=\"_blank\" rel=\"noopener\">kube-on-kube-operator 开发(一)</a></p>\n</li>\n<li><p><a href=\"http://blog.tianfeiyu.com/2019/08/07/kube_on_kube_operator_2/\" target=\"_blank\" rel=\"noopener\">kube-on-kube-operator 开发(二)</a></p>\n</li>\n</ul>\n<p>本文是介绍 kubernetes-operator 开发的第三篇，前几篇已经提到过 kubernetes-operator 的主要目标是实现以下三种场景中的集群管理：</p>\n<ul>\n<li>kube-on-kube</li>\n<li>kube-to-kube</li>\n<li>kube-to-cloud-kube</li>\n</ul>\n<p>目前笔者主要在开发 kube-to-kube，这一节会介绍 kube-to-kube 中如何使用二进制方式部署一个集群，问什么要先支持部署二进制集群呢，可以参考之前的文章。目前 kubernetes-operator 中部署集群是通过 ansible 调用笔者写的一些脚本部署的，由于 kubernetes 二进制文件比较大，暂时仅支持离线部署，部署前请下载好所需的二进制文件，笔者也提供了部署 v1.14 需要的所有二进制文件、镜像、yaml 等。</p>\n<p>二进制安装 kubernetes 最困难的地方就在于其复杂的认证(Authentication)及鉴权(Authorization)机制，上篇文章已经介绍了 kubernetes 中的认证与鉴权机制以及其中的证书链，若安装过程中有疑问请参考  <a href=\"http://blog.tianfeiyu.com/2019/08/18/k8s_auth_rbac/\" target=\"_blank\" rel=\"noopener\">浅析 kubernetes 的认证与鉴权机制</a>。</p>\n<p>使用 kubernetes-operator 管理集群时首选需要有一个元集群，元集群可以使用 minkube 或者 kind 部署一个单机版集群，然后将 kubernetes-operator 部署到该集群中再通过创建 CR 来部署一个业务集群，最后使用该业务集群作为元集群即可，或者也可以使用 kubernetes-operator 中部署业务集群的方式来部署元集群。</p>\n<blockquote>\n<p>部署集群前请先克隆 <a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-operator</a> 和 <a href=\"https://github.com/gosoon/kubernetes-utils\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-utils</a> 项目，部署集群所需要的一些工具、配置以及 bin 文件都存放在这两个项目中，你也可以使用自己的配置。</p>\n</blockquote>\n<h3 id=\"准备环境\"><a href=\"#准备环境\" class=\"headerlink\" title=\"准备环境\"></a>准备环境</h3><p>禁用防火墙：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ systemctl stop firewalld</span><br><span class=\"line\">$ systemctl disable firewalld</span><br></pre></td></tr></table></figure>\n<p>禁用 SELinux：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ setenforce 0</span><br><span class=\"line\">$ sed -i &apos;s/^SELINUX=enforcing$/SELINUX=permissive/&apos; /etc/selinux/config</span><br></pre></td></tr></table></figure>\n<p>关闭 swap：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">swapoff -a</span><br></pre></td></tr></table></figure>\n<p>修改内核参数：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf</span><br><span class=\"line\">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class=\"line\">net.bridge.bridge-nf-call-iptables = 1</span><br><span class=\"line\">EOF</span><br><span class=\"line\">sysctl --system</span><br></pre></td></tr></table></figure>\n<h3 id=\"配置-CA-及创建-TLS-证书\"><a href=\"#配置-CA-及创建-TLS-证书\" class=\"headerlink\" title=\"配置 CA 及创建 TLS 证书\"></a>配置 CA 及创建 TLS 证书</h3><p>安装证书生成工具，本文使用 cfssl</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-utils/scripts/bin/certs/* /usr/bin/</span><br></pre></td></tr></table></figure>\n<h4 id=\"etcd\"><a href=\"#etcd\" class=\"headerlink\" title=\"etcd\"></a>etcd</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; etcd-root-ca-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;CN&quot;: &quot;etcd-root-ca&quot;,</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 4096</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;O&quot;: &quot;etcd&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;etcd Security&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;C&quot;: &quot;CN&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;ca&quot;: &#123;</span><br><span class=\"line\">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; etcd-gencert.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;signing&quot;: &#123;</span><br><span class=\"line\">    &quot;default&quot;: &#123;</span><br><span class=\"line\">        &quot;usages&quot;: [</span><br><span class=\"line\">          &quot;signing&quot;,</span><br><span class=\"line\">          &quot;key encipherment&quot;,</span><br><span class=\"line\">          &quot;server auth&quot;,</span><br><span class=\"line\">          &quot;client auth&quot;</span><br><span class=\"line\">        ],</span><br><span class=\"line\">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; etcd-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 2048</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;O&quot;: &quot;etcd&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;etcd Security&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;C&quot;: &quot;CN&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;CN&quot;: &quot;etcd&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; config-etcd-peer.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    &quot;key&quot;: &#123;</span><br><span class=\"line\">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">        &quot;size&quot;: 2048</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;names&quot;: [</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            &quot;O&quot;: &quot;etcd&quot;,</span><br><span class=\"line\">            &quot;OU&quot;: &quot;etcd Security&quot;,</span><br><span class=\"line\">            &quot;L&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;ST&quot;: &quot;Beijing&quot;,</span><br><span class=\"line\">            &quot;C&quot;: &quot;CN&quot;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    ],</span><br><span class=\"line\">    &quot;CN&quot;: &quot;etcd&quot;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson -bare output/ca</span><br><span class=\"line\"></span><br><span class=\"line\">// 指定 etcd hosts，etcd server 和 etcd peer 中必须包含所有 etcd 的 hosts，eg：ETCD_HOSTS=&quot;10.0.4.15，10.0.2.15&quot;</span><br><span class=\"line\"># etcd server</span><br><span class=\"line\">$ cfssl gencert \\</span><br><span class=\"line\">  -ca=output/ca.pem \\</span><br><span class=\"line\">  -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">  -config=ca-config.json \\</span><br><span class=\"line\">  -hostname=127.0.0.1,$&#123;ETCD_HOSTS&#125; \\</span><br><span class=\"line\">  -profile=server \\</span><br><span class=\"line\">  server.json | cfssljson -bare output/etcd-server</span><br><span class=\"line\">  </span><br><span class=\"line\"># etcd peer</span><br><span class=\"line\">$ cfssl gencert \\</span><br><span class=\"line\">  -ca=output/ca.pem \\</span><br><span class=\"line\">  -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">  -config=ca-config.json \\</span><br><span class=\"line\">  -hostname=127.0.0.1,$&#123;ETCD_HOSTS&#125; \\</span><br><span class=\"line\">  -profile=peer \\</span><br><span class=\"line\">  server.json | cfssljson -bare output/etcd-peer</span><br></pre></td></tr></table></figure>\n<p>生成证书后反解 etcd server 和 peer 证书校验 ip 是否正确：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cfssl certinfo -cert etcd-peer.pem</span><br></pre></td></tr></table></figure>\n<h4 id=\"master\"><a href=\"#master\" class=\"headerlink\" title=\"master\"></a>master</h4><p>由于 master 组件的 CSR 配置与 kubernetes 中的认证与鉴权相关联，需要严格按照 kubernetes 中默认的 RBAC 进行配置，每个组件都有默认的 user 或者 group。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; ca-config.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;signing&quot;: &#123;</span><br><span class=\"line\">    &quot;default&quot;: &#123;</span><br><span class=\"line\">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class=\"line\">    &#125;,</span><br><span class=\"line\">    &quot;profiles&quot;: &#123;</span><br><span class=\"line\">      &quot;kubernetes&quot;: &#123;</span><br><span class=\"line\">        &quot;usages&quot;: [&quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot;],</span><br><span class=\"line\">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// kube-apiserver csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; kube-apiserver-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kube-controller-manager csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; kube-controller-manager-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kube-scheduler csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; kube-scheduler-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kubelet csr，请替换 nodeName</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; kubelet-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:node:&lt;nodeName&gt;&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:nodes&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// apiserver client csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt; apiserver-kubelet-client-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:kubelet-api-admin&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">      &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">      &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kube-proxy csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt;  kube-proxy-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;system:kube-proxy&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:node-proxier&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">// kubectl csr</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt;  admin-csr.json</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;CN&quot;: &quot;admin&quot;,</span><br><span class=\"line\">  &quot;key&quot;: &#123;</span><br><span class=\"line\">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class=\"line\">    &quot;size&quot;: 2048</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;names&quot;: [</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">      &quot;C&quot;: &quot;China&quot;,</span><br><span class=\"line\">      &quot;L&quot;: &quot;Shanghai&quot;,</span><br><span class=\"line\">      &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class=\"line\">      &quot;OU&quot;: &quot;Kubernetes&quot;,</span><br><span class=\"line\">      &quot;ST&quot;: &quot;Shanghai&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ cfssl gencert -initca ca-csr.json | cfssljson -bare output/ca</span><br><span class=\"line\"></span><br><span class=\"line\">为了保证客户端与 Kubernetes API 的认证，Kubernetes API Server 凭证中必需包含 master 的静态 IP 地址,在 hostname 中指定</span><br><span class=\"line\"># apiserver</span><br><span class=\"line\">$ cfssl gencert \\</span><br><span class=\"line\">  -ca=output/ca.pem \\</span><br><span class=\"line\">  -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">  -config=ca-config.json \\</span><br><span class=\"line\">  -hostname=10.250.0.1,$&#123;MASTER_HOSTS&#125;,$&#123;MASTER_VIP&#125;,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc \\</span><br><span class=\"line\">  -profile=kubernetes \\</span><br><span class=\"line\">  kube-apiserver-csr.json | cfssljson -bare output/kube-apiserver</span><br><span class=\"line\"></span><br><span class=\"line\"># kubelet</span><br><span class=\"line\">for node in `echo $&#123;NODE_HOSTS&#125; | tr &apos;,&apos; &apos; &apos;`;do</span><br><span class=\"line\">    cfssl gencert \\</span><br><span class=\"line\">      -ca=output/ca.pem \\</span><br><span class=\"line\">      -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">      -config=ca-config.json \\</span><br><span class=\"line\">      -hostname=$&#123;NODE_HOSTS&#125; \\</span><br><span class=\"line\">      -profile=kubernetes \\</span><br><span class=\"line\">      kubelet-csr.json | cfssljson -bare output/kubelet</span><br><span class=\"line\">done</span><br><span class=\"line\"></span><br><span class=\"line\"># other component</span><br><span class=\"line\">for component in kube-controller-manager kube-scheduler kube-proxy apiserver-kubelet-client admin service-account;do</span><br><span class=\"line\">    cfssl gencert \\</span><br><span class=\"line\">      -ca=output/ca.pem \\</span><br><span class=\"line\">      -ca-key=output/ca-key.pem \\</span><br><span class=\"line\">      -config=ca-config.json \\</span><br><span class=\"line\">      -profile=kubernetes \\</span><br><span class=\"line\">      $&#123;component&#125;-csr.json | cfssljson -bare output/$&#123;component&#125;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<h3 id=\"生成-kubeconfig\"><a href=\"#生成-kubeconfig\" class=\"headerlink\" title=\"生成 kubeconfig\"></a>生成 kubeconfig</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 替换 apiserver </span><br><span class=\"line\">KUBE_APISERVER=&quot;https://10.0.4.15:6443&quot;</span><br><span class=\"line\">CERTS_DIR=&quot;/etc/kubernetes/ssl&quot;</span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kubectl 配置文件</span><br><span class=\"line\">echo &quot;Create kubectl kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kubectl.kubeconfig</span><br><span class=\"line\">kubectl config set-credentials &quot;system:masters&quot; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/admin.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/admin-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kubectl.kubeconfig</span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:masters \\</span><br><span class=\"line\">  --kubeconfig=output/kubectl.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kubectl.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kube-controller-manager 配置文件</span><br><span class=\"line\">echo &quot;Create kube-controller-manager kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kube-controller-manager.kubeconfig</span><br><span class=\"line\">kubectl config set-credentials &quot;system:kube-controller-manager&quot; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/kube-controller-manager.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/kube-controller-manager-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kube-controller-manager.kubeconfig</span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:kube-controller-manager \\</span><br><span class=\"line\">  --kubeconfig=output/kube-controller-manager.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kube-controller-manager.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kube-scheduler 配置文件</span><br><span class=\"line\">echo &quot;Create kube-scheduler kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kube-scheduler.kubeconfig</span><br><span class=\"line\">kubectl config set-credentials &quot;system:kube-scheduler&quot; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/kube-scheduler.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/kube-scheduler-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kube-scheduler.kubeconfig</span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:kube-scheduler \\</span><br><span class=\"line\">  --kubeconfig=output/kube-scheduler.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kube-scheduler.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kubelet 配置文件,需要添加对应的 nodeName</span><br><span class=\"line\">echo &quot;Create kubelet kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\">kubectl config set-credentials system:node:$&#123;node&#125; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/kubelet.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/kubelet-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:node:$&#123;node&#125; \\</span><br><span class=\"line\">  --kubeconfig=$&#123;CERTS_DIR&#125;/kubelet-$&#123;node&#125;.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfig</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 生成 kube-proxy 配置文件</span><br><span class=\"line\">echo &quot;Create kube-proxy kubeconfig...&quot;</span><br><span class=\"line\">kubectl config set-cluster kubernetes \\</span><br><span class=\"line\">  --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --server=$&#123;KUBE_APISERVER&#125; \\</span><br><span class=\"line\">  --kubeconfig=output/kube-proxy.kubeconfig</span><br><span class=\"line\">kubectl config set-credentials &quot;system:kube-proxy&quot; \\</span><br><span class=\"line\">  --client-certificate=$&#123;CERTS_DIR&#125;/kube-proxy.pem \\</span><br><span class=\"line\">  --client-key=$&#123;CERTS_DIR&#125;/kube-proxy-key.pem \\</span><br><span class=\"line\">  --embed-certs=true \\</span><br><span class=\"line\">  --kubeconfig=output/kube-proxy.kubeconfig</span><br><span class=\"line\">kubectl config set-context default \\</span><br><span class=\"line\">  --cluster=kubernetes \\</span><br><span class=\"line\">  --user=system:kube-proxy \\</span><br><span class=\"line\">  --kubeconfig=output/kube-proxy.kubeconfig</span><br><span class=\"line\">kubectl config use-context default --kubeconfig=output/kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署\"><a href=\"#部署\" class=\"headerlink\" title=\"部署\"></a>部署</h3><h4 id=\"部署-etcd\"><a href=\"#部署-etcd\" class=\"headerlink\" title=\"部署 etcd\"></a>部署 etcd</h4><p>拷贝证书文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp output/* /etc/etcd/ssl/</span><br></pre></td></tr></table></figure>\n<p>拷贝 bin 文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-utils/scripts/bin/etcd_v3.3.13/* /usr/bin/</span><br></pre></td></tr></table></figure>\n<p>拷贝配置文件，配置文件中的 ip 需要手动替换掉：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-operator/scripts/config/etcd/etcd.conf /etc/etcd/</span><br></pre></td></tr></table></figure>\n<h4 id=\"部署-k8s-master-组件\"><a href=\"#部署-k8s-master-组件\" class=\"headerlink\" title=\"部署 k8s master 组件\"></a>部署 k8s master 组件</h4><p>拷贝证书文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp output/* /etc/kubernetes/ssl/</span><br></pre></td></tr></table></figure>\n<p>拷贝 bin 文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-utils/scripts/bin/kubernetes_v1.14.0/* /usr/bin/</span><br></pre></td></tr></table></figure>\n<p>拷贝配置文件，配置文件中的 ip 需要手动替换掉：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-operator/scripts/config/master/* /etc/kubernetes/</span><br></pre></td></tr></table></figure>\n<h4 id=\"部署-k8s-node-组件\"><a href=\"#部署-k8s-node-组件\" class=\"headerlink\" title=\"部署 k8s node 组件\"></a>部署 k8s node 组件</h4><p>部署 docker，拷贝 bin 文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-utils/scripts/bin/docker-ce-18.06.1.ce/*  /usr/bin/</span><br></pre></td></tr></table></figure>\n<p>拷贝证书文件：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp output/* /etc/kubernetes/ssl/</span><br></pre></td></tr></table></figure>\n<p>拷贝配置文件，配置文件中的 ip 需要手动替换掉：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-operator/scripts/config/node/* /etc/kubernetes/</span><br></pre></td></tr></table></figure>\n<h4 id=\"创建-systemd-文件\"><a href=\"#创建-systemd-文件\" class=\"headerlink\" title=\"创建 systemd 文件\"></a>创建 systemd 文件</h4><p>拷贝所有服务的 systemd 文件：</p>\n<p>拷贝配置文件，配置文件中的 ip 需要手动替换掉：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ cp kubernetes-operator/scripts/systemd/* /usr/lib/systemd/system/</span><br></pre></td></tr></table></figure>\n<h3 id=\"启动服务\"><a href=\"#启动服务\" class=\"headerlink\" title=\"启动服务\"></a>启动服务</h3><p>首先启动 etcd 服务，etcd 所部署的几个节点需要同时启动，否则服务会启动失败。</p>\n<p>然后依次启动 master 上的组件和 node 上的组件。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要讲述了 kubernetes-operator 中 kube-to-kube 部署集群的方式，介绍了主要的部署步骤，文中部署集群所有的操作都提供了脚本的方式：<a href=\"https://github.com/gosoon/kubernetes-operator/tree/master/scripts。\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-operator/tree/master/scripts。</a> kube-to-kube 的部署方式暂时是以 ansible + 自定义脚本的方式部署，部署方式也在持续更新与完善中。接下来会继续开发 kube-on-kube 的部署方式，kube-on-kube 会将业务集群的 master 组件部署在元集群中，kube-on-kube 方式暂时会采用对 kubeadm 封装的形式进行部署。</p>\n"},{"title":"使用 kind 部署单机版 kubernetes 集群","date":"2019-09-06T02:30:30.000Z","type":"kind","_content":"\nkubernetes 从一发布开始其学习门槛就比较高，首先就是部署难，用户要想学习 kubernetes 必须要过部署这一关，社区也推出了多个部署工具帮助简化集群的部署，社区中推出的部署工具主要目标有两大类，部署测试环境与生产环境，本节主要讲述测试环境的部署，目前社区已经有多套部署方案了：\n\n- https://github.com/bsycorp/kind\n- https://github.com/ubuntu/microk8s\n- https://github.com/kinvolk/kube-spawn\n- https://github.com/kubernetes/minikube\n- https://github.com/danderson/virtuakube\n- https://github.com/kubernetes-sigs/kubeadm-dind-cluster\n\n而本文主要讲述使用 [kind](https://github.com/kubernetes-sigs/kind)（Kubernetes In Docker）部署 k8s 集群，因为 kind 使用起来实在太简单了，特别适用于在本机部署测试环境。\n\n\n\nkind 的原理就是将 k8s 所需要的所有组件，全部部署在一个 docker 容器中，只需要一个镜像即可部署一套 k8s 环境，其底层是使用 kubeadm 进行部署，CRI 使用 Containerd，CNI 使用 weave。下面就来看看如何使用 kind 部署一套 kubernetes 环境，在使用 kind 前你需要确保目标机器已经安装了 docker 服务。\n\n### 一、使用 kind 部署 k8s 集群\n\n> 以下安装环境为 mac os。\n\n安装 kind ：\n\n```\n$ wget https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-darwin-amd64\n$ chmod +x kind-darwin-amd64\n$ mv kind-darwin-amd64 /usr/local/bin/kind\n```\n\n使用 kind 部署 kubernetes 集群：\n\n```\n// 默认的 cluster name 为 kind，可以使用 --name 指定\n$ kind create cluster\nCreating cluster \"kind\" ...\n ✓ Ensuring node image (kindest/node:v1.15.3) 🖼\n ✓ Preparing nodes 📦\n ✓ Creating kubeadm config 📜\n ✓ Starting control-plane 🕹️\n ✓ Installing CNI 🔌\n ✓ Installing StorageClass 💾\nCluster creation complete. You can now use the cluster with:\n```\n\n使用 **kind create cluster** 安装，是没有指定任何配置文件的安装方式。从安装打印出的输出来看，分为 6 步：\n1. 安装基础镜像 kindest/node:v1.15.4，这个镜像里面包含了所需要的二进制文件、配置文件以及 k8s 左右组件镜像的 tar 包\n2. 准备 node，检查环境、启动镜像等工作\n3. 生成 kubeadm 的配置，然后使用 kubeadm 安装，和直接使用 kubeadm 的步骤类似\n4. 启动服务\n5. 部署 CNI 插件，kind 默认使用 weave。\n6. 创建 StorageClass。\n\n\n\n```\n// 查看 kubeconfig path\n$ kind get kubeconfig-path\n/Users/feiyu/.kube/kind-config-kind\n\n$ export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"kind\")\"\n```\n\nkind 还有多个子命令，此处不再一一详解。\n\n\n\n```\n// 查看集群信息，\n$ kubectl cluster-info\nKubernetes master is running at https://127.0.0.1:55387\nKubeDNS is running at https://127.0.0.1:55387/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n\n// 查看本地的 kind 容器\n$ docker ps\nCONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                                  NAMES\ne26545538cc7        kindest/node:v1.15.3   \"/usr/local/bin/entr…\"   15 minutes ago      Up 15 minutes       55387/tcp, 127.0.0.1:55387->6443/tcp   kind-control-plane\n```\n\n可以看到，kind 容器暴露的 6443 端口映射在本机的一个随机端口(55387)上。\n\n\n\n```\n// 查看 node 的详细信息，可以看到 cni 为 containerd\n$ kubectl describe node kind-control-plane\n...\n Container Runtime Version:  containerd://1.2.6-0ubuntu1\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nPodCIDR:                     10.244.0.0/24\nExternalID:                  kind-control-plane\n...\n\n\n# 进入 kind 容器查看 k8s 的配置，和单独使用 kubeadm 时一致\n$ docker exec -it e26545538cc  bash\nroot@kind-control-plane:~# ls /etc/kubernetes/\nadmin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf\nroot@kind-control-plane:~# ls /etc/kubernetes/manifests/\netcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml\n\n\n# 查看 cni 配置\nroot@kind-control-plane:/etc/kubernetes# cat /var/lib/kubelet/kubeadm-flags.env\nKUBELET_KUBEADM_ARGS=\"--container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail-swap-on=false --node-ip=172.17.0.2\"\n\n\n# 查看容器的状态\nroot@kind-control-plane:~# crictl pods\nPOD ID              CREATED             STATE               NAME                                         NAMESPACE           ATTEMPT\nfc8700af77ca2       About an hour ago   Ready               coredns-5c98db65d4-bjxl2                     kube-system         0\n6378297d32811       About an hour ago   Ready               coredns-5c98db65d4-q2drh                     kube-system         0\n124b42a35e0d1       About an hour ago   Ready               kube-proxy-99nc9                             kube-system         0\n54b9511069534       About an hour ago   Ready               kindnet-xz8dp                                kube-system         0\n61cb720ddece8       About an hour ago   Ready               etcd-kind-control-plane                      kube-system         0\n4514b98de1a44       About an hour ago   Ready               kube-scheduler-kind-control-plane            kube-system         0\n9a29dbebc8dd1       About an hour ago   Ready               kube-controller-manager-kind-control-plane   kube-system         0\nab028c5f5a3e5       About an hour ago   Ready               kube-apiserver-kind-control-plane            kube-system         0\n```\n\n删除集群：\n\n```\n$ kind delete cluster\n```\n\nkind 也支持创建多 master 以及多 work 节点的集群，需要自定义 yaml 配置：\n\n```\n# a cluster with 3 control-plane nodes and 3 workers\nkind: Cluster\napiVersion: kind.sigs.k8s.io/v1alpha3\nnodes:\n- role: control-plane\n- role: control-plane\n- role: control-plane\n- role: worker\n- role: worker\n- role: worker\n\n// 创建集群指定 config\n$ kind create cluster --config kind.yaml\n```\n\n\n\nkind 还支持自定义映射的端口号、支持使用自定义镜像仓库、支持启用 Feature Gates 等多个功能，详细的使用请参考官方文档 [quick-start](https://kind.sigs.k8s.io/docs/user/quick-start/)。\n\n\n\n### 二、本地测试\n\n既然 kind 不能用作生产环境，那怎么在本地测试时使用呢？由于 k8s 的新版已经全面启用了 TLS，不再支持非安全端口，访问 APIServer 的接口都需要认证，但是本地测试不需要那么麻烦，如下所示，为匿名用户设置访问权限即可。\n\n```\n// 为匿名用户关联 RBAC 规则\n$ kubectl create clusterrolebinding system:anonymous --clusterrole=cluster-admin --user=system:anonymous\n\n// 请求相关的 API\n$ curl -k https://127.0.0.1:55387/api/v1/nodes\n{\n  \"kind\": \"NodeList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"selfLink\": \"/api/v1/nodes\",\n    \"resourceVersion\": \"11844\"\n  },\n  \"items\": [\n  ...\n```\n\n","source":"_posts/kind_deploy.md","raw":"---\ntitle: 使用 kind 部署单机版 kubernetes 集群\ndate: 2019-09-06 10:30:30\ntags: [\"kind\",\"deploy\"]\ntype: \"kind\"\n\n---\n\nkubernetes 从一发布开始其学习门槛就比较高，首先就是部署难，用户要想学习 kubernetes 必须要过部署这一关，社区也推出了多个部署工具帮助简化集群的部署，社区中推出的部署工具主要目标有两大类，部署测试环境与生产环境，本节主要讲述测试环境的部署，目前社区已经有多套部署方案了：\n\n- https://github.com/bsycorp/kind\n- https://github.com/ubuntu/microk8s\n- https://github.com/kinvolk/kube-spawn\n- https://github.com/kubernetes/minikube\n- https://github.com/danderson/virtuakube\n- https://github.com/kubernetes-sigs/kubeadm-dind-cluster\n\n而本文主要讲述使用 [kind](https://github.com/kubernetes-sigs/kind)（Kubernetes In Docker）部署 k8s 集群，因为 kind 使用起来实在太简单了，特别适用于在本机部署测试环境。\n\n\n\nkind 的原理就是将 k8s 所需要的所有组件，全部部署在一个 docker 容器中，只需要一个镜像即可部署一套 k8s 环境，其底层是使用 kubeadm 进行部署，CRI 使用 Containerd，CNI 使用 weave。下面就来看看如何使用 kind 部署一套 kubernetes 环境，在使用 kind 前你需要确保目标机器已经安装了 docker 服务。\n\n### 一、使用 kind 部署 k8s 集群\n\n> 以下安装环境为 mac os。\n\n安装 kind ：\n\n```\n$ wget https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-darwin-amd64\n$ chmod +x kind-darwin-amd64\n$ mv kind-darwin-amd64 /usr/local/bin/kind\n```\n\n使用 kind 部署 kubernetes 集群：\n\n```\n// 默认的 cluster name 为 kind，可以使用 --name 指定\n$ kind create cluster\nCreating cluster \"kind\" ...\n ✓ Ensuring node image (kindest/node:v1.15.3) 🖼\n ✓ Preparing nodes 📦\n ✓ Creating kubeadm config 📜\n ✓ Starting control-plane 🕹️\n ✓ Installing CNI 🔌\n ✓ Installing StorageClass 💾\nCluster creation complete. You can now use the cluster with:\n```\n\n使用 **kind create cluster** 安装，是没有指定任何配置文件的安装方式。从安装打印出的输出来看，分为 6 步：\n1. 安装基础镜像 kindest/node:v1.15.4，这个镜像里面包含了所需要的二进制文件、配置文件以及 k8s 左右组件镜像的 tar 包\n2. 准备 node，检查环境、启动镜像等工作\n3. 生成 kubeadm 的配置，然后使用 kubeadm 安装，和直接使用 kubeadm 的步骤类似\n4. 启动服务\n5. 部署 CNI 插件，kind 默认使用 weave。\n6. 创建 StorageClass。\n\n\n\n```\n// 查看 kubeconfig path\n$ kind get kubeconfig-path\n/Users/feiyu/.kube/kind-config-kind\n\n$ export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"kind\")\"\n```\n\nkind 还有多个子命令，此处不再一一详解。\n\n\n\n```\n// 查看集群信息，\n$ kubectl cluster-info\nKubernetes master is running at https://127.0.0.1:55387\nKubeDNS is running at https://127.0.0.1:55387/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n\n// 查看本地的 kind 容器\n$ docker ps\nCONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                                  NAMES\ne26545538cc7        kindest/node:v1.15.3   \"/usr/local/bin/entr…\"   15 minutes ago      Up 15 minutes       55387/tcp, 127.0.0.1:55387->6443/tcp   kind-control-plane\n```\n\n可以看到，kind 容器暴露的 6443 端口映射在本机的一个随机端口(55387)上。\n\n\n\n```\n// 查看 node 的详细信息，可以看到 cni 为 containerd\n$ kubectl describe node kind-control-plane\n...\n Container Runtime Version:  containerd://1.2.6-0ubuntu1\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nPodCIDR:                     10.244.0.0/24\nExternalID:                  kind-control-plane\n...\n\n\n# 进入 kind 容器查看 k8s 的配置，和单独使用 kubeadm 时一致\n$ docker exec -it e26545538cc  bash\nroot@kind-control-plane:~# ls /etc/kubernetes/\nadmin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf\nroot@kind-control-plane:~# ls /etc/kubernetes/manifests/\netcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml\n\n\n# 查看 cni 配置\nroot@kind-control-plane:/etc/kubernetes# cat /var/lib/kubelet/kubeadm-flags.env\nKUBELET_KUBEADM_ARGS=\"--container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail-swap-on=false --node-ip=172.17.0.2\"\n\n\n# 查看容器的状态\nroot@kind-control-plane:~# crictl pods\nPOD ID              CREATED             STATE               NAME                                         NAMESPACE           ATTEMPT\nfc8700af77ca2       About an hour ago   Ready               coredns-5c98db65d4-bjxl2                     kube-system         0\n6378297d32811       About an hour ago   Ready               coredns-5c98db65d4-q2drh                     kube-system         0\n124b42a35e0d1       About an hour ago   Ready               kube-proxy-99nc9                             kube-system         0\n54b9511069534       About an hour ago   Ready               kindnet-xz8dp                                kube-system         0\n61cb720ddece8       About an hour ago   Ready               etcd-kind-control-plane                      kube-system         0\n4514b98de1a44       About an hour ago   Ready               kube-scheduler-kind-control-plane            kube-system         0\n9a29dbebc8dd1       About an hour ago   Ready               kube-controller-manager-kind-control-plane   kube-system         0\nab028c5f5a3e5       About an hour ago   Ready               kube-apiserver-kind-control-plane            kube-system         0\n```\n\n删除集群：\n\n```\n$ kind delete cluster\n```\n\nkind 也支持创建多 master 以及多 work 节点的集群，需要自定义 yaml 配置：\n\n```\n# a cluster with 3 control-plane nodes and 3 workers\nkind: Cluster\napiVersion: kind.sigs.k8s.io/v1alpha3\nnodes:\n- role: control-plane\n- role: control-plane\n- role: control-plane\n- role: worker\n- role: worker\n- role: worker\n\n// 创建集群指定 config\n$ kind create cluster --config kind.yaml\n```\n\n\n\nkind 还支持自定义映射的端口号、支持使用自定义镜像仓库、支持启用 Feature Gates 等多个功能，详细的使用请参考官方文档 [quick-start](https://kind.sigs.k8s.io/docs/user/quick-start/)。\n\n\n\n### 二、本地测试\n\n既然 kind 不能用作生产环境，那怎么在本地测试时使用呢？由于 k8s 的新版已经全面启用了 TLS，不再支持非安全端口，访问 APIServer 的接口都需要认证，但是本地测试不需要那么麻烦，如下所示，为匿名用户设置访问权限即可。\n\n```\n// 为匿名用户关联 RBAC 规则\n$ kubectl create clusterrolebinding system:anonymous --clusterrole=cluster-admin --user=system:anonymous\n\n// 请求相关的 API\n$ curl -k https://127.0.0.1:55387/api/v1/nodes\n{\n  \"kind\": \"NodeList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"selfLink\": \"/api/v1/nodes\",\n    \"resourceVersion\": \"11844\"\n  },\n  \"items\": [\n  ...\n```\n\n","slug":"kind_deploy","published":1,"updated":"2019-09-06T02:36:38.481Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59k000yapwn2sicnf0w","content":"<p>kubernetes 从一发布开始其学习门槛就比较高，首先就是部署难，用户要想学习 kubernetes 必须要过部署这一关，社区也推出了多个部署工具帮助简化集群的部署，社区中推出的部署工具主要目标有两大类，部署测试环境与生产环境，本节主要讲述测试环境的部署，目前社区已经有多套部署方案了：</p>\n<ul>\n<li><a href=\"https://github.com/bsycorp/kind\" target=\"_blank\" rel=\"noopener\">https://github.com/bsycorp/kind</a></li>\n<li><a href=\"https://github.com/ubuntu/microk8s\" target=\"_blank\" rel=\"noopener\">https://github.com/ubuntu/microk8s</a></li>\n<li><a href=\"https://github.com/kinvolk/kube-spawn\" target=\"_blank\" rel=\"noopener\">https://github.com/kinvolk/kube-spawn</a></li>\n<li><a href=\"https://github.com/kubernetes/minikube\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/minikube</a></li>\n<li><a href=\"https://github.com/danderson/virtuakube\" target=\"_blank\" rel=\"noopener\">https://github.com/danderson/virtuakube</a></li>\n<li><a href=\"https://github.com/kubernetes-sigs/kubeadm-dind-cluster\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes-sigs/kubeadm-dind-cluster</a></li>\n</ul>\n<p>而本文主要讲述使用 <a href=\"https://github.com/kubernetes-sigs/kind\" target=\"_blank\" rel=\"noopener\">kind</a>（Kubernetes In Docker）部署 k8s 集群，因为 kind 使用起来实在太简单了，特别适用于在本机部署测试环境。</p>\n<p>kind 的原理就是将 k8s 所需要的所有组件，全部部署在一个 docker 容器中，只需要一个镜像即可部署一套 k8s 环境，其底层是使用 kubeadm 进行部署，CRI 使用 Containerd，CNI 使用 weave。下面就来看看如何使用 kind 部署一套 kubernetes 环境，在使用 kind 前你需要确保目标机器已经安装了 docker 服务。</p>\n<h3 id=\"一、使用-kind-部署-k8s-集群\"><a href=\"#一、使用-kind-部署-k8s-集群\" class=\"headerlink\" title=\"一、使用 kind 部署 k8s 集群\"></a>一、使用 kind 部署 k8s 集群</h3><blockquote>\n<p>以下安装环境为 mac os。</p>\n</blockquote>\n<p>安装 kind ：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ wget https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-darwin-amd64</span><br><span class=\"line\">$ chmod +x kind-darwin-amd64</span><br><span class=\"line\">$ mv kind-darwin-amd64 /usr/local/bin/kind</span><br></pre></td></tr></table></figure>\n<p>使用 kind 部署 kubernetes 集群：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 默认的 cluster name 为 kind，可以使用 --name 指定</span><br><span class=\"line\">$ kind create cluster</span><br><span class=\"line\">Creating cluster &quot;kind&quot; ...</span><br><span class=\"line\"> ✓ Ensuring node image (kindest/node:v1.15.3) 🖼</span><br><span class=\"line\"> ✓ Preparing nodes 📦</span><br><span class=\"line\"> ✓ Creating kubeadm config 📜</span><br><span class=\"line\"> ✓ Starting control-plane 🕹️</span><br><span class=\"line\"> ✓ Installing CNI 🔌</span><br><span class=\"line\"> ✓ Installing StorageClass 💾</span><br><span class=\"line\">Cluster creation complete. You can now use the cluster with:</span><br></pre></td></tr></table></figure>\n<p>使用 <strong>kind create cluster</strong> 安装，是没有指定任何配置文件的安装方式。从安装打印出的输出来看，分为 6 步：</p>\n<ol>\n<li>安装基础镜像 kindest/node:v1.15.4，这个镜像里面包含了所需要的二进制文件、配置文件以及 k8s 左右组件镜像的 tar 包</li>\n<li>准备 node，检查环境、启动镜像等工作</li>\n<li>生成 kubeadm 的配置，然后使用 kubeadm 安装，和直接使用 kubeadm 的步骤类似</li>\n<li>启动服务</li>\n<li>部署 CNI 插件，kind 默认使用 weave。</li>\n<li>创建 StorageClass。</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看 kubeconfig path</span><br><span class=\"line\">$ kind get kubeconfig-path</span><br><span class=\"line\">/Users/feiyu/.kube/kind-config-kind</span><br><span class=\"line\"></span><br><span class=\"line\">$ export KUBECONFIG=&quot;$(kind get kubeconfig-path --name=&quot;kind&quot;)&quot;</span><br></pre></td></tr></table></figure>\n<p>kind 还有多个子命令，此处不再一一详解。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看集群信息，</span><br><span class=\"line\">$ kubectl cluster-info</span><br><span class=\"line\">Kubernetes master is running at https://127.0.0.1:55387</span><br><span class=\"line\">KubeDNS is running at https://127.0.0.1:55387/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><br><span class=\"line\"></span><br><span class=\"line\">To further debug and diagnose cluster problems, use &apos;kubectl cluster-info dump&apos;.</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 查看本地的 kind 容器</span><br><span class=\"line\">$ docker ps</span><br><span class=\"line\">CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                                  NAMES</span><br><span class=\"line\">e26545538cc7        kindest/node:v1.15.3   &quot;/usr/local/bin/entr…&quot;   15 minutes ago      Up 15 minutes       55387/tcp, 127.0.0.1:55387-&gt;6443/tcp   kind-control-plane</span><br></pre></td></tr></table></figure>\n<p>可以看到，kind 容器暴露的 6443 端口映射在本机的一个随机端口(55387)上。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看 node 的详细信息，可以看到 cni 为 containerd</span><br><span class=\"line\">$ kubectl describe node kind-control-plane</span><br><span class=\"line\">...</span><br><span class=\"line\"> Container Runtime Version:  containerd://1.2.6-0ubuntu1</span><br><span class=\"line\"> Kubelet Version:            v1.15.3</span><br><span class=\"line\"> Kube-Proxy Version:         v1.15.3</span><br><span class=\"line\">PodCIDR:                     10.244.0.0/24</span><br><span class=\"line\">ExternalID:                  kind-control-plane</span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 进入 kind 容器查看 k8s 的配置，和单独使用 kubeadm 时一致</span><br><span class=\"line\">$ docker exec -it e26545538cc  bash</span><br><span class=\"line\">root@kind-control-plane:~# ls /etc/kubernetes/</span><br><span class=\"line\">admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf</span><br><span class=\"line\">root@kind-control-plane:~# ls /etc/kubernetes/manifests/</span><br><span class=\"line\">etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 查看 cni 配置</span><br><span class=\"line\">root@kind-control-plane:/etc/kubernetes# cat /var/lib/kubelet/kubeadm-flags.env</span><br><span class=\"line\">KUBELET_KUBEADM_ARGS=&quot;--container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail-swap-on=false --node-ip=172.17.0.2&quot;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 查看容器的状态</span><br><span class=\"line\">root@kind-control-plane:~# crictl pods</span><br><span class=\"line\">POD ID              CREATED             STATE               NAME                                         NAMESPACE           ATTEMPT</span><br><span class=\"line\">fc8700af77ca2       About an hour ago   Ready               coredns-5c98db65d4-bjxl2                     kube-system         0</span><br><span class=\"line\">6378297d32811       About an hour ago   Ready               coredns-5c98db65d4-q2drh                     kube-system         0</span><br><span class=\"line\">124b42a35e0d1       About an hour ago   Ready               kube-proxy-99nc9                             kube-system         0</span><br><span class=\"line\">54b9511069534       About an hour ago   Ready               kindnet-xz8dp                                kube-system         0</span><br><span class=\"line\">61cb720ddece8       About an hour ago   Ready               etcd-kind-control-plane                      kube-system         0</span><br><span class=\"line\">4514b98de1a44       About an hour ago   Ready               kube-scheduler-kind-control-plane            kube-system         0</span><br><span class=\"line\">9a29dbebc8dd1       About an hour ago   Ready               kube-controller-manager-kind-control-plane   kube-system         0</span><br><span class=\"line\">ab028c5f5a3e5       About an hour ago   Ready               kube-apiserver-kind-control-plane            kube-system         0</span><br></pre></td></tr></table></figure>\n<p>删除集群：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kind delete cluster</span><br></pre></td></tr></table></figure>\n<p>kind 也支持创建多 master 以及多 work 节点的集群，需要自定义 yaml 配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># a cluster with 3 control-plane nodes and 3 workers</span><br><span class=\"line\">kind: Cluster</span><br><span class=\"line\">apiVersion: kind.sigs.k8s.io/v1alpha3</span><br><span class=\"line\">nodes:</span><br><span class=\"line\">- role: control-plane</span><br><span class=\"line\">- role: control-plane</span><br><span class=\"line\">- role: control-plane</span><br><span class=\"line\">- role: worker</span><br><span class=\"line\">- role: worker</span><br><span class=\"line\">- role: worker</span><br><span class=\"line\"></span><br><span class=\"line\">// 创建集群指定 config</span><br><span class=\"line\">$ kind create cluster --config kind.yaml</span><br></pre></td></tr></table></figure>\n<p>kind 还支持自定义映射的端口号、支持使用自定义镜像仓库、支持启用 Feature Gates 等多个功能，详细的使用请参考官方文档 <a href=\"https://kind.sigs.k8s.io/docs/user/quick-start/\" target=\"_blank\" rel=\"noopener\">quick-start</a>。</p>\n<h3 id=\"二、本地测试\"><a href=\"#二、本地测试\" class=\"headerlink\" title=\"二、本地测试\"></a>二、本地测试</h3><p>既然 kind 不能用作生产环境，那怎么在本地测试时使用呢？由于 k8s 的新版已经全面启用了 TLS，不再支持非安全端口，访问 APIServer 的接口都需要认证，但是本地测试不需要那么麻烦，如下所示，为匿名用户设置访问权限即可。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 为匿名用户关联 RBAC 规则</span><br><span class=\"line\">$ kubectl create clusterrolebinding system:anonymous --clusterrole=cluster-admin --user=system:anonymous</span><br><span class=\"line\"></span><br><span class=\"line\">// 请求相关的 API</span><br><span class=\"line\">$ curl -k https://127.0.0.1:55387/api/v1/nodes</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;kind&quot;: &quot;NodeList&quot;,</span><br><span class=\"line\">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class=\"line\">  &quot;metadata&quot;: &#123;</span><br><span class=\"line\">    &quot;selfLink&quot;: &quot;/api/v1/nodes&quot;,</span><br><span class=\"line\">    &quot;resourceVersion&quot;: &quot;11844&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;items&quot;: [</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>kubernetes 从一发布开始其学习门槛就比较高，首先就是部署难，用户要想学习 kubernetes 必须要过部署这一关，社区也推出了多个部署工具帮助简化集群的部署，社区中推出的部署工具主要目标有两大类，部署测试环境与生产环境，本节主要讲述测试环境的部署，目前社区已经有多套部署方案了：</p>\n<ul>\n<li><a href=\"https://github.com/bsycorp/kind\" target=\"_blank\" rel=\"noopener\">https://github.com/bsycorp/kind</a></li>\n<li><a href=\"https://github.com/ubuntu/microk8s\" target=\"_blank\" rel=\"noopener\">https://github.com/ubuntu/microk8s</a></li>\n<li><a href=\"https://github.com/kinvolk/kube-spawn\" target=\"_blank\" rel=\"noopener\">https://github.com/kinvolk/kube-spawn</a></li>\n<li><a href=\"https://github.com/kubernetes/minikube\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/minikube</a></li>\n<li><a href=\"https://github.com/danderson/virtuakube\" target=\"_blank\" rel=\"noopener\">https://github.com/danderson/virtuakube</a></li>\n<li><a href=\"https://github.com/kubernetes-sigs/kubeadm-dind-cluster\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes-sigs/kubeadm-dind-cluster</a></li>\n</ul>\n<p>而本文主要讲述使用 <a href=\"https://github.com/kubernetes-sigs/kind\" target=\"_blank\" rel=\"noopener\">kind</a>（Kubernetes In Docker）部署 k8s 集群，因为 kind 使用起来实在太简单了，特别适用于在本机部署测试环境。</p>\n<p>kind 的原理就是将 k8s 所需要的所有组件，全部部署在一个 docker 容器中，只需要一个镜像即可部署一套 k8s 环境，其底层是使用 kubeadm 进行部署，CRI 使用 Containerd，CNI 使用 weave。下面就来看看如何使用 kind 部署一套 kubernetes 环境，在使用 kind 前你需要确保目标机器已经安装了 docker 服务。</p>\n<h3 id=\"一、使用-kind-部署-k8s-集群\"><a href=\"#一、使用-kind-部署-k8s-集群\" class=\"headerlink\" title=\"一、使用 kind 部署 k8s 集群\"></a>一、使用 kind 部署 k8s 集群</h3><blockquote>\n<p>以下安装环境为 mac os。</p>\n</blockquote>\n<p>安装 kind ：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ wget https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-darwin-amd64</span><br><span class=\"line\">$ chmod +x kind-darwin-amd64</span><br><span class=\"line\">$ mv kind-darwin-amd64 /usr/local/bin/kind</span><br></pre></td></tr></table></figure>\n<p>使用 kind 部署 kubernetes 集群：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 默认的 cluster name 为 kind，可以使用 --name 指定</span><br><span class=\"line\">$ kind create cluster</span><br><span class=\"line\">Creating cluster &quot;kind&quot; ...</span><br><span class=\"line\"> ✓ Ensuring node image (kindest/node:v1.15.3) 🖼</span><br><span class=\"line\"> ✓ Preparing nodes 📦</span><br><span class=\"line\"> ✓ Creating kubeadm config 📜</span><br><span class=\"line\"> ✓ Starting control-plane 🕹️</span><br><span class=\"line\"> ✓ Installing CNI 🔌</span><br><span class=\"line\"> ✓ Installing StorageClass 💾</span><br><span class=\"line\">Cluster creation complete. You can now use the cluster with:</span><br></pre></td></tr></table></figure>\n<p>使用 <strong>kind create cluster</strong> 安装，是没有指定任何配置文件的安装方式。从安装打印出的输出来看，分为 6 步：</p>\n<ol>\n<li>安装基础镜像 kindest/node:v1.15.4，这个镜像里面包含了所需要的二进制文件、配置文件以及 k8s 左右组件镜像的 tar 包</li>\n<li>准备 node，检查环境、启动镜像等工作</li>\n<li>生成 kubeadm 的配置，然后使用 kubeadm 安装，和直接使用 kubeadm 的步骤类似</li>\n<li>启动服务</li>\n<li>部署 CNI 插件，kind 默认使用 weave。</li>\n<li>创建 StorageClass。</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看 kubeconfig path</span><br><span class=\"line\">$ kind get kubeconfig-path</span><br><span class=\"line\">/Users/feiyu/.kube/kind-config-kind</span><br><span class=\"line\"></span><br><span class=\"line\">$ export KUBECONFIG=&quot;$(kind get kubeconfig-path --name=&quot;kind&quot;)&quot;</span><br></pre></td></tr></table></figure>\n<p>kind 还有多个子命令，此处不再一一详解。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看集群信息，</span><br><span class=\"line\">$ kubectl cluster-info</span><br><span class=\"line\">Kubernetes master is running at https://127.0.0.1:55387</span><br><span class=\"line\">KubeDNS is running at https://127.0.0.1:55387/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><br><span class=\"line\"></span><br><span class=\"line\">To further debug and diagnose cluster problems, use &apos;kubectl cluster-info dump&apos;.</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">// 查看本地的 kind 容器</span><br><span class=\"line\">$ docker ps</span><br><span class=\"line\">CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                                  NAMES</span><br><span class=\"line\">e26545538cc7        kindest/node:v1.15.3   &quot;/usr/local/bin/entr…&quot;   15 minutes ago      Up 15 minutes       55387/tcp, 127.0.0.1:55387-&gt;6443/tcp   kind-control-plane</span><br></pre></td></tr></table></figure>\n<p>可以看到，kind 容器暴露的 6443 端口映射在本机的一个随机端口(55387)上。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看 node 的详细信息，可以看到 cni 为 containerd</span><br><span class=\"line\">$ kubectl describe node kind-control-plane</span><br><span class=\"line\">...</span><br><span class=\"line\"> Container Runtime Version:  containerd://1.2.6-0ubuntu1</span><br><span class=\"line\"> Kubelet Version:            v1.15.3</span><br><span class=\"line\"> Kube-Proxy Version:         v1.15.3</span><br><span class=\"line\">PodCIDR:                     10.244.0.0/24</span><br><span class=\"line\">ExternalID:                  kind-control-plane</span><br><span class=\"line\">...</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 进入 kind 容器查看 k8s 的配置，和单独使用 kubeadm 时一致</span><br><span class=\"line\">$ docker exec -it e26545538cc  bash</span><br><span class=\"line\">root@kind-control-plane:~# ls /etc/kubernetes/</span><br><span class=\"line\">admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf</span><br><span class=\"line\">root@kind-control-plane:~# ls /etc/kubernetes/manifests/</span><br><span class=\"line\">etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 查看 cni 配置</span><br><span class=\"line\">root@kind-control-plane:/etc/kubernetes# cat /var/lib/kubelet/kubeadm-flags.env</span><br><span class=\"line\">KUBELET_KUBEADM_ARGS=&quot;--container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail-swap-on=false --node-ip=172.17.0.2&quot;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># 查看容器的状态</span><br><span class=\"line\">root@kind-control-plane:~# crictl pods</span><br><span class=\"line\">POD ID              CREATED             STATE               NAME                                         NAMESPACE           ATTEMPT</span><br><span class=\"line\">fc8700af77ca2       About an hour ago   Ready               coredns-5c98db65d4-bjxl2                     kube-system         0</span><br><span class=\"line\">6378297d32811       About an hour ago   Ready               coredns-5c98db65d4-q2drh                     kube-system         0</span><br><span class=\"line\">124b42a35e0d1       About an hour ago   Ready               kube-proxy-99nc9                             kube-system         0</span><br><span class=\"line\">54b9511069534       About an hour ago   Ready               kindnet-xz8dp                                kube-system         0</span><br><span class=\"line\">61cb720ddece8       About an hour ago   Ready               etcd-kind-control-plane                      kube-system         0</span><br><span class=\"line\">4514b98de1a44       About an hour ago   Ready               kube-scheduler-kind-control-plane            kube-system         0</span><br><span class=\"line\">9a29dbebc8dd1       About an hour ago   Ready               kube-controller-manager-kind-control-plane   kube-system         0</span><br><span class=\"line\">ab028c5f5a3e5       About an hour ago   Ready               kube-apiserver-kind-control-plane            kube-system         0</span><br></pre></td></tr></table></figure>\n<p>删除集群：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kind delete cluster</span><br></pre></td></tr></table></figure>\n<p>kind 也支持创建多 master 以及多 work 节点的集群，需要自定义 yaml 配置：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># a cluster with 3 control-plane nodes and 3 workers</span><br><span class=\"line\">kind: Cluster</span><br><span class=\"line\">apiVersion: kind.sigs.k8s.io/v1alpha3</span><br><span class=\"line\">nodes:</span><br><span class=\"line\">- role: control-plane</span><br><span class=\"line\">- role: control-plane</span><br><span class=\"line\">- role: control-plane</span><br><span class=\"line\">- role: worker</span><br><span class=\"line\">- role: worker</span><br><span class=\"line\">- role: worker</span><br><span class=\"line\"></span><br><span class=\"line\">// 创建集群指定 config</span><br><span class=\"line\">$ kind create cluster --config kind.yaml</span><br></pre></td></tr></table></figure>\n<p>kind 还支持自定义映射的端口号、支持使用自定义镜像仓库、支持启用 Feature Gates 等多个功能，详细的使用请参考官方文档 <a href=\"https://kind.sigs.k8s.io/docs/user/quick-start/\" target=\"_blank\" rel=\"noopener\">quick-start</a>。</p>\n<h3 id=\"二、本地测试\"><a href=\"#二、本地测试\" class=\"headerlink\" title=\"二、本地测试\"></a>二、本地测试</h3><p>既然 kind 不能用作生产环境，那怎么在本地测试时使用呢？由于 k8s 的新版已经全面启用了 TLS，不再支持非安全端口，访问 APIServer 的接口都需要认证，但是本地测试不需要那么麻烦，如下所示，为匿名用户设置访问权限即可。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 为匿名用户关联 RBAC 规则</span><br><span class=\"line\">$ kubectl create clusterrolebinding system:anonymous --clusterrole=cluster-admin --user=system:anonymous</span><br><span class=\"line\"></span><br><span class=\"line\">// 请求相关的 API</span><br><span class=\"line\">$ curl -k https://127.0.0.1:55387/api/v1/nodes</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;kind&quot;: &quot;NodeList&quot;,</span><br><span class=\"line\">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class=\"line\">  &quot;metadata&quot;: &#123;</span><br><span class=\"line\">    &quot;selfLink&quot;: &quot;/api/v1/nodes&quot;,</span><br><span class=\"line\">    &quot;resourceVersion&quot;: &quot;11844&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;items&quot;: [</span><br><span class=\"line\">  ...</span><br></pre></td></tr></table></figure>\n"},{"title":"kube-on-kube-operator 开发(一)","date":"2019-08-05T08:37:30.000Z","type":"kubernetes-operator","_content":"\nkubernetes 已经成为容器时代的分布式操作系统内核，目前也是所有公有云提供商的标配，在国内，阿里云、腾讯云、华为云这样的公有云大厂商都支持一键部署 kubernetes 集群，而 kubernetes 集群自动化管理则是迫切需要解决的问题。对于大部分不熟悉 kubernetes 而要上云的小白用户就强烈需要一个被托管及能自动化运维的集群，他们平时只是进行业务的部署与变更，只需要对 kubernetes 中部分概念了解即可。同样在私有云场景下，笔者所待过的几个大小公司一般都会维护多套集群，集群的运维工作就是一个很大的挑战，反观各大厂同样要有效可靠的管理大规模集群，kube-on-kube-operator 是一个很好的解决方案。\n\n所谓 kube-on-kube-operator，就是将 kubernetes 运行在 kubernetes 上，用 kubernetes 托管 kubernetes 方式来自动化管理集群。和所有 operator 的功能类似，系统会定时检测集群当前状态，判断是否与目标状态一致，出现不一致时，operator 会发起一系列操作，驱动集群达到目标状态。今年 kubeCon 上，雅虎日本也分享了其管理大规模 kubernetes 集群的方法，4000 节点构建了 400 个 kubernetes 集群，同样采用的是 kube-on-kube-operator 架构，以 kubernetes as a service 的形式使用。\n\n### kubernetes-operator 设计参考\n\n记得 kube-on-kube-operator 的概念最初是在去年的 kubeCon China 上蚂蚁金服提出来的，先看看蚂蚁金服以及腾讯云 kube-on-kube-operator 的设计思路，其实腾讯云的架构和蚂蚁金服的是类似的。以下是蚂蚁金服的架构设计图：\n\n![](http://cdn.tianfeiyu.com/image-20190805163304515.png)\n\n\n\n首先部署一套 kubernetes 元集群，通过元集群部署业务集群，业务集群的 master 组件都分布在一台宿主上，该宿主以 node 节点的方式挂载在元集群中。在私有云场景下，这样的部署方式有一个很明显的问题就是元集群节点跨机房，虽然业务集群的 master 与 node 都是在同一个机房，但是元集群中的 node 节点大部分是分布在不同的机房，有些公司在不同机房之间会有网络上的限制，有可能网络不通或者只能使用专线连接。在公有云场景下，元集群自己有一套独立的 vpc 网络，它要怎么和用户的 vpc 结点进行通信呢？腾讯云的做法是利用 vpc 提供的弹性网卡能力，将这个弹性网卡直接绑定到运行 apiserver 的 pod 中，运行 master  的这个pod 既加入了元集群的 vpc，又加入了用户的 vpc，也就是一个 pod 同时在两个网络中，这样就可以很好的去实现和用户 node 相关的互通。这种方式都是通过 kubernetes  API 去管理 master 组件的，master 组件的升级以及故障自愈都可以通过 kubernetes 提供的方式实现。\n\n\n\n### kubernetes-operator 设计\n\n![kubernetes-operator 架构](http://cdn.tianfeiyu.com/image-20190804152312149.png)\n\n\n\n> kubernetes-operator 项目地址：[https://github.com/gosoon/kubernetes-operator](https://github.com/gosoon/kubernetes-operator)\n\n目前该项目的主要目标是实现以下三种场景中的集群管理：\n\n- kube-on-kube\n- kube-to-kube\n- kube-to-cloud-kube\n\nkubernetes-operator 不仅是要实现 kube-on-kube 架构，还有 kube-to-kube，kube-to-cloud-kube，kube-to-kube 即 kubernetes 集群管理业务独立的 kubernetes 集群，两个集群相互独立。kube-to-cloud-kube 即 kubernetes 集群管理多云环境上的 kubernetes 集群。\n\n\n\n上面是项目的架构图，红色的线段表示对集群生命周期管理的一个操作，涉及集群的创建、删除、扩缩容、升级等，蓝色线段是对集群应用的操作，集群中应用的创建、删除、发布更新等，kubernetes-proxy 是一个 API 代理，所有涉及 API 的调用都要通过 kubernetes-proxy。左边部署有 kubernetes-operator 的是元集群，kubernetes-operator 使用 etcd 仅存储部分配置信息，其管理业务集群的生命周期，支持三种集群的创建方式，第一种方式就是可以创建出类似蚂蚁金服这种直接将业务集群 master 运行在元集群，node 节点在业务集群，第二种是以二进制方式创建业务集群，其中业务集群的 master 以及 node 都是在业务集群所在的机房，第三种方式就是在各种公有云厂商创建集群，以一种统一的方式管理公有云上的集群，也可以称作融合云。\n\n\n\n#### 项目结构\n\n总体来说，项目暂时分为三大块：\n\n- kubernetes proxy：支持 API 透传、访问控制等功能；\n- 控制器：也就是 kubernetes-operator，管理业务集群的生命周期；\n- 集群部署模块：用来部署业务集群，目前主要在开发第二种方式使用二进制部署业务集群；\n- kubernetes 应用安装模块：在新建完成的集群中部署监控、日志采集、镜像仓库、helm 等组件；\n\n\n\n#### 控制器\n\n控制器也就是 Operator + CR，目前开发 operator 的方式已知的有三种：\n\n-  自定义 controller 的方式：kube-controller-manager 中所有的 controller 就是以自定义 controller 的方式，这种方式是最原生的方式，需要开发者了解 kubernetes 中的代码生成，informer 的使用等。\n- operator-sdk 的方式：一个开发 Operator 的框架，对于一些不熟悉 kubernetes 的可以使用 operator-sdk 的方式，这种方式让开发者更注重业务的实现，但是不够灵活。\n- kubebuilder 的方式：kubebuilder 是开发 controller manager 的框架，controller manager 会管理一个或者多个 operator。\n\n\n\nkubebuilder, operator-sdk 都是对controller-runtime做了封装, controller runtime又是对client-go shardInfromer 做的封装，本质上其实都一样的。kubernetes-operator 使用的是自定义 controller  的方式，如果想要更深入的学习 kubernetes，非自定义 controller 方式莫属了，kube-controller-manager 组件中的各种 controller 都是使用这种方式开发的，完全可以按照官方这种套路来开发。在 kubernetes 中，目前有两种方式可以定义一个新对象，一是 CustomResourceDefinition（CRD）、二是 Aggregation ApiServer（AA），其中 CRD 是相对简单也是目前应用比较广的方法。kubernetes-operator 采用 CRD 的方式。\n\n\n\n#### 集群部署\n\n其实项目中最难的是集群部署这一部分，部署集群目前有两种方式，二进制部署和容器化部署，但是都有一些开源工具的支持。手动部署一个二进制集群需要熟悉 docker 的部署、etcd 的部署、角色证书的创建、RBAC 授权、网络配置、yaml 文件编写、kubernetes 集群运维等等，总之手动部署一个二进制集群是非常麻烦的，但是要真正会用 kubernetes 是逃不了部署这一步的。第二种方式就是以容器化的方式部署，这种部署方式相对来说比较简单，有现成的工具直接傻瓜式操作就能部署成功。但是我目前选择的是使用二进制的部署方式，由于自己运维过二进制的 kubernetes 集群，对于私有云场景一般都是直接将集群部署在物理机上，作为生产环境，自己认为容器化的方式部署还不是非常成熟的，目前工作过的大小公司中，生产环境暂时没有以容器化的方式运行集群。所以 kubernetes-operator 中目前主要支持的就是使用二进制部署集群。\n\n\n\n目前比较成熟的用于生产环境的 kubernetes 集群部署工具有：kubeadm、kubespary、kops、rancher、kubeasz 等。kubeadm、kubespary、kops 都是官方开源的产品，kubeadm 使用容器化的方式部署，需要手动执行一些部署命令，暂时无法完全自动化部署。kubespary 是对 kubeadm 的一层封装，使用 ansible + kubeadm 的方式自动化进行部署，据说阿里云就是使用 kubespary 部署集群的。在公有云的环境(GCP、AWS)通常使用 kops  部署起来更方便些。kubeasz 是使用 ansible 自动化的方式部署二进制集群，目前也已经比较成熟了。\n\n\n\n#### 应用安装\n\n- 监控：当然是使用 promethus；\n- 日志采集：使用 filebeat 或者基于 filebeat 封装的一些组件如 logpilot，其他的还有 logkit 等都可以尝试使用；\n- 镜像仓库：当然是使用 harbor；\n- HPA：组件以及应用的自动扩缩容；\n\n应用安装使用 helm 的方式进行安装。\n\n\n\n#### 集群升级\n\n若以二进制部署最好是替换二进制文件的方式进行升级，若使用容器化部署，master 部署在元集群中可以使用 kubernetes 的滚动方式升级否则要以修改 manifest 文件的方式。\n\n集群升级包括配置和版本的升级，集群部署完成后，master 的配置改动不会很频繁，由于要进行性能上的优化以及业务的支持，对于 node 组件上的配置升级还是比较多的。对于集群的版本升级，升级的难度系数随着版本的跨度增大而增大，若按照官方的升级流程，一般不会出现异常。升级操作一般都是先升 master 再升 node，在工作中经历的几次版本升级中，每次升级完 master 后理论上不会再回退了，除非升级过程中有问题，否则升级完成后已经很难回退了，master 升级完成后 APIServer 的一些 API 还有 pod 的字段都有可能改变，master 版本回退后一些已存在的应用可能会异常，或者还可以参考 openshift 的蓝绿升级方式。二进制部署的集群尽量以替换二进制文件的方式进行升级，对于容器化部署的集群，可以直接使用 kubernetes 的滚动方式升级或者是修改 manifest 文件的方式。\n\n目前蚂蚁金服 kube-on-kube-operator 架构中在业务集群中会部署一个 node-operator，node-operator 会记录 master 组件的镜像、默认启动参数等信息，其作用就是节点配置管理、集群组件升级以及节点故障自愈，未来在项目中也会实现基于此的方式。\n\n\n\n### 后期计划\n\n- 支持部署 k3s、kubeedge：5G 时代，边缘计算将是非常火的，目前各大厂商也都在此布局，所以支持部署 k3s、kubeedge 这些专门支持边缘计算的产品还是非常有必要的。\n- 支持使用 kops 部署\n- 支持部署多版本 k8s\n- node-operator 开发，支持集群的配置管理、自动化升级、故障自愈等功能\n- 用户及权限管理：操作集群用户的权限和 kubernetes 中 RBAC 规则绑定\n- Kubernetes-operator 一些功能的扩展和完善\n\n\n\n参考：\n\n[腾讯云容器服务TKE：一键部署实践](https://mp.weixin.qq.com/s/WScGf3DRDC8ryyrf_tY-Qw)\n\n[一年时间打造全球最大规模之一的Kubernetes集群，蚂蚁金服怎么做到的？](https://mp.weixin.qq.com/s/bJrMNxKMn89TzmpEyIZrRg)\n\n[https://github.com/gosoon/kubernetes-operator](https://github.com/gosoon/kubernetes-operator)\n","source":"_posts/kube_on_kube_operator_1.md","raw":"---\ntitle: kube-on-kube-operator 开发(一)\ndate: 2019-08-05 16:37:30\ntags: [\"operator\",\"kube-on-kube\"]\ntype: \"kubernetes-operator\"\n\n---\n\nkubernetes 已经成为容器时代的分布式操作系统内核，目前也是所有公有云提供商的标配，在国内，阿里云、腾讯云、华为云这样的公有云大厂商都支持一键部署 kubernetes 集群，而 kubernetes 集群自动化管理则是迫切需要解决的问题。对于大部分不熟悉 kubernetes 而要上云的小白用户就强烈需要一个被托管及能自动化运维的集群，他们平时只是进行业务的部署与变更，只需要对 kubernetes 中部分概念了解即可。同样在私有云场景下，笔者所待过的几个大小公司一般都会维护多套集群，集群的运维工作就是一个很大的挑战，反观各大厂同样要有效可靠的管理大规模集群，kube-on-kube-operator 是一个很好的解决方案。\n\n所谓 kube-on-kube-operator，就是将 kubernetes 运行在 kubernetes 上，用 kubernetes 托管 kubernetes 方式来自动化管理集群。和所有 operator 的功能类似，系统会定时检测集群当前状态，判断是否与目标状态一致，出现不一致时，operator 会发起一系列操作，驱动集群达到目标状态。今年 kubeCon 上，雅虎日本也分享了其管理大规模 kubernetes 集群的方法，4000 节点构建了 400 个 kubernetes 集群，同样采用的是 kube-on-kube-operator 架构，以 kubernetes as a service 的形式使用。\n\n### kubernetes-operator 设计参考\n\n记得 kube-on-kube-operator 的概念最初是在去年的 kubeCon China 上蚂蚁金服提出来的，先看看蚂蚁金服以及腾讯云 kube-on-kube-operator 的设计思路，其实腾讯云的架构和蚂蚁金服的是类似的。以下是蚂蚁金服的架构设计图：\n\n![](http://cdn.tianfeiyu.com/image-20190805163304515.png)\n\n\n\n首先部署一套 kubernetes 元集群，通过元集群部署业务集群，业务集群的 master 组件都分布在一台宿主上，该宿主以 node 节点的方式挂载在元集群中。在私有云场景下，这样的部署方式有一个很明显的问题就是元集群节点跨机房，虽然业务集群的 master 与 node 都是在同一个机房，但是元集群中的 node 节点大部分是分布在不同的机房，有些公司在不同机房之间会有网络上的限制，有可能网络不通或者只能使用专线连接。在公有云场景下，元集群自己有一套独立的 vpc 网络，它要怎么和用户的 vpc 结点进行通信呢？腾讯云的做法是利用 vpc 提供的弹性网卡能力，将这个弹性网卡直接绑定到运行 apiserver 的 pod 中，运行 master  的这个pod 既加入了元集群的 vpc，又加入了用户的 vpc，也就是一个 pod 同时在两个网络中，这样就可以很好的去实现和用户 node 相关的互通。这种方式都是通过 kubernetes  API 去管理 master 组件的，master 组件的升级以及故障自愈都可以通过 kubernetes 提供的方式实现。\n\n\n\n### kubernetes-operator 设计\n\n![kubernetes-operator 架构](http://cdn.tianfeiyu.com/image-20190804152312149.png)\n\n\n\n> kubernetes-operator 项目地址：[https://github.com/gosoon/kubernetes-operator](https://github.com/gosoon/kubernetes-operator)\n\n目前该项目的主要目标是实现以下三种场景中的集群管理：\n\n- kube-on-kube\n- kube-to-kube\n- kube-to-cloud-kube\n\nkubernetes-operator 不仅是要实现 kube-on-kube 架构，还有 kube-to-kube，kube-to-cloud-kube，kube-to-kube 即 kubernetes 集群管理业务独立的 kubernetes 集群，两个集群相互独立。kube-to-cloud-kube 即 kubernetes 集群管理多云环境上的 kubernetes 集群。\n\n\n\n上面是项目的架构图，红色的线段表示对集群生命周期管理的一个操作，涉及集群的创建、删除、扩缩容、升级等，蓝色线段是对集群应用的操作，集群中应用的创建、删除、发布更新等，kubernetes-proxy 是一个 API 代理，所有涉及 API 的调用都要通过 kubernetes-proxy。左边部署有 kubernetes-operator 的是元集群，kubernetes-operator 使用 etcd 仅存储部分配置信息，其管理业务集群的生命周期，支持三种集群的创建方式，第一种方式就是可以创建出类似蚂蚁金服这种直接将业务集群 master 运行在元集群，node 节点在业务集群，第二种是以二进制方式创建业务集群，其中业务集群的 master 以及 node 都是在业务集群所在的机房，第三种方式就是在各种公有云厂商创建集群，以一种统一的方式管理公有云上的集群，也可以称作融合云。\n\n\n\n#### 项目结构\n\n总体来说，项目暂时分为三大块：\n\n- kubernetes proxy：支持 API 透传、访问控制等功能；\n- 控制器：也就是 kubernetes-operator，管理业务集群的生命周期；\n- 集群部署模块：用来部署业务集群，目前主要在开发第二种方式使用二进制部署业务集群；\n- kubernetes 应用安装模块：在新建完成的集群中部署监控、日志采集、镜像仓库、helm 等组件；\n\n\n\n#### 控制器\n\n控制器也就是 Operator + CR，目前开发 operator 的方式已知的有三种：\n\n-  自定义 controller 的方式：kube-controller-manager 中所有的 controller 就是以自定义 controller 的方式，这种方式是最原生的方式，需要开发者了解 kubernetes 中的代码生成，informer 的使用等。\n- operator-sdk 的方式：一个开发 Operator 的框架，对于一些不熟悉 kubernetes 的可以使用 operator-sdk 的方式，这种方式让开发者更注重业务的实现，但是不够灵活。\n- kubebuilder 的方式：kubebuilder 是开发 controller manager 的框架，controller manager 会管理一个或者多个 operator。\n\n\n\nkubebuilder, operator-sdk 都是对controller-runtime做了封装, controller runtime又是对client-go shardInfromer 做的封装，本质上其实都一样的。kubernetes-operator 使用的是自定义 controller  的方式，如果想要更深入的学习 kubernetes，非自定义 controller 方式莫属了，kube-controller-manager 组件中的各种 controller 都是使用这种方式开发的，完全可以按照官方这种套路来开发。在 kubernetes 中，目前有两种方式可以定义一个新对象，一是 CustomResourceDefinition（CRD）、二是 Aggregation ApiServer（AA），其中 CRD 是相对简单也是目前应用比较广的方法。kubernetes-operator 采用 CRD 的方式。\n\n\n\n#### 集群部署\n\n其实项目中最难的是集群部署这一部分，部署集群目前有两种方式，二进制部署和容器化部署，但是都有一些开源工具的支持。手动部署一个二进制集群需要熟悉 docker 的部署、etcd 的部署、角色证书的创建、RBAC 授权、网络配置、yaml 文件编写、kubernetes 集群运维等等，总之手动部署一个二进制集群是非常麻烦的，但是要真正会用 kubernetes 是逃不了部署这一步的。第二种方式就是以容器化的方式部署，这种部署方式相对来说比较简单，有现成的工具直接傻瓜式操作就能部署成功。但是我目前选择的是使用二进制的部署方式，由于自己运维过二进制的 kubernetes 集群，对于私有云场景一般都是直接将集群部署在物理机上，作为生产环境，自己认为容器化的方式部署还不是非常成熟的，目前工作过的大小公司中，生产环境暂时没有以容器化的方式运行集群。所以 kubernetes-operator 中目前主要支持的就是使用二进制部署集群。\n\n\n\n目前比较成熟的用于生产环境的 kubernetes 集群部署工具有：kubeadm、kubespary、kops、rancher、kubeasz 等。kubeadm、kubespary、kops 都是官方开源的产品，kubeadm 使用容器化的方式部署，需要手动执行一些部署命令，暂时无法完全自动化部署。kubespary 是对 kubeadm 的一层封装，使用 ansible + kubeadm 的方式自动化进行部署，据说阿里云就是使用 kubespary 部署集群的。在公有云的环境(GCP、AWS)通常使用 kops  部署起来更方便些。kubeasz 是使用 ansible 自动化的方式部署二进制集群，目前也已经比较成熟了。\n\n\n\n#### 应用安装\n\n- 监控：当然是使用 promethus；\n- 日志采集：使用 filebeat 或者基于 filebeat 封装的一些组件如 logpilot，其他的还有 logkit 等都可以尝试使用；\n- 镜像仓库：当然是使用 harbor；\n- HPA：组件以及应用的自动扩缩容；\n\n应用安装使用 helm 的方式进行安装。\n\n\n\n#### 集群升级\n\n若以二进制部署最好是替换二进制文件的方式进行升级，若使用容器化部署，master 部署在元集群中可以使用 kubernetes 的滚动方式升级否则要以修改 manifest 文件的方式。\n\n集群升级包括配置和版本的升级，集群部署完成后，master 的配置改动不会很频繁，由于要进行性能上的优化以及业务的支持，对于 node 组件上的配置升级还是比较多的。对于集群的版本升级，升级的难度系数随着版本的跨度增大而增大，若按照官方的升级流程，一般不会出现异常。升级操作一般都是先升 master 再升 node，在工作中经历的几次版本升级中，每次升级完 master 后理论上不会再回退了，除非升级过程中有问题，否则升级完成后已经很难回退了，master 升级完成后 APIServer 的一些 API 还有 pod 的字段都有可能改变，master 版本回退后一些已存在的应用可能会异常，或者还可以参考 openshift 的蓝绿升级方式。二进制部署的集群尽量以替换二进制文件的方式进行升级，对于容器化部署的集群，可以直接使用 kubernetes 的滚动方式升级或者是修改 manifest 文件的方式。\n\n目前蚂蚁金服 kube-on-kube-operator 架构中在业务集群中会部署一个 node-operator，node-operator 会记录 master 组件的镜像、默认启动参数等信息，其作用就是节点配置管理、集群组件升级以及节点故障自愈，未来在项目中也会实现基于此的方式。\n\n\n\n### 后期计划\n\n- 支持部署 k3s、kubeedge：5G 时代，边缘计算将是非常火的，目前各大厂商也都在此布局，所以支持部署 k3s、kubeedge 这些专门支持边缘计算的产品还是非常有必要的。\n- 支持使用 kops 部署\n- 支持部署多版本 k8s\n- node-operator 开发，支持集群的配置管理、自动化升级、故障自愈等功能\n- 用户及权限管理：操作集群用户的权限和 kubernetes 中 RBAC 规则绑定\n- Kubernetes-operator 一些功能的扩展和完善\n\n\n\n参考：\n\n[腾讯云容器服务TKE：一键部署实践](https://mp.weixin.qq.com/s/WScGf3DRDC8ryyrf_tY-Qw)\n\n[一年时间打造全球最大规模之一的Kubernetes集群，蚂蚁金服怎么做到的？](https://mp.weixin.qq.com/s/bJrMNxKMn89TzmpEyIZrRg)\n\n[https://github.com/gosoon/kubernetes-operator](https://github.com/gosoon/kubernetes-operator)\n","slug":"kube_on_kube_operator_1","published":1,"updated":"2019-08-08T01:36:07.960Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59l0010apwn0xhnvj2v","content":"<p>kubernetes 已经成为容器时代的分布式操作系统内核，目前也是所有公有云提供商的标配，在国内，阿里云、腾讯云、华为云这样的公有云大厂商都支持一键部署 kubernetes 集群，而 kubernetes 集群自动化管理则是迫切需要解决的问题。对于大部分不熟悉 kubernetes 而要上云的小白用户就强烈需要一个被托管及能自动化运维的集群，他们平时只是进行业务的部署与变更，只需要对 kubernetes 中部分概念了解即可。同样在私有云场景下，笔者所待过的几个大小公司一般都会维护多套集群，集群的运维工作就是一个很大的挑战，反观各大厂同样要有效可靠的管理大规模集群，kube-on-kube-operator 是一个很好的解决方案。</p>\n<p>所谓 kube-on-kube-operator，就是将 kubernetes 运行在 kubernetes 上，用 kubernetes 托管 kubernetes 方式来自动化管理集群。和所有 operator 的功能类似，系统会定时检测集群当前状态，判断是否与目标状态一致，出现不一致时，operator 会发起一系列操作，驱动集群达到目标状态。今年 kubeCon 上，雅虎日本也分享了其管理大规模 kubernetes 集群的方法，4000 节点构建了 400 个 kubernetes 集群，同样采用的是 kube-on-kube-operator 架构，以 kubernetes as a service 的形式使用。</p>\n<h3 id=\"kubernetes-operator-设计参考\"><a href=\"#kubernetes-operator-设计参考\" class=\"headerlink\" title=\"kubernetes-operator 设计参考\"></a>kubernetes-operator 设计参考</h3><p>记得 kube-on-kube-operator 的概念最初是在去年的 kubeCon China 上蚂蚁金服提出来的，先看看蚂蚁金服以及腾讯云 kube-on-kube-operator 的设计思路，其实腾讯云的架构和蚂蚁金服的是类似的。以下是蚂蚁金服的架构设计图：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20190805163304515.png\" alt=\"\"></p>\n<p>首先部署一套 kubernetes 元集群，通过元集群部署业务集群，业务集群的 master 组件都分布在一台宿主上，该宿主以 node 节点的方式挂载在元集群中。在私有云场景下，这样的部署方式有一个很明显的问题就是元集群节点跨机房，虽然业务集群的 master 与 node 都是在同一个机房，但是元集群中的 node 节点大部分是分布在不同的机房，有些公司在不同机房之间会有网络上的限制，有可能网络不通或者只能使用专线连接。在公有云场景下，元集群自己有一套独立的 vpc 网络，它要怎么和用户的 vpc 结点进行通信呢？腾讯云的做法是利用 vpc 提供的弹性网卡能力，将这个弹性网卡直接绑定到运行 apiserver 的 pod 中，运行 master  的这个pod 既加入了元集群的 vpc，又加入了用户的 vpc，也就是一个 pod 同时在两个网络中，这样就可以很好的去实现和用户 node 相关的互通。这种方式都是通过 kubernetes  API 去管理 master 组件的，master 组件的升级以及故障自愈都可以通过 kubernetes 提供的方式实现。</p>\n<h3 id=\"kubernetes-operator-设计\"><a href=\"#kubernetes-operator-设计\" class=\"headerlink\" title=\"kubernetes-operator 设计\"></a>kubernetes-operator 设计</h3><p><img src=\"http://cdn.tianfeiyu.com/image-20190804152312149.png\" alt=\"kubernetes-operator 架构\"></p>\n<blockquote>\n<p>kubernetes-operator 项目地址：<a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-operator</a></p>\n</blockquote>\n<p>目前该项目的主要目标是实现以下三种场景中的集群管理：</p>\n<ul>\n<li>kube-on-kube</li>\n<li>kube-to-kube</li>\n<li>kube-to-cloud-kube</li>\n</ul>\n<p>kubernetes-operator 不仅是要实现 kube-on-kube 架构，还有 kube-to-kube，kube-to-cloud-kube，kube-to-kube 即 kubernetes 集群管理业务独立的 kubernetes 集群，两个集群相互独立。kube-to-cloud-kube 即 kubernetes 集群管理多云环境上的 kubernetes 集群。</p>\n<p>上面是项目的架构图，红色的线段表示对集群生命周期管理的一个操作，涉及集群的创建、删除、扩缩容、升级等，蓝色线段是对集群应用的操作，集群中应用的创建、删除、发布更新等，kubernetes-proxy 是一个 API 代理，所有涉及 API 的调用都要通过 kubernetes-proxy。左边部署有 kubernetes-operator 的是元集群，kubernetes-operator 使用 etcd 仅存储部分配置信息，其管理业务集群的生命周期，支持三种集群的创建方式，第一种方式就是可以创建出类似蚂蚁金服这种直接将业务集群 master 运行在元集群，node 节点在业务集群，第二种是以二进制方式创建业务集群，其中业务集群的 master 以及 node 都是在业务集群所在的机房，第三种方式就是在各种公有云厂商创建集群，以一种统一的方式管理公有云上的集群，也可以称作融合云。</p>\n<h4 id=\"项目结构\"><a href=\"#项目结构\" class=\"headerlink\" title=\"项目结构\"></a>项目结构</h4><p>总体来说，项目暂时分为三大块：</p>\n<ul>\n<li>kubernetes proxy：支持 API 透传、访问控制等功能；</li>\n<li>控制器：也就是 kubernetes-operator，管理业务集群的生命周期；</li>\n<li>集群部署模块：用来部署业务集群，目前主要在开发第二种方式使用二进制部署业务集群；</li>\n<li>kubernetes 应用安装模块：在新建完成的集群中部署监控、日志采集、镜像仓库、helm 等组件；</li>\n</ul>\n<h4 id=\"控制器\"><a href=\"#控制器\" class=\"headerlink\" title=\"控制器\"></a>控制器</h4><p>控制器也就是 Operator + CR，目前开发 operator 的方式已知的有三种：</p>\n<ul>\n<li>自定义 controller 的方式：kube-controller-manager 中所有的 controller 就是以自定义 controller 的方式，这种方式是最原生的方式，需要开发者了解 kubernetes 中的代码生成，informer 的使用等。</li>\n<li>operator-sdk 的方式：一个开发 Operator 的框架，对于一些不熟悉 kubernetes 的可以使用 operator-sdk 的方式，这种方式让开发者更注重业务的实现，但是不够灵活。</li>\n<li>kubebuilder 的方式：kubebuilder 是开发 controller manager 的框架，controller manager 会管理一个或者多个 operator。</li>\n</ul>\n<p>kubebuilder, operator-sdk 都是对controller-runtime做了封装, controller runtime又是对client-go shardInfromer 做的封装，本质上其实都一样的。kubernetes-operator 使用的是自定义 controller  的方式，如果想要更深入的学习 kubernetes，非自定义 controller 方式莫属了，kube-controller-manager 组件中的各种 controller 都是使用这种方式开发的，完全可以按照官方这种套路来开发。在 kubernetes 中，目前有两种方式可以定义一个新对象，一是 CustomResourceDefinition（CRD）、二是 Aggregation ApiServer（AA），其中 CRD 是相对简单也是目前应用比较广的方法。kubernetes-operator 采用 CRD 的方式。</p>\n<h4 id=\"集群部署\"><a href=\"#集群部署\" class=\"headerlink\" title=\"集群部署\"></a>集群部署</h4><p>其实项目中最难的是集群部署这一部分，部署集群目前有两种方式，二进制部署和容器化部署，但是都有一些开源工具的支持。手动部署一个二进制集群需要熟悉 docker 的部署、etcd 的部署、角色证书的创建、RBAC 授权、网络配置、yaml 文件编写、kubernetes 集群运维等等，总之手动部署一个二进制集群是非常麻烦的，但是要真正会用 kubernetes 是逃不了部署这一步的。第二种方式就是以容器化的方式部署，这种部署方式相对来说比较简单，有现成的工具直接傻瓜式操作就能部署成功。但是我目前选择的是使用二进制的部署方式，由于自己运维过二进制的 kubernetes 集群，对于私有云场景一般都是直接将集群部署在物理机上，作为生产环境，自己认为容器化的方式部署还不是非常成熟的，目前工作过的大小公司中，生产环境暂时没有以容器化的方式运行集群。所以 kubernetes-operator 中目前主要支持的就是使用二进制部署集群。</p>\n<p>目前比较成熟的用于生产环境的 kubernetes 集群部署工具有：kubeadm、kubespary、kops、rancher、kubeasz 等。kubeadm、kubespary、kops 都是官方开源的产品，kubeadm 使用容器化的方式部署，需要手动执行一些部署命令，暂时无法完全自动化部署。kubespary 是对 kubeadm 的一层封装，使用 ansible + kubeadm 的方式自动化进行部署，据说阿里云就是使用 kubespary 部署集群的。在公有云的环境(GCP、AWS)通常使用 kops  部署起来更方便些。kubeasz 是使用 ansible 自动化的方式部署二进制集群，目前也已经比较成熟了。</p>\n<h4 id=\"应用安装\"><a href=\"#应用安装\" class=\"headerlink\" title=\"应用安装\"></a>应用安装</h4><ul>\n<li>监控：当然是使用 promethus；</li>\n<li>日志采集：使用 filebeat 或者基于 filebeat 封装的一些组件如 logpilot，其他的还有 logkit 等都可以尝试使用；</li>\n<li>镜像仓库：当然是使用 harbor；</li>\n<li>HPA：组件以及应用的自动扩缩容；</li>\n</ul>\n<p>应用安装使用 helm 的方式进行安装。</p>\n<h4 id=\"集群升级\"><a href=\"#集群升级\" class=\"headerlink\" title=\"集群升级\"></a>集群升级</h4><p>若以二进制部署最好是替换二进制文件的方式进行升级，若使用容器化部署，master 部署在元集群中可以使用 kubernetes 的滚动方式升级否则要以修改 manifest 文件的方式。</p>\n<p>集群升级包括配置和版本的升级，集群部署完成后，master 的配置改动不会很频繁，由于要进行性能上的优化以及业务的支持，对于 node 组件上的配置升级还是比较多的。对于集群的版本升级，升级的难度系数随着版本的跨度增大而增大，若按照官方的升级流程，一般不会出现异常。升级操作一般都是先升 master 再升 node，在工作中经历的几次版本升级中，每次升级完 master 后理论上不会再回退了，除非升级过程中有问题，否则升级完成后已经很难回退了，master 升级完成后 APIServer 的一些 API 还有 pod 的字段都有可能改变，master 版本回退后一些已存在的应用可能会异常，或者还可以参考 openshift 的蓝绿升级方式。二进制部署的集群尽量以替换二进制文件的方式进行升级，对于容器化部署的集群，可以直接使用 kubernetes 的滚动方式升级或者是修改 manifest 文件的方式。</p>\n<p>目前蚂蚁金服 kube-on-kube-operator 架构中在业务集群中会部署一个 node-operator，node-operator 会记录 master 组件的镜像、默认启动参数等信息，其作用就是节点配置管理、集群组件升级以及节点故障自愈，未来在项目中也会实现基于此的方式。</p>\n<h3 id=\"后期计划\"><a href=\"#后期计划\" class=\"headerlink\" title=\"后期计划\"></a>后期计划</h3><ul>\n<li>支持部署 k3s、kubeedge：5G 时代，边缘计算将是非常火的，目前各大厂商也都在此布局，所以支持部署 k3s、kubeedge 这些专门支持边缘计算的产品还是非常有必要的。</li>\n<li>支持使用 kops 部署</li>\n<li>支持部署多版本 k8s</li>\n<li>node-operator 开发，支持集群的配置管理、自动化升级、故障自愈等功能</li>\n<li>用户及权限管理：操作集群用户的权限和 kubernetes 中 RBAC 规则绑定</li>\n<li>Kubernetes-operator 一些功能的扩展和完善</li>\n</ul>\n<p>参考：</p>\n<p><a href=\"https://mp.weixin.qq.com/s/WScGf3DRDC8ryyrf_tY-Qw\" target=\"_blank\" rel=\"noopener\">腾讯云容器服务TKE：一键部署实践</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/bJrMNxKMn89TzmpEyIZrRg\" target=\"_blank\" rel=\"noopener\">一年时间打造全球最大规模之一的Kubernetes集群，蚂蚁金服怎么做到的？</a></p>\n<p><a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-operator</a></p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/\">kube-on-kube-operator 开发(三)</a></li><li><a href=\"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/\">kube-on-kube-operator 开发(二)</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>kubernetes 已经成为容器时代的分布式操作系统内核，目前也是所有公有云提供商的标配，在国内，阿里云、腾讯云、华为云这样的公有云大厂商都支持一键部署 kubernetes 集群，而 kubernetes 集群自动化管理则是迫切需要解决的问题。对于大部分不熟悉 kubernetes 而要上云的小白用户就强烈需要一个被托管及能自动化运维的集群，他们平时只是进行业务的部署与变更，只需要对 kubernetes 中部分概念了解即可。同样在私有云场景下，笔者所待过的几个大小公司一般都会维护多套集群，集群的运维工作就是一个很大的挑战，反观各大厂同样要有效可靠的管理大规模集群，kube-on-kube-operator 是一个很好的解决方案。</p>\n<p>所谓 kube-on-kube-operator，就是将 kubernetes 运行在 kubernetes 上，用 kubernetes 托管 kubernetes 方式来自动化管理集群。和所有 operator 的功能类似，系统会定时检测集群当前状态，判断是否与目标状态一致，出现不一致时，operator 会发起一系列操作，驱动集群达到目标状态。今年 kubeCon 上，雅虎日本也分享了其管理大规模 kubernetes 集群的方法，4000 节点构建了 400 个 kubernetes 集群，同样采用的是 kube-on-kube-operator 架构，以 kubernetes as a service 的形式使用。</p>\n<h3 id=\"kubernetes-operator-设计参考\"><a href=\"#kubernetes-operator-设计参考\" class=\"headerlink\" title=\"kubernetes-operator 设计参考\"></a>kubernetes-operator 设计参考</h3><p>记得 kube-on-kube-operator 的概念最初是在去年的 kubeCon China 上蚂蚁金服提出来的，先看看蚂蚁金服以及腾讯云 kube-on-kube-operator 的设计思路，其实腾讯云的架构和蚂蚁金服的是类似的。以下是蚂蚁金服的架构设计图：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20190805163304515.png\" alt=\"\"></p>\n<p>首先部署一套 kubernetes 元集群，通过元集群部署业务集群，业务集群的 master 组件都分布在一台宿主上，该宿主以 node 节点的方式挂载在元集群中。在私有云场景下，这样的部署方式有一个很明显的问题就是元集群节点跨机房，虽然业务集群的 master 与 node 都是在同一个机房，但是元集群中的 node 节点大部分是分布在不同的机房，有些公司在不同机房之间会有网络上的限制，有可能网络不通或者只能使用专线连接。在公有云场景下，元集群自己有一套独立的 vpc 网络，它要怎么和用户的 vpc 结点进行通信呢？腾讯云的做法是利用 vpc 提供的弹性网卡能力，将这个弹性网卡直接绑定到运行 apiserver 的 pod 中，运行 master  的这个pod 既加入了元集群的 vpc，又加入了用户的 vpc，也就是一个 pod 同时在两个网络中，这样就可以很好的去实现和用户 node 相关的互通。这种方式都是通过 kubernetes  API 去管理 master 组件的，master 组件的升级以及故障自愈都可以通过 kubernetes 提供的方式实现。</p>\n<h3 id=\"kubernetes-operator-设计\"><a href=\"#kubernetes-operator-设计\" class=\"headerlink\" title=\"kubernetes-operator 设计\"></a>kubernetes-operator 设计</h3><p><img src=\"http://cdn.tianfeiyu.com/image-20190804152312149.png\" alt=\"kubernetes-operator 架构\"></p>\n<blockquote>\n<p>kubernetes-operator 项目地址：<a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-operator</a></p>\n</blockquote>\n<p>目前该项目的主要目标是实现以下三种场景中的集群管理：</p>\n<ul>\n<li>kube-on-kube</li>\n<li>kube-to-kube</li>\n<li>kube-to-cloud-kube</li>\n</ul>\n<p>kubernetes-operator 不仅是要实现 kube-on-kube 架构，还有 kube-to-kube，kube-to-cloud-kube，kube-to-kube 即 kubernetes 集群管理业务独立的 kubernetes 集群，两个集群相互独立。kube-to-cloud-kube 即 kubernetes 集群管理多云环境上的 kubernetes 集群。</p>\n<p>上面是项目的架构图，红色的线段表示对集群生命周期管理的一个操作，涉及集群的创建、删除、扩缩容、升级等，蓝色线段是对集群应用的操作，集群中应用的创建、删除、发布更新等，kubernetes-proxy 是一个 API 代理，所有涉及 API 的调用都要通过 kubernetes-proxy。左边部署有 kubernetes-operator 的是元集群，kubernetes-operator 使用 etcd 仅存储部分配置信息，其管理业务集群的生命周期，支持三种集群的创建方式，第一种方式就是可以创建出类似蚂蚁金服这种直接将业务集群 master 运行在元集群，node 节点在业务集群，第二种是以二进制方式创建业务集群，其中业务集群的 master 以及 node 都是在业务集群所在的机房，第三种方式就是在各种公有云厂商创建集群，以一种统一的方式管理公有云上的集群，也可以称作融合云。</p>\n<h4 id=\"项目结构\"><a href=\"#项目结构\" class=\"headerlink\" title=\"项目结构\"></a>项目结构</h4><p>总体来说，项目暂时分为三大块：</p>\n<ul>\n<li>kubernetes proxy：支持 API 透传、访问控制等功能；</li>\n<li>控制器：也就是 kubernetes-operator，管理业务集群的生命周期；</li>\n<li>集群部署模块：用来部署业务集群，目前主要在开发第二种方式使用二进制部署业务集群；</li>\n<li>kubernetes 应用安装模块：在新建完成的集群中部署监控、日志采集、镜像仓库、helm 等组件；</li>\n</ul>\n<h4 id=\"控制器\"><a href=\"#控制器\" class=\"headerlink\" title=\"控制器\"></a>控制器</h4><p>控制器也就是 Operator + CR，目前开发 operator 的方式已知的有三种：</p>\n<ul>\n<li>自定义 controller 的方式：kube-controller-manager 中所有的 controller 就是以自定义 controller 的方式，这种方式是最原生的方式，需要开发者了解 kubernetes 中的代码生成，informer 的使用等。</li>\n<li>operator-sdk 的方式：一个开发 Operator 的框架，对于一些不熟悉 kubernetes 的可以使用 operator-sdk 的方式，这种方式让开发者更注重业务的实现，但是不够灵活。</li>\n<li>kubebuilder 的方式：kubebuilder 是开发 controller manager 的框架，controller manager 会管理一个或者多个 operator。</li>\n</ul>\n<p>kubebuilder, operator-sdk 都是对controller-runtime做了封装, controller runtime又是对client-go shardInfromer 做的封装，本质上其实都一样的。kubernetes-operator 使用的是自定义 controller  的方式，如果想要更深入的学习 kubernetes，非自定义 controller 方式莫属了，kube-controller-manager 组件中的各种 controller 都是使用这种方式开发的，完全可以按照官方这种套路来开发。在 kubernetes 中，目前有两种方式可以定义一个新对象，一是 CustomResourceDefinition（CRD）、二是 Aggregation ApiServer（AA），其中 CRD 是相对简单也是目前应用比较广的方法。kubernetes-operator 采用 CRD 的方式。</p>\n<h4 id=\"集群部署\"><a href=\"#集群部署\" class=\"headerlink\" title=\"集群部署\"></a>集群部署</h4><p>其实项目中最难的是集群部署这一部分，部署集群目前有两种方式，二进制部署和容器化部署，但是都有一些开源工具的支持。手动部署一个二进制集群需要熟悉 docker 的部署、etcd 的部署、角色证书的创建、RBAC 授权、网络配置、yaml 文件编写、kubernetes 集群运维等等，总之手动部署一个二进制集群是非常麻烦的，但是要真正会用 kubernetes 是逃不了部署这一步的。第二种方式就是以容器化的方式部署，这种部署方式相对来说比较简单，有现成的工具直接傻瓜式操作就能部署成功。但是我目前选择的是使用二进制的部署方式，由于自己运维过二进制的 kubernetes 集群，对于私有云场景一般都是直接将集群部署在物理机上，作为生产环境，自己认为容器化的方式部署还不是非常成熟的，目前工作过的大小公司中，生产环境暂时没有以容器化的方式运行集群。所以 kubernetes-operator 中目前主要支持的就是使用二进制部署集群。</p>\n<p>目前比较成熟的用于生产环境的 kubernetes 集群部署工具有：kubeadm、kubespary、kops、rancher、kubeasz 等。kubeadm、kubespary、kops 都是官方开源的产品，kubeadm 使用容器化的方式部署，需要手动执行一些部署命令，暂时无法完全自动化部署。kubespary 是对 kubeadm 的一层封装，使用 ansible + kubeadm 的方式自动化进行部署，据说阿里云就是使用 kubespary 部署集群的。在公有云的环境(GCP、AWS)通常使用 kops  部署起来更方便些。kubeasz 是使用 ansible 自动化的方式部署二进制集群，目前也已经比较成熟了。</p>\n<h4 id=\"应用安装\"><a href=\"#应用安装\" class=\"headerlink\" title=\"应用安装\"></a>应用安装</h4><ul>\n<li>监控：当然是使用 promethus；</li>\n<li>日志采集：使用 filebeat 或者基于 filebeat 封装的一些组件如 logpilot，其他的还有 logkit 等都可以尝试使用；</li>\n<li>镜像仓库：当然是使用 harbor；</li>\n<li>HPA：组件以及应用的自动扩缩容；</li>\n</ul>\n<p>应用安装使用 helm 的方式进行安装。</p>\n<h4 id=\"集群升级\"><a href=\"#集群升级\" class=\"headerlink\" title=\"集群升级\"></a>集群升级</h4><p>若以二进制部署最好是替换二进制文件的方式进行升级，若使用容器化部署，master 部署在元集群中可以使用 kubernetes 的滚动方式升级否则要以修改 manifest 文件的方式。</p>\n<p>集群升级包括配置和版本的升级，集群部署完成后，master 的配置改动不会很频繁，由于要进行性能上的优化以及业务的支持，对于 node 组件上的配置升级还是比较多的。对于集群的版本升级，升级的难度系数随着版本的跨度增大而增大，若按照官方的升级流程，一般不会出现异常。升级操作一般都是先升 master 再升 node，在工作中经历的几次版本升级中，每次升级完 master 后理论上不会再回退了，除非升级过程中有问题，否则升级完成后已经很难回退了，master 升级完成后 APIServer 的一些 API 还有 pod 的字段都有可能改变，master 版本回退后一些已存在的应用可能会异常，或者还可以参考 openshift 的蓝绿升级方式。二进制部署的集群尽量以替换二进制文件的方式进行升级，对于容器化部署的集群，可以直接使用 kubernetes 的滚动方式升级或者是修改 manifest 文件的方式。</p>\n<p>目前蚂蚁金服 kube-on-kube-operator 架构中在业务集群中会部署一个 node-operator，node-operator 会记录 master 组件的镜像、默认启动参数等信息，其作用就是节点配置管理、集群组件升级以及节点故障自愈，未来在项目中也会实现基于此的方式。</p>\n<h3 id=\"后期计划\"><a href=\"#后期计划\" class=\"headerlink\" title=\"后期计划\"></a>后期计划</h3><ul>\n<li>支持部署 k3s、kubeedge：5G 时代，边缘计算将是非常火的，目前各大厂商也都在此布局，所以支持部署 k3s、kubeedge 这些专门支持边缘计算的产品还是非常有必要的。</li>\n<li>支持使用 kops 部署</li>\n<li>支持部署多版本 k8s</li>\n<li>node-operator 开发，支持集群的配置管理、自动化升级、故障自愈等功能</li>\n<li>用户及权限管理：操作集群用户的权限和 kubernetes 中 RBAC 规则绑定</li>\n<li>Kubernetes-operator 一些功能的扩展和完善</li>\n</ul>\n<p>参考：</p>\n<p><a href=\"https://mp.weixin.qq.com/s/WScGf3DRDC8ryyrf_tY-Qw\" target=\"_blank\" rel=\"noopener\">腾讯云容器服务TKE：一键部署实践</a></p>\n<p><a href=\"https://mp.weixin.qq.com/s/bJrMNxKMn89TzmpEyIZrRg\" target=\"_blank\" rel=\"noopener\">一年时间打造全球最大规模之一的Kubernetes集群，蚂蚁金服怎么做到的？</a></p>\n<p><a href=\"https://github.com/gosoon/kubernetes-operator\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubernetes-operator</a></p>\n"},{"title":"kubernetes 版本多久该升级一次","date":"2019-09-26T07:13:30.000Z","type":"release version","_content":"\nkubernetes  社区每三个月发布一个新版本，可以说发布新版本的速度非常快，当然，在生产环境中版本升级的速度可能跟不上新版本发布的速度，那么确保目前使用的版本还处于社区的维护阶段就非常重要了，kubernetes 官方对各个版本支持的时间是多长呢？\n\nkubernetes 发行版通常支持9个月，在此期间，如果发现严重的bug或安全问题，会在对应的分支发布补丁版本。比如，当前版本为 v1.10.1，当社区修复一些 bug 后，就会发布 v1.10.2 版本。\n\n官方支持时间说明如下：\n\n| Kubernetes version | Release month  | End-of-life-month |\n| :----------------- | :------------: | :---------------- |\n| v1.6.x             |   March 2017   | December 2017     |\n| v1.7.x             |   June 2017    | March 2018        |\n| v1.8.x             | September 2017 | June 2018         |\n| v1.9.x             | December 2017  | September 2018    |\n| v1.10.x            |   March 2018   | December 2018     |\n| v1.11.x            |   June 2018    | March 2019        |\n| v1.12.x            | September 2018 | June 2019         |\n| v1.13.x            | December 2018  | September 2019    |\n| v1.14.x            |   March 2019   | December 2019     |\n| v1.15.x            |   June 2019    | March 2020        |\n| v1.16.x            | September 2019 | June 2020         |\n\n到目前为止，v1.13.x 以前的版本已经停止支持了，请尽快升级至高版本。\n\n\n\n### kubernetes 版本发布流程\n\n>  翻译自官方文档：[Kubernetes Release Versioning](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md)\n\n **说明**：**Kube X.Y.Z** 代表 kubernetes 已经发布的版本（git tag），这个版本包含所有的组件：apiserver, kubelet, kubectl, etc. (**X** 表示主版本号, **Y** 是此版本号, **Z** 是补丁版本。)\n\n\n\n#### 版本发布时间\n\n##### 次版本发布计划与时间表\n\n- Kube X.Y.0-alpha.W, W > 0 ( 分支：master)\n  - Alpha 版本大约每两周直接从 master 分支发布一次。\n  - 没有 cherrypick 版本。如有有严重的 bug 被修复，可以基于 master 分支提前创建一个新版本。\n\n- Kube X.Y.Z-beta.W (分支: release-X.Y)\n\n  - 当 master 完成 Kube X.Y 的功能后，在距 X.Y.0 发布前两周会停掉 release-X.Y 分支，只将一些比较重要的 PR cherry-pick 到 X.Y。\n\n  - 该分支会被标记为  X.Y.0-beta.0，master 分支会被移到  X.Y+1.0-alpha.0。\n\n    ![](http://cdn.tianfeiyu.com/image-20190925195015864.png)\n\n  - 如果 X.Y.0-beta.0 的功能有缺陷，还会发布其他的 beta 版本 (X.Y.0-beta.W | W > 0) 。\n\n- Kube X.Y.0 (分支: release-X.Y)\n\n  - 最终的 release 版本会提前两周从 release-X.Y 分支上产生。\n  - 在同一分支的同一 commit  处也会被标记为 X.Y.1-beta.0。\n  - 在 X.Y.0 发布 3-4 个月后会发布 X.(Y-1).0。\n\n- Kube X.Y.Z, Z > 0 (分支: release-X.Y)\n\n  - 当 cherrypick commits 到 release-X.Y 分支时，若有需要，也会发布相应的[补丁版本](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#patch-releases) （X.Y.Z-beta.W）。\n  - X.Y.Z 是直接从 release-X.Y 分支上产生的，当使用 beta 版本在更新 pkg/version/base.go 后会被标记为 X.Y.Z+1-beta.0。\n\n- Kube X.Y.Z, Z > 0 (分支: release-X.Y.Z)\n\n  - 这是一个特殊的 tag，如果在上一个 release 分支后有重大的 bug 被修复，会有一个 X.Y.Z tag。\n\n  - release-X.Y.Z 分支会被停掉以确保补丁版本是最新的。\n\n  - 如果还有重要 bug 被修复会再有一个补丁版本  X.Y.(Z+1)。\n\n  - 一般不会有[补丁版本](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#patch-releases)，补丁版本仅用于一些重大 bug 的修复。\n\n  -  可以参考[#19849](https://issues.k8s.io/19849)看看补丁版本的作用。\n\n\n\n#### 主版本时间线\n\n主版本暂时没有预期发布的时间点，也没有公布 2.0.0 的标准。到目前为止，我们还没有对任何类型的不兼容更改(例如，组件参数更改)。之前讨论过在发布 2.0.0 后 删除 `v1` API group/version，但目前没有这样做的计划。\n\n\n\n#### 支持的组件版本与兼容版本\n\n我们希望用户在生产中使用 kubernetes 最稳定的版本，但升级版本需要一些时间，尤其是对于生产环境中的关键组件。我们也希望用户更新到最新的补丁版本，补丁版本中包含一些重要的 bugfix，希望用户尽快升级。\n\nkubernetes 对各组件的版本也有一定的兼容性。具体的兼容策略是： slave组件可以与master组件最多延迟两个版本(minor version)，但是不能比 master 组件新。client 不能与 master 组件落后一个次版本，但是可以高一个版本，也就是说： v1.3 的 master 可以与 v1.1，v1.2，v1.3 的 slave 组件一起使用，与 v1.2，v1.3，v1.4 client 一起使用。\n\n此外，我们希望一次“支持”三个次版本，“支持”意味着我们希望用户在生产环境中运行该版本，虽然我们可能对于不在支持的版本进行 bugfix。例如，当 v1.3 发布时，将不再支持 v1.0。此外新版本每三个月发行一次，也就是说一个版本仅支持 9 个月。\n\n\n\n### 升级策略\n\n用户可以使用滚动方式升级，一次升级一个小版本，不建议直接跨度两个及以上小版本，升级时先升级 master 再升级 node 节点。\n\n以下是在实际升级过程中的一些经验：\n\n金丝雀部署：即灰度升级，若使用二进制部署，则在原有集群直接替换二进制进行升级，运维代价小，不会导致服务中断；若以 pod 方式部署的 master 组件直接替换镜像进行升级，若以 deployment 方式部署 master 组件，对于 apiserver 可以参考阿里的经验，设置 maxSurge=3 的方式升级，以避免升级过程带来的性能抖动，但所有的 node 组件依然需要替换二进制升级。\n\n蓝绿部署：搭建一套新的集群，这种方式升级方式比较麻烦，涉及到数据迁移，IP 更换操作，对于部分业务不适用，风险不可控。\n\n\n\n可以看到，kubernetes 社区的更新速度非常快，坚决不建议自己维护一套 kubernetes 版本，每次升级巨麻烦，将所有修改过的 commit cherry-pick 到每个新版本上，也容易出错，有些新版本的改动也比较大，之前修改过的地方在新版中有可能已经被移除或放在别的位置了。\n\n详细的升级策略可以参考：[kubernetes集群升级的正确姿势](https://www.cnblogs.com/gaorong/p/11266629.html)。\n\n\n\n### 结论\n\nkubernetes 每三个月发布一个版本，社区仅维护最新的三个版本，一个版本的维护时间为 9 个月，请尽量保持生产环境的版本在社区维护范围内，版本升级时尽量保持小版本滚动升级，不建议跨多个版本升级。\n\n\n参考：\nhttps://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\nhttps://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md\n\n","source":"_posts/k8s_release_version.md","raw":"---\ntitle: kubernetes 版本多久该升级一次\ndate: 2019-09-26 15:13:30\ntags: [\"release version\",\"\"]\ntype: \"release version\"\n\n---\n\nkubernetes  社区每三个月发布一个新版本，可以说发布新版本的速度非常快，当然，在生产环境中版本升级的速度可能跟不上新版本发布的速度，那么确保目前使用的版本还处于社区的维护阶段就非常重要了，kubernetes 官方对各个版本支持的时间是多长呢？\n\nkubernetes 发行版通常支持9个月，在此期间，如果发现严重的bug或安全问题，会在对应的分支发布补丁版本。比如，当前版本为 v1.10.1，当社区修复一些 bug 后，就会发布 v1.10.2 版本。\n\n官方支持时间说明如下：\n\n| Kubernetes version | Release month  | End-of-life-month |\n| :----------------- | :------------: | :---------------- |\n| v1.6.x             |   March 2017   | December 2017     |\n| v1.7.x             |   June 2017    | March 2018        |\n| v1.8.x             | September 2017 | June 2018         |\n| v1.9.x             | December 2017  | September 2018    |\n| v1.10.x            |   March 2018   | December 2018     |\n| v1.11.x            |   June 2018    | March 2019        |\n| v1.12.x            | September 2018 | June 2019         |\n| v1.13.x            | December 2018  | September 2019    |\n| v1.14.x            |   March 2019   | December 2019     |\n| v1.15.x            |   June 2019    | March 2020        |\n| v1.16.x            | September 2019 | June 2020         |\n\n到目前为止，v1.13.x 以前的版本已经停止支持了，请尽快升级至高版本。\n\n\n\n### kubernetes 版本发布流程\n\n>  翻译自官方文档：[Kubernetes Release Versioning](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md)\n\n **说明**：**Kube X.Y.Z** 代表 kubernetes 已经发布的版本（git tag），这个版本包含所有的组件：apiserver, kubelet, kubectl, etc. (**X** 表示主版本号, **Y** 是此版本号, **Z** 是补丁版本。)\n\n\n\n#### 版本发布时间\n\n##### 次版本发布计划与时间表\n\n- Kube X.Y.0-alpha.W, W > 0 ( 分支：master)\n  - Alpha 版本大约每两周直接从 master 分支发布一次。\n  - 没有 cherrypick 版本。如有有严重的 bug 被修复，可以基于 master 分支提前创建一个新版本。\n\n- Kube X.Y.Z-beta.W (分支: release-X.Y)\n\n  - 当 master 完成 Kube X.Y 的功能后，在距 X.Y.0 发布前两周会停掉 release-X.Y 分支，只将一些比较重要的 PR cherry-pick 到 X.Y。\n\n  - 该分支会被标记为  X.Y.0-beta.0，master 分支会被移到  X.Y+1.0-alpha.0。\n\n    ![](http://cdn.tianfeiyu.com/image-20190925195015864.png)\n\n  - 如果 X.Y.0-beta.0 的功能有缺陷，还会发布其他的 beta 版本 (X.Y.0-beta.W | W > 0) 。\n\n- Kube X.Y.0 (分支: release-X.Y)\n\n  - 最终的 release 版本会提前两周从 release-X.Y 分支上产生。\n  - 在同一分支的同一 commit  处也会被标记为 X.Y.1-beta.0。\n  - 在 X.Y.0 发布 3-4 个月后会发布 X.(Y-1).0。\n\n- Kube X.Y.Z, Z > 0 (分支: release-X.Y)\n\n  - 当 cherrypick commits 到 release-X.Y 分支时，若有需要，也会发布相应的[补丁版本](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#patch-releases) （X.Y.Z-beta.W）。\n  - X.Y.Z 是直接从 release-X.Y 分支上产生的，当使用 beta 版本在更新 pkg/version/base.go 后会被标记为 X.Y.Z+1-beta.0。\n\n- Kube X.Y.Z, Z > 0 (分支: release-X.Y.Z)\n\n  - 这是一个特殊的 tag，如果在上一个 release 分支后有重大的 bug 被修复，会有一个 X.Y.Z tag。\n\n  - release-X.Y.Z 分支会被停掉以确保补丁版本是最新的。\n\n  - 如果还有重要 bug 被修复会再有一个补丁版本  X.Y.(Z+1)。\n\n  - 一般不会有[补丁版本](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#patch-releases)，补丁版本仅用于一些重大 bug 的修复。\n\n  -  可以参考[#19849](https://issues.k8s.io/19849)看看补丁版本的作用。\n\n\n\n#### 主版本时间线\n\n主版本暂时没有预期发布的时间点，也没有公布 2.0.0 的标准。到目前为止，我们还没有对任何类型的不兼容更改(例如，组件参数更改)。之前讨论过在发布 2.0.0 后 删除 `v1` API group/version，但目前没有这样做的计划。\n\n\n\n#### 支持的组件版本与兼容版本\n\n我们希望用户在生产中使用 kubernetes 最稳定的版本，但升级版本需要一些时间，尤其是对于生产环境中的关键组件。我们也希望用户更新到最新的补丁版本，补丁版本中包含一些重要的 bugfix，希望用户尽快升级。\n\nkubernetes 对各组件的版本也有一定的兼容性。具体的兼容策略是： slave组件可以与master组件最多延迟两个版本(minor version)，但是不能比 master 组件新。client 不能与 master 组件落后一个次版本，但是可以高一个版本，也就是说： v1.3 的 master 可以与 v1.1，v1.2，v1.3 的 slave 组件一起使用，与 v1.2，v1.3，v1.4 client 一起使用。\n\n此外，我们希望一次“支持”三个次版本，“支持”意味着我们希望用户在生产环境中运行该版本，虽然我们可能对于不在支持的版本进行 bugfix。例如，当 v1.3 发布时，将不再支持 v1.0。此外新版本每三个月发行一次，也就是说一个版本仅支持 9 个月。\n\n\n\n### 升级策略\n\n用户可以使用滚动方式升级，一次升级一个小版本，不建议直接跨度两个及以上小版本，升级时先升级 master 再升级 node 节点。\n\n以下是在实际升级过程中的一些经验：\n\n金丝雀部署：即灰度升级，若使用二进制部署，则在原有集群直接替换二进制进行升级，运维代价小，不会导致服务中断；若以 pod 方式部署的 master 组件直接替换镜像进行升级，若以 deployment 方式部署 master 组件，对于 apiserver 可以参考阿里的经验，设置 maxSurge=3 的方式升级，以避免升级过程带来的性能抖动，但所有的 node 组件依然需要替换二进制升级。\n\n蓝绿部署：搭建一套新的集群，这种方式升级方式比较麻烦，涉及到数据迁移，IP 更换操作，对于部分业务不适用，风险不可控。\n\n\n\n可以看到，kubernetes 社区的更新速度非常快，坚决不建议自己维护一套 kubernetes 版本，每次升级巨麻烦，将所有修改过的 commit cherry-pick 到每个新版本上，也容易出错，有些新版本的改动也比较大，之前修改过的地方在新版中有可能已经被移除或放在别的位置了。\n\n详细的升级策略可以参考：[kubernetes集群升级的正确姿势](https://www.cnblogs.com/gaorong/p/11266629.html)。\n\n\n\n### 结论\n\nkubernetes 每三个月发布一个版本，社区仅维护最新的三个版本，一个版本的维护时间为 9 个月，请尽量保持生产环境的版本在社区维护范围内，版本升级时尽量保持小版本滚动升级，不建议跨多个版本升级。\n\n\n参考：\nhttps://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\nhttps://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md\n\n","slug":"k8s_release_version","published":1,"updated":"2019-09-26T11:54:56.651Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59n0011apwnopz4pp34","content":"<p>kubernetes  社区每三个月发布一个新版本，可以说发布新版本的速度非常快，当然，在生产环境中版本升级的速度可能跟不上新版本发布的速度，那么确保目前使用的版本还处于社区的维护阶段就非常重要了，kubernetes 官方对各个版本支持的时间是多长呢？</p>\n<p>kubernetes 发行版通常支持9个月，在此期间，如果发现严重的bug或安全问题，会在对应的分支发布补丁版本。比如，当前版本为 v1.10.1，当社区修复一些 bug 后，就会发布 v1.10.2 版本。</p>\n<p>官方支持时间说明如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Kubernetes version</th>\n<th style=\"text-align:center\">Release month</th>\n<th style=\"text-align:left\">End-of-life-month</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">v1.6.x</td>\n<td style=\"text-align:center\">March 2017</td>\n<td style=\"text-align:left\">December 2017</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.7.x</td>\n<td style=\"text-align:center\">June 2017</td>\n<td style=\"text-align:left\">March 2018</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.8.x</td>\n<td style=\"text-align:center\">September 2017</td>\n<td style=\"text-align:left\">June 2018</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.9.x</td>\n<td style=\"text-align:center\">December 2017</td>\n<td style=\"text-align:left\">September 2018</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.10.x</td>\n<td style=\"text-align:center\">March 2018</td>\n<td style=\"text-align:left\">December 2018</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.11.x</td>\n<td style=\"text-align:center\">June 2018</td>\n<td style=\"text-align:left\">March 2019</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.12.x</td>\n<td style=\"text-align:center\">September 2018</td>\n<td style=\"text-align:left\">June 2019</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.13.x</td>\n<td style=\"text-align:center\">December 2018</td>\n<td style=\"text-align:left\">September 2019</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.14.x</td>\n<td style=\"text-align:center\">March 2019</td>\n<td style=\"text-align:left\">December 2019</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.15.x</td>\n<td style=\"text-align:center\">June 2019</td>\n<td style=\"text-align:left\">March 2020</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.16.x</td>\n<td style=\"text-align:center\">September 2019</td>\n<td style=\"text-align:left\">June 2020</td>\n</tr>\n</tbody>\n</table>\n<p>到目前为止，v1.13.x 以前的版本已经停止支持了，请尽快升级至高版本。</p>\n<h3 id=\"kubernetes-版本发布流程\"><a href=\"#kubernetes-版本发布流程\" class=\"headerlink\" title=\"kubernetes 版本发布流程\"></a>kubernetes 版本发布流程</h3><blockquote>\n<p> 翻译自官方文档：<a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md\" target=\"_blank\" rel=\"noopener\">Kubernetes Release Versioning</a></p>\n</blockquote>\n<p> <strong>说明</strong>：<strong>Kube X.Y.Z</strong> 代表 kubernetes 已经发布的版本（git tag），这个版本包含所有的组件：apiserver, kubelet, kubectl, etc. (<strong>X</strong> 表示主版本号, <strong>Y</strong> 是此版本号, <strong>Z</strong> 是补丁版本。)</p>\n<h4 id=\"版本发布时间\"><a href=\"#版本发布时间\" class=\"headerlink\" title=\"版本发布时间\"></a>版本发布时间</h4><h5 id=\"次版本发布计划与时间表\"><a href=\"#次版本发布计划与时间表\" class=\"headerlink\" title=\"次版本发布计划与时间表\"></a>次版本发布计划与时间表</h5><ul>\n<li><p>Kube X.Y.0-alpha.W, W &gt; 0 ( 分支：master)</p>\n<ul>\n<li>Alpha 版本大约每两周直接从 master 分支发布一次。</li>\n<li>没有 cherrypick 版本。如有有严重的 bug 被修复，可以基于 master 分支提前创建一个新版本。</li>\n</ul>\n</li>\n<li><p>Kube X.Y.Z-beta.W (分支: release-X.Y)</p>\n<ul>\n<li><p>当 master 完成 Kube X.Y 的功能后，在距 X.Y.0 发布前两周会停掉 release-X.Y 分支，只将一些比较重要的 PR cherry-pick 到 X.Y。</p>\n</li>\n<li><p>该分支会被标记为  X.Y.0-beta.0，master 分支会被移到  X.Y+1.0-alpha.0。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20190925195015864.png\" alt=\"\"></p>\n</li>\n<li><p>如果 X.Y.0-beta.0 的功能有缺陷，还会发布其他的 beta 版本 (X.Y.0-beta.W | W &gt; 0) 。</p>\n</li>\n</ul>\n</li>\n<li><p>Kube X.Y.0 (分支: release-X.Y)</p>\n<ul>\n<li>最终的 release 版本会提前两周从 release-X.Y 分支上产生。</li>\n<li>在同一分支的同一 commit  处也会被标记为 X.Y.1-beta.0。</li>\n<li>在 X.Y.0 发布 3-4 个月后会发布 X.(Y-1).0。</li>\n</ul>\n</li>\n<li><p>Kube X.Y.Z, Z &gt; 0 (分支: release-X.Y)</p>\n<ul>\n<li>当 cherrypick commits 到 release-X.Y 分支时，若有需要，也会发布相应的<a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#patch-releases\" target=\"_blank\" rel=\"noopener\">补丁版本</a> （X.Y.Z-beta.W）。</li>\n<li>X.Y.Z 是直接从 release-X.Y 分支上产生的，当使用 beta 版本在更新 pkg/version/base.go 后会被标记为 X.Y.Z+1-beta.0。</li>\n</ul>\n</li>\n<li><p>Kube X.Y.Z, Z &gt; 0 (分支: release-X.Y.Z)</p>\n<ul>\n<li><p>这是一个特殊的 tag，如果在上一个 release 分支后有重大的 bug 被修复，会有一个 X.Y.Z tag。</p>\n</li>\n<li><p>release-X.Y.Z 分支会被停掉以确保补丁版本是最新的。</p>\n</li>\n<li><p>如果还有重要 bug 被修复会再有一个补丁版本  X.Y.(Z+1)。</p>\n</li>\n<li><p>一般不会有<a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#patch-releases\" target=\"_blank\" rel=\"noopener\">补丁版本</a>，补丁版本仅用于一些重大 bug 的修复。</p>\n</li>\n<li><p>可以参考<a href=\"https://issues.k8s.io/19849\" target=\"_blank\" rel=\"noopener\">#19849</a>看看补丁版本的作用。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"主版本时间线\"><a href=\"#主版本时间线\" class=\"headerlink\" title=\"主版本时间线\"></a>主版本时间线</h4><p>主版本暂时没有预期发布的时间点，也没有公布 2.0.0 的标准。到目前为止，我们还没有对任何类型的不兼容更改(例如，组件参数更改)。之前讨论过在发布 2.0.0 后 删除 <code>v1</code> API group/version，但目前没有这样做的计划。</p>\n<h4 id=\"支持的组件版本与兼容版本\"><a href=\"#支持的组件版本与兼容版本\" class=\"headerlink\" title=\"支持的组件版本与兼容版本\"></a>支持的组件版本与兼容版本</h4><p>我们希望用户在生产中使用 kubernetes 最稳定的版本，但升级版本需要一些时间，尤其是对于生产环境中的关键组件。我们也希望用户更新到最新的补丁版本，补丁版本中包含一些重要的 bugfix，希望用户尽快升级。</p>\n<p>kubernetes 对各组件的版本也有一定的兼容性。具体的兼容策略是： slave组件可以与master组件最多延迟两个版本(minor version)，但是不能比 master 组件新。client 不能与 master 组件落后一个次版本，但是可以高一个版本，也就是说： v1.3 的 master 可以与 v1.1，v1.2，v1.3 的 slave 组件一起使用，与 v1.2，v1.3，v1.4 client 一起使用。</p>\n<p>此外，我们希望一次“支持”三个次版本，“支持”意味着我们希望用户在生产环境中运行该版本，虽然我们可能对于不在支持的版本进行 bugfix。例如，当 v1.3 发布时，将不再支持 v1.0。此外新版本每三个月发行一次，也就是说一个版本仅支持 9 个月。</p>\n<h3 id=\"升级策略\"><a href=\"#升级策略\" class=\"headerlink\" title=\"升级策略\"></a>升级策略</h3><p>用户可以使用滚动方式升级，一次升级一个小版本，不建议直接跨度两个及以上小版本，升级时先升级 master 再升级 node 节点。</p>\n<p>以下是在实际升级过程中的一些经验：</p>\n<p>金丝雀部署：即灰度升级，若使用二进制部署，则在原有集群直接替换二进制进行升级，运维代价小，不会导致服务中断；若以 pod 方式部署的 master 组件直接替换镜像进行升级，若以 deployment 方式部署 master 组件，对于 apiserver 可以参考阿里的经验，设置 maxSurge=3 的方式升级，以避免升级过程带来的性能抖动，但所有的 node 组件依然需要替换二进制升级。</p>\n<p>蓝绿部署：搭建一套新的集群，这种方式升级方式比较麻烦，涉及到数据迁移，IP 更换操作，对于部分业务不适用，风险不可控。</p>\n<p>可以看到，kubernetes 社区的更新速度非常快，坚决不建议自己维护一套 kubernetes 版本，每次升级巨麻烦，将所有修改过的 commit cherry-pick 到每个新版本上，也容易出错，有些新版本的改动也比较大，之前修改过的地方在新版中有可能已经被移除或放在别的位置了。</p>\n<p>详细的升级策略可以参考：<a href=\"https://www.cnblogs.com/gaorong/p/11266629.html\" target=\"_blank\" rel=\"noopener\">kubernetes集群升级的正确姿势</a>。</p>\n<h3 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h3><p>kubernetes 每三个月发布一个版本，社区仅维护最新的三个版本，一个版本的维护时间为 9 个月，请尽量保持生产环境的版本在社区维护范围内，版本升级时尽量保持小版本滚动升级，不建议跨多个版本升级。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</a><br><a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>kubernetes  社区每三个月发布一个新版本，可以说发布新版本的速度非常快，当然，在生产环境中版本升级的速度可能跟不上新版本发布的速度，那么确保目前使用的版本还处于社区的维护阶段就非常重要了，kubernetes 官方对各个版本支持的时间是多长呢？</p>\n<p>kubernetes 发行版通常支持9个月，在此期间，如果发现严重的bug或安全问题，会在对应的分支发布补丁版本。比如，当前版本为 v1.10.1，当社区修复一些 bug 后，就会发布 v1.10.2 版本。</p>\n<p>官方支持时间说明如下：</p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Kubernetes version</th>\n<th style=\"text-align:center\">Release month</th>\n<th style=\"text-align:left\">End-of-life-month</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">v1.6.x</td>\n<td style=\"text-align:center\">March 2017</td>\n<td style=\"text-align:left\">December 2017</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.7.x</td>\n<td style=\"text-align:center\">June 2017</td>\n<td style=\"text-align:left\">March 2018</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.8.x</td>\n<td style=\"text-align:center\">September 2017</td>\n<td style=\"text-align:left\">June 2018</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.9.x</td>\n<td style=\"text-align:center\">December 2017</td>\n<td style=\"text-align:left\">September 2018</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.10.x</td>\n<td style=\"text-align:center\">March 2018</td>\n<td style=\"text-align:left\">December 2018</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.11.x</td>\n<td style=\"text-align:center\">June 2018</td>\n<td style=\"text-align:left\">March 2019</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.12.x</td>\n<td style=\"text-align:center\">September 2018</td>\n<td style=\"text-align:left\">June 2019</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.13.x</td>\n<td style=\"text-align:center\">December 2018</td>\n<td style=\"text-align:left\">September 2019</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.14.x</td>\n<td style=\"text-align:center\">March 2019</td>\n<td style=\"text-align:left\">December 2019</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.15.x</td>\n<td style=\"text-align:center\">June 2019</td>\n<td style=\"text-align:left\">March 2020</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">v1.16.x</td>\n<td style=\"text-align:center\">September 2019</td>\n<td style=\"text-align:left\">June 2020</td>\n</tr>\n</tbody>\n</table>\n<p>到目前为止，v1.13.x 以前的版本已经停止支持了，请尽快升级至高版本。</p>\n<h3 id=\"kubernetes-版本发布流程\"><a href=\"#kubernetes-版本发布流程\" class=\"headerlink\" title=\"kubernetes 版本发布流程\"></a>kubernetes 版本发布流程</h3><blockquote>\n<p> 翻译自官方文档：<a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md\" target=\"_blank\" rel=\"noopener\">Kubernetes Release Versioning</a></p>\n</blockquote>\n<p> <strong>说明</strong>：<strong>Kube X.Y.Z</strong> 代表 kubernetes 已经发布的版本（git tag），这个版本包含所有的组件：apiserver, kubelet, kubectl, etc. (<strong>X</strong> 表示主版本号, <strong>Y</strong> 是此版本号, <strong>Z</strong> 是补丁版本。)</p>\n<h4 id=\"版本发布时间\"><a href=\"#版本发布时间\" class=\"headerlink\" title=\"版本发布时间\"></a>版本发布时间</h4><h5 id=\"次版本发布计划与时间表\"><a href=\"#次版本发布计划与时间表\" class=\"headerlink\" title=\"次版本发布计划与时间表\"></a>次版本发布计划与时间表</h5><ul>\n<li><p>Kube X.Y.0-alpha.W, W &gt; 0 ( 分支：master)</p>\n<ul>\n<li>Alpha 版本大约每两周直接从 master 分支发布一次。</li>\n<li>没有 cherrypick 版本。如有有严重的 bug 被修复，可以基于 master 分支提前创建一个新版本。</li>\n</ul>\n</li>\n<li><p>Kube X.Y.Z-beta.W (分支: release-X.Y)</p>\n<ul>\n<li><p>当 master 完成 Kube X.Y 的功能后，在距 X.Y.0 发布前两周会停掉 release-X.Y 分支，只将一些比较重要的 PR cherry-pick 到 X.Y。</p>\n</li>\n<li><p>该分支会被标记为  X.Y.0-beta.0，master 分支会被移到  X.Y+1.0-alpha.0。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/image-20190925195015864.png\" alt=\"\"></p>\n</li>\n<li><p>如果 X.Y.0-beta.0 的功能有缺陷，还会发布其他的 beta 版本 (X.Y.0-beta.W | W &gt; 0) 。</p>\n</li>\n</ul>\n</li>\n<li><p>Kube X.Y.0 (分支: release-X.Y)</p>\n<ul>\n<li>最终的 release 版本会提前两周从 release-X.Y 分支上产生。</li>\n<li>在同一分支的同一 commit  处也会被标记为 X.Y.1-beta.0。</li>\n<li>在 X.Y.0 发布 3-4 个月后会发布 X.(Y-1).0。</li>\n</ul>\n</li>\n<li><p>Kube X.Y.Z, Z &gt; 0 (分支: release-X.Y)</p>\n<ul>\n<li>当 cherrypick commits 到 release-X.Y 分支时，若有需要，也会发布相应的<a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#patch-releases\" target=\"_blank\" rel=\"noopener\">补丁版本</a> （X.Y.Z-beta.W）。</li>\n<li>X.Y.Z 是直接从 release-X.Y 分支上产生的，当使用 beta 版本在更新 pkg/version/base.go 后会被标记为 X.Y.Z+1-beta.0。</li>\n</ul>\n</li>\n<li><p>Kube X.Y.Z, Z &gt; 0 (分支: release-X.Y.Z)</p>\n<ul>\n<li><p>这是一个特殊的 tag，如果在上一个 release 分支后有重大的 bug 被修复，会有一个 X.Y.Z tag。</p>\n</li>\n<li><p>release-X.Y.Z 分支会被停掉以确保补丁版本是最新的。</p>\n</li>\n<li><p>如果还有重要 bug 被修复会再有一个补丁版本  X.Y.(Z+1)。</p>\n</li>\n<li><p>一般不会有<a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#patch-releases\" target=\"_blank\" rel=\"noopener\">补丁版本</a>，补丁版本仅用于一些重大 bug 的修复。</p>\n</li>\n<li><p>可以参考<a href=\"https://issues.k8s.io/19849\" target=\"_blank\" rel=\"noopener\">#19849</a>看看补丁版本的作用。</p>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"主版本时间线\"><a href=\"#主版本时间线\" class=\"headerlink\" title=\"主版本时间线\"></a>主版本时间线</h4><p>主版本暂时没有预期发布的时间点，也没有公布 2.0.0 的标准。到目前为止，我们还没有对任何类型的不兼容更改(例如，组件参数更改)。之前讨论过在发布 2.0.0 后 删除 <code>v1</code> API group/version，但目前没有这样做的计划。</p>\n<h4 id=\"支持的组件版本与兼容版本\"><a href=\"#支持的组件版本与兼容版本\" class=\"headerlink\" title=\"支持的组件版本与兼容版本\"></a>支持的组件版本与兼容版本</h4><p>我们希望用户在生产中使用 kubernetes 最稳定的版本，但升级版本需要一些时间，尤其是对于生产环境中的关键组件。我们也希望用户更新到最新的补丁版本，补丁版本中包含一些重要的 bugfix，希望用户尽快升级。</p>\n<p>kubernetes 对各组件的版本也有一定的兼容性。具体的兼容策略是： slave组件可以与master组件最多延迟两个版本(minor version)，但是不能比 master 组件新。client 不能与 master 组件落后一个次版本，但是可以高一个版本，也就是说： v1.3 的 master 可以与 v1.1，v1.2，v1.3 的 slave 组件一起使用，与 v1.2，v1.3，v1.4 client 一起使用。</p>\n<p>此外，我们希望一次“支持”三个次版本，“支持”意味着我们希望用户在生产环境中运行该版本，虽然我们可能对于不在支持的版本进行 bugfix。例如，当 v1.3 发布时，将不再支持 v1.0。此外新版本每三个月发行一次，也就是说一个版本仅支持 9 个月。</p>\n<h3 id=\"升级策略\"><a href=\"#升级策略\" class=\"headerlink\" title=\"升级策略\"></a>升级策略</h3><p>用户可以使用滚动方式升级，一次升级一个小版本，不建议直接跨度两个及以上小版本，升级时先升级 master 再升级 node 节点。</p>\n<p>以下是在实际升级过程中的一些经验：</p>\n<p>金丝雀部署：即灰度升级，若使用二进制部署，则在原有集群直接替换二进制进行升级，运维代价小，不会导致服务中断；若以 pod 方式部署的 master 组件直接替换镜像进行升级，若以 deployment 方式部署 master 组件，对于 apiserver 可以参考阿里的经验，设置 maxSurge=3 的方式升级，以避免升级过程带来的性能抖动，但所有的 node 组件依然需要替换二进制升级。</p>\n<p>蓝绿部署：搭建一套新的集群，这种方式升级方式比较麻烦，涉及到数据迁移，IP 更换操作，对于部分业务不适用，风险不可控。</p>\n<p>可以看到，kubernetes 社区的更新速度非常快，坚决不建议自己维护一套 kubernetes 版本，每次升级巨麻烦，将所有修改过的 commit cherry-pick 到每个新版本上，也容易出错，有些新版本的改动也比较大，之前修改过的地方在新版中有可能已经被移除或放在别的位置了。</p>\n<p>详细的升级策略可以参考：<a href=\"https://www.cnblogs.com/gaorong/p/11266629.html\" target=\"_blank\" rel=\"noopener\">kubernetes集群升级的正确姿势</a>。</p>\n<h3 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h3><p>kubernetes 每三个月发布一个版本，社区仅维护最新的三个版本，一个版本的维护时间为 9 个月，请尽量保持生产环境的版本在社区维护范围内，版本升级时尽量保持小版本滚动升级，不建议跨多个版本升级。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</a><br><a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md</a></p>\n"},{"title":"kube-on-kube-operator 开发(二)","date":"2019-08-07T09:47:30.000Z","type":"kubernetes-operator","_content":"\n本文主要讲述 kubernetes-operator 的开发过程，kubernetes-operator 已经开发了一个多月，其核心功能已经实现，其中的架构以及功能设计主要来自于一些生产环境的经验以及自己从事 kubernetes 运维开发两年多的一些工作经验，如有问题望指正。\n\n\n### kubernetes-operator 组件介绍\n\nkubernetes-operator 中主要包含一个自定义的 controller 和一个 HTTP Server，如下图所示，controller 主要是监听 CRD 的变化以及使其达到终态，HTTP Server 提供了多个 RESTful API，用于操作 CRD(创建、删除、扩缩容、接收回调等)。\n\n![](http://cdn.tianfeiyu.com/operator-2.png)\n\n\n\n除此之外还有其他的组件，ansibleinit、precheck、admission-webhook，ansibleinit 是一个二进制文件用来作为容器内的 1 号进程，会调用 ansible 相关的命令以及处理信号、子进程收割等。precheck 主要用于在对集群操作前检查目标宿主机的环境，由于对集群的操作需要耗费数十秒，为了保证成功率需要在部署前检查宿主的环境。admission-webhook 暂时用于校验 CR 中字段，比如集群执行扩容操作时，master 等字段的值肯定是不能改变的。\n\n\n### kubernetes-operator 的开发\n\n下面主要讲 kubernetes-operator 中核心组件的开发，主要有以下几步：\n\n- 定义 CRD\n- 生成代码\n- 开发 controller\n- 开发 RESTful API\n\n#### 定义 CRD\n\n下面是 CRD 的定义，kubernetes-operator 中的自定义资源为 `KubernetesCluster`，项目中简称为 `ecs`。\n\n```\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kubernetesclusters.ecs.yun.com\nspec:\n  group: ecs.yun.com\n  names:\n    kind: KubernetesCluster\n    listKind: KubernetesClusterList\n    plural: kubernetesclusters\n    singular: kubernetescluster\n    shortNames:\n    - ecs\n  scope: Namespaced\n  subresources:\n    status: {}\n  version: v1\n  versions:\n  - name: v1\n    served: true\n    storage: true\n```\n\n将 CRD 部署到 kubernetes 集群中，CRD 中的自定义资源`KubernetesCluster`(CR) 就成为了 kubernetes 中的一种资源，和 pod、deployment 等类似。\n\n\n#### 生成代码\n\n生成代码可以参考上一篇文章[使用 code-generator 为 CustomResources 生成代码](http://blog.tianfeiyu.com/2019/08/06/code_generator/)，此处不再详解。\n\n\n\n#### 开发 controller\n\n如下所示是 controller 最简单的一个声明：\n\n```\nfor {\n  desired := getDesiredState()\n  current := getCurrentState()\n  makeChanges(desired, current)\n}\n```\n\n所有 controller 也都是以此进行演变的，controller 的代码模式或者套路可以参考[sample-controller](https://github.com/kubernetes/sample-controller) 或者 kube-controller-manager 中所有 [controller](https://github.com/kubernetes/kubernetes/tree/master/pkg/controller) 的实现。\n\n\n\n下面是 kubernetes-operator 中 controller 实现的一个流程图：\n\n![](http://cdn.tianfeiyu.com/operator-3.png)\n\n\n更新 CR 都是客户端的操作，所以在设计时客户端都是操作 annotation 中的字段，然后 operator 监听到相关的时间后会进行处理。例如，当用户要创建一个集群时，首先客户端将 `app.kubernetes.io/operation`设置为 **creating**，此时 operator watch 到 CR 变化后会处理新建集群的操作，operator 会创建一个用来部署集群的 job，以及创建 configmap 来保存本次的操作记录以及关联对应的 job，也能用来查询本次操作的日志，然后会更新 CR 中 status.phase 中的 **Creating** (新创建的 CR status.phase 为 \"\")，接下来为 CR 设置 finalizers，最后会启动一个 goroutine 检测 job 的状态。此时需要等待 job 的完成以及回调，若 job 失败或者超时都会被最后启动的 goroutine 检测到，job 成功与否都会触发更新 CR status.phase 的操作。若 job 执行完成成功回调，客户端会更新  `app.kubernetes.io/operation `为 **create-finished**，客户端更新完成后会触发一次事件，然后 operator 会将 status.phase 更新为 **Running** 状态，否则 job 异常 operator 会直接更新 status.phase 为 **Failed**。\n\n\n关于 CR 中  `app.kubernetes.io/operation` 字段以及 status.phase 中所有的定义请参见 [kubernetes-operator/pkg/enum/task.go](https://github.com/gosoon/kubernetes-operator/blob/master/pkg/enum/task.go)。\n\n\n#### 开发 RESTful API\n\n在前后端分离的场景中，RESTful API 的开发仅需要一个 route 框架即可，kubernetes-operator 中用的是  mux，具体的代码在 [kubernetes-operator/pkg/server](https://github.com/gosoon/kubernetes-operator/tree/master/pkg/server) 下。\n\n\n### 总结\n\n本文主要讲述了 kubernetes-operator 中主要的模块以及 controller 的具体实现，其中许多细节暂未提及到，详细的实现请参考代码，该项目只是笔者利用业余时间进行开发的，毕竟个人精力有限，笔者当前主要集中在 `kube-on-kube` 的开发上，在阅读文章或者代码的过程中如有问题可以随时留言，笔者会持续迭代版本。下一篇文章会讲述如何使用二进制文件部署 kubernetes 集群。\n\n\n参考：\n\nhttps://github.com/kubernetes/community/blob/8decfe4/contributors/devel/controllers.md  \n\nhttps://github.com/kubernetes/sample-controller  \n\nhttps://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html  \n\nhttps://www.cnblogs.com/gaorong/p/8854934.html\n\nhttps://yucs.github.io/2017/12/21/2017-12-21-operator/\n","source":"_posts/kube_on_kube_operator_2.md","raw":"---\ntitle: kube-on-kube-operator 开发(二)\ndate: 2019-08-07 17:47:30\ntags: [\"operator\",\"kube-on-kube\"]\ntype: \"kubernetes-operator\"\n\n---\n\n本文主要讲述 kubernetes-operator 的开发过程，kubernetes-operator 已经开发了一个多月，其核心功能已经实现，其中的架构以及功能设计主要来自于一些生产环境的经验以及自己从事 kubernetes 运维开发两年多的一些工作经验，如有问题望指正。\n\n\n### kubernetes-operator 组件介绍\n\nkubernetes-operator 中主要包含一个自定义的 controller 和一个 HTTP Server，如下图所示，controller 主要是监听 CRD 的变化以及使其达到终态，HTTP Server 提供了多个 RESTful API，用于操作 CRD(创建、删除、扩缩容、接收回调等)。\n\n![](http://cdn.tianfeiyu.com/operator-2.png)\n\n\n\n除此之外还有其他的组件，ansibleinit、precheck、admission-webhook，ansibleinit 是一个二进制文件用来作为容器内的 1 号进程，会调用 ansible 相关的命令以及处理信号、子进程收割等。precheck 主要用于在对集群操作前检查目标宿主机的环境，由于对集群的操作需要耗费数十秒，为了保证成功率需要在部署前检查宿主的环境。admission-webhook 暂时用于校验 CR 中字段，比如集群执行扩容操作时，master 等字段的值肯定是不能改变的。\n\n\n### kubernetes-operator 的开发\n\n下面主要讲 kubernetes-operator 中核心组件的开发，主要有以下几步：\n\n- 定义 CRD\n- 生成代码\n- 开发 controller\n- 开发 RESTful API\n\n#### 定义 CRD\n\n下面是 CRD 的定义，kubernetes-operator 中的自定义资源为 `KubernetesCluster`，项目中简称为 `ecs`。\n\n```\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: kubernetesclusters.ecs.yun.com\nspec:\n  group: ecs.yun.com\n  names:\n    kind: KubernetesCluster\n    listKind: KubernetesClusterList\n    plural: kubernetesclusters\n    singular: kubernetescluster\n    shortNames:\n    - ecs\n  scope: Namespaced\n  subresources:\n    status: {}\n  version: v1\n  versions:\n  - name: v1\n    served: true\n    storage: true\n```\n\n将 CRD 部署到 kubernetes 集群中，CRD 中的自定义资源`KubernetesCluster`(CR) 就成为了 kubernetes 中的一种资源，和 pod、deployment 等类似。\n\n\n#### 生成代码\n\n生成代码可以参考上一篇文章[使用 code-generator 为 CustomResources 生成代码](http://blog.tianfeiyu.com/2019/08/06/code_generator/)，此处不再详解。\n\n\n\n#### 开发 controller\n\n如下所示是 controller 最简单的一个声明：\n\n```\nfor {\n  desired := getDesiredState()\n  current := getCurrentState()\n  makeChanges(desired, current)\n}\n```\n\n所有 controller 也都是以此进行演变的，controller 的代码模式或者套路可以参考[sample-controller](https://github.com/kubernetes/sample-controller) 或者 kube-controller-manager 中所有 [controller](https://github.com/kubernetes/kubernetes/tree/master/pkg/controller) 的实现。\n\n\n\n下面是 kubernetes-operator 中 controller 实现的一个流程图：\n\n![](http://cdn.tianfeiyu.com/operator-3.png)\n\n\n更新 CR 都是客户端的操作，所以在设计时客户端都是操作 annotation 中的字段，然后 operator 监听到相关的时间后会进行处理。例如，当用户要创建一个集群时，首先客户端将 `app.kubernetes.io/operation`设置为 **creating**，此时 operator watch 到 CR 变化后会处理新建集群的操作，operator 会创建一个用来部署集群的 job，以及创建 configmap 来保存本次的操作记录以及关联对应的 job，也能用来查询本次操作的日志，然后会更新 CR 中 status.phase 中的 **Creating** (新创建的 CR status.phase 为 \"\")，接下来为 CR 设置 finalizers，最后会启动一个 goroutine 检测 job 的状态。此时需要等待 job 的完成以及回调，若 job 失败或者超时都会被最后启动的 goroutine 检测到，job 成功与否都会触发更新 CR status.phase 的操作。若 job 执行完成成功回调，客户端会更新  `app.kubernetes.io/operation `为 **create-finished**，客户端更新完成后会触发一次事件，然后 operator 会将 status.phase 更新为 **Running** 状态，否则 job 异常 operator 会直接更新 status.phase 为 **Failed**。\n\n\n关于 CR 中  `app.kubernetes.io/operation` 字段以及 status.phase 中所有的定义请参见 [kubernetes-operator/pkg/enum/task.go](https://github.com/gosoon/kubernetes-operator/blob/master/pkg/enum/task.go)。\n\n\n#### 开发 RESTful API\n\n在前后端分离的场景中，RESTful API 的开发仅需要一个 route 框架即可，kubernetes-operator 中用的是  mux，具体的代码在 [kubernetes-operator/pkg/server](https://github.com/gosoon/kubernetes-operator/tree/master/pkg/server) 下。\n\n\n### 总结\n\n本文主要讲述了 kubernetes-operator 中主要的模块以及 controller 的具体实现，其中许多细节暂未提及到，详细的实现请参考代码，该项目只是笔者利用业余时间进行开发的，毕竟个人精力有限，笔者当前主要集中在 `kube-on-kube` 的开发上，在阅读文章或者代码的过程中如有问题可以随时留言，笔者会持续迭代版本。下一篇文章会讲述如何使用二进制文件部署 kubernetes 集群。\n\n\n参考：\n\nhttps://github.com/kubernetes/community/blob/8decfe4/contributors/devel/controllers.md  \n\nhttps://github.com/kubernetes/sample-controller  \n\nhttps://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html  \n\nhttps://www.cnblogs.com/gaorong/p/8854934.html\n\nhttps://yucs.github.io/2017/12/21/2017-12-21-operator/\n","slug":"kube_on_kube_operator_2","published":1,"updated":"2019-08-08T02:11:09.250Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59o0012apwn3j4u0rp8","content":"<p>本文主要讲述 kubernetes-operator 的开发过程，kubernetes-operator 已经开发了一个多月，其核心功能已经实现，其中的架构以及功能设计主要来自于一些生产环境的经验以及自己从事 kubernetes 运维开发两年多的一些工作经验，如有问题望指正。</p>\n<h3 id=\"kubernetes-operator-组件介绍\"><a href=\"#kubernetes-operator-组件介绍\" class=\"headerlink\" title=\"kubernetes-operator 组件介绍\"></a>kubernetes-operator 组件介绍</h3><p>kubernetes-operator 中主要包含一个自定义的 controller 和一个 HTTP Server，如下图所示，controller 主要是监听 CRD 的变化以及使其达到终态，HTTP Server 提供了多个 RESTful API，用于操作 CRD(创建、删除、扩缩容、接收回调等)。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/operator-2.png\" alt=\"\"></p>\n<p>除此之外还有其他的组件，ansibleinit、precheck、admission-webhook，ansibleinit 是一个二进制文件用来作为容器内的 1 号进程，会调用 ansible 相关的命令以及处理信号、子进程收割等。precheck 主要用于在对集群操作前检查目标宿主机的环境，由于对集群的操作需要耗费数十秒，为了保证成功率需要在部署前检查宿主的环境。admission-webhook 暂时用于校验 CR 中字段，比如集群执行扩容操作时，master 等字段的值肯定是不能改变的。</p>\n<h3 id=\"kubernetes-operator-的开发\"><a href=\"#kubernetes-operator-的开发\" class=\"headerlink\" title=\"kubernetes-operator 的开发\"></a>kubernetes-operator 的开发</h3><p>下面主要讲 kubernetes-operator 中核心组件的开发，主要有以下几步：</p>\n<ul>\n<li>定义 CRD</li>\n<li>生成代码</li>\n<li>开发 controller</li>\n<li>开发 RESTful API</li>\n</ul>\n<h4 id=\"定义-CRD\"><a href=\"#定义-CRD\" class=\"headerlink\" title=\"定义 CRD\"></a>定义 CRD</h4><p>下面是 CRD 的定义，kubernetes-operator 中的自定义资源为 <code>KubernetesCluster</code>，项目中简称为 <code>ecs</code>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class=\"line\">kind: CustomResourceDefinition</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetesclusters.ecs.yun.com</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  group: ecs.yun.com</span><br><span class=\"line\">  names:</span><br><span class=\"line\">    kind: KubernetesCluster</span><br><span class=\"line\">    listKind: KubernetesClusterList</span><br><span class=\"line\">    plural: kubernetesclusters</span><br><span class=\"line\">    singular: kubernetescluster</span><br><span class=\"line\">    shortNames:</span><br><span class=\"line\">    - ecs</span><br><span class=\"line\">  scope: Namespaced</span><br><span class=\"line\">  subresources:</span><br><span class=\"line\">    status: &#123;&#125;</span><br><span class=\"line\">  version: v1</span><br><span class=\"line\">  versions:</span><br><span class=\"line\">  - name: v1</span><br><span class=\"line\">    served: true</span><br><span class=\"line\">    storage: true</span><br></pre></td></tr></table></figure>\n<p>将 CRD 部署到 kubernetes 集群中，CRD 中的自定义资源<code>KubernetesCluster</code>(CR) 就成为了 kubernetes 中的一种资源，和 pod、deployment 等类似。</p>\n<h4 id=\"生成代码\"><a href=\"#生成代码\" class=\"headerlink\" title=\"生成代码\"></a>生成代码</h4><p>生成代码可以参考上一篇文章<a href=\"http://blog.tianfeiyu.com/2019/08/06/code_generator/\" target=\"_blank\" rel=\"noopener\">使用 code-generator 为 CustomResources 生成代码</a>，此处不再详解。</p>\n<h4 id=\"开发-controller\"><a href=\"#开发-controller\" class=\"headerlink\" title=\"开发 controller\"></a>开发 controller</h4><p>如下所示是 controller 最简单的一个声明：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for &#123;</span><br><span class=\"line\">  desired := getDesiredState()</span><br><span class=\"line\">  current := getCurrentState()</span><br><span class=\"line\">  makeChanges(desired, current)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>所有 controller 也都是以此进行演变的，controller 的代码模式或者套路可以参考<a href=\"https://github.com/kubernetes/sample-controller\" target=\"_blank\" rel=\"noopener\">sample-controller</a> 或者 kube-controller-manager 中所有 <a href=\"https://github.com/kubernetes/kubernetes/tree/master/pkg/controller\" target=\"_blank\" rel=\"noopener\">controller</a> 的实现。</p>\n<p>下面是 kubernetes-operator 中 controller 实现的一个流程图：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/operator-3.png\" alt=\"\"></p>\n<p>更新 CR 都是客户端的操作，所以在设计时客户端都是操作 annotation 中的字段，然后 operator 监听到相关的时间后会进行处理。例如，当用户要创建一个集群时，首先客户端将 <code>app.kubernetes.io/operation</code>设置为 <strong>creating</strong>，此时 operator watch 到 CR 变化后会处理新建集群的操作，operator 会创建一个用来部署集群的 job，以及创建 configmap 来保存本次的操作记录以及关联对应的 job，也能用来查询本次操作的日志，然后会更新 CR 中 status.phase 中的 <strong>Creating</strong> (新创建的 CR status.phase 为 “”)，接下来为 CR 设置 finalizers，最后会启动一个 goroutine 检测 job 的状态。此时需要等待 job 的完成以及回调，若 job 失败或者超时都会被最后启动的 goroutine 检测到，job 成功与否都会触发更新 CR status.phase 的操作。若 job 执行完成成功回调，客户端会更新  <code>app.kubernetes.io/operation</code>为 <strong>create-finished</strong>，客户端更新完成后会触发一次事件，然后 operator 会将 status.phase 更新为 <strong>Running</strong> 状态，否则 job 异常 operator 会直接更新 status.phase 为 <strong>Failed</strong>。</p>\n<p>关于 CR 中  <code>app.kubernetes.io/operation</code> 字段以及 status.phase 中所有的定义请参见 <a href=\"https://github.com/gosoon/kubernetes-operator/blob/master/pkg/enum/task.go\" target=\"_blank\" rel=\"noopener\">kubernetes-operator/pkg/enum/task.go</a>。</p>\n<h4 id=\"开发-RESTful-API\"><a href=\"#开发-RESTful-API\" class=\"headerlink\" title=\"开发 RESTful API\"></a>开发 RESTful API</h4><p>在前后端分离的场景中，RESTful API 的开发仅需要一个 route 框架即可，kubernetes-operator 中用的是  mux，具体的代码在 <a href=\"https://github.com/gosoon/kubernetes-operator/tree/master/pkg/server\" target=\"_blank\" rel=\"noopener\">kubernetes-operator/pkg/server</a> 下。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要讲述了 kubernetes-operator 中主要的模块以及 controller 的具体实现，其中许多细节暂未提及到，详细的实现请参考代码，该项目只是笔者利用业余时间进行开发的，毕竟个人精力有限，笔者当前主要集中在 <code>kube-on-kube</code> 的开发上，在阅读文章或者代码的过程中如有问题可以随时留言，笔者会持续迭代版本。下一篇文章会讲述如何使用二进制文件部署 kubernetes 集群。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes/community/blob/8decfe4/contributors/devel/controllers.md\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/community/blob/8decfe4/contributors/devel/controllers.md</a>  </p>\n<p><a href=\"https://github.com/kubernetes/sample-controller\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/sample-controller</a>  </p>\n<p><a href=\"https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html\" target=\"_blank\" rel=\"noopener\">https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html</a>  </p>\n<p><a href=\"https://www.cnblogs.com/gaorong/p/8854934.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/gaorong/p/8854934.html</a></p>\n<p><a href=\"https://yucs.github.io/2017/12/21/2017-12-21-operator/\" target=\"_blank\" rel=\"noopener\">https://yucs.github.io/2017/12/21/2017-12-21-operator/</a></p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/\">kube-on-kube-operator 开发(三)</a></li><li><a href=\"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/\">kube-on-kube-operator 开发(一)</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>本文主要讲述 kubernetes-operator 的开发过程，kubernetes-operator 已经开发了一个多月，其核心功能已经实现，其中的架构以及功能设计主要来自于一些生产环境的经验以及自己从事 kubernetes 运维开发两年多的一些工作经验，如有问题望指正。</p>\n<h3 id=\"kubernetes-operator-组件介绍\"><a href=\"#kubernetes-operator-组件介绍\" class=\"headerlink\" title=\"kubernetes-operator 组件介绍\"></a>kubernetes-operator 组件介绍</h3><p>kubernetes-operator 中主要包含一个自定义的 controller 和一个 HTTP Server，如下图所示，controller 主要是监听 CRD 的变化以及使其达到终态，HTTP Server 提供了多个 RESTful API，用于操作 CRD(创建、删除、扩缩容、接收回调等)。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/operator-2.png\" alt=\"\"></p>\n<p>除此之外还有其他的组件，ansibleinit、precheck、admission-webhook，ansibleinit 是一个二进制文件用来作为容器内的 1 号进程，会调用 ansible 相关的命令以及处理信号、子进程收割等。precheck 主要用于在对集群操作前检查目标宿主机的环境，由于对集群的操作需要耗费数十秒，为了保证成功率需要在部署前检查宿主的环境。admission-webhook 暂时用于校验 CR 中字段，比如集群执行扩容操作时，master 等字段的值肯定是不能改变的。</p>\n<h3 id=\"kubernetes-operator-的开发\"><a href=\"#kubernetes-operator-的开发\" class=\"headerlink\" title=\"kubernetes-operator 的开发\"></a>kubernetes-operator 的开发</h3><p>下面主要讲 kubernetes-operator 中核心组件的开发，主要有以下几步：</p>\n<ul>\n<li>定义 CRD</li>\n<li>生成代码</li>\n<li>开发 controller</li>\n<li>开发 RESTful API</li>\n</ul>\n<h4 id=\"定义-CRD\"><a href=\"#定义-CRD\" class=\"headerlink\" title=\"定义 CRD\"></a>定义 CRD</h4><p>下面是 CRD 的定义，kubernetes-operator 中的自定义资源为 <code>KubernetesCluster</code>，项目中简称为 <code>ecs</code>。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class=\"line\">kind: CustomResourceDefinition</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: kubernetesclusters.ecs.yun.com</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  group: ecs.yun.com</span><br><span class=\"line\">  names:</span><br><span class=\"line\">    kind: KubernetesCluster</span><br><span class=\"line\">    listKind: KubernetesClusterList</span><br><span class=\"line\">    plural: kubernetesclusters</span><br><span class=\"line\">    singular: kubernetescluster</span><br><span class=\"line\">    shortNames:</span><br><span class=\"line\">    - ecs</span><br><span class=\"line\">  scope: Namespaced</span><br><span class=\"line\">  subresources:</span><br><span class=\"line\">    status: &#123;&#125;</span><br><span class=\"line\">  version: v1</span><br><span class=\"line\">  versions:</span><br><span class=\"line\">  - name: v1</span><br><span class=\"line\">    served: true</span><br><span class=\"line\">    storage: true</span><br></pre></td></tr></table></figure>\n<p>将 CRD 部署到 kubernetes 集群中，CRD 中的自定义资源<code>KubernetesCluster</code>(CR) 就成为了 kubernetes 中的一种资源，和 pod、deployment 等类似。</p>\n<h4 id=\"生成代码\"><a href=\"#生成代码\" class=\"headerlink\" title=\"生成代码\"></a>生成代码</h4><p>生成代码可以参考上一篇文章<a href=\"http://blog.tianfeiyu.com/2019/08/06/code_generator/\" target=\"_blank\" rel=\"noopener\">使用 code-generator 为 CustomResources 生成代码</a>，此处不再详解。</p>\n<h4 id=\"开发-controller\"><a href=\"#开发-controller\" class=\"headerlink\" title=\"开发 controller\"></a>开发 controller</h4><p>如下所示是 controller 最简单的一个声明：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">for &#123;</span><br><span class=\"line\">  desired := getDesiredState()</span><br><span class=\"line\">  current := getCurrentState()</span><br><span class=\"line\">  makeChanges(desired, current)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>所有 controller 也都是以此进行演变的，controller 的代码模式或者套路可以参考<a href=\"https://github.com/kubernetes/sample-controller\" target=\"_blank\" rel=\"noopener\">sample-controller</a> 或者 kube-controller-manager 中所有 <a href=\"https://github.com/kubernetes/kubernetes/tree/master/pkg/controller\" target=\"_blank\" rel=\"noopener\">controller</a> 的实现。</p>\n<p>下面是 kubernetes-operator 中 controller 实现的一个流程图：</p>\n<p><img src=\"http://cdn.tianfeiyu.com/operator-3.png\" alt=\"\"></p>\n<p>更新 CR 都是客户端的操作，所以在设计时客户端都是操作 annotation 中的字段，然后 operator 监听到相关的时间后会进行处理。例如，当用户要创建一个集群时，首先客户端将 <code>app.kubernetes.io/operation</code>设置为 <strong>creating</strong>，此时 operator watch 到 CR 变化后会处理新建集群的操作，operator 会创建一个用来部署集群的 job，以及创建 configmap 来保存本次的操作记录以及关联对应的 job，也能用来查询本次操作的日志，然后会更新 CR 中 status.phase 中的 <strong>Creating</strong> (新创建的 CR status.phase 为 “”)，接下来为 CR 设置 finalizers，最后会启动一个 goroutine 检测 job 的状态。此时需要等待 job 的完成以及回调，若 job 失败或者超时都会被最后启动的 goroutine 检测到，job 成功与否都会触发更新 CR status.phase 的操作。若 job 执行完成成功回调，客户端会更新  <code>app.kubernetes.io/operation</code>为 <strong>create-finished</strong>，客户端更新完成后会触发一次事件，然后 operator 会将 status.phase 更新为 <strong>Running</strong> 状态，否则 job 异常 operator 会直接更新 status.phase 为 <strong>Failed</strong>。</p>\n<p>关于 CR 中  <code>app.kubernetes.io/operation</code> 字段以及 status.phase 中所有的定义请参见 <a href=\"https://github.com/gosoon/kubernetes-operator/blob/master/pkg/enum/task.go\" target=\"_blank\" rel=\"noopener\">kubernetes-operator/pkg/enum/task.go</a>。</p>\n<h4 id=\"开发-RESTful-API\"><a href=\"#开发-RESTful-API\" class=\"headerlink\" title=\"开发 RESTful API\"></a>开发 RESTful API</h4><p>在前后端分离的场景中，RESTful API 的开发仅需要一个 route 框架即可，kubernetes-operator 中用的是  mux，具体的代码在 <a href=\"https://github.com/gosoon/kubernetes-operator/tree/master/pkg/server\" target=\"_blank\" rel=\"noopener\">kubernetes-operator/pkg/server</a> 下。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要讲述了 kubernetes-operator 中主要的模块以及 controller 的具体实现，其中许多细节暂未提及到，详细的实现请参考代码，该项目只是笔者利用业余时间进行开发的，毕竟个人精力有限，笔者当前主要集中在 <code>kube-on-kube</code> 的开发上，在阅读文章或者代码的过程中如有问题可以随时留言，笔者会持续迭代版本。下一篇文章会讲述如何使用二进制文件部署 kubernetes 集群。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes/community/blob/8decfe4/contributors/devel/controllers.md\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/community/blob/8decfe4/contributors/devel/controllers.md</a>  </p>\n<p><a href=\"https://github.com/kubernetes/sample-controller\" target=\"_blank\" rel=\"noopener\">https://github.com/kubernetes/sample-controller</a>  </p>\n<p><a href=\"https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html\" target=\"_blank\" rel=\"noopener\">https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html</a>  </p>\n<p><a href=\"https://www.cnblogs.com/gaorong/p/8854934.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/gaorong/p/8854934.html</a></p>\n<p><a href=\"https://yucs.github.io/2017/12/21/2017-12-21-operator/\" target=\"_blank\" rel=\"noopener\">https://yucs.github.io/2017/12/21/2017-12-21-operator/</a></p>\n"},{"title":"kube-scheduler 源码分析","date":"2019-10-21T09:30:30.000Z","type":"kube-scheduler","_content":"\n### kube-scheduler 的设计\n\nKube-scheduler 是 kubernetes 的核心组件之一，也是所有核心组件之间功能比较单一的，其代码也相对容易理解。kube-scheduler 的目的就是为每一个 pod 选择一个合适的 node，整体流程可以概括为三步，获取未调度的 podList，通过执行一系列调度算法为 pod 选择一个合适的 node，提交数据到 apiserver，其核心则是一系列调度算法的设计与执行。\n\n\n官方对 kube-scheduler 的调度流程描述 [The Kubernetes Scheduler](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md)：\n\n```\nFor given pod:\n\n    +---------------------------------------------+\n    |               Schedulable nodes:            |\n    |                                             |\n    | +--------+    +--------+      +--------+    |\n    | | node 1 |    | node 2 |      | node 3 |    |\n    | +--------+    +--------+      +--------+    |\n    |                                             |\n    +-------------------+-------------------------+\n                        |\n                        |\n                        v\n    +-------------------+-------------------------+\n\n    Pred. filters: node 3 doesn't have enough resource\n\n    +-------------------+-------------------------+\n                        |\n                        |\n                        v\n    +-------------------+-------------------------+\n    |             remaining nodes:                |\n    |   +--------+                 +--------+     |\n    |   | node 1 |                 | node 2 |     |\n    |   +--------+                 +--------+     |\n    |                                             |\n    +-------------------+-------------------------+\n                        |\n                        |\n                        v\n    +-------------------+-------------------------+\n\n    Priority function:    node 1: p=2\n                          node 2: p=5\n\n    +-------------------+-------------------------+\n                        |\n                        |\n                        v\n            select max{node priority} = node 2\n```\n\nkube-scheduler 目前包含两部分调度算法 predicates 和 priorities，首先执行 predicates 算法过滤部分 node 然后执行  priorities 算法为所有 node 打分，最后从所有 node 中选出分数最高的最为最佳的 node。 \n\n\n\n### kube-scheduler 源码分析\n\n> kubernetes 版本: v1.16\n\n\nkubernetes 中所有组件的启动流程都是类似的，首先会解析命令行参数、添加默认值，kube-scheduler 的默认参数在 `k8s.io/kubernetes/pkg/scheduler/apis/config/v1alpha1/defaults.go` 中定义的。然后会执行  run 方法启动主逻辑，下面直接看 kube-scheduler 的主逻辑 run 方法执行过程。\n\n\n`Run()` 方法主要做了以下工作：\n- 初始化 scheduler 对象\n- 启动 kube-scheduler server，kube-scheduler 监听 10251 和 10259 端口，10251 端口不需要认证，可以获取 healthz metrics 等信息，10259 为安全端口，需要认证\n- 启动所有的 informer\n- 执行 `sched.Run()` 方法，执行主调度逻辑\n\n`k8s.io/kubernetes/cmd/kube-scheduler/app/server.go:160`  \n\n```\nfunc Run(cc schedulerserverconfig.CompletedConfig, stopCh <-chan struct{}, registryOptions ...Option) error {\n    ......\n    // 1、初始化 scheduler 对象\n    sched, err := scheduler.New(......）\n    if err != nil {\n        return err\n    }\n\n    // 2、启动事件广播\n    if cc.Broadcaster != nil && cc.EventClient != nil {\n        cc.Broadcaster.StartRecordingToSink(stopCh)\n    }\n    if cc.LeaderElectionBroadcaster != nil && cc.CoreEventClient != nil {\n        cc.LeaderElectionBroadcaster.StartRecordingToSink(&corev1.EventSinkImpl{Interface: cc.CoreEventClient.Events(\"\")})\n    }\n\n    ......\n    // 3、启动 http server\n    if cc.InsecureServing != nil {\n        separateMetrics := cc.InsecureMetricsServing != nil\n        handler := buildHandlerChain(newHealthzHandler(&cc.ComponentConfig, separateMetrics, checks...), nil, nil)\n        if err := cc.InsecureServing.Serve(handler, 0, stopCh); err != nil {\n            return fmt.Errorf(\"failed to start healthz server: %v\", err)\n        }\n    }\n    ......\n    // 4、启动所有 informer\n    go cc.PodInformer.Informer().Run(stopCh)\n    cc.InformerFactory.Start(stopCh)\n\n    cc.InformerFactory.WaitForCacheSync(stopCh)\n\n    run := func(ctx context.Context) {\n        sched.Run()\n        <-ctx.Done()\n    }\n\n    ctx, cancel := context.WithCancel(context.TODO()) // TODO once Run() accepts a context, it should be used here\n    defer cancel()\n    go func() {\n        select {\n        case <-stopCh:\n            cancel()\n        case <-ctx.Done():\n        }\n    }()\n\n    // 5、选举 leader\n    if cc.LeaderElection != nil {\n        ......\n    }\n    // 6、执行 sched.Run() 方法\n    run(ctx)\n    return fmt.Errorf(\"finished without leader elect\")\n}\n```\n\n\n\n下面看一下 `scheduler.New()` 方法是如何初始化 scheduler 结构体的，该方法主要的功能是初始化默认的调度算法以及默认的调度器 GenericScheduler。\n\n- 创建 scheduler 配置文件\n- 根据默认的 DefaultProvider 初始化  schedulerAlgorithmSource 然后加载默认的预选及优选算法，然后初始化 GenericScheduler\n- 若启动参数提供了 policy config 则使用其覆盖默认的预选及优选算法并初始化 GenericScheduler，不过该参数现已被弃用\n\n`k8s.io/kubernetes/pkg/scheduler/scheduler.go:166`\n```\nfunc New(......) (*Scheduler, error) {\n\t......\n    // 1、创建 scheduler 的配置文件\n    configurator := factory.NewConfigFactory(&factory.ConfigFactoryArgs{\n  \t    ......\n  \t})\n    var config *factory.Config\n    source := schedulerAlgorithmSource\n    // 2、加载默认的调度算法\n    switch {\n    case source.Provider != nil:\n        // 使用默认的 ”DefaultProvider“ 初始化 config\n        sc, err := configurator.CreateFromProvider(*source.Provider)\n        if err != nil {\n            return nil, fmt.Errorf(\"couldn't create scheduler using provider %q: %v\", *source.Provider, err)\n        }\n        config = sc\n    case source.Policy != nil:\n        // 通过启动时指定的 policy source 加载 config\n\t......\n        config = sc\n    default:\n        return nil, fmt.Errorf(\"unsupported algorithm source: %v\", source)\n    }\n    // Additional tweaks to the config produced by the configurator.\n    config.Recorder = recorder\n    config.DisablePreemption = options.disablePreemption\n    config.StopEverything = stopCh\n\n    // 3.创建 scheduler 对象\n    sched := NewFromConfig(config)\n\t......\n    return sched, nil\n}\n```\n\n\n\n下面是 pod informer 的启动逻辑，只监听 status.phase 不为 succeeded 以及 failed 状态的 pod，即非 terminating 的 pod。\n\n`k8s.io/kubernetes/pkg/scheduler/factory/factory.go:527`\n````\nfunc NewPodInformer(client clientset.Interface, resyncPeriod time.Duration) coreinformers.PodInformer {\n    selector := fields.ParseSelectorOrDie(\n        \"status.phase!=\" + string(v1.PodSucceeded) +\n            \",status.phase!=\" + string(v1.PodFailed))\n    lw := cache.NewListWatchFromClient(client.CoreV1().RESTClient(), string(v1.ResourcePods), metav1.NamespaceAll, selector)\n    return &podInformer{\n        informer: cache.NewSharedIndexInformer(lw, &v1.Pod{}, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}),\n    }\n}\n````\n\n\n\n然后继续看 `Run()`  方法中最后执行的 `sched.Run()` 调度循环逻辑，若 informer 中的 cache 同步完成后会启动一个循环逻辑执行 `sched.scheduleOne` 方法。\n\n`k8s.io/kubernetes/pkg/scheduler/scheduler.go:313`   \n```\nfunc (sched *Scheduler) Run() {\n    if !sched.config.WaitForCacheSync() {\n        return\n    }\n\n    go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything)\n}\n```\n\n\n\n`scheduleOne()` 每次对一个 pod 进行调度，主要有以下步骤：\n\n- 从 scheduler 调度队列中取出一个 pod，如果该 pod 处于删除状态则跳过\n- 执行调度逻辑 `sched.schedule()` 返回通过预算及优选算法过滤后选出的最佳 node\n- 如果过滤算法没有选出合适的 node，则返回 core.FitError \n- 若没有合适的 node 会判断是否启用了抢占策略，若启用了则执行抢占机制\n- 判断是否需要 VolumeScheduling 特性\n- 执行 reserve plugin\n- pod 对应的 spec.NodeName 写上 scheduler 最终选择的 node，更新 scheduler cache\n- 请求 apiserver 异步处理最终的绑定操作，写入到 etcd\n- 执行 permit plugin\n- 执行 prebind plugin\n- 执行 postbind plugin\n\n`k8s.io/kubernetes/pkg/scheduler/scheduler.go:515`\n```\nfunc (sched *Scheduler) scheduleOne() {\n    fwk := sched.Framework\n\n    pod := sched.NextPod()\n    if pod == nil {\n        return\n    }\n    // 1.判断 pod 是否处于删除状态\n    if pod.DeletionTimestamp != nil {\n    \t......\n    }\n\t\t\n    // 2.执行调度策略选择 node\n    start := time.Now()\n    pluginContext := framework.NewPluginContext()\n    scheduleResult, err := sched.schedule(pod, pluginContext)\n    if err != nil {\n        if fitError, ok := err.(*core.FitError); ok {\n            // 3.若启用抢占机制则执行\n            if sched.DisablePreemption {\n            \t......\n            } else {\n                preemptionStartTime := time.Now()\n                sched.preempt(pluginContext, fwk, pod, fitError)\n                ......\n            }\n            ......\n            metrics.PodScheduleFailures.Inc()\n        } else {\n            klog.Errorf(\"error selecting node for pod: %v\", err)\n            metrics.PodScheduleErrors.Inc()\n        }\n        return\n    }\n    ......\n    assumedPod := pod.DeepCopy()\n\n    // 4.判断是否需要 VolumeScheduling 特性\n    allBound, err := sched.assumeVolumes(assumedPod, scheduleResult.SuggestedHost)\n    if err != nil {\n        klog.Errorf(\"error assuming volumes: %v\", err)\n        metrics.PodScheduleErrors.Inc()\n        return\n    }\n\n    // 5.执行 \"reserve\" plugins\n    if sts := fwk.RunReservePlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {\n        .....\n    }\n\n    // 6.为 pod 设置 NodeName 字段，更新 scheduler 缓存\n    err = sched.assume(assumedPod, scheduleResult.SuggestedHost)\n    if err != nil {\n        ......\n    }\n\n    // 7.异步请求 apiserver\n    go func() {\n        // Bind volumes first before Pod\n        if !allBound {\n            err := sched.bindVolumes(assumedPod)\n            if err != nil {\n\t\t\t\t......\n                return\n            }\n        }\n\n        // 8.执行 \"permit\" plugins\n        permitStatus := fwk.RunPermitPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)\n        if !permitStatus.IsSuccess() {\n\t\t\t......\n        }\n        // 9.执行 \"prebind\" plugins\n        preBindStatus := fwk.RunPreBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)\n        if !preBindStatus.IsSuccess() {\n            ......\n        }\n        err := sched.bind(assumedPod, scheduleResult.SuggestedHost, pluginContext)\n        ......\n        if err != nil {\n            ......\n        } else {\n            ......\n            // 10.执行 \"postbind\" plugins\n            fwk.RunPostBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)\n        }\n    }()\n}\n```\n\n\n\n`scheduleOne()`  中通过调用 `sched.schedule()` 来执行预选与优选算法处理：\n\n`k8s.io/kubernetes/pkg/scheduler/scheduler.go:337`\n```\nfunc (sched *Scheduler) schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (core.ScheduleResult, error) {\n    result, err := sched.Algorithm.Schedule(pod, pluginContext)\n    if err != nil {\n\t......\n    }\n    return result, err\n}\n```\n\n\n\n`sched.Algorithm` 是一个 interface，主要包含四个方法，GenericScheduler 是其具体的实现：\n\n`k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:131`\n```\ntype ScheduleAlgorithm interface {\n    Schedule(*v1.Pod, *framework.PluginContext) (scheduleResult ScheduleResult, err error)\n    Preempt(*framework.PluginContext, *v1.Pod, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error)\n    Predicates() map[string]predicates.FitPredicate\n    Prioritizers() []priorities.PriorityConfig\n}\n```\n\n- `Schedule()`：正常调度逻辑，包含预算与优选算法的执行\n- `Preempt()`：抢占策略，在 pod 调度发生失败的时候尝试抢占低优先级的 pod，函数返回发生抢占的 node，被 抢占的 pods 列表，nominated node name 需要被移除的 pods 列表以及 error\n- `Predicates()`：predicates 算法列表\n- `Prioritizers()`：prioritizers 算法列表\n\n\n\nkube-scheduler 提供的默认调度为 DefaultProvider，DefaultProvider 配置的 predicates 和 priorities policies 在 `k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults/defaults.go` 中定义，算法具体实现是在 `k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/` 和`k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/` 中，默认的算法如下所示：\n\n`pkg/scheduler/algorithmprovider/defaults/defaults.go`\n```\nfunc defaultPredicates() sets.String {\n    return sets.NewString(\n        predicates.NoVolumeZoneConflictPred,\n        predicates.MaxEBSVolumeCountPred,\n        predicates.MaxGCEPDVolumeCountPred,\n        predicates.MaxAzureDiskVolumeCountPred,\n        predicates.MaxCSIVolumeCountPred,\n        predicates.MatchInterPodAffinityPred,\n        predicates.NoDiskConflictPred,\n        predicates.GeneralPred,\n        predicates.CheckNodeMemoryPressurePred,\n        predicates.CheckNodeDiskPressurePred,\n        predicates.CheckNodePIDPressurePred,\n        predicates.CheckNodeConditionPred,\n        predicates.PodToleratesNodeTaintsPred,\n        predicates.CheckVolumeBindingPred,\n    )\n}\n\nfunc defaultPriorities() sets.String {\n    return sets.NewString(\n        priorities.SelectorSpreadPriority,\n        priorities.InterPodAffinityPriority,\n        priorities.LeastRequestedPriority,\n        priorities.BalancedResourceAllocation,\n        priorities.NodePreferAvoidPodsPriority,\n        priorities.NodeAffinityPriority,\n        priorities.TaintTolerationPriority,\n        priorities.ImageLocalityPriority,\n    )\n}\n```\n\n\n\n下面继续看 `sched.Algorithm.Schedule()` 调用具体调度算法的过程：\n\n- 检查 pod pvc 信息\n- 执行 prefilter plugins\n- 获取 scheduler cache 的快照，每次调度 pod 时都会获取一次快照\n- 执行 `g.findNodesThatFit()` 预选算法\n- 执行 postfilter plugin\n- 若 node  为 0 直接返回失败的 error，若 node 数为1 直接返回该 node\n- 执行 `g.priorityMetaProducer()` 获取 metaPrioritiesInterface，计算 pod 的metadata，检查该 node 上是否有相同 meta 的 pod\n- 执行 `PrioritizeNodes()` 算法\n- 执行 `g.selectHost()` 通过得分选择一个最佳的 node\n\n`k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:186`\n```\nfunc (g *genericScheduler) Schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (result ScheduleResult, err error) {\n    ......\n    // 1.检查 pod pvc \n    if err := podPassesBasicChecks(pod, g.pvcLister); err != nil {\n        return result, err\n    }\n\n    // 2.执行 \"prefilter\" plugins\n    preFilterStatus := g.framework.RunPreFilterPlugins(pluginContext, pod)\n    if !preFilterStatus.IsSuccess() {\n        return result, preFilterStatus.AsError()\n    }\n\n    // 3.获取 node 数量\n    numNodes := g.cache.NodeTree().NumNodes()\n    if numNodes == 0 {\n        return result, ErrNoNodesAvailable\n    }\n\n    // 4.快照 node 信息\n    if err := g.snapshot(); err != nil {\n        return result, err\n    }\n\t\t\n    // 5.执行预选算法\n    startPredicateEvalTime := time.Now()\n    filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(pluginContext, pod)\n    if err != nil {\n        return result, err\n    }\n    // 6.执行 \"postfilter\" plugins\n    postfilterStatus := g.framework.RunPostFilterPlugins(pluginContext, pod, filteredNodes, filteredNodesStatuses)\n    if !postfilterStatus.IsSuccess() {\n        return result, postfilterStatus.AsError()\n    }\n\n    // 7.预选后没有合适的 node 直接返回\n    if len(filteredNodes) == 0 {\n        ......\n    }\n\n    startPriorityEvalTime := time.Now()\n    // 8.若只有一个 node 则直接返回该 node\n    if len(filteredNodes) == 1 {\n        return ScheduleResult{\n            SuggestedHost:  filteredNodes[0].Name,\n            EvaluatedNodes: 1 + len(failedPredicateMap),\n            FeasibleNodes:  1,\n        }, nil\n    }\n    \n    // 9.获取 pod meta 信息，执行优选算法\n    metaPrioritiesInterface := g.priorityMetaProducer(pod, g.nodeInfoSnapshot.NodeInfoMap)\n    priorityList, err := PrioritizeNodes(pod, g.nodeInfoSnapshot.NodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders, g.framework,      pluginContext)\n    if err != nil {\n        return result, err\n    }\n\n    // 10.根据打分选择最佳的 node\n    host, err := g.selectHost(priorityList)\n    trace.Step(\"Selecting host done\")\n    return ScheduleResult{\n        SuggestedHost:  host,\n        EvaluatedNodes: len(filteredNodes) + len(failedPredicateMap),\n        FeasibleNodes:  len(filteredNodes),\n    }, err\n}\n```\n\n至此，scheduler 的整个过程分析完毕。\n\n\n\n### 总结\n\n本文主要对于 kube-scheduler v1.16 的调度流程进行了分析，但其中有大量的细节都暂未提及，包括预选算法以及优选算法的具体实现、优先级与抢占调度的实现、framework 的使用及实现，因篇幅有限，部分内容会在后文继续说明。\n\n\n\n参考：\n\n[The Kubernetes Scheduler](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md)\n\n[scheduling design proposals](https://github.com/kubernetes/community/tree/master/contributors/design-proposals/scheduling)\n\n\n\n","source":"_posts/kube_scheduler_process.md","raw":"---\ntitle: kube-scheduler 源码分析\ndate: 2019-10-21 17:30:30\ntags: [\"kube-scheduler\",]\ntype: \"kube-scheduler\"\n\n---\n\n### kube-scheduler 的设计\n\nKube-scheduler 是 kubernetes 的核心组件之一，也是所有核心组件之间功能比较单一的，其代码也相对容易理解。kube-scheduler 的目的就是为每一个 pod 选择一个合适的 node，整体流程可以概括为三步，获取未调度的 podList，通过执行一系列调度算法为 pod 选择一个合适的 node，提交数据到 apiserver，其核心则是一系列调度算法的设计与执行。\n\n\n官方对 kube-scheduler 的调度流程描述 [The Kubernetes Scheduler](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md)：\n\n```\nFor given pod:\n\n    +---------------------------------------------+\n    |               Schedulable nodes:            |\n    |                                             |\n    | +--------+    +--------+      +--------+    |\n    | | node 1 |    | node 2 |      | node 3 |    |\n    | +--------+    +--------+      +--------+    |\n    |                                             |\n    +-------------------+-------------------------+\n                        |\n                        |\n                        v\n    +-------------------+-------------------------+\n\n    Pred. filters: node 3 doesn't have enough resource\n\n    +-------------------+-------------------------+\n                        |\n                        |\n                        v\n    +-------------------+-------------------------+\n    |             remaining nodes:                |\n    |   +--------+                 +--------+     |\n    |   | node 1 |                 | node 2 |     |\n    |   +--------+                 +--------+     |\n    |                                             |\n    +-------------------+-------------------------+\n                        |\n                        |\n                        v\n    +-------------------+-------------------------+\n\n    Priority function:    node 1: p=2\n                          node 2: p=5\n\n    +-------------------+-------------------------+\n                        |\n                        |\n                        v\n            select max{node priority} = node 2\n```\n\nkube-scheduler 目前包含两部分调度算法 predicates 和 priorities，首先执行 predicates 算法过滤部分 node 然后执行  priorities 算法为所有 node 打分，最后从所有 node 中选出分数最高的最为最佳的 node。 \n\n\n\n### kube-scheduler 源码分析\n\n> kubernetes 版本: v1.16\n\n\nkubernetes 中所有组件的启动流程都是类似的，首先会解析命令行参数、添加默认值，kube-scheduler 的默认参数在 `k8s.io/kubernetes/pkg/scheduler/apis/config/v1alpha1/defaults.go` 中定义的。然后会执行  run 方法启动主逻辑，下面直接看 kube-scheduler 的主逻辑 run 方法执行过程。\n\n\n`Run()` 方法主要做了以下工作：\n- 初始化 scheduler 对象\n- 启动 kube-scheduler server，kube-scheduler 监听 10251 和 10259 端口，10251 端口不需要认证，可以获取 healthz metrics 等信息，10259 为安全端口，需要认证\n- 启动所有的 informer\n- 执行 `sched.Run()` 方法，执行主调度逻辑\n\n`k8s.io/kubernetes/cmd/kube-scheduler/app/server.go:160`  \n\n```\nfunc Run(cc schedulerserverconfig.CompletedConfig, stopCh <-chan struct{}, registryOptions ...Option) error {\n    ......\n    // 1、初始化 scheduler 对象\n    sched, err := scheduler.New(......）\n    if err != nil {\n        return err\n    }\n\n    // 2、启动事件广播\n    if cc.Broadcaster != nil && cc.EventClient != nil {\n        cc.Broadcaster.StartRecordingToSink(stopCh)\n    }\n    if cc.LeaderElectionBroadcaster != nil && cc.CoreEventClient != nil {\n        cc.LeaderElectionBroadcaster.StartRecordingToSink(&corev1.EventSinkImpl{Interface: cc.CoreEventClient.Events(\"\")})\n    }\n\n    ......\n    // 3、启动 http server\n    if cc.InsecureServing != nil {\n        separateMetrics := cc.InsecureMetricsServing != nil\n        handler := buildHandlerChain(newHealthzHandler(&cc.ComponentConfig, separateMetrics, checks...), nil, nil)\n        if err := cc.InsecureServing.Serve(handler, 0, stopCh); err != nil {\n            return fmt.Errorf(\"failed to start healthz server: %v\", err)\n        }\n    }\n    ......\n    // 4、启动所有 informer\n    go cc.PodInformer.Informer().Run(stopCh)\n    cc.InformerFactory.Start(stopCh)\n\n    cc.InformerFactory.WaitForCacheSync(stopCh)\n\n    run := func(ctx context.Context) {\n        sched.Run()\n        <-ctx.Done()\n    }\n\n    ctx, cancel := context.WithCancel(context.TODO()) // TODO once Run() accepts a context, it should be used here\n    defer cancel()\n    go func() {\n        select {\n        case <-stopCh:\n            cancel()\n        case <-ctx.Done():\n        }\n    }()\n\n    // 5、选举 leader\n    if cc.LeaderElection != nil {\n        ......\n    }\n    // 6、执行 sched.Run() 方法\n    run(ctx)\n    return fmt.Errorf(\"finished without leader elect\")\n}\n```\n\n\n\n下面看一下 `scheduler.New()` 方法是如何初始化 scheduler 结构体的，该方法主要的功能是初始化默认的调度算法以及默认的调度器 GenericScheduler。\n\n- 创建 scheduler 配置文件\n- 根据默认的 DefaultProvider 初始化  schedulerAlgorithmSource 然后加载默认的预选及优选算法，然后初始化 GenericScheduler\n- 若启动参数提供了 policy config 则使用其覆盖默认的预选及优选算法并初始化 GenericScheduler，不过该参数现已被弃用\n\n`k8s.io/kubernetes/pkg/scheduler/scheduler.go:166`\n```\nfunc New(......) (*Scheduler, error) {\n\t......\n    // 1、创建 scheduler 的配置文件\n    configurator := factory.NewConfigFactory(&factory.ConfigFactoryArgs{\n  \t    ......\n  \t})\n    var config *factory.Config\n    source := schedulerAlgorithmSource\n    // 2、加载默认的调度算法\n    switch {\n    case source.Provider != nil:\n        // 使用默认的 ”DefaultProvider“ 初始化 config\n        sc, err := configurator.CreateFromProvider(*source.Provider)\n        if err != nil {\n            return nil, fmt.Errorf(\"couldn't create scheduler using provider %q: %v\", *source.Provider, err)\n        }\n        config = sc\n    case source.Policy != nil:\n        // 通过启动时指定的 policy source 加载 config\n\t......\n        config = sc\n    default:\n        return nil, fmt.Errorf(\"unsupported algorithm source: %v\", source)\n    }\n    // Additional tweaks to the config produced by the configurator.\n    config.Recorder = recorder\n    config.DisablePreemption = options.disablePreemption\n    config.StopEverything = stopCh\n\n    // 3.创建 scheduler 对象\n    sched := NewFromConfig(config)\n\t......\n    return sched, nil\n}\n```\n\n\n\n下面是 pod informer 的启动逻辑，只监听 status.phase 不为 succeeded 以及 failed 状态的 pod，即非 terminating 的 pod。\n\n`k8s.io/kubernetes/pkg/scheduler/factory/factory.go:527`\n````\nfunc NewPodInformer(client clientset.Interface, resyncPeriod time.Duration) coreinformers.PodInformer {\n    selector := fields.ParseSelectorOrDie(\n        \"status.phase!=\" + string(v1.PodSucceeded) +\n            \",status.phase!=\" + string(v1.PodFailed))\n    lw := cache.NewListWatchFromClient(client.CoreV1().RESTClient(), string(v1.ResourcePods), metav1.NamespaceAll, selector)\n    return &podInformer{\n        informer: cache.NewSharedIndexInformer(lw, &v1.Pod{}, resyncPeriod, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}),\n    }\n}\n````\n\n\n\n然后继续看 `Run()`  方法中最后执行的 `sched.Run()` 调度循环逻辑，若 informer 中的 cache 同步完成后会启动一个循环逻辑执行 `sched.scheduleOne` 方法。\n\n`k8s.io/kubernetes/pkg/scheduler/scheduler.go:313`   \n```\nfunc (sched *Scheduler) Run() {\n    if !sched.config.WaitForCacheSync() {\n        return\n    }\n\n    go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything)\n}\n```\n\n\n\n`scheduleOne()` 每次对一个 pod 进行调度，主要有以下步骤：\n\n- 从 scheduler 调度队列中取出一个 pod，如果该 pod 处于删除状态则跳过\n- 执行调度逻辑 `sched.schedule()` 返回通过预算及优选算法过滤后选出的最佳 node\n- 如果过滤算法没有选出合适的 node，则返回 core.FitError \n- 若没有合适的 node 会判断是否启用了抢占策略，若启用了则执行抢占机制\n- 判断是否需要 VolumeScheduling 特性\n- 执行 reserve plugin\n- pod 对应的 spec.NodeName 写上 scheduler 最终选择的 node，更新 scheduler cache\n- 请求 apiserver 异步处理最终的绑定操作，写入到 etcd\n- 执行 permit plugin\n- 执行 prebind plugin\n- 执行 postbind plugin\n\n`k8s.io/kubernetes/pkg/scheduler/scheduler.go:515`\n```\nfunc (sched *Scheduler) scheduleOne() {\n    fwk := sched.Framework\n\n    pod := sched.NextPod()\n    if pod == nil {\n        return\n    }\n    // 1.判断 pod 是否处于删除状态\n    if pod.DeletionTimestamp != nil {\n    \t......\n    }\n\t\t\n    // 2.执行调度策略选择 node\n    start := time.Now()\n    pluginContext := framework.NewPluginContext()\n    scheduleResult, err := sched.schedule(pod, pluginContext)\n    if err != nil {\n        if fitError, ok := err.(*core.FitError); ok {\n            // 3.若启用抢占机制则执行\n            if sched.DisablePreemption {\n            \t......\n            } else {\n                preemptionStartTime := time.Now()\n                sched.preempt(pluginContext, fwk, pod, fitError)\n                ......\n            }\n            ......\n            metrics.PodScheduleFailures.Inc()\n        } else {\n            klog.Errorf(\"error selecting node for pod: %v\", err)\n            metrics.PodScheduleErrors.Inc()\n        }\n        return\n    }\n    ......\n    assumedPod := pod.DeepCopy()\n\n    // 4.判断是否需要 VolumeScheduling 特性\n    allBound, err := sched.assumeVolumes(assumedPod, scheduleResult.SuggestedHost)\n    if err != nil {\n        klog.Errorf(\"error assuming volumes: %v\", err)\n        metrics.PodScheduleErrors.Inc()\n        return\n    }\n\n    // 5.执行 \"reserve\" plugins\n    if sts := fwk.RunReservePlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {\n        .....\n    }\n\n    // 6.为 pod 设置 NodeName 字段，更新 scheduler 缓存\n    err = sched.assume(assumedPod, scheduleResult.SuggestedHost)\n    if err != nil {\n        ......\n    }\n\n    // 7.异步请求 apiserver\n    go func() {\n        // Bind volumes first before Pod\n        if !allBound {\n            err := sched.bindVolumes(assumedPod)\n            if err != nil {\n\t\t\t\t......\n                return\n            }\n        }\n\n        // 8.执行 \"permit\" plugins\n        permitStatus := fwk.RunPermitPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)\n        if !permitStatus.IsSuccess() {\n\t\t\t......\n        }\n        // 9.执行 \"prebind\" plugins\n        preBindStatus := fwk.RunPreBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)\n        if !preBindStatus.IsSuccess() {\n            ......\n        }\n        err := sched.bind(assumedPod, scheduleResult.SuggestedHost, pluginContext)\n        ......\n        if err != nil {\n            ......\n        } else {\n            ......\n            // 10.执行 \"postbind\" plugins\n            fwk.RunPostBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)\n        }\n    }()\n}\n```\n\n\n\n`scheduleOne()`  中通过调用 `sched.schedule()` 来执行预选与优选算法处理：\n\n`k8s.io/kubernetes/pkg/scheduler/scheduler.go:337`\n```\nfunc (sched *Scheduler) schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (core.ScheduleResult, error) {\n    result, err := sched.Algorithm.Schedule(pod, pluginContext)\n    if err != nil {\n\t......\n    }\n    return result, err\n}\n```\n\n\n\n`sched.Algorithm` 是一个 interface，主要包含四个方法，GenericScheduler 是其具体的实现：\n\n`k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:131`\n```\ntype ScheduleAlgorithm interface {\n    Schedule(*v1.Pod, *framework.PluginContext) (scheduleResult ScheduleResult, err error)\n    Preempt(*framework.PluginContext, *v1.Pod, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error)\n    Predicates() map[string]predicates.FitPredicate\n    Prioritizers() []priorities.PriorityConfig\n}\n```\n\n- `Schedule()`：正常调度逻辑，包含预算与优选算法的执行\n- `Preempt()`：抢占策略，在 pod 调度发生失败的时候尝试抢占低优先级的 pod，函数返回发生抢占的 node，被 抢占的 pods 列表，nominated node name 需要被移除的 pods 列表以及 error\n- `Predicates()`：predicates 算法列表\n- `Prioritizers()`：prioritizers 算法列表\n\n\n\nkube-scheduler 提供的默认调度为 DefaultProvider，DefaultProvider 配置的 predicates 和 priorities policies 在 `k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults/defaults.go` 中定义，算法具体实现是在 `k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/` 和`k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/` 中，默认的算法如下所示：\n\n`pkg/scheduler/algorithmprovider/defaults/defaults.go`\n```\nfunc defaultPredicates() sets.String {\n    return sets.NewString(\n        predicates.NoVolumeZoneConflictPred,\n        predicates.MaxEBSVolumeCountPred,\n        predicates.MaxGCEPDVolumeCountPred,\n        predicates.MaxAzureDiskVolumeCountPred,\n        predicates.MaxCSIVolumeCountPred,\n        predicates.MatchInterPodAffinityPred,\n        predicates.NoDiskConflictPred,\n        predicates.GeneralPred,\n        predicates.CheckNodeMemoryPressurePred,\n        predicates.CheckNodeDiskPressurePred,\n        predicates.CheckNodePIDPressurePred,\n        predicates.CheckNodeConditionPred,\n        predicates.PodToleratesNodeTaintsPred,\n        predicates.CheckVolumeBindingPred,\n    )\n}\n\nfunc defaultPriorities() sets.String {\n    return sets.NewString(\n        priorities.SelectorSpreadPriority,\n        priorities.InterPodAffinityPriority,\n        priorities.LeastRequestedPriority,\n        priorities.BalancedResourceAllocation,\n        priorities.NodePreferAvoidPodsPriority,\n        priorities.NodeAffinityPriority,\n        priorities.TaintTolerationPriority,\n        priorities.ImageLocalityPriority,\n    )\n}\n```\n\n\n\n下面继续看 `sched.Algorithm.Schedule()` 调用具体调度算法的过程：\n\n- 检查 pod pvc 信息\n- 执行 prefilter plugins\n- 获取 scheduler cache 的快照，每次调度 pod 时都会获取一次快照\n- 执行 `g.findNodesThatFit()` 预选算法\n- 执行 postfilter plugin\n- 若 node  为 0 直接返回失败的 error，若 node 数为1 直接返回该 node\n- 执行 `g.priorityMetaProducer()` 获取 metaPrioritiesInterface，计算 pod 的metadata，检查该 node 上是否有相同 meta 的 pod\n- 执行 `PrioritizeNodes()` 算法\n- 执行 `g.selectHost()` 通过得分选择一个最佳的 node\n\n`k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:186`\n```\nfunc (g *genericScheduler) Schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (result ScheduleResult, err error) {\n    ......\n    // 1.检查 pod pvc \n    if err := podPassesBasicChecks(pod, g.pvcLister); err != nil {\n        return result, err\n    }\n\n    // 2.执行 \"prefilter\" plugins\n    preFilterStatus := g.framework.RunPreFilterPlugins(pluginContext, pod)\n    if !preFilterStatus.IsSuccess() {\n        return result, preFilterStatus.AsError()\n    }\n\n    // 3.获取 node 数量\n    numNodes := g.cache.NodeTree().NumNodes()\n    if numNodes == 0 {\n        return result, ErrNoNodesAvailable\n    }\n\n    // 4.快照 node 信息\n    if err := g.snapshot(); err != nil {\n        return result, err\n    }\n\t\t\n    // 5.执行预选算法\n    startPredicateEvalTime := time.Now()\n    filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(pluginContext, pod)\n    if err != nil {\n        return result, err\n    }\n    // 6.执行 \"postfilter\" plugins\n    postfilterStatus := g.framework.RunPostFilterPlugins(pluginContext, pod, filteredNodes, filteredNodesStatuses)\n    if !postfilterStatus.IsSuccess() {\n        return result, postfilterStatus.AsError()\n    }\n\n    // 7.预选后没有合适的 node 直接返回\n    if len(filteredNodes) == 0 {\n        ......\n    }\n\n    startPriorityEvalTime := time.Now()\n    // 8.若只有一个 node 则直接返回该 node\n    if len(filteredNodes) == 1 {\n        return ScheduleResult{\n            SuggestedHost:  filteredNodes[0].Name,\n            EvaluatedNodes: 1 + len(failedPredicateMap),\n            FeasibleNodes:  1,\n        }, nil\n    }\n    \n    // 9.获取 pod meta 信息，执行优选算法\n    metaPrioritiesInterface := g.priorityMetaProducer(pod, g.nodeInfoSnapshot.NodeInfoMap)\n    priorityList, err := PrioritizeNodes(pod, g.nodeInfoSnapshot.NodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders, g.framework,      pluginContext)\n    if err != nil {\n        return result, err\n    }\n\n    // 10.根据打分选择最佳的 node\n    host, err := g.selectHost(priorityList)\n    trace.Step(\"Selecting host done\")\n    return ScheduleResult{\n        SuggestedHost:  host,\n        EvaluatedNodes: len(filteredNodes) + len(failedPredicateMap),\n        FeasibleNodes:  len(filteredNodes),\n    }, err\n}\n```\n\n至此，scheduler 的整个过程分析完毕。\n\n\n\n### 总结\n\n本文主要对于 kube-scheduler v1.16 的调度流程进行了分析，但其中有大量的细节都暂未提及，包括预选算法以及优选算法的具体实现、优先级与抢占调度的实现、framework 的使用及实现，因篇幅有限，部分内容会在后文继续说明。\n\n\n\n参考：\n\n[The Kubernetes Scheduler](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md)\n\n[scheduling design proposals](https://github.com/kubernetes/community/tree/master/contributors/design-proposals/scheduling)\n\n\n\n","slug":"kube_scheduler_process","published":1,"updated":"2019-10-21T09:49:12.782Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59o0014apwng6dgzqzx","content":"<h3 id=\"kube-scheduler-的设计\"><a href=\"#kube-scheduler-的设计\" class=\"headerlink\" title=\"kube-scheduler 的设计\"></a>kube-scheduler 的设计</h3><p>Kube-scheduler 是 kubernetes 的核心组件之一，也是所有核心组件之间功能比较单一的，其代码也相对容易理解。kube-scheduler 的目的就是为每一个 pod 选择一个合适的 node，整体流程可以概括为三步，获取未调度的 podList，通过执行一系列调度算法为 pod 选择一个合适的 node，提交数据到 apiserver，其核心则是一系列调度算法的设计与执行。</p>\n<p>官方对 kube-scheduler 的调度流程描述 <a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md\" target=\"_blank\" rel=\"noopener\">The Kubernetes Scheduler</a>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">For given pod:</span><br><span class=\"line\"></span><br><span class=\"line\">    +---------------------------------------------+</span><br><span class=\"line\">    |               Schedulable nodes:            |</span><br><span class=\"line\">    |                                             |</span><br><span class=\"line\">    | +--------+    +--------+      +--------+    |</span><br><span class=\"line\">    | | node 1 |    | node 2 |      | node 3 |    |</span><br><span class=\"line\">    | +--------+    +--------+      +--------+    |</span><br><span class=\"line\">    |                                             |</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        v</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\"></span><br><span class=\"line\">    Pred. filters: node 3 doesn&apos;t have enough resource</span><br><span class=\"line\"></span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        v</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">    |             remaining nodes:                |</span><br><span class=\"line\">    |   +--------+                 +--------+     |</span><br><span class=\"line\">    |   | node 1 |                 | node 2 |     |</span><br><span class=\"line\">    |   +--------+                 +--------+     |</span><br><span class=\"line\">    |                                             |</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        v</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\"></span><br><span class=\"line\">    Priority function:    node 1: p=2</span><br><span class=\"line\">                          node 2: p=5</span><br><span class=\"line\"></span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        v</span><br><span class=\"line\">            select max&#123;node priority&#125; = node 2</span><br></pre></td></tr></table></figure>\n<p>kube-scheduler 目前包含两部分调度算法 predicates 和 priorities，首先执行 predicates 算法过滤部分 node 然后执行  priorities 算法为所有 node 打分，最后从所有 node 中选出分数最高的最为最佳的 node。 </p>\n<h3 id=\"kube-scheduler-源码分析\"><a href=\"#kube-scheduler-源码分析\" class=\"headerlink\" title=\"kube-scheduler 源码分析\"></a>kube-scheduler 源码分析</h3><blockquote>\n<p>kubernetes 版本: v1.16</p>\n</blockquote>\n<p>kubernetes 中所有组件的启动流程都是类似的，首先会解析命令行参数、添加默认值，kube-scheduler 的默认参数在 <code>k8s.io/kubernetes/pkg/scheduler/apis/config/v1alpha1/defaults.go</code> 中定义的。然后会执行  run 方法启动主逻辑，下面直接看 kube-scheduler 的主逻辑 run 方法执行过程。</p>\n<p><code>Run()</code> 方法主要做了以下工作：</p>\n<ul>\n<li>初始化 scheduler 对象</li>\n<li>启动 kube-scheduler server，kube-scheduler 监听 10251 和 10259 端口，10251 端口不需要认证，可以获取 healthz metrics 等信息，10259 为安全端口，需要认证</li>\n<li>启动所有的 informer</li>\n<li>执行 <code>sched.Run()</code> 方法，执行主调度逻辑</li>\n</ul>\n<p><code>k8s.io/kubernetes/cmd/kube-scheduler/app/server.go:160</code>  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func Run(cc schedulerserverconfig.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;, registryOptions ...Option) error &#123;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    // 1、初始化 scheduler 对象</span><br><span class=\"line\">    sched, err := scheduler.New(......）</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 2、启动事件广播</span><br><span class=\"line\">    if cc.Broadcaster != nil &amp;&amp; cc.EventClient != nil &#123;</span><br><span class=\"line\">        cc.Broadcaster.StartRecordingToSink(stopCh)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if cc.LeaderElectionBroadcaster != nil &amp;&amp; cc.CoreEventClient != nil &#123;</span><br><span class=\"line\">        cc.LeaderElectionBroadcaster.StartRecordingToSink(&amp;corev1.EventSinkImpl&#123;Interface: cc.CoreEventClient.Events(&quot;&quot;)&#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ......</span><br><span class=\"line\">    // 3、启动 http server</span><br><span class=\"line\">    if cc.InsecureServing != nil &#123;</span><br><span class=\"line\">        separateMetrics := cc.InsecureMetricsServing != nil</span><br><span class=\"line\">        handler := buildHandlerChain(newHealthzHandler(&amp;cc.ComponentConfig, separateMetrics, checks...), nil, nil)</span><br><span class=\"line\">        if err := cc.InsecureServing.Serve(handler, 0, stopCh); err != nil &#123;</span><br><span class=\"line\">            return fmt.Errorf(&quot;failed to start healthz server: %v&quot;, err)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    // 4、启动所有 informer</span><br><span class=\"line\">    go cc.PodInformer.Informer().Run(stopCh)</span><br><span class=\"line\">    cc.InformerFactory.Start(stopCh)</span><br><span class=\"line\"></span><br><span class=\"line\">    cc.InformerFactory.WaitForCacheSync(stopCh)</span><br><span class=\"line\"></span><br><span class=\"line\">    run := func(ctx context.Context) &#123;</span><br><span class=\"line\">        sched.Run()</span><br><span class=\"line\">        &lt;-ctx.Done()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ctx, cancel := context.WithCancel(context.TODO()) // TODO once Run() accepts a context, it should be used here</span><br><span class=\"line\">    defer cancel()</span><br><span class=\"line\">    go func() &#123;</span><br><span class=\"line\">        select &#123;</span><br><span class=\"line\">        case &lt;-stopCh:</span><br><span class=\"line\">            cancel()</span><br><span class=\"line\">        case &lt;-ctx.Done():</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">    // 5、选举 leader</span><br><span class=\"line\">    if cc.LeaderElection != nil &#123;</span><br><span class=\"line\">        ......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 6、执行 sched.Run() 方法</span><br><span class=\"line\">    run(ctx)</span><br><span class=\"line\">    return fmt.Errorf(&quot;finished without leader elect&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>下面看一下 <code>scheduler.New()</code> 方法是如何初始化 scheduler 结构体的，该方法主要的功能是初始化默认的调度算法以及默认的调度器 GenericScheduler。</p>\n<ul>\n<li>创建 scheduler 配置文件</li>\n<li>根据默认的 DefaultProvider 初始化  schedulerAlgorithmSource 然后加载默认的预选及优选算法，然后初始化 GenericScheduler</li>\n<li>若启动参数提供了 policy config 则使用其覆盖默认的预选及优选算法并初始化 GenericScheduler，不过该参数现已被弃用</li>\n</ul>\n<p><code>k8s.io/kubernetes/pkg/scheduler/scheduler.go:166</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func New(......) (*Scheduler, error) &#123;</span><br><span class=\"line\">\t......</span><br><span class=\"line\">    // 1、创建 scheduler 的配置文件</span><br><span class=\"line\">    configurator := factory.NewConfigFactory(&amp;factory.ConfigFactoryArgs&#123;</span><br><span class=\"line\">  \t    ......</span><br><span class=\"line\">  \t&#125;)</span><br><span class=\"line\">    var config *factory.Config</span><br><span class=\"line\">    source := schedulerAlgorithmSource</span><br><span class=\"line\">    // 2、加载默认的调度算法</span><br><span class=\"line\">    switch &#123;</span><br><span class=\"line\">    case source.Provider != nil:</span><br><span class=\"line\">        // 使用默认的 ”DefaultProvider“ 初始化 config</span><br><span class=\"line\">        sc, err := configurator.CreateFromProvider(*source.Provider)</span><br><span class=\"line\">        if err != nil &#123;</span><br><span class=\"line\">            return nil, fmt.Errorf(&quot;couldn&apos;t create scheduler using provider %q: %v&quot;, *source.Provider, err)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        config = sc</span><br><span class=\"line\">    case source.Policy != nil:</span><br><span class=\"line\">        // 通过启动时指定的 policy source 加载 config</span><br><span class=\"line\">\t......</span><br><span class=\"line\">        config = sc</span><br><span class=\"line\">    default:</span><br><span class=\"line\">        return nil, fmt.Errorf(&quot;unsupported algorithm source: %v&quot;, source)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // Additional tweaks to the config produced by the configurator.</span><br><span class=\"line\">    config.Recorder = recorder</span><br><span class=\"line\">    config.DisablePreemption = options.disablePreemption</span><br><span class=\"line\">    config.StopEverything = stopCh</span><br><span class=\"line\"></span><br><span class=\"line\">    // 3.创建 scheduler 对象</span><br><span class=\"line\">    sched := NewFromConfig(config)</span><br><span class=\"line\">\t......</span><br><span class=\"line\">    return sched, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>下面是 pod informer 的启动逻辑，只监听 status.phase 不为 succeeded 以及 failed 状态的 pod，即非 terminating 的 pod。</p>\n<p><code>k8s.io/kubernetes/pkg/scheduler/factory/factory.go:527</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewPodInformer(client clientset.Interface, resyncPeriod time.Duration) coreinformers.PodInformer &#123;</span><br><span class=\"line\">    selector := fields.ParseSelectorOrDie(</span><br><span class=\"line\">        &quot;status.phase!=&quot; + string(v1.PodSucceeded) +</span><br><span class=\"line\">            &quot;,status.phase!=&quot; + string(v1.PodFailed))</span><br><span class=\"line\">    lw := cache.NewListWatchFromClient(client.CoreV1().RESTClient(), string(v1.ResourcePods), metav1.NamespaceAll, selector)</span><br><span class=\"line\">    return &amp;podInformer&#123;</span><br><span class=\"line\">        informer: cache.NewSharedIndexInformer(lw, &amp;v1.Pod&#123;&#125;, resyncPeriod, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;),</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>然后继续看 <code>Run()</code>  方法中最后执行的 <code>sched.Run()</code> 调度循环逻辑，若 informer 中的 cache 同步完成后会启动一个循环逻辑执行 <code>sched.scheduleOne</code> 方法。</p>\n<p><code>k8s.io/kubernetes/pkg/scheduler/scheduler.go:313</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (sched *Scheduler) Run() &#123;</span><br><span class=\"line\">    if !sched.config.WaitForCacheSync() &#123;</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>scheduleOne()</code> 每次对一个 pod 进行调度，主要有以下步骤：</p>\n<ul>\n<li>从 scheduler 调度队列中取出一个 pod，如果该 pod 处于删除状态则跳过</li>\n<li>执行调度逻辑 <code>sched.schedule()</code> 返回通过预算及优选算法过滤后选出的最佳 node</li>\n<li>如果过滤算法没有选出合适的 node，则返回 core.FitError </li>\n<li>若没有合适的 node 会判断是否启用了抢占策略，若启用了则执行抢占机制</li>\n<li>判断是否需要 VolumeScheduling 特性</li>\n<li>执行 reserve plugin</li>\n<li>pod 对应的 spec.NodeName 写上 scheduler 最终选择的 node，更新 scheduler cache</li>\n<li>请求 apiserver 异步处理最终的绑定操作，写入到 etcd</li>\n<li>执行 permit plugin</li>\n<li>执行 prebind plugin</li>\n<li>执行 postbind plugin</li>\n</ul>\n<p><code>k8s.io/kubernetes/pkg/scheduler/scheduler.go:515</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (sched *Scheduler) scheduleOne() &#123;</span><br><span class=\"line\">    fwk := sched.Framework</span><br><span class=\"line\"></span><br><span class=\"line\">    pod := sched.NextPod()</span><br><span class=\"line\">    if pod == nil &#123;</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 1.判断 pod 是否处于删除状态</span><br><span class=\"line\">    if pod.DeletionTimestamp != nil &#123;</span><br><span class=\"line\">    \t......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">    // 2.执行调度策略选择 node</span><br><span class=\"line\">    start := time.Now()</span><br><span class=\"line\">    pluginContext := framework.NewPluginContext()</span><br><span class=\"line\">    scheduleResult, err := sched.schedule(pod, pluginContext)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        if fitError, ok := err.(*core.FitError); ok &#123;</span><br><span class=\"line\">            // 3.若启用抢占机制则执行</span><br><span class=\"line\">            if sched.DisablePreemption &#123;</span><br><span class=\"line\">            \t......</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                preemptionStartTime := time.Now()</span><br><span class=\"line\">                sched.preempt(pluginContext, fwk, pod, fitError)</span><br><span class=\"line\">                ......</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            ......</span><br><span class=\"line\">            metrics.PodScheduleFailures.Inc()</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            klog.Errorf(&quot;error selecting node for pod: %v&quot;, err)</span><br><span class=\"line\">            metrics.PodScheduleErrors.Inc()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    assumedPod := pod.DeepCopy()</span><br><span class=\"line\"></span><br><span class=\"line\">    // 4.判断是否需要 VolumeScheduling 特性</span><br><span class=\"line\">    allBound, err := sched.assumeVolumes(assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        klog.Errorf(&quot;error assuming volumes: %v&quot;, err)</span><br><span class=\"line\">        metrics.PodScheduleErrors.Inc()</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 5.执行 &quot;reserve&quot; plugins</span><br><span class=\"line\">    if sts := fwk.RunReservePlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() &#123;</span><br><span class=\"line\">        .....</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 6.为 pod 设置 NodeName 字段，更新 scheduler 缓存</span><br><span class=\"line\">    err = sched.assume(assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        ......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 7.异步请求 apiserver</span><br><span class=\"line\">    go func() &#123;</span><br><span class=\"line\">        // Bind volumes first before Pod</span><br><span class=\"line\">        if !allBound &#123;</span><br><span class=\"line\">            err := sched.bindVolumes(assumedPod)</span><br><span class=\"line\">            if err != nil &#123;</span><br><span class=\"line\">\t\t\t\t......</span><br><span class=\"line\">                return</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        // 8.执行 &quot;permit&quot; plugins</span><br><span class=\"line\">        permitStatus := fwk.RunPermitPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">        if !permitStatus.IsSuccess() &#123;</span><br><span class=\"line\">\t\t\t......</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        // 9.执行 &quot;prebind&quot; plugins</span><br><span class=\"line\">        preBindStatus := fwk.RunPreBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">        if !preBindStatus.IsSuccess() &#123;</span><br><span class=\"line\">            ......</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        err := sched.bind(assumedPod, scheduleResult.SuggestedHost, pluginContext)</span><br><span class=\"line\">        ......</span><br><span class=\"line\">        if err != nil &#123;</span><br><span class=\"line\">            ......</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            ......</span><br><span class=\"line\">            // 10.执行 &quot;postbind&quot; plugins</span><br><span class=\"line\">            fwk.RunPostBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>scheduleOne()</code>  中通过调用 <code>sched.schedule()</code> 来执行预选与优选算法处理：</p>\n<p><code>k8s.io/kubernetes/pkg/scheduler/scheduler.go:337</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (sched *Scheduler) schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (core.ScheduleResult, error) &#123;</span><br><span class=\"line\">    result, err := sched.Algorithm.Schedule(pod, pluginContext)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">\t......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return result, err</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>sched.Algorithm</code> 是一个 interface，主要包含四个方法，GenericScheduler 是其具体的实现：</p>\n<p><code>k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:131</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type ScheduleAlgorithm interface &#123;</span><br><span class=\"line\">    Schedule(*v1.Pod, *framework.PluginContext) (scheduleResult ScheduleResult, err error)</span><br><span class=\"line\">    Preempt(*framework.PluginContext, *v1.Pod, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error)</span><br><span class=\"line\">    Predicates() map[string]predicates.FitPredicate</span><br><span class=\"line\">    Prioritizers() []priorities.PriorityConfig</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li><code>Schedule()</code>：正常调度逻辑，包含预算与优选算法的执行</li>\n<li><code>Preempt()</code>：抢占策略，在 pod 调度发生失败的时候尝试抢占低优先级的 pod，函数返回发生抢占的 node，被 抢占的 pods 列表，nominated node name 需要被移除的 pods 列表以及 error</li>\n<li><code>Predicates()</code>：predicates 算法列表</li>\n<li><code>Prioritizers()</code>：prioritizers 算法列表</li>\n</ul>\n<p>kube-scheduler 提供的默认调度为 DefaultProvider，DefaultProvider 配置的 predicates 和 priorities policies 在 <code>k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults/defaults.go</code> 中定义，算法具体实现是在 <code>k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/</code> 和<code>k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/</code> 中，默认的算法如下所示：</p>\n<p><code>pkg/scheduler/algorithmprovider/defaults/defaults.go</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func defaultPredicates() sets.String &#123;</span><br><span class=\"line\">    return sets.NewString(</span><br><span class=\"line\">        predicates.NoVolumeZoneConflictPred,</span><br><span class=\"line\">        predicates.MaxEBSVolumeCountPred,</span><br><span class=\"line\">        predicates.MaxGCEPDVolumeCountPred,</span><br><span class=\"line\">        predicates.MaxAzureDiskVolumeCountPred,</span><br><span class=\"line\">        predicates.MaxCSIVolumeCountPred,</span><br><span class=\"line\">        predicates.MatchInterPodAffinityPred,</span><br><span class=\"line\">        predicates.NoDiskConflictPred,</span><br><span class=\"line\">        predicates.GeneralPred,</span><br><span class=\"line\">        predicates.CheckNodeMemoryPressurePred,</span><br><span class=\"line\">        predicates.CheckNodeDiskPressurePred,</span><br><span class=\"line\">        predicates.CheckNodePIDPressurePred,</span><br><span class=\"line\">        predicates.CheckNodeConditionPred,</span><br><span class=\"line\">        predicates.PodToleratesNodeTaintsPred,</span><br><span class=\"line\">        predicates.CheckVolumeBindingPred,</span><br><span class=\"line\">    )</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func defaultPriorities() sets.String &#123;</span><br><span class=\"line\">    return sets.NewString(</span><br><span class=\"line\">        priorities.SelectorSpreadPriority,</span><br><span class=\"line\">        priorities.InterPodAffinityPriority,</span><br><span class=\"line\">        priorities.LeastRequestedPriority,</span><br><span class=\"line\">        priorities.BalancedResourceAllocation,</span><br><span class=\"line\">        priorities.NodePreferAvoidPodsPriority,</span><br><span class=\"line\">        priorities.NodeAffinityPriority,</span><br><span class=\"line\">        priorities.TaintTolerationPriority,</span><br><span class=\"line\">        priorities.ImageLocalityPriority,</span><br><span class=\"line\">    )</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>下面继续看 <code>sched.Algorithm.Schedule()</code> 调用具体调度算法的过程：</p>\n<ul>\n<li>检查 pod pvc 信息</li>\n<li>执行 prefilter plugins</li>\n<li>获取 scheduler cache 的快照，每次调度 pod 时都会获取一次快照</li>\n<li>执行 <code>g.findNodesThatFit()</code> 预选算法</li>\n<li>执行 postfilter plugin</li>\n<li>若 node  为 0 直接返回失败的 error，若 node 数为1 直接返回该 node</li>\n<li>执行 <code>g.priorityMetaProducer()</code> 获取 metaPrioritiesInterface，计算 pod 的metadata，检查该 node 上是否有相同 meta 的 pod</li>\n<li>执行 <code>PrioritizeNodes()</code> 算法</li>\n<li>执行 <code>g.selectHost()</code> 通过得分选择一个最佳的 node</li>\n</ul>\n<p><code>k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:186</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (g *genericScheduler) Schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (result ScheduleResult, err error) &#123;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    // 1.检查 pod pvc </span><br><span class=\"line\">    if err := podPassesBasicChecks(pod, g.pvcLister); err != nil &#123;</span><br><span class=\"line\">        return result, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 2.执行 &quot;prefilter&quot; plugins</span><br><span class=\"line\">    preFilterStatus := g.framework.RunPreFilterPlugins(pluginContext, pod)</span><br><span class=\"line\">    if !preFilterStatus.IsSuccess() &#123;</span><br><span class=\"line\">        return result, preFilterStatus.AsError()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 3.获取 node 数量</span><br><span class=\"line\">    numNodes := g.cache.NodeTree().NumNodes()</span><br><span class=\"line\">    if numNodes == 0 &#123;</span><br><span class=\"line\">        return result, ErrNoNodesAvailable</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 4.快照 node 信息</span><br><span class=\"line\">    if err := g.snapshot(); err != nil &#123;</span><br><span class=\"line\">        return result, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">    // 5.执行预选算法</span><br><span class=\"line\">    startPredicateEvalTime := time.Now()</span><br><span class=\"line\">    filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(pluginContext, pod)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return result, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 6.执行 &quot;postfilter&quot; plugins</span><br><span class=\"line\">    postfilterStatus := g.framework.RunPostFilterPlugins(pluginContext, pod, filteredNodes, filteredNodesStatuses)</span><br><span class=\"line\">    if !postfilterStatus.IsSuccess() &#123;</span><br><span class=\"line\">        return result, postfilterStatus.AsError()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 7.预选后没有合适的 node 直接返回</span><br><span class=\"line\">    if len(filteredNodes) == 0 &#123;</span><br><span class=\"line\">        ......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    startPriorityEvalTime := time.Now()</span><br><span class=\"line\">    // 8.若只有一个 node 则直接返回该 node</span><br><span class=\"line\">    if len(filteredNodes) == 1 &#123;</span><br><span class=\"line\">        return ScheduleResult&#123;</span><br><span class=\"line\">            SuggestedHost:  filteredNodes[0].Name,</span><br><span class=\"line\">            EvaluatedNodes: 1 + len(failedPredicateMap),</span><br><span class=\"line\">            FeasibleNodes:  1,</span><br><span class=\"line\">        &#125;, nil</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 9.获取 pod meta 信息，执行优选算法</span><br><span class=\"line\">    metaPrioritiesInterface := g.priorityMetaProducer(pod, g.nodeInfoSnapshot.NodeInfoMap)</span><br><span class=\"line\">    priorityList, err := PrioritizeNodes(pod, g.nodeInfoSnapshot.NodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders, g.framework,      pluginContext)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return result, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 10.根据打分选择最佳的 node</span><br><span class=\"line\">    host, err := g.selectHost(priorityList)</span><br><span class=\"line\">    trace.Step(&quot;Selecting host done&quot;)</span><br><span class=\"line\">    return ScheduleResult&#123;</span><br><span class=\"line\">        SuggestedHost:  host,</span><br><span class=\"line\">        EvaluatedNodes: len(filteredNodes) + len(failedPredicateMap),</span><br><span class=\"line\">        FeasibleNodes:  len(filteredNodes),</span><br><span class=\"line\">    &#125;, err</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>至此，scheduler 的整个过程分析完毕。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要对于 kube-scheduler v1.16 的调度流程进行了分析，但其中有大量的细节都暂未提及，包括预选算法以及优选算法的具体实现、优先级与抢占调度的实现、framework 的使用及实现，因篇幅有限，部分内容会在后文继续说明。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md\" target=\"_blank\" rel=\"noopener\">The Kubernetes Scheduler</a></p>\n<p><a href=\"https://github.com/kubernetes/community/tree/master/contributors/design-proposals/scheduling\" target=\"_blank\" rel=\"noopener\">scheduling design proposals</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<h3 id=\"kube-scheduler-的设计\"><a href=\"#kube-scheduler-的设计\" class=\"headerlink\" title=\"kube-scheduler 的设计\"></a>kube-scheduler 的设计</h3><p>Kube-scheduler 是 kubernetes 的核心组件之一，也是所有核心组件之间功能比较单一的，其代码也相对容易理解。kube-scheduler 的目的就是为每一个 pod 选择一个合适的 node，整体流程可以概括为三步，获取未调度的 podList，通过执行一系列调度算法为 pod 选择一个合适的 node，提交数据到 apiserver，其核心则是一系列调度算法的设计与执行。</p>\n<p>官方对 kube-scheduler 的调度流程描述 <a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md\" target=\"_blank\" rel=\"noopener\">The Kubernetes Scheduler</a>：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">For given pod:</span><br><span class=\"line\"></span><br><span class=\"line\">    +---------------------------------------------+</span><br><span class=\"line\">    |               Schedulable nodes:            |</span><br><span class=\"line\">    |                                             |</span><br><span class=\"line\">    | +--------+    +--------+      +--------+    |</span><br><span class=\"line\">    | | node 1 |    | node 2 |      | node 3 |    |</span><br><span class=\"line\">    | +--------+    +--------+      +--------+    |</span><br><span class=\"line\">    |                                             |</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        v</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\"></span><br><span class=\"line\">    Pred. filters: node 3 doesn&apos;t have enough resource</span><br><span class=\"line\"></span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        v</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">    |             remaining nodes:                |</span><br><span class=\"line\">    |   +--------+                 +--------+     |</span><br><span class=\"line\">    |   | node 1 |                 | node 2 |     |</span><br><span class=\"line\">    |   +--------+                 +--------+     |</span><br><span class=\"line\">    |                                             |</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        v</span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\"></span><br><span class=\"line\">    Priority function:    node 1: p=2</span><br><span class=\"line\">                          node 2: p=5</span><br><span class=\"line\"></span><br><span class=\"line\">    +-------------------+-------------------------+</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        |</span><br><span class=\"line\">                        v</span><br><span class=\"line\">            select max&#123;node priority&#125; = node 2</span><br></pre></td></tr></table></figure>\n<p>kube-scheduler 目前包含两部分调度算法 predicates 和 priorities，首先执行 predicates 算法过滤部分 node 然后执行  priorities 算法为所有 node 打分，最后从所有 node 中选出分数最高的最为最佳的 node。 </p>\n<h3 id=\"kube-scheduler-源码分析\"><a href=\"#kube-scheduler-源码分析\" class=\"headerlink\" title=\"kube-scheduler 源码分析\"></a>kube-scheduler 源码分析</h3><blockquote>\n<p>kubernetes 版本: v1.16</p>\n</blockquote>\n<p>kubernetes 中所有组件的启动流程都是类似的，首先会解析命令行参数、添加默认值，kube-scheduler 的默认参数在 <code>k8s.io/kubernetes/pkg/scheduler/apis/config/v1alpha1/defaults.go</code> 中定义的。然后会执行  run 方法启动主逻辑，下面直接看 kube-scheduler 的主逻辑 run 方法执行过程。</p>\n<p><code>Run()</code> 方法主要做了以下工作：</p>\n<ul>\n<li>初始化 scheduler 对象</li>\n<li>启动 kube-scheduler server，kube-scheduler 监听 10251 和 10259 端口，10251 端口不需要认证，可以获取 healthz metrics 等信息，10259 为安全端口，需要认证</li>\n<li>启动所有的 informer</li>\n<li>执行 <code>sched.Run()</code> 方法，执行主调度逻辑</li>\n</ul>\n<p><code>k8s.io/kubernetes/cmd/kube-scheduler/app/server.go:160</code>  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func Run(cc schedulerserverconfig.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;, registryOptions ...Option) error &#123;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    // 1、初始化 scheduler 对象</span><br><span class=\"line\">    sched, err := scheduler.New(......）</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 2、启动事件广播</span><br><span class=\"line\">    if cc.Broadcaster != nil &amp;&amp; cc.EventClient != nil &#123;</span><br><span class=\"line\">        cc.Broadcaster.StartRecordingToSink(stopCh)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if cc.LeaderElectionBroadcaster != nil &amp;&amp; cc.CoreEventClient != nil &#123;</span><br><span class=\"line\">        cc.LeaderElectionBroadcaster.StartRecordingToSink(&amp;corev1.EventSinkImpl&#123;Interface: cc.CoreEventClient.Events(&quot;&quot;)&#125;)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ......</span><br><span class=\"line\">    // 3、启动 http server</span><br><span class=\"line\">    if cc.InsecureServing != nil &#123;</span><br><span class=\"line\">        separateMetrics := cc.InsecureMetricsServing != nil</span><br><span class=\"line\">        handler := buildHandlerChain(newHealthzHandler(&amp;cc.ComponentConfig, separateMetrics, checks...), nil, nil)</span><br><span class=\"line\">        if err := cc.InsecureServing.Serve(handler, 0, stopCh); err != nil &#123;</span><br><span class=\"line\">            return fmt.Errorf(&quot;failed to start healthz server: %v&quot;, err)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    // 4、启动所有 informer</span><br><span class=\"line\">    go cc.PodInformer.Informer().Run(stopCh)</span><br><span class=\"line\">    cc.InformerFactory.Start(stopCh)</span><br><span class=\"line\"></span><br><span class=\"line\">    cc.InformerFactory.WaitForCacheSync(stopCh)</span><br><span class=\"line\"></span><br><span class=\"line\">    run := func(ctx context.Context) &#123;</span><br><span class=\"line\">        sched.Run()</span><br><span class=\"line\">        &lt;-ctx.Done()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ctx, cancel := context.WithCancel(context.TODO()) // TODO once Run() accepts a context, it should be used here</span><br><span class=\"line\">    defer cancel()</span><br><span class=\"line\">    go func() &#123;</span><br><span class=\"line\">        select &#123;</span><br><span class=\"line\">        case &lt;-stopCh:</span><br><span class=\"line\">            cancel()</span><br><span class=\"line\">        case &lt;-ctx.Done():</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;()</span><br><span class=\"line\"></span><br><span class=\"line\">    // 5、选举 leader</span><br><span class=\"line\">    if cc.LeaderElection != nil &#123;</span><br><span class=\"line\">        ......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 6、执行 sched.Run() 方法</span><br><span class=\"line\">    run(ctx)</span><br><span class=\"line\">    return fmt.Errorf(&quot;finished without leader elect&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>下面看一下 <code>scheduler.New()</code> 方法是如何初始化 scheduler 结构体的，该方法主要的功能是初始化默认的调度算法以及默认的调度器 GenericScheduler。</p>\n<ul>\n<li>创建 scheduler 配置文件</li>\n<li>根据默认的 DefaultProvider 初始化  schedulerAlgorithmSource 然后加载默认的预选及优选算法，然后初始化 GenericScheduler</li>\n<li>若启动参数提供了 policy config 则使用其覆盖默认的预选及优选算法并初始化 GenericScheduler，不过该参数现已被弃用</li>\n</ul>\n<p><code>k8s.io/kubernetes/pkg/scheduler/scheduler.go:166</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func New(......) (*Scheduler, error) &#123;</span><br><span class=\"line\">\t......</span><br><span class=\"line\">    // 1、创建 scheduler 的配置文件</span><br><span class=\"line\">    configurator := factory.NewConfigFactory(&amp;factory.ConfigFactoryArgs&#123;</span><br><span class=\"line\">  \t    ......</span><br><span class=\"line\">  \t&#125;)</span><br><span class=\"line\">    var config *factory.Config</span><br><span class=\"line\">    source := schedulerAlgorithmSource</span><br><span class=\"line\">    // 2、加载默认的调度算法</span><br><span class=\"line\">    switch &#123;</span><br><span class=\"line\">    case source.Provider != nil:</span><br><span class=\"line\">        // 使用默认的 ”DefaultProvider“ 初始化 config</span><br><span class=\"line\">        sc, err := configurator.CreateFromProvider(*source.Provider)</span><br><span class=\"line\">        if err != nil &#123;</span><br><span class=\"line\">            return nil, fmt.Errorf(&quot;couldn&apos;t create scheduler using provider %q: %v&quot;, *source.Provider, err)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        config = sc</span><br><span class=\"line\">    case source.Policy != nil:</span><br><span class=\"line\">        // 通过启动时指定的 policy source 加载 config</span><br><span class=\"line\">\t......</span><br><span class=\"line\">        config = sc</span><br><span class=\"line\">    default:</span><br><span class=\"line\">        return nil, fmt.Errorf(&quot;unsupported algorithm source: %v&quot;, source)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // Additional tweaks to the config produced by the configurator.</span><br><span class=\"line\">    config.Recorder = recorder</span><br><span class=\"line\">    config.DisablePreemption = options.disablePreemption</span><br><span class=\"line\">    config.StopEverything = stopCh</span><br><span class=\"line\"></span><br><span class=\"line\">    // 3.创建 scheduler 对象</span><br><span class=\"line\">    sched := NewFromConfig(config)</span><br><span class=\"line\">\t......</span><br><span class=\"line\">    return sched, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>下面是 pod informer 的启动逻辑，只监听 status.phase 不为 succeeded 以及 failed 状态的 pod，即非 terminating 的 pod。</p>\n<p><code>k8s.io/kubernetes/pkg/scheduler/factory/factory.go:527</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewPodInformer(client clientset.Interface, resyncPeriod time.Duration) coreinformers.PodInformer &#123;</span><br><span class=\"line\">    selector := fields.ParseSelectorOrDie(</span><br><span class=\"line\">        &quot;status.phase!=&quot; + string(v1.PodSucceeded) +</span><br><span class=\"line\">            &quot;,status.phase!=&quot; + string(v1.PodFailed))</span><br><span class=\"line\">    lw := cache.NewListWatchFromClient(client.CoreV1().RESTClient(), string(v1.ResourcePods), metav1.NamespaceAll, selector)</span><br><span class=\"line\">    return &amp;podInformer&#123;</span><br><span class=\"line\">        informer: cache.NewSharedIndexInformer(lw, &amp;v1.Pod&#123;&#125;, resyncPeriod, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;),</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>然后继续看 <code>Run()</code>  方法中最后执行的 <code>sched.Run()</code> 调度循环逻辑，若 informer 中的 cache 同步完成后会启动一个循环逻辑执行 <code>sched.scheduleOne</code> 方法。</p>\n<p><code>k8s.io/kubernetes/pkg/scheduler/scheduler.go:313</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (sched *Scheduler) Run() &#123;</span><br><span class=\"line\">    if !sched.config.WaitForCacheSync() &#123;</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>scheduleOne()</code> 每次对一个 pod 进行调度，主要有以下步骤：</p>\n<ul>\n<li>从 scheduler 调度队列中取出一个 pod，如果该 pod 处于删除状态则跳过</li>\n<li>执行调度逻辑 <code>sched.schedule()</code> 返回通过预算及优选算法过滤后选出的最佳 node</li>\n<li>如果过滤算法没有选出合适的 node，则返回 core.FitError </li>\n<li>若没有合适的 node 会判断是否启用了抢占策略，若启用了则执行抢占机制</li>\n<li>判断是否需要 VolumeScheduling 特性</li>\n<li>执行 reserve plugin</li>\n<li>pod 对应的 spec.NodeName 写上 scheduler 最终选择的 node，更新 scheduler cache</li>\n<li>请求 apiserver 异步处理最终的绑定操作，写入到 etcd</li>\n<li>执行 permit plugin</li>\n<li>执行 prebind plugin</li>\n<li>执行 postbind plugin</li>\n</ul>\n<p><code>k8s.io/kubernetes/pkg/scheduler/scheduler.go:515</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (sched *Scheduler) scheduleOne() &#123;</span><br><span class=\"line\">    fwk := sched.Framework</span><br><span class=\"line\"></span><br><span class=\"line\">    pod := sched.NextPod()</span><br><span class=\"line\">    if pod == nil &#123;</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 1.判断 pod 是否处于删除状态</span><br><span class=\"line\">    if pod.DeletionTimestamp != nil &#123;</span><br><span class=\"line\">    \t......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">    // 2.执行调度策略选择 node</span><br><span class=\"line\">    start := time.Now()</span><br><span class=\"line\">    pluginContext := framework.NewPluginContext()</span><br><span class=\"line\">    scheduleResult, err := sched.schedule(pod, pluginContext)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        if fitError, ok := err.(*core.FitError); ok &#123;</span><br><span class=\"line\">            // 3.若启用抢占机制则执行</span><br><span class=\"line\">            if sched.DisablePreemption &#123;</span><br><span class=\"line\">            \t......</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                preemptionStartTime := time.Now()</span><br><span class=\"line\">                sched.preempt(pluginContext, fwk, pod, fitError)</span><br><span class=\"line\">                ......</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            ......</span><br><span class=\"line\">            metrics.PodScheduleFailures.Inc()</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            klog.Errorf(&quot;error selecting node for pod: %v&quot;, err)</span><br><span class=\"line\">            metrics.PodScheduleErrors.Inc()</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    assumedPod := pod.DeepCopy()</span><br><span class=\"line\"></span><br><span class=\"line\">    // 4.判断是否需要 VolumeScheduling 特性</span><br><span class=\"line\">    allBound, err := sched.assumeVolumes(assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        klog.Errorf(&quot;error assuming volumes: %v&quot;, err)</span><br><span class=\"line\">        metrics.PodScheduleErrors.Inc()</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 5.执行 &quot;reserve&quot; plugins</span><br><span class=\"line\">    if sts := fwk.RunReservePlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() &#123;</span><br><span class=\"line\">        .....</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 6.为 pod 设置 NodeName 字段，更新 scheduler 缓存</span><br><span class=\"line\">    err = sched.assume(assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        ......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 7.异步请求 apiserver</span><br><span class=\"line\">    go func() &#123;</span><br><span class=\"line\">        // Bind volumes first before Pod</span><br><span class=\"line\">        if !allBound &#123;</span><br><span class=\"line\">            err := sched.bindVolumes(assumedPod)</span><br><span class=\"line\">            if err != nil &#123;</span><br><span class=\"line\">\t\t\t\t......</span><br><span class=\"line\">                return</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        // 8.执行 &quot;permit&quot; plugins</span><br><span class=\"line\">        permitStatus := fwk.RunPermitPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">        if !permitStatus.IsSuccess() &#123;</span><br><span class=\"line\">\t\t\t......</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        // 9.执行 &quot;prebind&quot; plugins</span><br><span class=\"line\">        preBindStatus := fwk.RunPreBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">        if !preBindStatus.IsSuccess() &#123;</span><br><span class=\"line\">            ......</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        err := sched.bind(assumedPod, scheduleResult.SuggestedHost, pluginContext)</span><br><span class=\"line\">        ......</span><br><span class=\"line\">        if err != nil &#123;</span><br><span class=\"line\">            ......</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            ......</span><br><span class=\"line\">            // 10.执行 &quot;postbind&quot; plugins</span><br><span class=\"line\">            fwk.RunPostBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>scheduleOne()</code>  中通过调用 <code>sched.schedule()</code> 来执行预选与优选算法处理：</p>\n<p><code>k8s.io/kubernetes/pkg/scheduler/scheduler.go:337</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (sched *Scheduler) schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (core.ScheduleResult, error) &#123;</span><br><span class=\"line\">    result, err := sched.Algorithm.Schedule(pod, pluginContext)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">\t......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return result, err</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p><code>sched.Algorithm</code> 是一个 interface，主要包含四个方法，GenericScheduler 是其具体的实现：</p>\n<p><code>k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:131</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">type ScheduleAlgorithm interface &#123;</span><br><span class=\"line\">    Schedule(*v1.Pod, *framework.PluginContext) (scheduleResult ScheduleResult, err error)</span><br><span class=\"line\">    Preempt(*framework.PluginContext, *v1.Pod, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error)</span><br><span class=\"line\">    Predicates() map[string]predicates.FitPredicate</span><br><span class=\"line\">    Prioritizers() []priorities.PriorityConfig</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<ul>\n<li><code>Schedule()</code>：正常调度逻辑，包含预算与优选算法的执行</li>\n<li><code>Preempt()</code>：抢占策略，在 pod 调度发生失败的时候尝试抢占低优先级的 pod，函数返回发生抢占的 node，被 抢占的 pods 列表，nominated node name 需要被移除的 pods 列表以及 error</li>\n<li><code>Predicates()</code>：predicates 算法列表</li>\n<li><code>Prioritizers()</code>：prioritizers 算法列表</li>\n</ul>\n<p>kube-scheduler 提供的默认调度为 DefaultProvider，DefaultProvider 配置的 predicates 和 priorities policies 在 <code>k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults/defaults.go</code> 中定义，算法具体实现是在 <code>k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/</code> 和<code>k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/</code> 中，默认的算法如下所示：</p>\n<p><code>pkg/scheduler/algorithmprovider/defaults/defaults.go</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func defaultPredicates() sets.String &#123;</span><br><span class=\"line\">    return sets.NewString(</span><br><span class=\"line\">        predicates.NoVolumeZoneConflictPred,</span><br><span class=\"line\">        predicates.MaxEBSVolumeCountPred,</span><br><span class=\"line\">        predicates.MaxGCEPDVolumeCountPred,</span><br><span class=\"line\">        predicates.MaxAzureDiskVolumeCountPred,</span><br><span class=\"line\">        predicates.MaxCSIVolumeCountPred,</span><br><span class=\"line\">        predicates.MatchInterPodAffinityPred,</span><br><span class=\"line\">        predicates.NoDiskConflictPred,</span><br><span class=\"line\">        predicates.GeneralPred,</span><br><span class=\"line\">        predicates.CheckNodeMemoryPressurePred,</span><br><span class=\"line\">        predicates.CheckNodeDiskPressurePred,</span><br><span class=\"line\">        predicates.CheckNodePIDPressurePred,</span><br><span class=\"line\">        predicates.CheckNodeConditionPred,</span><br><span class=\"line\">        predicates.PodToleratesNodeTaintsPred,</span><br><span class=\"line\">        predicates.CheckVolumeBindingPred,</span><br><span class=\"line\">    )</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">func defaultPriorities() sets.String &#123;</span><br><span class=\"line\">    return sets.NewString(</span><br><span class=\"line\">        priorities.SelectorSpreadPriority,</span><br><span class=\"line\">        priorities.InterPodAffinityPriority,</span><br><span class=\"line\">        priorities.LeastRequestedPriority,</span><br><span class=\"line\">        priorities.BalancedResourceAllocation,</span><br><span class=\"line\">        priorities.NodePreferAvoidPodsPriority,</span><br><span class=\"line\">        priorities.NodeAffinityPriority,</span><br><span class=\"line\">        priorities.TaintTolerationPriority,</span><br><span class=\"line\">        priorities.ImageLocalityPriority,</span><br><span class=\"line\">    )</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>下面继续看 <code>sched.Algorithm.Schedule()</code> 调用具体调度算法的过程：</p>\n<ul>\n<li>检查 pod pvc 信息</li>\n<li>执行 prefilter plugins</li>\n<li>获取 scheduler cache 的快照，每次调度 pod 时都会获取一次快照</li>\n<li>执行 <code>g.findNodesThatFit()</code> 预选算法</li>\n<li>执行 postfilter plugin</li>\n<li>若 node  为 0 直接返回失败的 error，若 node 数为1 直接返回该 node</li>\n<li>执行 <code>g.priorityMetaProducer()</code> 获取 metaPrioritiesInterface，计算 pod 的metadata，检查该 node 上是否有相同 meta 的 pod</li>\n<li>执行 <code>PrioritizeNodes()</code> 算法</li>\n<li>执行 <code>g.selectHost()</code> 通过得分选择一个最佳的 node</li>\n</ul>\n<p><code>k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:186</code><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (g *genericScheduler) Schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (result ScheduleResult, err error) &#123;</span><br><span class=\"line\">    ......</span><br><span class=\"line\">    // 1.检查 pod pvc </span><br><span class=\"line\">    if err := podPassesBasicChecks(pod, g.pvcLister); err != nil &#123;</span><br><span class=\"line\">        return result, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 2.执行 &quot;prefilter&quot; plugins</span><br><span class=\"line\">    preFilterStatus := g.framework.RunPreFilterPlugins(pluginContext, pod)</span><br><span class=\"line\">    if !preFilterStatus.IsSuccess() &#123;</span><br><span class=\"line\">        return result, preFilterStatus.AsError()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 3.获取 node 数量</span><br><span class=\"line\">    numNodes := g.cache.NodeTree().NumNodes()</span><br><span class=\"line\">    if numNodes == 0 &#123;</span><br><span class=\"line\">        return result, ErrNoNodesAvailable</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 4.快照 node 信息</span><br><span class=\"line\">    if err := g.snapshot(); err != nil &#123;</span><br><span class=\"line\">        return result, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">    // 5.执行预选算法</span><br><span class=\"line\">    startPredicateEvalTime := time.Now()</span><br><span class=\"line\">    filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(pluginContext, pod)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return result, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    // 6.执行 &quot;postfilter&quot; plugins</span><br><span class=\"line\">    postfilterStatus := g.framework.RunPostFilterPlugins(pluginContext, pod, filteredNodes, filteredNodesStatuses)</span><br><span class=\"line\">    if !postfilterStatus.IsSuccess() &#123;</span><br><span class=\"line\">        return result, postfilterStatus.AsError()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 7.预选后没有合适的 node 直接返回</span><br><span class=\"line\">    if len(filteredNodes) == 0 &#123;</span><br><span class=\"line\">        ......</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    startPriorityEvalTime := time.Now()</span><br><span class=\"line\">    // 8.若只有一个 node 则直接返回该 node</span><br><span class=\"line\">    if len(filteredNodes) == 1 &#123;</span><br><span class=\"line\">        return ScheduleResult&#123;</span><br><span class=\"line\">            SuggestedHost:  filteredNodes[0].Name,</span><br><span class=\"line\">            EvaluatedNodes: 1 + len(failedPredicateMap),</span><br><span class=\"line\">            FeasibleNodes:  1,</span><br><span class=\"line\">        &#125;, nil</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 9.获取 pod meta 信息，执行优选算法</span><br><span class=\"line\">    metaPrioritiesInterface := g.priorityMetaProducer(pod, g.nodeInfoSnapshot.NodeInfoMap)</span><br><span class=\"line\">    priorityList, err := PrioritizeNodes(pod, g.nodeInfoSnapshot.NodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders, g.framework,      pluginContext)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return result, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 10.根据打分选择最佳的 node</span><br><span class=\"line\">    host, err := g.selectHost(priorityList)</span><br><span class=\"line\">    trace.Step(&quot;Selecting host done&quot;)</span><br><span class=\"line\">    return ScheduleResult&#123;</span><br><span class=\"line\">        SuggestedHost:  host,</span><br><span class=\"line\">        EvaluatedNodes: len(filteredNodes) + len(failedPredicateMap),</span><br><span class=\"line\">        FeasibleNodes:  len(filteredNodes),</span><br><span class=\"line\">    &#125;, err</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>至此，scheduler 的整个过程分析完毕。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要对于 kube-scheduler v1.16 的调度流程进行了分析，但其中有大量的细节都暂未提及，包括预选算法以及优选算法的具体实现、优先级与抢占调度的实现、framework 的使用及实现，因篇幅有限，部分内容会在后文继续说明。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md\" target=\"_blank\" rel=\"noopener\">The Kubernetes Scheduler</a></p>\n<p><a href=\"https://github.com/kubernetes/community/tree/master/contributors/design-proposals/scheduling\" target=\"_blank\" rel=\"noopener\">scheduling design proposals</a></p>\n"},{"title":"kubernetes 中 kubeconfig 的用法","date":"2019-01-09T11:28:30.000Z","type":"kubeconfig","_content":"\n用于配置集群访问信息的文件叫作 kubeconfig 文件，在开启了 TLS 的集群中，每次与集群交互时都需要身份认证，生产环境一般使用证书进行认证，其认证所需要的信息会放在 kubeconfig 文件中。此外，k8s 的组件都可以使用 kubeconfig 连接 apiserver，[client-go ](https://github.com/kubernetes/client-go/blob/master/examples/create-update-delete-deployment/main.go)、operator、helm 等其他组件也使用 kubeconfig 访问 apiserver。\n\n## 一、kubeconfig 配置文件的生成\n\nkubeconfig 的一个示例：\n```\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: xxx\n    server: https://xxx:6443\n  name: cluster1\n- cluster:\n    certificate-authority-data: xxx\n    server: https://xxx:6443\n  name: cluster2\ncontexts:\n- context:\n    cluster: cluster1\n    user: kubelet\n  name: cluster1-context\n- context:\n    cluster: cluster2\n    user: kubelet\n  name: cluster2-context\ncurrent-context: cluster1-context\nkind: Config\npreferences: {}\nusers:\n- name: kubelet\n  user:\n    client-certificate-data: xxx\n    client-key-data: xxx\n```\n\napiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。\n\n#### 1、clusters模块\ncluster中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构。\n\n可以使用 `kubectl config set-cluster` 添加或修改 cluster 条目。\n\n#### 2、users 模块\nuser 定义用于向 kubernetes 集群进行身份验证的客户端凭据。\n\n可用凭证有 `client-certificate、client-key、token 和 username/password`。 \n`username/password` 和 `token` 是二者只能选择一个，但 `client-certificate` 和 `client-key` 可以分别与它们组合。\n\n可以使用 `kubectl config set-credentials` 添加或者修改 user 条目。\n\n#### 3、contexts 模块\n\ncontext 定义了一个命名的`cluster、user、namespace`元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。\n\n三个都是可选的；\n仅使用 cluster、user、namespace 之一指定上下文，或指定`none`。 \n\n未指定的值或在加载的 kubeconfig 中没有相应条目的命名值将被替换为默认值。\n加载和合并 kubeconfig 文件的规则很简单，但有很多，具体可以查看[加载和合并kubeconfig规则](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/)。\n\n可以使用`kubectl config set-context`添加或修改上下文条目。\n\n#### 4、current-context 模块\n\ncurrent-context 是作为`cluster、user、namespace`元组的 key，\n当 kubectl 从该文件中加载配置的时候会被默认使用。\n\n可以在 kubectl 命令行里覆盖这些值，通过分别传入`--context=CONTEXT、--cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE`。\n以上示例中若不指定 context 则默认使用 cluster1-context。\n```\nkubectl  get node --kubeconfig=./kubeconfig --context=cluster2-context\n```\n可以使用 `kubectl config use-context` 更改 current-context。\n\n#### 5、kubectl 生成 kubeconfig 的示例\n\nkubectl 可以快速生成 kubeconfig，以下是一个示例：\n```\n$ kubectl config set-credentials myself --username=admin --password=secret\n$ kubectl config set-cluster local-server --server=http://localhost:8080\n$ kubectl config set-context default-context --cluster=local-server --user=myself\n$ kubectl config use-context cluster-context\n$ kubectl config set contexts.default-context.namespace the-right-prefix\n$ kubectl config view\n```\n\n若使用手写 kubeconfig 的方式，推荐一个工具 [kubeval](https://github.com/garethr/kubeval)，可以校验 kubernetes yaml 或 json 格式的配置文件是否正确。\n\n## 二、使用 kubeconfig 文件配置 kuebctl 跨集群认证\n\nkubectl 作为操作 k8s 的一个客户端工具，只要为 kubectl 提供连接 apiserver 的配置(kubeconfig)，kubectl 可以在任何地方操作该集群，当然，若 kubeconfig 文件中配置多个集群，kubectl 也可以轻松地在多个集群之间切换。\n\nkubectl 加载配置文件的顺序：\n1、kubectl 默认连接本机的 8080 端口\n2、从 $HOME/.kube 目录下查找文件名为 config 的文件\n3、通过设置环境变量 KUBECONFIG 或者通过设置去指定其它 kubeconfig 文件\n```\n# 设置 KUBECONFIG 的环境变量\nexport KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig\n# 指定 kubeconfig 文件\nkubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig\n# 使用不同的 context 在多个集群之间切换\nkubectl  get node --kubeconfig=./kubeconfig --context=cluster1-context\n```\n开篇的示例就是多集群认证方式配置的一种。\n\n参考：\nhttps://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\nhttps://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\n","source":"_posts/kubeconfig.md","raw":"---\ntitle: kubernetes 中 kubeconfig 的用法\ndate: 2019-01-09 19:28:30\ntags: \"kubeconfig\"\ntype: \"kubeconfig\"\n\n---\n\n用于配置集群访问信息的文件叫作 kubeconfig 文件，在开启了 TLS 的集群中，每次与集群交互时都需要身份认证，生产环境一般使用证书进行认证，其认证所需要的信息会放在 kubeconfig 文件中。此外，k8s 的组件都可以使用 kubeconfig 连接 apiserver，[client-go ](https://github.com/kubernetes/client-go/blob/master/examples/create-update-delete-deployment/main.go)、operator、helm 等其他组件也使用 kubeconfig 访问 apiserver。\n\n## 一、kubeconfig 配置文件的生成\n\nkubeconfig 的一个示例：\n```\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: xxx\n    server: https://xxx:6443\n  name: cluster1\n- cluster:\n    certificate-authority-data: xxx\n    server: https://xxx:6443\n  name: cluster2\ncontexts:\n- context:\n    cluster: cluster1\n    user: kubelet\n  name: cluster1-context\n- context:\n    cluster: cluster2\n    user: kubelet\n  name: cluster2-context\ncurrent-context: cluster1-context\nkind: Config\npreferences: {}\nusers:\n- name: kubelet\n  user:\n    client-certificate-data: xxx\n    client-key-data: xxx\n```\n\napiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。\n\n#### 1、clusters模块\ncluster中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构。\n\n可以使用 `kubectl config set-cluster` 添加或修改 cluster 条目。\n\n#### 2、users 模块\nuser 定义用于向 kubernetes 集群进行身份验证的客户端凭据。\n\n可用凭证有 `client-certificate、client-key、token 和 username/password`。 \n`username/password` 和 `token` 是二者只能选择一个，但 `client-certificate` 和 `client-key` 可以分别与它们组合。\n\n可以使用 `kubectl config set-credentials` 添加或者修改 user 条目。\n\n#### 3、contexts 模块\n\ncontext 定义了一个命名的`cluster、user、namespace`元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。\n\n三个都是可选的；\n仅使用 cluster、user、namespace 之一指定上下文，或指定`none`。 \n\n未指定的值或在加载的 kubeconfig 中没有相应条目的命名值将被替换为默认值。\n加载和合并 kubeconfig 文件的规则很简单，但有很多，具体可以查看[加载和合并kubeconfig规则](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/)。\n\n可以使用`kubectl config set-context`添加或修改上下文条目。\n\n#### 4、current-context 模块\n\ncurrent-context 是作为`cluster、user、namespace`元组的 key，\n当 kubectl 从该文件中加载配置的时候会被默认使用。\n\n可以在 kubectl 命令行里覆盖这些值，通过分别传入`--context=CONTEXT、--cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE`。\n以上示例中若不指定 context 则默认使用 cluster1-context。\n```\nkubectl  get node --kubeconfig=./kubeconfig --context=cluster2-context\n```\n可以使用 `kubectl config use-context` 更改 current-context。\n\n#### 5、kubectl 生成 kubeconfig 的示例\n\nkubectl 可以快速生成 kubeconfig，以下是一个示例：\n```\n$ kubectl config set-credentials myself --username=admin --password=secret\n$ kubectl config set-cluster local-server --server=http://localhost:8080\n$ kubectl config set-context default-context --cluster=local-server --user=myself\n$ kubectl config use-context cluster-context\n$ kubectl config set contexts.default-context.namespace the-right-prefix\n$ kubectl config view\n```\n\n若使用手写 kubeconfig 的方式，推荐一个工具 [kubeval](https://github.com/garethr/kubeval)，可以校验 kubernetes yaml 或 json 格式的配置文件是否正确。\n\n## 二、使用 kubeconfig 文件配置 kuebctl 跨集群认证\n\nkubectl 作为操作 k8s 的一个客户端工具，只要为 kubectl 提供连接 apiserver 的配置(kubeconfig)，kubectl 可以在任何地方操作该集群，当然，若 kubeconfig 文件中配置多个集群，kubectl 也可以轻松地在多个集群之间切换。\n\nkubectl 加载配置文件的顺序：\n1、kubectl 默认连接本机的 8080 端口\n2、从 $HOME/.kube 目录下查找文件名为 config 的文件\n3、通过设置环境变量 KUBECONFIG 或者通过设置去指定其它 kubeconfig 文件\n```\n# 设置 KUBECONFIG 的环境变量\nexport KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig\n# 指定 kubeconfig 文件\nkubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig\n# 使用不同的 context 在多个集群之间切换\nkubectl  get node --kubeconfig=./kubeconfig --context=cluster1-context\n```\n开篇的示例就是多集群认证方式配置的一种。\n\n参考：\nhttps://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\nhttps://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\n","slug":"kubeconfig","published":1,"updated":"2019-06-01T14:26:16.309Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59p0015apwng52xrzxh","content":"<p>用于配置集群访问信息的文件叫作 kubeconfig 文件，在开启了 TLS 的集群中，每次与集群交互时都需要身份认证，生产环境一般使用证书进行认证，其认证所需要的信息会放在 kubeconfig 文件中。此外，k8s 的组件都可以使用 kubeconfig 连接 apiserver，<a href=\"https://github.com/kubernetes/client-go/blob/master/examples/create-update-delete-deployment/main.go\" target=\"_blank\" rel=\"noopener\">client-go </a>、operator、helm 等其他组件也使用 kubeconfig 访问 apiserver。</p>\n<h2 id=\"一、kubeconfig-配置文件的生成\"><a href=\"#一、kubeconfig-配置文件的生成\" class=\"headerlink\" title=\"一、kubeconfig 配置文件的生成\"></a>一、kubeconfig 配置文件的生成</h2><p>kubeconfig 的一个示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    certificate-authority-data: xxx</span><br><span class=\"line\">    server: https://xxx:6443</span><br><span class=\"line\">  name: cluster1</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    certificate-authority-data: xxx</span><br><span class=\"line\">    server: https://xxx:6443</span><br><span class=\"line\">  name: cluster2</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: cluster1</span><br><span class=\"line\">    user: kubelet</span><br><span class=\"line\">  name: cluster1-context</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: cluster2</span><br><span class=\"line\">    user: kubelet</span><br><span class=\"line\">  name: cluster2-context</span><br><span class=\"line\">current-context: cluster1-context</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">users:</span><br><span class=\"line\">- name: kubelet</span><br><span class=\"line\">  user:</span><br><span class=\"line\">    client-certificate-data: xxx</span><br><span class=\"line\">    client-key-data: xxx</span><br></pre></td></tr></table></figure></p>\n<p>apiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。</p>\n<h4 id=\"1、clusters模块\"><a href=\"#1、clusters模块\" class=\"headerlink\" title=\"1、clusters模块\"></a>1、clusters模块</h4><p>cluster中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构。</p>\n<p>可以使用 <code>kubectl config set-cluster</code> 添加或修改 cluster 条目。</p>\n<h4 id=\"2、users-模块\"><a href=\"#2、users-模块\" class=\"headerlink\" title=\"2、users 模块\"></a>2、users 模块</h4><p>user 定义用于向 kubernetes 集群进行身份验证的客户端凭据。</p>\n<p>可用凭证有 <code>client-certificate、client-key、token 和 username/password</code>。<br><code>username/password</code> 和 <code>token</code> 是二者只能选择一个，但 <code>client-certificate</code> 和 <code>client-key</code> 可以分别与它们组合。</p>\n<p>可以使用 <code>kubectl config set-credentials</code> 添加或者修改 user 条目。</p>\n<h4 id=\"3、contexts-模块\"><a href=\"#3、contexts-模块\" class=\"headerlink\" title=\"3、contexts 模块\"></a>3、contexts 模块</h4><p>context 定义了一个命名的<code>cluster、user、namespace</code>元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。</p>\n<p>三个都是可选的；<br>仅使用 cluster、user、namespace 之一指定上下文，或指定<code>none</code>。 </p>\n<p>未指定的值或在加载的 kubeconfig 中没有相应条目的命名值将被替换为默认值。<br>加载和合并 kubeconfig 文件的规则很简单，但有很多，具体可以查看<a href=\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\" target=\"_blank\" rel=\"noopener\">加载和合并kubeconfig规则</a>。</p>\n<p>可以使用<code>kubectl config set-context</code>添加或修改上下文条目。</p>\n<h4 id=\"4、current-context-模块\"><a href=\"#4、current-context-模块\" class=\"headerlink\" title=\"4、current-context 模块\"></a>4、current-context 模块</h4><p>current-context 是作为<code>cluster、user、namespace</code>元组的 key，<br>当 kubectl 从该文件中加载配置的时候会被默认使用。</p>\n<p>可以在 kubectl 命令行里覆盖这些值，通过分别传入<code>--context=CONTEXT、--cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE</code>。<br>以上示例中若不指定 context 则默认使用 cluster1-context。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl  get node --kubeconfig=./kubeconfig --context=cluster2-context</span><br></pre></td></tr></table></figure></p>\n<p>可以使用 <code>kubectl config use-context</code> 更改 current-context。</p>\n<h4 id=\"5、kubectl-生成-kubeconfig-的示例\"><a href=\"#5、kubectl-生成-kubeconfig-的示例\" class=\"headerlink\" title=\"5、kubectl 生成 kubeconfig 的示例\"></a>5、kubectl 生成 kubeconfig 的示例</h4><p>kubectl 可以快速生成 kubeconfig，以下是一个示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl config set-credentials myself --username=admin --password=secret</span><br><span class=\"line\">$ kubectl config set-cluster local-server --server=http://localhost:8080</span><br><span class=\"line\">$ kubectl config set-context default-context --cluster=local-server --user=myself</span><br><span class=\"line\">$ kubectl config use-context cluster-context</span><br><span class=\"line\">$ kubectl config set contexts.default-context.namespace the-right-prefix</span><br><span class=\"line\">$ kubectl config view</span><br></pre></td></tr></table></figure></p>\n<p>若使用手写 kubeconfig 的方式，推荐一个工具 <a href=\"https://github.com/garethr/kubeval\" target=\"_blank\" rel=\"noopener\">kubeval</a>，可以校验 kubernetes yaml 或 json 格式的配置文件是否正确。</p>\n<h2 id=\"二、使用-kubeconfig-文件配置-kuebctl-跨集群认证\"><a href=\"#二、使用-kubeconfig-文件配置-kuebctl-跨集群认证\" class=\"headerlink\" title=\"二、使用 kubeconfig 文件配置 kuebctl 跨集群认证\"></a>二、使用 kubeconfig 文件配置 kuebctl 跨集群认证</h2><p>kubectl 作为操作 k8s 的一个客户端工具，只要为 kubectl 提供连接 apiserver 的配置(kubeconfig)，kubectl 可以在任何地方操作该集群，当然，若 kubeconfig 文件中配置多个集群，kubectl 也可以轻松地在多个集群之间切换。</p>\n<p>kubectl 加载配置文件的顺序：<br>1、kubectl 默认连接本机的 8080 端口<br>2、从 $HOME/.kube 目录下查找文件名为 config 的文件<br>3、通过设置环境变量 KUBECONFIG 或者通过设置去指定其它 kubeconfig 文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 设置 KUBECONFIG 的环境变量</span><br><span class=\"line\">export KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig</span><br><span class=\"line\"># 指定 kubeconfig 文件</span><br><span class=\"line\">kubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig</span><br><span class=\"line\"># 使用不同的 context 在多个集群之间切换</span><br><span class=\"line\">kubectl  get node --kubeconfig=./kubeconfig --context=cluster1-context</span><br></pre></td></tr></table></figure></p>\n<p>开篇的示例就是多集群认证方式配置的一种。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/</a><br><a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>用于配置集群访问信息的文件叫作 kubeconfig 文件，在开启了 TLS 的集群中，每次与集群交互时都需要身份认证，生产环境一般使用证书进行认证，其认证所需要的信息会放在 kubeconfig 文件中。此外，k8s 的组件都可以使用 kubeconfig 连接 apiserver，<a href=\"https://github.com/kubernetes/client-go/blob/master/examples/create-update-delete-deployment/main.go\" target=\"_blank\" rel=\"noopener\">client-go </a>、operator、helm 等其他组件也使用 kubeconfig 访问 apiserver。</p>\n<h2 id=\"一、kubeconfig-配置文件的生成\"><a href=\"#一、kubeconfig-配置文件的生成\" class=\"headerlink\" title=\"一、kubeconfig 配置文件的生成\"></a>一、kubeconfig 配置文件的生成</h2><p>kubeconfig 的一个示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">clusters:</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    certificate-authority-data: xxx</span><br><span class=\"line\">    server: https://xxx:6443</span><br><span class=\"line\">  name: cluster1</span><br><span class=\"line\">- cluster:</span><br><span class=\"line\">    certificate-authority-data: xxx</span><br><span class=\"line\">    server: https://xxx:6443</span><br><span class=\"line\">  name: cluster2</span><br><span class=\"line\">contexts:</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: cluster1</span><br><span class=\"line\">    user: kubelet</span><br><span class=\"line\">  name: cluster1-context</span><br><span class=\"line\">- context:</span><br><span class=\"line\">    cluster: cluster2</span><br><span class=\"line\">    user: kubelet</span><br><span class=\"line\">  name: cluster2-context</span><br><span class=\"line\">current-context: cluster1-context</span><br><span class=\"line\">kind: Config</span><br><span class=\"line\">preferences: &#123;&#125;</span><br><span class=\"line\">users:</span><br><span class=\"line\">- name: kubelet</span><br><span class=\"line\">  user:</span><br><span class=\"line\">    client-certificate-data: xxx</span><br><span class=\"line\">    client-key-data: xxx</span><br></pre></td></tr></table></figure></p>\n<p>apiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。</p>\n<h4 id=\"1、clusters模块\"><a href=\"#1、clusters模块\" class=\"headerlink\" title=\"1、clusters模块\"></a>1、clusters模块</h4><p>cluster中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构。</p>\n<p>可以使用 <code>kubectl config set-cluster</code> 添加或修改 cluster 条目。</p>\n<h4 id=\"2、users-模块\"><a href=\"#2、users-模块\" class=\"headerlink\" title=\"2、users 模块\"></a>2、users 模块</h4><p>user 定义用于向 kubernetes 集群进行身份验证的客户端凭据。</p>\n<p>可用凭证有 <code>client-certificate、client-key、token 和 username/password</code>。<br><code>username/password</code> 和 <code>token</code> 是二者只能选择一个，但 <code>client-certificate</code> 和 <code>client-key</code> 可以分别与它们组合。</p>\n<p>可以使用 <code>kubectl config set-credentials</code> 添加或者修改 user 条目。</p>\n<h4 id=\"3、contexts-模块\"><a href=\"#3、contexts-模块\" class=\"headerlink\" title=\"3、contexts 模块\"></a>3、contexts 模块</h4><p>context 定义了一个命名的<code>cluster、user、namespace</code>元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。</p>\n<p>三个都是可选的；<br>仅使用 cluster、user、namespace 之一指定上下文，或指定<code>none</code>。 </p>\n<p>未指定的值或在加载的 kubeconfig 中没有相应条目的命名值将被替换为默认值。<br>加载和合并 kubeconfig 文件的规则很简单，但有很多，具体可以查看<a href=\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\" target=\"_blank\" rel=\"noopener\">加载和合并kubeconfig规则</a>。</p>\n<p>可以使用<code>kubectl config set-context</code>添加或修改上下文条目。</p>\n<h4 id=\"4、current-context-模块\"><a href=\"#4、current-context-模块\" class=\"headerlink\" title=\"4、current-context 模块\"></a>4、current-context 模块</h4><p>current-context 是作为<code>cluster、user、namespace</code>元组的 key，<br>当 kubectl 从该文件中加载配置的时候会被默认使用。</p>\n<p>可以在 kubectl 命令行里覆盖这些值，通过分别传入<code>--context=CONTEXT、--cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE</code>。<br>以上示例中若不指定 context 则默认使用 cluster1-context。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl  get node --kubeconfig=./kubeconfig --context=cluster2-context</span><br></pre></td></tr></table></figure></p>\n<p>可以使用 <code>kubectl config use-context</code> 更改 current-context。</p>\n<h4 id=\"5、kubectl-生成-kubeconfig-的示例\"><a href=\"#5、kubectl-生成-kubeconfig-的示例\" class=\"headerlink\" title=\"5、kubectl 生成 kubeconfig 的示例\"></a>5、kubectl 生成 kubeconfig 的示例</h4><p>kubectl 可以快速生成 kubeconfig，以下是一个示例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl config set-credentials myself --username=admin --password=secret</span><br><span class=\"line\">$ kubectl config set-cluster local-server --server=http://localhost:8080</span><br><span class=\"line\">$ kubectl config set-context default-context --cluster=local-server --user=myself</span><br><span class=\"line\">$ kubectl config use-context cluster-context</span><br><span class=\"line\">$ kubectl config set contexts.default-context.namespace the-right-prefix</span><br><span class=\"line\">$ kubectl config view</span><br></pre></td></tr></table></figure></p>\n<p>若使用手写 kubeconfig 的方式，推荐一个工具 <a href=\"https://github.com/garethr/kubeval\" target=\"_blank\" rel=\"noopener\">kubeval</a>，可以校验 kubernetes yaml 或 json 格式的配置文件是否正确。</p>\n<h2 id=\"二、使用-kubeconfig-文件配置-kuebctl-跨集群认证\"><a href=\"#二、使用-kubeconfig-文件配置-kuebctl-跨集群认证\" class=\"headerlink\" title=\"二、使用 kubeconfig 文件配置 kuebctl 跨集群认证\"></a>二、使用 kubeconfig 文件配置 kuebctl 跨集群认证</h2><p>kubectl 作为操作 k8s 的一个客户端工具，只要为 kubectl 提供连接 apiserver 的配置(kubeconfig)，kubectl 可以在任何地方操作该集群，当然，若 kubeconfig 文件中配置多个集群，kubectl 也可以轻松地在多个集群之间切换。</p>\n<p>kubectl 加载配置文件的顺序：<br>1、kubectl 默认连接本机的 8080 端口<br>2、从 $HOME/.kube 目录下查找文件名为 config 的文件<br>3、通过设置环境变量 KUBECONFIG 或者通过设置去指定其它 kubeconfig 文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 设置 KUBECONFIG 的环境变量</span><br><span class=\"line\">export KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig</span><br><span class=\"line\"># 指定 kubeconfig 文件</span><br><span class=\"line\">kubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig</span><br><span class=\"line\"># 使用不同的 context 在多个集群之间切换</span><br><span class=\"line\">kubectl  get node --kubeconfig=./kubeconfig --context=cluster1-context</span><br></pre></td></tr></table></figure></p>\n<p>开篇的示例就是多集群认证方式配置的一种。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/</a><br><a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/</a></p>\n"},{"title":"使用插件扩展 kubectl","date":"2019-05-16T03:20:30.000Z","type":"kubectl plugin","_content":"由于笔者所维护的集群规模较大，经常需要使用 kubectl 来排查一些问题，但是 kubectl 功能有限，有些操作还是需要写一个脚本对 kubectl 做一些封装才能达到目的。比如我经常做的一个操作就是排查一下线上哪些宿主的 cpu/memory request 使用率超过某个阈值，kubectl 并不能直接看到一个 master 下所有宿主的 request 使用率，但可以使用 `kubectl describe node xxx`查看某个宿主机的 request 使用率，所以只好写一个脚本来扫一遍了。\n\n```\n#!/bin/bash\n\necho -e \"node\\tcpu_requets  memory_requets\"\nfor i in `kubectl get node | grep -v NAME | awk '{print $1}'`;do\n    res=$(kubectl describe node $i  | grep -A 3 \"Resource\")\n    cpu_requets=$(echo ${res} | awk '{print $9}' | awk -F '%' '{print $1}' | awk -F '(' '{print $2}')\n    memory_requets=$(echo ${res} | awk '{print $14}' | awk -F '%' '{print $1}' | awk -F '(' '{print $2}')\n    echo -e \"$i\\t${cpu_requets} \\t${memory_requets}\"\ndone\n```\n\n类似的需求比较多，此处不一一列举，这种操作经常需要做，虽然写一个脚本也能完全搞定，但确实比较 low，也不便提供给别人使用，基于此了解到目前官方对 kubectl 的插件机制做了一些改进，对 kubectl 的扩展也比较容易，所以下文会带你了解一下 kubectl 的扩展功能。\n\n\n\n#### 一、编写 kubectl 插件\n\nkubectl 命令从 `v1.8.0` 版本开始支持插件机制，之后的版本中我们都可以对 `kubectl` 命令进行扩展，kubernetes 在 `v1.12` 以后插件可以直接是以 kubectl- 开头命令的一个二进制文件，插件机制在 `v1.14` 进入 GA 状态，这种改进是希望用户以二进制文件形式可以扩展自己的 kubectl 子命令。当然，kubectl 插件机制是与语言无关的，也就是说你可以用任何语言编写插件。\n\n\n\n如 [kubernetes 官方文档](https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/)中描述，只要将二进制文件放在系统 PATH 下，kubectl 即可识别，二进制文件类似 `kubectl-foo-bar`，并且在使用时 kubectl 会匹配最长的二进制文件。\n\n官方建议使用  [k8s.io/cli-runtime](https://github.com/kubernetes/cli-runtime) 库进行编写，若你的插件需要支持一些命令行参数，可以参考使用，官方也给了一个例子 [sample-cli-plugin](https://github.com/kubernetes/sample-cli-plugin)。\n\n\n\n还是回到最初的问题，对于获取一个集群写所有 node 的资源使用率，笔者基于也编写了一个简单的插件。\n\n```\n// 安装插件\n\n$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o bin/kubectl-view-node-resource cmd/view-node-resource/main.go\n\n$ mv bin/kubectl-view-node-resource /usr/bin/   \n```\n\n\n\n使用 `kubectl plugin list` 查看 PATH 下有哪些可用的插件。\n\n```\n// 查看插件\n\n$ kubectl plugin list\nThe following kubectl-compatible plugins are available:\n\n/usr/bin/kubectl-view-node-resource\n```\n\n```\n// 使用插件\n\n$ kubectl view node taints --help\nA longer description that spans multiple lines and likely contains\nexamples and usage of using your application. For example:\nCobra is a CLI library for Go that empowers applications.\nThis application is a tool to generate the needed files\nto quickly create a Cobra application.\n\nUsage:\n  view-node-taints [flags]\n\nFlags:\n      --config string   config file (default is $HOME/.view-node-taints.yaml)\n  -h, --help            help for view-node-taints\n  -t, --toggle          Help message for toggle\n\n\n$ kubectl view node resource\n Name            PodCount  CPURequests  MemoryRequests  CPULimits     MemoryLimits\n 192.168.1.110   4         0 (0.00%)    6.4 (41.26%)    8 (100.00%)   16.0 (103.14%)\n```\n\n\n\n此外，还开发一个查看集群下所有 node taints 的插件，kubectl 支持查看宿主的 label，但是没有直接查看所有宿主 taints 的命令，插件效果如下：\n\n```\n$ kubectl view node taints\n Name            Status                       Age   Version                         Taints\n 192.168.1.110   Ready,SchedulingDisabled     49d   v1.8.1-35+9406f9d9909c61-dirty  enabledDiskSchedule=true:NoSchedule\n```\n\n> 插件代码地址：[kubectl-plugin](https://github.com/gosoon/kubectl-plugin)\n\n\n\n#### 二、kubectl 插件管理工具 krew\n\n上文讲了如何编写一个插件，但是官方也提供一个插件库并提供了一个插件管理工具 [krew](https://github.com/kubernetes-sigs/krew)  ，[krew](https://github.com/kubernetes-sigs/krew) 是 kubectl 插件的管理器，使用 krew 可以轻松的查找、安装和管理 kubectl 插件，它类似于 yum、apt、 dnf，krew 也可以帮助你将已写好的插件在多个平台上打包和分发，krew 自己也作为一个 kubectl 插件存在。\n\n> krew 仅支持在 v1.12 及之后的版本中使用。\n\n\n\n1、安装 krew\n\n```\n$ (\n  set -x; cd \"$(mktemp -d)\" &&\n  curl -fsSLO \"https://storage.googleapis.com/krew/v0.2.1/krew.{tar.gz,yaml}\" &&\n  tar zxvf krew.tar.gz &&\n  ./krew-\"$(uname | tr '[:upper:]' '[:lower:]')_amd64\" install \\\n    --manifest=krew.yaml --archive=krew.tar.gz\n)\n\n$ export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n```\n\n2、krew 的使用\n\n```\n$ kubectl krew search               \t\t\t\t\t\t\t\t# show all plugins\n$ kubectl krew install view-secret  \t\t\t\t\t\t\t\t# install a plugin named \"view-secret\"\n$ kubectl view-secret default-token-4cwvh           # use the plugin\n$ kubectl krew upgrade              \t\t\t\t\t\t\t\t# upgrade installed plugins\n$ kubectl krew remove view-secret   \t\t\t\t\t\t\t\t# uninstall a plugin\n```\n\n若想让你自己的插件加入到 krew 的索引中，可以参考：[how to package and publish a plugin for krew](https://github.com/kubernetes-sigs/krew/blob/master/docs/DEVELOPER_GUIDE.md)。\n\n\n\n参考：\n\n[kubectl 插件命明规范](https://github.com/kubernetes-sigs/krew/blob/master/docs/NAMING_GUIDE.md)\n\nhttps://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\n\nhttps://github.com/gosoon/kubectl-plugin\n","source":"_posts/kubectl_plugin.md","raw":"---\ntitle: 使用插件扩展 kubectl\ndate: 2019-05-16 11:20:30\ntags: \"kubectl plugin\"\ntype: \"kubectl plugin\"\n\n---\n由于笔者所维护的集群规模较大，经常需要使用 kubectl 来排查一些问题，但是 kubectl 功能有限，有些操作还是需要写一个脚本对 kubectl 做一些封装才能达到目的。比如我经常做的一个操作就是排查一下线上哪些宿主的 cpu/memory request 使用率超过某个阈值，kubectl 并不能直接看到一个 master 下所有宿主的 request 使用率，但可以使用 `kubectl describe node xxx`查看某个宿主机的 request 使用率，所以只好写一个脚本来扫一遍了。\n\n```\n#!/bin/bash\n\necho -e \"node\\tcpu_requets  memory_requets\"\nfor i in `kubectl get node | grep -v NAME | awk '{print $1}'`;do\n    res=$(kubectl describe node $i  | grep -A 3 \"Resource\")\n    cpu_requets=$(echo ${res} | awk '{print $9}' | awk -F '%' '{print $1}' | awk -F '(' '{print $2}')\n    memory_requets=$(echo ${res} | awk '{print $14}' | awk -F '%' '{print $1}' | awk -F '(' '{print $2}')\n    echo -e \"$i\\t${cpu_requets} \\t${memory_requets}\"\ndone\n```\n\n类似的需求比较多，此处不一一列举，这种操作经常需要做，虽然写一个脚本也能完全搞定，但确实比较 low，也不便提供给别人使用，基于此了解到目前官方对 kubectl 的插件机制做了一些改进，对 kubectl 的扩展也比较容易，所以下文会带你了解一下 kubectl 的扩展功能。\n\n\n\n#### 一、编写 kubectl 插件\n\nkubectl 命令从 `v1.8.0` 版本开始支持插件机制，之后的版本中我们都可以对 `kubectl` 命令进行扩展，kubernetes 在 `v1.12` 以后插件可以直接是以 kubectl- 开头命令的一个二进制文件，插件机制在 `v1.14` 进入 GA 状态，这种改进是希望用户以二进制文件形式可以扩展自己的 kubectl 子命令。当然，kubectl 插件机制是与语言无关的，也就是说你可以用任何语言编写插件。\n\n\n\n如 [kubernetes 官方文档](https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/)中描述，只要将二进制文件放在系统 PATH 下，kubectl 即可识别，二进制文件类似 `kubectl-foo-bar`，并且在使用时 kubectl 会匹配最长的二进制文件。\n\n官方建议使用  [k8s.io/cli-runtime](https://github.com/kubernetes/cli-runtime) 库进行编写，若你的插件需要支持一些命令行参数，可以参考使用，官方也给了一个例子 [sample-cli-plugin](https://github.com/kubernetes/sample-cli-plugin)。\n\n\n\n还是回到最初的问题，对于获取一个集群写所有 node 的资源使用率，笔者基于也编写了一个简单的插件。\n\n```\n// 安装插件\n\n$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o bin/kubectl-view-node-resource cmd/view-node-resource/main.go\n\n$ mv bin/kubectl-view-node-resource /usr/bin/   \n```\n\n\n\n使用 `kubectl plugin list` 查看 PATH 下有哪些可用的插件。\n\n```\n// 查看插件\n\n$ kubectl plugin list\nThe following kubectl-compatible plugins are available:\n\n/usr/bin/kubectl-view-node-resource\n```\n\n```\n// 使用插件\n\n$ kubectl view node taints --help\nA longer description that spans multiple lines and likely contains\nexamples and usage of using your application. For example:\nCobra is a CLI library for Go that empowers applications.\nThis application is a tool to generate the needed files\nto quickly create a Cobra application.\n\nUsage:\n  view-node-taints [flags]\n\nFlags:\n      --config string   config file (default is $HOME/.view-node-taints.yaml)\n  -h, --help            help for view-node-taints\n  -t, --toggle          Help message for toggle\n\n\n$ kubectl view node resource\n Name            PodCount  CPURequests  MemoryRequests  CPULimits     MemoryLimits\n 192.168.1.110   4         0 (0.00%)    6.4 (41.26%)    8 (100.00%)   16.0 (103.14%)\n```\n\n\n\n此外，还开发一个查看集群下所有 node taints 的插件，kubectl 支持查看宿主的 label，但是没有直接查看所有宿主 taints 的命令，插件效果如下：\n\n```\n$ kubectl view node taints\n Name            Status                       Age   Version                         Taints\n 192.168.1.110   Ready,SchedulingDisabled     49d   v1.8.1-35+9406f9d9909c61-dirty  enabledDiskSchedule=true:NoSchedule\n```\n\n> 插件代码地址：[kubectl-plugin](https://github.com/gosoon/kubectl-plugin)\n\n\n\n#### 二、kubectl 插件管理工具 krew\n\n上文讲了如何编写一个插件，但是官方也提供一个插件库并提供了一个插件管理工具 [krew](https://github.com/kubernetes-sigs/krew)  ，[krew](https://github.com/kubernetes-sigs/krew) 是 kubectl 插件的管理器，使用 krew 可以轻松的查找、安装和管理 kubectl 插件，它类似于 yum、apt、 dnf，krew 也可以帮助你将已写好的插件在多个平台上打包和分发，krew 自己也作为一个 kubectl 插件存在。\n\n> krew 仅支持在 v1.12 及之后的版本中使用。\n\n\n\n1、安装 krew\n\n```\n$ (\n  set -x; cd \"$(mktemp -d)\" &&\n  curl -fsSLO \"https://storage.googleapis.com/krew/v0.2.1/krew.{tar.gz,yaml}\" &&\n  tar zxvf krew.tar.gz &&\n  ./krew-\"$(uname | tr '[:upper:]' '[:lower:]')_amd64\" install \\\n    --manifest=krew.yaml --archive=krew.tar.gz\n)\n\n$ export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n```\n\n2、krew 的使用\n\n```\n$ kubectl krew search               \t\t\t\t\t\t\t\t# show all plugins\n$ kubectl krew install view-secret  \t\t\t\t\t\t\t\t# install a plugin named \"view-secret\"\n$ kubectl view-secret default-token-4cwvh           # use the plugin\n$ kubectl krew upgrade              \t\t\t\t\t\t\t\t# upgrade installed plugins\n$ kubectl krew remove view-secret   \t\t\t\t\t\t\t\t# uninstall a plugin\n```\n\n若想让你自己的插件加入到 krew 的索引中，可以参考：[how to package and publish a plugin for krew](https://github.com/kubernetes-sigs/krew/blob/master/docs/DEVELOPER_GUIDE.md)。\n\n\n\n参考：\n\n[kubectl 插件命明规范](https://github.com/kubernetes-sigs/krew/blob/master/docs/NAMING_GUIDE.md)\n\nhttps://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\n\nhttps://github.com/gosoon/kubectl-plugin\n","slug":"kubectl_plugin","published":1,"updated":"2019-06-01T14:26:16.309Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59q0017apwnxjy2qojx","content":"<p>由于笔者所维护的集群规模较大，经常需要使用 kubectl 来排查一些问题，但是 kubectl 功能有限，有些操作还是需要写一个脚本对 kubectl 做一些封装才能达到目的。比如我经常做的一个操作就是排查一下线上哪些宿主的 cpu/memory request 使用率超过某个阈值，kubectl 并不能直接看到一个 master 下所有宿主的 request 使用率，但可以使用 <code>kubectl describe node xxx</code>查看某个宿主机的 request 使用率，所以只好写一个脚本来扫一遍了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">echo -e &quot;node\\tcpu_requets  memory_requets&quot;</span><br><span class=\"line\">for i in `kubectl get node | grep -v NAME | awk &apos;&#123;print $1&#125;&apos;`;do</span><br><span class=\"line\">    res=$(kubectl describe node $i  | grep -A 3 &quot;Resource&quot;)</span><br><span class=\"line\">    cpu_requets=$(echo $&#123;res&#125; | awk &apos;&#123;print $9&#125;&apos; | awk -F &apos;%&apos; &apos;&#123;print $1&#125;&apos; | awk -F &apos;(&apos; &apos;&#123;print $2&#125;&apos;)</span><br><span class=\"line\">    memory_requets=$(echo $&#123;res&#125; | awk &apos;&#123;print $14&#125;&apos; | awk -F &apos;%&apos; &apos;&#123;print $1&#125;&apos; | awk -F &apos;(&apos; &apos;&#123;print $2&#125;&apos;)</span><br><span class=\"line\">    echo -e &quot;$i\\t$&#123;cpu_requets&#125; \\t$&#123;memory_requets&#125;&quot;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>类似的需求比较多，此处不一一列举，这种操作经常需要做，虽然写一个脚本也能完全搞定，但确实比较 low，也不便提供给别人使用，基于此了解到目前官方对 kubectl 的插件机制做了一些改进，对 kubectl 的扩展也比较容易，所以下文会带你了解一下 kubectl 的扩展功能。</p>\n<h4 id=\"一、编写-kubectl-插件\"><a href=\"#一、编写-kubectl-插件\" class=\"headerlink\" title=\"一、编写 kubectl 插件\"></a>一、编写 kubectl 插件</h4><p>kubectl 命令从 <code>v1.8.0</code> 版本开始支持插件机制，之后的版本中我们都可以对 <code>kubectl</code> 命令进行扩展，kubernetes 在 <code>v1.12</code> 以后插件可以直接是以 kubectl- 开头命令的一个二进制文件，插件机制在 <code>v1.14</code> 进入 GA 状态，这种改进是希望用户以二进制文件形式可以扩展自己的 kubectl 子命令。当然，kubectl 插件机制是与语言无关的，也就是说你可以用任何语言编写插件。</p>\n<p>如 <a href=\"https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\" target=\"_blank\" rel=\"noopener\">kubernetes 官方文档</a>中描述，只要将二进制文件放在系统 PATH 下，kubectl 即可识别，二进制文件类似 <code>kubectl-foo-bar</code>，并且在使用时 kubectl 会匹配最长的二进制文件。</p>\n<p>官方建议使用  <a href=\"https://github.com/kubernetes/cli-runtime\" target=\"_blank\" rel=\"noopener\">k8s.io/cli-runtime</a> 库进行编写，若你的插件需要支持一些命令行参数，可以参考使用，官方也给了一个例子 <a href=\"https://github.com/kubernetes/sample-cli-plugin\" target=\"_blank\" rel=\"noopener\">sample-cli-plugin</a>。</p>\n<p>还是回到最初的问题，对于获取一个集群写所有 node 的资源使用率，笔者基于也编写了一个简单的插件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 安装插件</span><br><span class=\"line\"></span><br><span class=\"line\">$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o bin/kubectl-view-node-resource cmd/view-node-resource/main.go</span><br><span class=\"line\"></span><br><span class=\"line\">$ mv bin/kubectl-view-node-resource /usr/bin/</span><br></pre></td></tr></table></figure>\n<p>使用 <code>kubectl plugin list</code> 查看 PATH 下有哪些可用的插件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看插件</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl plugin list</span><br><span class=\"line\">The following kubectl-compatible plugins are available:</span><br><span class=\"line\"></span><br><span class=\"line\">/usr/bin/kubectl-view-node-resource</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 使用插件</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl view node taints --help</span><br><span class=\"line\">A longer description that spans multiple lines and likely contains</span><br><span class=\"line\">examples and usage of using your application. For example:</span><br><span class=\"line\">Cobra is a CLI library for Go that empowers applications.</span><br><span class=\"line\">This application is a tool to generate the needed files</span><br><span class=\"line\">to quickly create a Cobra application.</span><br><span class=\"line\"></span><br><span class=\"line\">Usage:</span><br><span class=\"line\">  view-node-taints [flags]</span><br><span class=\"line\"></span><br><span class=\"line\">Flags:</span><br><span class=\"line\">      --config string   config file (default is $HOME/.view-node-taints.yaml)</span><br><span class=\"line\">  -h, --help            help for view-node-taints</span><br><span class=\"line\">  -t, --toggle          Help message for toggle</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl view node resource</span><br><span class=\"line\"> Name            PodCount  CPURequests  MemoryRequests  CPULimits     MemoryLimits</span><br><span class=\"line\"> 192.168.1.110   4         0 (0.00%)    6.4 (41.26%)    8 (100.00%)   16.0 (103.14%)</span><br></pre></td></tr></table></figure>\n<p>此外，还开发一个查看集群下所有 node taints 的插件，kubectl 支持查看宿主的 label，但是没有直接查看所有宿主 taints 的命令，插件效果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl view node taints</span><br><span class=\"line\"> Name            Status                       Age   Version                         Taints</span><br><span class=\"line\"> 192.168.1.110   Ready,SchedulingDisabled     49d   v1.8.1-35+9406f9d9909c61-dirty  enabledDiskSchedule=true:NoSchedule</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>插件代码地址：<a href=\"https://github.com/gosoon/kubectl-plugin\" target=\"_blank\" rel=\"noopener\">kubectl-plugin</a></p>\n</blockquote>\n<h4 id=\"二、kubectl-插件管理工具-krew\"><a href=\"#二、kubectl-插件管理工具-krew\" class=\"headerlink\" title=\"二、kubectl 插件管理工具 krew\"></a>二、kubectl 插件管理工具 krew</h4><p>上文讲了如何编写一个插件，但是官方也提供一个插件库并提供了一个插件管理工具 <a href=\"https://github.com/kubernetes-sigs/krew\" target=\"_blank\" rel=\"noopener\">krew</a>  ，<a href=\"https://github.com/kubernetes-sigs/krew\" target=\"_blank\" rel=\"noopener\">krew</a> 是 kubectl 插件的管理器，使用 krew 可以轻松的查找、安装和管理 kubectl 插件，它类似于 yum、apt、 dnf，krew 也可以帮助你将已写好的插件在多个平台上打包和分发，krew 自己也作为一个 kubectl 插件存在。</p>\n<blockquote>\n<p>krew 仅支持在 v1.12 及之后的版本中使用。</p>\n</blockquote>\n<p>1、安装 krew</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ (</span><br><span class=\"line\">  set -x; cd &quot;$(mktemp -d)&quot; &amp;&amp;</span><br><span class=\"line\">  curl -fsSLO &quot;https://storage.googleapis.com/krew/v0.2.1/krew.&#123;tar.gz,yaml&#125;&quot; &amp;&amp;</span><br><span class=\"line\">  tar zxvf krew.tar.gz &amp;&amp;</span><br><span class=\"line\">  ./krew-&quot;$(uname | tr &apos;[:upper:]&apos; &apos;[:lower:]&apos;)_amd64&quot; install \\</span><br><span class=\"line\">    --manifest=krew.yaml --archive=krew.tar.gz</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">$ export PATH=&quot;$&#123;KREW_ROOT:-$HOME/.krew&#125;/bin:$PATH&quot;</span><br></pre></td></tr></table></figure>\n<p>2、krew 的使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl krew search               \t\t\t\t\t\t\t\t# show all plugins</span><br><span class=\"line\">$ kubectl krew install view-secret  \t\t\t\t\t\t\t\t# install a plugin named &quot;view-secret&quot;</span><br><span class=\"line\">$ kubectl view-secret default-token-4cwvh           # use the plugin</span><br><span class=\"line\">$ kubectl krew upgrade              \t\t\t\t\t\t\t\t# upgrade installed plugins</span><br><span class=\"line\">$ kubectl krew remove view-secret   \t\t\t\t\t\t\t\t# uninstall a plugin</span><br></pre></td></tr></table></figure>\n<p>若想让你自己的插件加入到 krew 的索引中，可以参考：<a href=\"https://github.com/kubernetes-sigs/krew/blob/master/docs/DEVELOPER_GUIDE.md\" target=\"_blank\" rel=\"noopener\">how to package and publish a plugin for krew</a>。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes-sigs/krew/blob/master/docs/NAMING_GUIDE.md\" target=\"_blank\" rel=\"noopener\">kubectl 插件命明规范</a></p>\n<p><a href=\"https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/</a></p>\n<p><a href=\"https://github.com/gosoon/kubectl-plugin\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubectl-plugin</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>由于笔者所维护的集群规模较大，经常需要使用 kubectl 来排查一些问题，但是 kubectl 功能有限，有些操作还是需要写一个脚本对 kubectl 做一些封装才能达到目的。比如我经常做的一个操作就是排查一下线上哪些宿主的 cpu/memory request 使用率超过某个阈值，kubectl 并不能直接看到一个 master 下所有宿主的 request 使用率，但可以使用 <code>kubectl describe node xxx</code>查看某个宿主机的 request 使用率，所以只好写一个脚本来扫一遍了。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">#!/bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">echo -e &quot;node\\tcpu_requets  memory_requets&quot;</span><br><span class=\"line\">for i in `kubectl get node | grep -v NAME | awk &apos;&#123;print $1&#125;&apos;`;do</span><br><span class=\"line\">    res=$(kubectl describe node $i  | grep -A 3 &quot;Resource&quot;)</span><br><span class=\"line\">    cpu_requets=$(echo $&#123;res&#125; | awk &apos;&#123;print $9&#125;&apos; | awk -F &apos;%&apos; &apos;&#123;print $1&#125;&apos; | awk -F &apos;(&apos; &apos;&#123;print $2&#125;&apos;)</span><br><span class=\"line\">    memory_requets=$(echo $&#123;res&#125; | awk &apos;&#123;print $14&#125;&apos; | awk -F &apos;%&apos; &apos;&#123;print $1&#125;&apos; | awk -F &apos;(&apos; &apos;&#123;print $2&#125;&apos;)</span><br><span class=\"line\">    echo -e &quot;$i\\t$&#123;cpu_requets&#125; \\t$&#123;memory_requets&#125;&quot;</span><br><span class=\"line\">done</span><br></pre></td></tr></table></figure>\n<p>类似的需求比较多，此处不一一列举，这种操作经常需要做，虽然写一个脚本也能完全搞定，但确实比较 low，也不便提供给别人使用，基于此了解到目前官方对 kubectl 的插件机制做了一些改进，对 kubectl 的扩展也比较容易，所以下文会带你了解一下 kubectl 的扩展功能。</p>\n<h4 id=\"一、编写-kubectl-插件\"><a href=\"#一、编写-kubectl-插件\" class=\"headerlink\" title=\"一、编写 kubectl 插件\"></a>一、编写 kubectl 插件</h4><p>kubectl 命令从 <code>v1.8.0</code> 版本开始支持插件机制，之后的版本中我们都可以对 <code>kubectl</code> 命令进行扩展，kubernetes 在 <code>v1.12</code> 以后插件可以直接是以 kubectl- 开头命令的一个二进制文件，插件机制在 <code>v1.14</code> 进入 GA 状态，这种改进是希望用户以二进制文件形式可以扩展自己的 kubectl 子命令。当然，kubectl 插件机制是与语言无关的，也就是说你可以用任何语言编写插件。</p>\n<p>如 <a href=\"https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\" target=\"_blank\" rel=\"noopener\">kubernetes 官方文档</a>中描述，只要将二进制文件放在系统 PATH 下，kubectl 即可识别，二进制文件类似 <code>kubectl-foo-bar</code>，并且在使用时 kubectl 会匹配最长的二进制文件。</p>\n<p>官方建议使用  <a href=\"https://github.com/kubernetes/cli-runtime\" target=\"_blank\" rel=\"noopener\">k8s.io/cli-runtime</a> 库进行编写，若你的插件需要支持一些命令行参数，可以参考使用，官方也给了一个例子 <a href=\"https://github.com/kubernetes/sample-cli-plugin\" target=\"_blank\" rel=\"noopener\">sample-cli-plugin</a>。</p>\n<p>还是回到最初的问题，对于获取一个集群写所有 node 的资源使用率，笔者基于也编写了一个简单的插件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 安装插件</span><br><span class=\"line\"></span><br><span class=\"line\">$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o bin/kubectl-view-node-resource cmd/view-node-resource/main.go</span><br><span class=\"line\"></span><br><span class=\"line\">$ mv bin/kubectl-view-node-resource /usr/bin/</span><br></pre></td></tr></table></figure>\n<p>使用 <code>kubectl plugin list</code> 查看 PATH 下有哪些可用的插件。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 查看插件</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl plugin list</span><br><span class=\"line\">The following kubectl-compatible plugins are available:</span><br><span class=\"line\"></span><br><span class=\"line\">/usr/bin/kubectl-view-node-resource</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 使用插件</span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl view node taints --help</span><br><span class=\"line\">A longer description that spans multiple lines and likely contains</span><br><span class=\"line\">examples and usage of using your application. For example:</span><br><span class=\"line\">Cobra is a CLI library for Go that empowers applications.</span><br><span class=\"line\">This application is a tool to generate the needed files</span><br><span class=\"line\">to quickly create a Cobra application.</span><br><span class=\"line\"></span><br><span class=\"line\">Usage:</span><br><span class=\"line\">  view-node-taints [flags]</span><br><span class=\"line\"></span><br><span class=\"line\">Flags:</span><br><span class=\"line\">      --config string   config file (default is $HOME/.view-node-taints.yaml)</span><br><span class=\"line\">  -h, --help            help for view-node-taints</span><br><span class=\"line\">  -t, --toggle          Help message for toggle</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">$ kubectl view node resource</span><br><span class=\"line\"> Name            PodCount  CPURequests  MemoryRequests  CPULimits     MemoryLimits</span><br><span class=\"line\"> 192.168.1.110   4         0 (0.00%)    6.4 (41.26%)    8 (100.00%)   16.0 (103.14%)</span><br></pre></td></tr></table></figure>\n<p>此外，还开发一个查看集群下所有 node taints 的插件，kubectl 支持查看宿主的 label，但是没有直接查看所有宿主 taints 的命令，插件效果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl view node taints</span><br><span class=\"line\"> Name            Status                       Age   Version                         Taints</span><br><span class=\"line\"> 192.168.1.110   Ready,SchedulingDisabled     49d   v1.8.1-35+9406f9d9909c61-dirty  enabledDiskSchedule=true:NoSchedule</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>插件代码地址：<a href=\"https://github.com/gosoon/kubectl-plugin\" target=\"_blank\" rel=\"noopener\">kubectl-plugin</a></p>\n</blockquote>\n<h4 id=\"二、kubectl-插件管理工具-krew\"><a href=\"#二、kubectl-插件管理工具-krew\" class=\"headerlink\" title=\"二、kubectl 插件管理工具 krew\"></a>二、kubectl 插件管理工具 krew</h4><p>上文讲了如何编写一个插件，但是官方也提供一个插件库并提供了一个插件管理工具 <a href=\"https://github.com/kubernetes-sigs/krew\" target=\"_blank\" rel=\"noopener\">krew</a>  ，<a href=\"https://github.com/kubernetes-sigs/krew\" target=\"_blank\" rel=\"noopener\">krew</a> 是 kubectl 插件的管理器，使用 krew 可以轻松的查找、安装和管理 kubectl 插件，它类似于 yum、apt、 dnf，krew 也可以帮助你将已写好的插件在多个平台上打包和分发，krew 自己也作为一个 kubectl 插件存在。</p>\n<blockquote>\n<p>krew 仅支持在 v1.12 及之后的版本中使用。</p>\n</blockquote>\n<p>1、安装 krew</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ (</span><br><span class=\"line\">  set -x; cd &quot;$(mktemp -d)&quot; &amp;&amp;</span><br><span class=\"line\">  curl -fsSLO &quot;https://storage.googleapis.com/krew/v0.2.1/krew.&#123;tar.gz,yaml&#125;&quot; &amp;&amp;</span><br><span class=\"line\">  tar zxvf krew.tar.gz &amp;&amp;</span><br><span class=\"line\">  ./krew-&quot;$(uname | tr &apos;[:upper:]&apos; &apos;[:lower:]&apos;)_amd64&quot; install \\</span><br><span class=\"line\">    --manifest=krew.yaml --archive=krew.tar.gz</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">$ export PATH=&quot;$&#123;KREW_ROOT:-$HOME/.krew&#125;/bin:$PATH&quot;</span><br></pre></td></tr></table></figure>\n<p>2、krew 的使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl krew search               \t\t\t\t\t\t\t\t# show all plugins</span><br><span class=\"line\">$ kubectl krew install view-secret  \t\t\t\t\t\t\t\t# install a plugin named &quot;view-secret&quot;</span><br><span class=\"line\">$ kubectl view-secret default-token-4cwvh           # use the plugin</span><br><span class=\"line\">$ kubectl krew upgrade              \t\t\t\t\t\t\t\t# upgrade installed plugins</span><br><span class=\"line\">$ kubectl krew remove view-secret   \t\t\t\t\t\t\t\t# uninstall a plugin</span><br></pre></td></tr></table></figure>\n<p>若想让你自己的插件加入到 krew 的索引中，可以参考：<a href=\"https://github.com/kubernetes-sigs/krew/blob/master/docs/DEVELOPER_GUIDE.md\" target=\"_blank\" rel=\"noopener\">how to package and publish a plugin for krew</a>。</p>\n<p>参考：</p>\n<p><a href=\"https://github.com/kubernetes-sigs/krew/blob/master/docs/NAMING_GUIDE.md\" target=\"_blank\" rel=\"noopener\">kubectl 插件命明规范</a></p>\n<p><a href=\"https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/</a></p>\n<p><a href=\"https://github.com/gosoon/kubectl-plugin\" target=\"_blank\" rel=\"noopener\">https://github.com/gosoon/kubectl-plugin</a></p>\n"},{"title":"kubeadm 安装 kubernetes","date":"2019-01-17T02:11:30.000Z","type":"kubeadm","_content":"\nkubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中，当前版本为 GA，暂不建议用于部署生产环境，其先进的设计理念可以借鉴。\n\n## 一、kubeadm 原理介绍\n\nkubeadm 会在初始化的机器上首先部署 kubelet 服务，kubelet 创建 pod 的方式有三种，其中一种就是监控指定目下（/etc/kubernetes/manifests）容器状态的变化然后进行相应的操作。kubeadm 启动 kubelet 后会在 /etc/kubernetes/manifests 目录下创建出 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 四个组件 static pod 的 yaml 文件，此时 kubelet 监测到该目录下有 yaml 文件便会将其创建为对应的 pod，最终 kube-apiserver、kube-controller-manager、kube-scheduler 以及 etcd 会以 static pod 的方式运行。\n\n\n> 本次安装 kubernetes 版本：v1.12.0\n\n当前宿主机系统与内核版本：\n```\n$ uname -r\n3.10.0-514.16.1.el7.x86_64\n\n$ cat /etc/redhat-release\nCentOS Linux release 7.2.1511 (Core)\n```\n## 二、安装前的准备工作\n```\n# 关闭swap\n$ sudo swapoff -a\n\n# 关闭selinux\n$ sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux \n$ setenforce 0\n\n# 关闭防火墙\n$ systemctl disable firewalld.service && systemctl stop firewalld.service\n\n# 配置转发相关参数\n$ cat << EOF >> /etc/sysctl.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nvm.swappiness=0\nEOF \n$ sysctl -p\n```\n\n## 三、安装 Docker CE \n \n> 本次安装的 docker 版本：docker-ce-18.06.1.ce\n\n```\n# Install Docker CE\n## Set up the repository\n### Install required packages.\nyum install yum-utils device-mapper-persistent-data lvm2\n\n### Add docker repository.\nyum-config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/centos/docker-ce.repo\n\n## Install docker ce.\nyum update && yum install docker-ce-18.06.1.ce\n\n## Create /etc/docker directory.\nmkdir /etc/docker\n\n# Setup daemon.\ncat > /etc/docker/daemon.json <<EOF\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"100m\"\n  },\n  \"storage-driver\": \"overlay2\",\n  \"storage-opts\": [\n    \"overlay2.override_kernel_check=true\"\n  ]\n}\nEOF\n\nmkdir -p /etc/systemd/system/docker.service.d\n\n# Restart docker.\nsystemctl daemon-reload\nsystemctl restart docker\n```\n参考：https://kubernetes.io/docs/setup/cri/\n\n## 四、安装 kubernetes master 组件\n\n使用 kubeadm 初始化集群：\n```\n$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16\n[init] using Kubernetes version: v1.12.0\n[preflight] running pre-flight checks\n[preflight/images] Pulling images required for setting up a Kubernetes cluster\n[preflight/images] This might take a minute or two, depending on the speed of your internet connection\n[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'\n[kubelet] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[preflight] Activating the kubelet service\n[certificates] Using the existing front-proxy-client certificate and key.\n[certificates] Using the existing etcd/server certificate and key.\n[certificates] Using the existing etcd/peer certificate and key.\n[certificates] Using the existing etcd/healthcheck-client certificate and key.\n[certificates] Using the existing apiserver-etcd-client certificate and key.\n[certificates] Using the existing apiserver certificate and key.\n[certificates] Using the existing apiserver-kubelet-client certificate and key.\n[certificates] valid certificates and keys now exist in \"/etc/kubernetes/pki\"\n[certificates] Using the existing sa key.\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/admin.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/kubelet.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/controller-manager.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/scheduler.conf\"\n[controlplane] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"\n[etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\"\n[init] waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\"\n[init] this might take a minute or longer if the control plane images have to be pulled\n[apiclient] All control plane components are healthy after 14.002350 seconds\n[uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config-1.12\" in namespace kube-system with the configuration for the kubelets in the cluster\n[markmaster] Marking the node 192.168.1.110 as master by adding the label \"node-role.kubernetes.io/master=''\"\n[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"192.168.1.110\" as an annotation\n[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt\n[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstraptoken] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes master has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23\n\n```\n\n这个命令会自动执行以下步骤：\n\n- 系统状态检查\n- 生成 token\n- 生成自签名 CA 和 client 端证书\n- 生成 kubeconfig 用于 kubelet 连接 API server\n- 为 Master 组件生成 Static Pod manifests，并放到 /etc/kubernetes/manifests 目录中\n- 配置 RBAC 并设置 Master node 只运行控制平面组件\n- 创建附加服务，比如 kube-proxy 和 CoreDNS\n\n\n配置 kubetl 认证信息： \n```\n $ mkdir -p $HOME/.kube\n $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n $ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n将本机作为 node 加入到 master 中：\n```\n$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash\n```\nkubeadm 默认 master 节点不作为 node 节点使用，初始化完成后会给 master 节点打上 taint 标签，若单机部署，使用以下命令去掉 taint 标签：\n```\n$ kubectl taint nodes --all node-role.kubernetes.io/master-\n```\n\n查看各组件是否正常运行：\n\n```\n$ kubectl get pod -n kube-system\nNAME                                     READY   STATUS             RESTARTS   AGE\ncoredns-99b9bb8bd-pgh5t                  1/1     Running            0          48m\netcd                                     1/1     Running            2          48m\nkube-apiserver                           1/1     Running            1          48m\nkube-controller-manager                  1/1     Running            0          49m\nkube-flannel-ds-amd64-b5rjg              1/1     Running            0          31m\nkube-proxy-c8ktg                         1/1     Running            0          48m\nkube-scheduler                           1/1     Running            2          48m\n```\n\n## 五、安装 kubernetes 网络\n\nkubernetes 本身是不提供网络方案的，但是有很多开源组件可以帮助我们打通容器和容器之间的网络，实现 Kubernetes 要求的网络模型。从实现原理上来说大致分为以下两种：\n- overlay 网络，通过封包解包的方式构造一个隧道，代表的方案有 flannel(udp/vxlan）、weave、calico(ipip)，openvswitch 等\n- 通过路由来实现(更改 iptables 等手段)，flannel(host-gw)，calico(bgp)，macvlan 等\n\n当然每种方案都有自己适合的场景，flannel 和 calico 是两种最常见的网络方案，我们要根据自己的实际需要进行选择。此次安装选择 flannel 网络：\n\n\n此操作也会为 flannel 创建对应的 RBAC 规则，flannel 会以 daemonset 的方式创建出来：\n```\n$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```\n\n创建一个 pod 验证集群是否正常：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n```\n\n## 六、kubeadm 其他相关的操作\n\n1、删除安装:\n```\n$ kubeadm reset\n```\n2、版本升级\n```\n# 查看可升级的版本\n$ kubeadm upgrade plan\n\n# 升级至指定版本\n$ kubeadm upgrade apply [version]\n```\n>  1. 要执行升级，需要先将 kubeadm 升级到对应的版本；\n>  2. kubeadm 并不负责 kubelet 的升级，需要在升级完 master 组件后，手工对 kubelet 进行升级。\n\n\n## 七、创建过程中的一些 case 记录\n\n##### 1、flannel 容器启动报错：pod cidr not assgned\n\n需要在  /etc/kubernetes/manifests/kube-controller-manager.yaml 文件中添加以下配置：\n\n--allocate-node-cidrs=true\n--cluster-cidr=10.244.0.0/16\n\n参考：https://github.com/coreos/flannel/issues/728\n\n##### 2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory\n\n```\n$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line\n$ grub2-mkconfig -o /boot/grub2/grub.cfg\n$ shutdown -r now\n```\n参考：https://github.com/containernetworking/cni/issues/569\n\n##### 3、kubeadm 证书有效期问题\n\n默认情况下，kubeadm 会生成集群运行所需的所有证书，我们也可以通过提供自己的证书来覆盖此行为。要做到这一点，必须把它们放在 --cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录（默认目录为 /etc/kubernetes/pki），如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。所以只要我们自己提供一个有效期很长的证书去覆盖掉默认的证书就可以来避免这个的问题。\n\n##### 4、kubeadm join 时 token 无法生效\n\ntoken 的失效为24小时，若忘记或者 token 过期可以使用 `kubeadm token create` 重新生成 token。\n\n## 八、总结\n本篇文章讲述了使用 kubeadm 来搭建一个 kubernetes 集群，kubeadm 暂时还不建议用于生产环境，若部署生产环境请使用二进制文件。kubeadm 搭建出的集群还是有很多不完善的地方，比如，集群 master 组件的参数配置问题，官方默认的并不会满足需求，有许多参数需要根据实际情况进行修改。\n\n\n参考：\n[Creating a single master cluster with kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/)\n[kubeadm 工作原理](https://github.com/feiskyer/kubernetes-handbook/blob/master/components/kubeadm.md)\n[DockOne微信分享（一六三）：Kubernetes官方集群部署工具kubeadm原理解析](http://dockone.io/article/4645)\n[centos7.2 安装k8s v1.11.0](https://segmentfault.com/a/1190000015787725)\n\n","source":"_posts/kubeadm.md","raw":"---\ntitle: kubeadm 安装 kubernetes\ndate: 2019-01-17 10:11:30\ntags: \"kubeadm\"\ntype: \"kubeadm\"\n\n---\n\nkubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中，当前版本为 GA，暂不建议用于部署生产环境，其先进的设计理念可以借鉴。\n\n## 一、kubeadm 原理介绍\n\nkubeadm 会在初始化的机器上首先部署 kubelet 服务，kubelet 创建 pod 的方式有三种，其中一种就是监控指定目下（/etc/kubernetes/manifests）容器状态的变化然后进行相应的操作。kubeadm 启动 kubelet 后会在 /etc/kubernetes/manifests 目录下创建出 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 四个组件 static pod 的 yaml 文件，此时 kubelet 监测到该目录下有 yaml 文件便会将其创建为对应的 pod，最终 kube-apiserver、kube-controller-manager、kube-scheduler 以及 etcd 会以 static pod 的方式运行。\n\n\n> 本次安装 kubernetes 版本：v1.12.0\n\n当前宿主机系统与内核版本：\n```\n$ uname -r\n3.10.0-514.16.1.el7.x86_64\n\n$ cat /etc/redhat-release\nCentOS Linux release 7.2.1511 (Core)\n```\n## 二、安装前的准备工作\n```\n# 关闭swap\n$ sudo swapoff -a\n\n# 关闭selinux\n$ sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux \n$ setenforce 0\n\n# 关闭防火墙\n$ systemctl disable firewalld.service && systemctl stop firewalld.service\n\n# 配置转发相关参数\n$ cat << EOF >> /etc/sysctl.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nvm.swappiness=0\nEOF \n$ sysctl -p\n```\n\n## 三、安装 Docker CE \n \n> 本次安装的 docker 版本：docker-ce-18.06.1.ce\n\n```\n# Install Docker CE\n## Set up the repository\n### Install required packages.\nyum install yum-utils device-mapper-persistent-data lvm2\n\n### Add docker repository.\nyum-config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/centos/docker-ce.repo\n\n## Install docker ce.\nyum update && yum install docker-ce-18.06.1.ce\n\n## Create /etc/docker directory.\nmkdir /etc/docker\n\n# Setup daemon.\ncat > /etc/docker/daemon.json <<EOF\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"100m\"\n  },\n  \"storage-driver\": \"overlay2\",\n  \"storage-opts\": [\n    \"overlay2.override_kernel_check=true\"\n  ]\n}\nEOF\n\nmkdir -p /etc/systemd/system/docker.service.d\n\n# Restart docker.\nsystemctl daemon-reload\nsystemctl restart docker\n```\n参考：https://kubernetes.io/docs/setup/cri/\n\n## 四、安装 kubernetes master 组件\n\n使用 kubeadm 初始化集群：\n```\n$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16\n[init] using Kubernetes version: v1.12.0\n[preflight] running pre-flight checks\n[preflight/images] Pulling images required for setting up a Kubernetes cluster\n[preflight/images] This might take a minute or two, depending on the speed of your internet connection\n[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'\n[kubelet] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[preflight] Activating the kubelet service\n[certificates] Using the existing front-proxy-client certificate and key.\n[certificates] Using the existing etcd/server certificate and key.\n[certificates] Using the existing etcd/peer certificate and key.\n[certificates] Using the existing etcd/healthcheck-client certificate and key.\n[certificates] Using the existing apiserver-etcd-client certificate and key.\n[certificates] Using the existing apiserver certificate and key.\n[certificates] Using the existing apiserver-kubelet-client certificate and key.\n[certificates] valid certificates and keys now exist in \"/etc/kubernetes/pki\"\n[certificates] Using the existing sa key.\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/admin.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/kubelet.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/controller-manager.conf\"\n[kubeconfig] Using existing up-to-date KubeConfig file: \"/etc/kubernetes/scheduler.conf\"\n[controlplane] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"\n[etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\"\n[init] waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\"\n[init] this might take a minute or longer if the control plane images have to be pulled\n[apiclient] All control plane components are healthy after 14.002350 seconds\n[uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config-1.12\" in namespace kube-system with the configuration for the kubelets in the cluster\n[markmaster] Marking the node 192.168.1.110 as master by adding the label \"node-role.kubernetes.io/master=''\"\n[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"192.168.1.110\" as an annotation\n[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt\n[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstraptoken] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes master has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23\n\n```\n\n这个命令会自动执行以下步骤：\n\n- 系统状态检查\n- 生成 token\n- 生成自签名 CA 和 client 端证书\n- 生成 kubeconfig 用于 kubelet 连接 API server\n- 为 Master 组件生成 Static Pod manifests，并放到 /etc/kubernetes/manifests 目录中\n- 配置 RBAC 并设置 Master node 只运行控制平面组件\n- 创建附加服务，比如 kube-proxy 和 CoreDNS\n\n\n配置 kubetl 认证信息： \n```\n $ mkdir -p $HOME/.kube\n $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n $ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n将本机作为 node 加入到 master 中：\n```\n$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash\n```\nkubeadm 默认 master 节点不作为 node 节点使用，初始化完成后会给 master 节点打上 taint 标签，若单机部署，使用以下命令去掉 taint 标签：\n```\n$ kubectl taint nodes --all node-role.kubernetes.io/master-\n```\n\n查看各组件是否正常运行：\n\n```\n$ kubectl get pod -n kube-system\nNAME                                     READY   STATUS             RESTARTS   AGE\ncoredns-99b9bb8bd-pgh5t                  1/1     Running            0          48m\netcd                                     1/1     Running            2          48m\nkube-apiserver                           1/1     Running            1          48m\nkube-controller-manager                  1/1     Running            0          49m\nkube-flannel-ds-amd64-b5rjg              1/1     Running            0          31m\nkube-proxy-c8ktg                         1/1     Running            0          48m\nkube-scheduler                           1/1     Running            2          48m\n```\n\n## 五、安装 kubernetes 网络\n\nkubernetes 本身是不提供网络方案的，但是有很多开源组件可以帮助我们打通容器和容器之间的网络，实现 Kubernetes 要求的网络模型。从实现原理上来说大致分为以下两种：\n- overlay 网络，通过封包解包的方式构造一个隧道，代表的方案有 flannel(udp/vxlan）、weave、calico(ipip)，openvswitch 等\n- 通过路由来实现(更改 iptables 等手段)，flannel(host-gw)，calico(bgp)，macvlan 等\n\n当然每种方案都有自己适合的场景，flannel 和 calico 是两种最常见的网络方案，我们要根据自己的实际需要进行选择。此次安装选择 flannel 网络：\n\n\n此操作也会为 flannel 创建对应的 RBAC 规则，flannel 会以 daemonset 的方式创建出来：\n```\n$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```\n\n创建一个 pod 验证集群是否正常：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n```\n\n## 六、kubeadm 其他相关的操作\n\n1、删除安装:\n```\n$ kubeadm reset\n```\n2、版本升级\n```\n# 查看可升级的版本\n$ kubeadm upgrade plan\n\n# 升级至指定版本\n$ kubeadm upgrade apply [version]\n```\n>  1. 要执行升级，需要先将 kubeadm 升级到对应的版本；\n>  2. kubeadm 并不负责 kubelet 的升级，需要在升级完 master 组件后，手工对 kubelet 进行升级。\n\n\n## 七、创建过程中的一些 case 记录\n\n##### 1、flannel 容器启动报错：pod cidr not assgned\n\n需要在  /etc/kubernetes/manifests/kube-controller-manager.yaml 文件中添加以下配置：\n\n--allocate-node-cidrs=true\n--cluster-cidr=10.244.0.0/16\n\n参考：https://github.com/coreos/flannel/issues/728\n\n##### 2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory\n\n```\n$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line\n$ grub2-mkconfig -o /boot/grub2/grub.cfg\n$ shutdown -r now\n```\n参考：https://github.com/containernetworking/cni/issues/569\n\n##### 3、kubeadm 证书有效期问题\n\n默认情况下，kubeadm 会生成集群运行所需的所有证书，我们也可以通过提供自己的证书来覆盖此行为。要做到这一点，必须把它们放在 --cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录（默认目录为 /etc/kubernetes/pki），如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。所以只要我们自己提供一个有效期很长的证书去覆盖掉默认的证书就可以来避免这个的问题。\n\n##### 4、kubeadm join 时 token 无法生效\n\ntoken 的失效为24小时，若忘记或者 token 过期可以使用 `kubeadm token create` 重新生成 token。\n\n## 八、总结\n本篇文章讲述了使用 kubeadm 来搭建一个 kubernetes 集群，kubeadm 暂时还不建议用于生产环境，若部署生产环境请使用二进制文件。kubeadm 搭建出的集群还是有很多不完善的地方，比如，集群 master 组件的参数配置问题，官方默认的并不会满足需求，有许多参数需要根据实际情况进行修改。\n\n\n参考：\n[Creating a single master cluster with kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/)\n[kubeadm 工作原理](https://github.com/feiskyer/kubernetes-handbook/blob/master/components/kubeadm.md)\n[DockOne微信分享（一六三）：Kubernetes官方集群部署工具kubeadm原理解析](http://dockone.io/article/4645)\n[centos7.2 安装k8s v1.11.0](https://segmentfault.com/a/1190000015787725)\n\n","slug":"kubeadm","published":1,"updated":"2019-06-01T14:26:16.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59r0019apwn9y2gd0sn","content":"<p>kubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中，当前版本为 GA，暂不建议用于部署生产环境，其先进的设计理念可以借鉴。</p>\n<h2 id=\"一、kubeadm-原理介绍\"><a href=\"#一、kubeadm-原理介绍\" class=\"headerlink\" title=\"一、kubeadm 原理介绍\"></a>一、kubeadm 原理介绍</h2><p>kubeadm 会在初始化的机器上首先部署 kubelet 服务，kubelet 创建 pod 的方式有三种，其中一种就是监控指定目下（/etc/kubernetes/manifests）容器状态的变化然后进行相应的操作。kubeadm 启动 kubelet 后会在 /etc/kubernetes/manifests 目录下创建出 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 四个组件 static pod 的 yaml 文件，此时 kubelet 监测到该目录下有 yaml 文件便会将其创建为对应的 pod，最终 kube-apiserver、kube-controller-manager、kube-scheduler 以及 etcd 会以 static pod 的方式运行。</p>\n<blockquote>\n<p>本次安装 kubernetes 版本：v1.12.0</p>\n</blockquote>\n<p>当前宿主机系统与内核版本：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ uname -r</span><br><span class=\"line\">3.10.0-514.16.1.el7.x86_64</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat /etc/redhat-release</span><br><span class=\"line\">CentOS Linux release 7.2.1511 (Core)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"二、安装前的准备工作\"><a href=\"#二、安装前的准备工作\" class=\"headerlink\" title=\"二、安装前的准备工作\"></a>二、安装前的准备工作</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 关闭swap</span><br><span class=\"line\">$ sudo swapoff -a</span><br><span class=\"line\"></span><br><span class=\"line\"># 关闭selinux</span><br><span class=\"line\">$ sed -i &apos;s/SELINUX=permissive/SELINUX=disabled/&apos; /etc/sysconfig/selinux </span><br><span class=\"line\">$ setenforce 0</span><br><span class=\"line\"></span><br><span class=\"line\"># 关闭防火墙</span><br><span class=\"line\">$ systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.service</span><br><span class=\"line\"></span><br><span class=\"line\"># 配置转发相关参数</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class=\"line\">net.bridge.bridge-nf-call-iptables = 1</span><br><span class=\"line\">vm.swappiness=0</span><br><span class=\"line\">EOF </span><br><span class=\"line\">$ sysctl -p</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、安装-Docker-CE\"><a href=\"#三、安装-Docker-CE\" class=\"headerlink\" title=\"三、安装 Docker CE\"></a>三、安装 Docker CE</h2><blockquote>\n<p>本次安装的 docker 版本：docker-ce-18.06.1.ce</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Install Docker CE</span><br><span class=\"line\">## Set up the repository</span><br><span class=\"line\">### Install required packages.</span><br><span class=\"line\">yum install yum-utils device-mapper-persistent-data lvm2</span><br><span class=\"line\"></span><br><span class=\"line\">### Add docker repository.</span><br><span class=\"line\">yum-config-manager \\</span><br><span class=\"line\">    --add-repo \\</span><br><span class=\"line\">    https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class=\"line\"></span><br><span class=\"line\">## Install docker ce.</span><br><span class=\"line\">yum update &amp;&amp; yum install docker-ce-18.06.1.ce</span><br><span class=\"line\"></span><br><span class=\"line\">## Create /etc/docker directory.</span><br><span class=\"line\">mkdir /etc/docker</span><br><span class=\"line\"></span><br><span class=\"line\"># Setup daemon.</span><br><span class=\"line\">cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class=\"line\">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class=\"line\">  &quot;log-opts&quot;: &#123;</span><br><span class=\"line\">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class=\"line\">  &quot;storage-opts&quot;: [</span><br><span class=\"line\">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">mkdir -p /etc/systemd/system/docker.service.d</span><br><span class=\"line\"></span><br><span class=\"line\"># Restart docker.</span><br><span class=\"line\">systemctl daemon-reload</span><br><span class=\"line\">systemctl restart docker</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://kubernetes.io/docs/setup/cri/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/setup/cri/</a></p>\n<h2 id=\"四、安装-kubernetes-master-组件\"><a href=\"#四、安装-kubernetes-master-组件\" class=\"headerlink\" title=\"四、安装 kubernetes master 组件\"></a>四、安装 kubernetes master 组件</h2><p>使用 kubeadm 初始化集群：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16</span><br><span class=\"line\">[init] using Kubernetes version: v1.12.0</span><br><span class=\"line\">[preflight] running pre-flight checks</span><br><span class=\"line\">[preflight/images] Pulling images required for setting up a Kubernetes cluster</span><br><span class=\"line\">[preflight/images] This might take a minute or two, depending on the speed of your internet connection</span><br><span class=\"line\">[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class=\"line\">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class=\"line\">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class=\"line\">[preflight] Activating the kubelet service</span><br><span class=\"line\">[certificates] Using the existing front-proxy-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/server certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/peer certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/healthcheck-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver-etcd-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver-kubelet-client certificate and key.</span><br><span class=\"line\">[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class=\"line\">[certificates] Using the existing sa key.</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/kubelet.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/controller-manager.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/scheduler.conf&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class=\"line\">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class=\"line\">[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;</span><br><span class=\"line\">[init] this might take a minute or longer if the control plane images have to be pulled</span><br><span class=\"line\">[apiclient] All control plane components are healthy after 14.002350 seconds</span><br><span class=\"line\">[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class=\"line\">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class=\"line\">[markmaster] Marking the node 192.168.1.110 as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class=\"line\">[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class=\"line\">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;192.168.1.110&quot; as an annotation</span><br><span class=\"line\">[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class=\"line\">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class=\"line\">[addons] Applied essential addon: CoreDNS</span><br><span class=\"line\">[addons] Applied essential addon: kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">Your Kubernetes master has initialized successfully!</span><br><span class=\"line\"></span><br><span class=\"line\">To start using your cluster, you need to run the following as a regular user:</span><br><span class=\"line\"></span><br><span class=\"line\">  mkdir -p $HOME/.kube</span><br><span class=\"line\">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class=\"line\"></span><br><span class=\"line\">You should now deploy a pod network to the cluster.</span><br><span class=\"line\">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class=\"line\">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class=\"line\"></span><br><span class=\"line\">You can now join any number of machines by running the following on each node</span><br><span class=\"line\">as root:</span><br><span class=\"line\"></span><br><span class=\"line\">  kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23</span><br></pre></td></tr></table></figure></p>\n<p>这个命令会自动执行以下步骤：</p>\n<ul>\n<li>系统状态检查</li>\n<li>生成 token</li>\n<li>生成自签名 CA 和 client 端证书</li>\n<li>生成 kubeconfig 用于 kubelet 连接 API server</li>\n<li>为 Master 组件生成 Static Pod manifests，并放到 /etc/kubernetes/manifests 目录中</li>\n<li>配置 RBAC 并设置 Master node 只运行控制平面组件</li>\n<li>创建附加服务，比如 kube-proxy 和 CoreDNS</li>\n</ul>\n<p>配置 kubetl 认证信息：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -p $HOME/.kube</span><br><span class=\"line\">$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure></p>\n<p>将本机作为 node 加入到 master 中：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash</span><br></pre></td></tr></table></figure></p>\n<p>kubeadm 默认 master 节点不作为 node 节点使用，初始化完成后会给 master 节点打上 taint 标签，若单机部署，使用以下命令去掉 taint 标签：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl taint nodes --all node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure></p>\n<p>查看各组件是否正常运行：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pod -n kube-system</span><br><span class=\"line\">NAME                                     READY   STATUS             RESTARTS   AGE</span><br><span class=\"line\">coredns-99b9bb8bd-pgh5t                  1/1     Running            0          48m</span><br><span class=\"line\">etcd                                     1/1     Running            2          48m</span><br><span class=\"line\">kube-apiserver                           1/1     Running            1          48m</span><br><span class=\"line\">kube-controller-manager                  1/1     Running            0          49m</span><br><span class=\"line\">kube-flannel-ds-amd64-b5rjg              1/1     Running            0          31m</span><br><span class=\"line\">kube-proxy-c8ktg                         1/1     Running            0          48m</span><br><span class=\"line\">kube-scheduler                           1/1     Running            2          48m</span><br></pre></td></tr></table></figure>\n<h2 id=\"五、安装-kubernetes-网络\"><a href=\"#五、安装-kubernetes-网络\" class=\"headerlink\" title=\"五、安装 kubernetes 网络\"></a>五、安装 kubernetes 网络</h2><p>kubernetes 本身是不提供网络方案的，但是有很多开源组件可以帮助我们打通容器和容器之间的网络，实现 Kubernetes 要求的网络模型。从实现原理上来说大致分为以下两种：</p>\n<ul>\n<li>overlay 网络，通过封包解包的方式构造一个隧道，代表的方案有 flannel(udp/vxlan）、weave、calico(ipip)，openvswitch 等</li>\n<li>通过路由来实现(更改 iptables 等手段)，flannel(host-gw)，calico(bgp)，macvlan 等</li>\n</ul>\n<p>当然每种方案都有自己适合的场景，flannel 和 calico 是两种最常见的网络方案，我们要根据自己的实际需要进行选择。此次安装选择 flannel 网络：</p>\n<p>此操作也会为 flannel 创建对应的 RBAC 规则，flannel 会以 daemonset 的方式创建出来：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure></p>\n<p>创建一个 pod 验证集群是否正常：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: nginx</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    name: nginx</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">  - name: nginx</span><br><span class=\"line\">    image: nginx</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">    - containerPort: 80</span><br></pre></td></tr></table></figure>\n<h2 id=\"六、kubeadm-其他相关的操作\"><a href=\"#六、kubeadm-其他相关的操作\" class=\"headerlink\" title=\"六、kubeadm 其他相关的操作\"></a>六、kubeadm 其他相关的操作</h2><p>1、删除安装:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm reset</span><br></pre></td></tr></table></figure></p>\n<p>2、版本升级<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 查看可升级的版本</span><br><span class=\"line\">$ kubeadm upgrade plan</span><br><span class=\"line\"></span><br><span class=\"line\"># 升级至指定版本</span><br><span class=\"line\">$ kubeadm upgrade apply [version]</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<ol>\n<li>要执行升级，需要先将 kubeadm 升级到对应的版本；</li>\n<li>kubeadm 并不负责 kubelet 的升级，需要在升级完 master 组件后，手工对 kubelet 进行升级。</li>\n</ol>\n</blockquote>\n<h2 id=\"七、创建过程中的一些-case-记录\"><a href=\"#七、创建过程中的一些-case-记录\" class=\"headerlink\" title=\"七、创建过程中的一些 case 记录\"></a>七、创建过程中的一些 case 记录</h2><h5 id=\"1、flannel-容器启动报错：pod-cidr-not-assgned\"><a href=\"#1、flannel-容器启动报错：pod-cidr-not-assgned\" class=\"headerlink\" title=\"1、flannel 容器启动报错：pod cidr not assgned\"></a>1、flannel 容器启动报错：pod cidr not assgned</h5><p>需要在  /etc/kubernetes/manifests/kube-controller-manager.yaml 文件中添加以下配置：</p>\n<p>–allocate-node-cidrs=true<br>–cluster-cidr=10.244.0.0/16</p>\n<p>参考：<a href=\"https://github.com/coreos/flannel/issues/728\" target=\"_blank\" rel=\"noopener\">https://github.com/coreos/flannel/issues/728</a></p>\n<h5 id=\"2、coredns-容器启动失败报错：-proc-sys-net-ipv6-conf-eth0-accept-dad-no-such-file-or-directory\"><a href=\"#2、coredns-容器启动失败报错：-proc-sys-net-ipv6-conf-eth0-accept-dad-no-such-file-or-directory\" class=\"headerlink\" title=\"2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory\"></a>2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line</span><br><span class=\"line\">$ grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class=\"line\">$ shutdown -r now</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://github.com/containernetworking/cni/issues/569\" target=\"_blank\" rel=\"noopener\">https://github.com/containernetworking/cni/issues/569</a></p>\n<h5 id=\"3、kubeadm-证书有效期问题\"><a href=\"#3、kubeadm-证书有效期问题\" class=\"headerlink\" title=\"3、kubeadm 证书有效期问题\"></a>3、kubeadm 证书有效期问题</h5><p>默认情况下，kubeadm 会生成集群运行所需的所有证书，我们也可以通过提供自己的证书来覆盖此行为。要做到这一点，必须把它们放在 –cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录（默认目录为 /etc/kubernetes/pki），如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。所以只要我们自己提供一个有效期很长的证书去覆盖掉默认的证书就可以来避免这个的问题。</p>\n<h5 id=\"4、kubeadm-join-时-token-无法生效\"><a href=\"#4、kubeadm-join-时-token-无法生效\" class=\"headerlink\" title=\"4、kubeadm join 时 token 无法生效\"></a>4、kubeadm join 时 token 无法生效</h5><p>token 的失效为24小时，若忘记或者 token 过期可以使用 <code>kubeadm token create</code> 重新生成 token。</p>\n<h2 id=\"八、总结\"><a href=\"#八、总结\" class=\"headerlink\" title=\"八、总结\"></a>八、总结</h2><p>本篇文章讲述了使用 kubeadm 来搭建一个 kubernetes 集群，kubeadm 暂时还不建议用于生产环境，若部署生产环境请使用二进制文件。kubeadm 搭建出的集群还是有很多不完善的地方，比如，集群 master 组件的参数配置问题，官方默认的并不会满足需求，有许多参数需要根据实际情况进行修改。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\" target=\"_blank\" rel=\"noopener\">Creating a single master cluster with kubeadm</a><br><a href=\"https://github.com/feiskyer/kubernetes-handbook/blob/master/components/kubeadm.md\" target=\"_blank\" rel=\"noopener\">kubeadm 工作原理</a><br><a href=\"http://dockone.io/article/4645\" target=\"_blank\" rel=\"noopener\">DockOne微信分享（一六三）：Kubernetes官方集群部署工具kubeadm原理解析</a><br><a href=\"https://segmentfault.com/a/1190000015787725\" target=\"_blank\" rel=\"noopener\">centos7.2 安装k8s v1.11.0</a></p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>kubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中，当前版本为 GA，暂不建议用于部署生产环境，其先进的设计理念可以借鉴。</p>\n<h2 id=\"一、kubeadm-原理介绍\"><a href=\"#一、kubeadm-原理介绍\" class=\"headerlink\" title=\"一、kubeadm 原理介绍\"></a>一、kubeadm 原理介绍</h2><p>kubeadm 会在初始化的机器上首先部署 kubelet 服务，kubelet 创建 pod 的方式有三种，其中一种就是监控指定目下（/etc/kubernetes/manifests）容器状态的变化然后进行相应的操作。kubeadm 启动 kubelet 后会在 /etc/kubernetes/manifests 目录下创建出 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 四个组件 static pod 的 yaml 文件，此时 kubelet 监测到该目录下有 yaml 文件便会将其创建为对应的 pod，最终 kube-apiserver、kube-controller-manager、kube-scheduler 以及 etcd 会以 static pod 的方式运行。</p>\n<blockquote>\n<p>本次安装 kubernetes 版本：v1.12.0</p>\n</blockquote>\n<p>当前宿主机系统与内核版本：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ uname -r</span><br><span class=\"line\">3.10.0-514.16.1.el7.x86_64</span><br><span class=\"line\"></span><br><span class=\"line\">$ cat /etc/redhat-release</span><br><span class=\"line\">CentOS Linux release 7.2.1511 (Core)</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"二、安装前的准备工作\"><a href=\"#二、安装前的准备工作\" class=\"headerlink\" title=\"二、安装前的准备工作\"></a>二、安装前的准备工作</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 关闭swap</span><br><span class=\"line\">$ sudo swapoff -a</span><br><span class=\"line\"></span><br><span class=\"line\"># 关闭selinux</span><br><span class=\"line\">$ sed -i &apos;s/SELINUX=permissive/SELINUX=disabled/&apos; /etc/sysconfig/selinux </span><br><span class=\"line\">$ setenforce 0</span><br><span class=\"line\"></span><br><span class=\"line\"># 关闭防火墙</span><br><span class=\"line\">$ systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.service</span><br><span class=\"line\"></span><br><span class=\"line\"># 配置转发相关参数</span><br><span class=\"line\">$ cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.conf</span><br><span class=\"line\">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class=\"line\">net.bridge.bridge-nf-call-iptables = 1</span><br><span class=\"line\">vm.swappiness=0</span><br><span class=\"line\">EOF </span><br><span class=\"line\">$ sysctl -p</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、安装-Docker-CE\"><a href=\"#三、安装-Docker-CE\" class=\"headerlink\" title=\"三、安装 Docker CE\"></a>三、安装 Docker CE</h2><blockquote>\n<p>本次安装的 docker 版本：docker-ce-18.06.1.ce</p>\n</blockquote>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Install Docker CE</span><br><span class=\"line\">## Set up the repository</span><br><span class=\"line\">### Install required packages.</span><br><span class=\"line\">yum install yum-utils device-mapper-persistent-data lvm2</span><br><span class=\"line\"></span><br><span class=\"line\">### Add docker repository.</span><br><span class=\"line\">yum-config-manager \\</span><br><span class=\"line\">    --add-repo \\</span><br><span class=\"line\">    https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class=\"line\"></span><br><span class=\"line\">## Install docker ce.</span><br><span class=\"line\">yum update &amp;&amp; yum install docker-ce-18.06.1.ce</span><br><span class=\"line\"></span><br><span class=\"line\">## Create /etc/docker directory.</span><br><span class=\"line\">mkdir /etc/docker</span><br><span class=\"line\"></span><br><span class=\"line\"># Setup daemon.</span><br><span class=\"line\">cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class=\"line\">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class=\"line\">  &quot;log-opts&quot;: &#123;</span><br><span class=\"line\">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class=\"line\">  &#125;,</span><br><span class=\"line\">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class=\"line\">  &quot;storage-opts&quot;: [</span><br><span class=\"line\">    &quot;overlay2.override_kernel_check=true&quot;</span><br><span class=\"line\">  ]</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">EOF</span><br><span class=\"line\"></span><br><span class=\"line\">mkdir -p /etc/systemd/system/docker.service.d</span><br><span class=\"line\"></span><br><span class=\"line\"># Restart docker.</span><br><span class=\"line\">systemctl daemon-reload</span><br><span class=\"line\">systemctl restart docker</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://kubernetes.io/docs/setup/cri/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/setup/cri/</a></p>\n<h2 id=\"四、安装-kubernetes-master-组件\"><a href=\"#四、安装-kubernetes-master-组件\" class=\"headerlink\" title=\"四、安装 kubernetes master 组件\"></a>四、安装 kubernetes master 组件</h2><p>使用 kubeadm 初始化集群：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16</span><br><span class=\"line\">[init] using Kubernetes version: v1.12.0</span><br><span class=\"line\">[preflight] running pre-flight checks</span><br><span class=\"line\">[preflight/images] Pulling images required for setting up a Kubernetes cluster</span><br><span class=\"line\">[preflight/images] This might take a minute or two, depending on the speed of your internet connection</span><br><span class=\"line\">[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;</span><br><span class=\"line\">[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class=\"line\">[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class=\"line\">[preflight] Activating the kubelet service</span><br><span class=\"line\">[certificates] Using the existing front-proxy-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/server certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/peer certificate and key.</span><br><span class=\"line\">[certificates] Using the existing etcd/healthcheck-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver-etcd-client certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver certificate and key.</span><br><span class=\"line\">[certificates] Using the existing apiserver-kubelet-client certificate and key.</span><br><span class=\"line\">[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;</span><br><span class=\"line\">[certificates] Using the existing sa key.</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/admin.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/kubelet.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/controller-manager.conf&quot;</span><br><span class=\"line\">[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/scheduler.conf&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span><br><span class=\"line\">[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span><br><span class=\"line\">[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span><br><span class=\"line\">[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;</span><br><span class=\"line\">[init] this might take a minute or longer if the control plane images have to be pulled</span><br><span class=\"line\">[apiclient] All control plane components are healthy after 14.002350 seconds</span><br><span class=\"line\">[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class=\"line\">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class=\"line\">[markmaster] Marking the node 192.168.1.110 as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;</span><br><span class=\"line\">[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class=\"line\">[patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;192.168.1.110&quot; as an annotation</span><br><span class=\"line\">[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class=\"line\">[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class=\"line\">[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class=\"line\">[addons] Applied essential addon: CoreDNS</span><br><span class=\"line\">[addons] Applied essential addon: kube-proxy</span><br><span class=\"line\"></span><br><span class=\"line\">Your Kubernetes master has initialized successfully!</span><br><span class=\"line\"></span><br><span class=\"line\">To start using your cluster, you need to run the following as a regular user:</span><br><span class=\"line\"></span><br><span class=\"line\">  mkdir -p $HOME/.kube</span><br><span class=\"line\">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class=\"line\"></span><br><span class=\"line\">You should now deploy a pod network to the cluster.</span><br><span class=\"line\">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class=\"line\">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class=\"line\"></span><br><span class=\"line\">You can now join any number of machines by running the following on each node</span><br><span class=\"line\">as root:</span><br><span class=\"line\"></span><br><span class=\"line\">  kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23</span><br></pre></td></tr></table></figure></p>\n<p>这个命令会自动执行以下步骤：</p>\n<ul>\n<li>系统状态检查</li>\n<li>生成 token</li>\n<li>生成自签名 CA 和 client 端证书</li>\n<li>生成 kubeconfig 用于 kubelet 连接 API server</li>\n<li>为 Master 组件生成 Static Pod manifests，并放到 /etc/kubernetes/manifests 目录中</li>\n<li>配置 RBAC 并设置 Master node 只运行控制平面组件</li>\n<li>创建附加服务，比如 kube-proxy 和 CoreDNS</li>\n</ul>\n<p>配置 kubetl 认证信息：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ mkdir -p $HOME/.kube</span><br><span class=\"line\">$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class=\"line\">$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure></p>\n<p>将本机作为 node 加入到 master 中：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash</span><br></pre></td></tr></table></figure></p>\n<p>kubeadm 默认 master 节点不作为 node 节点使用，初始化完成后会给 master 节点打上 taint 标签，若单机部署，使用以下命令去掉 taint 标签：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl taint nodes --all node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure></p>\n<p>查看各组件是否正常运行：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl get pod -n kube-system</span><br><span class=\"line\">NAME                                     READY   STATUS             RESTARTS   AGE</span><br><span class=\"line\">coredns-99b9bb8bd-pgh5t                  1/1     Running            0          48m</span><br><span class=\"line\">etcd                                     1/1     Running            2          48m</span><br><span class=\"line\">kube-apiserver                           1/1     Running            1          48m</span><br><span class=\"line\">kube-controller-manager                  1/1     Running            0          49m</span><br><span class=\"line\">kube-flannel-ds-amd64-b5rjg              1/1     Running            0          31m</span><br><span class=\"line\">kube-proxy-c8ktg                         1/1     Running            0          48m</span><br><span class=\"line\">kube-scheduler                           1/1     Running            2          48m</span><br></pre></td></tr></table></figure>\n<h2 id=\"五、安装-kubernetes-网络\"><a href=\"#五、安装-kubernetes-网络\" class=\"headerlink\" title=\"五、安装 kubernetes 网络\"></a>五、安装 kubernetes 网络</h2><p>kubernetes 本身是不提供网络方案的，但是有很多开源组件可以帮助我们打通容器和容器之间的网络，实现 Kubernetes 要求的网络模型。从实现原理上来说大致分为以下两种：</p>\n<ul>\n<li>overlay 网络，通过封包解包的方式构造一个隧道，代表的方案有 flannel(udp/vxlan）、weave、calico(ipip)，openvswitch 等</li>\n<li>通过路由来实现(更改 iptables 等手段)，flannel(host-gw)，calico(bgp)，macvlan 等</li>\n</ul>\n<p>当然每种方案都有自己适合的场景，flannel 和 calico 是两种最常见的网络方案，我们要根据自己的实际需要进行选择。此次安装选择 flannel 网络：</p>\n<p>此操作也会为 flannel 创建对应的 RBAC 规则，flannel 会以 daemonset 的方式创建出来：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure></p>\n<p>创建一个 pod 验证集群是否正常：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">apiVersion: v1</span><br><span class=\"line\">kind: Pod</span><br><span class=\"line\">metadata:</span><br><span class=\"line\">  name: nginx</span><br><span class=\"line\">  labels:</span><br><span class=\"line\">    name: nginx</span><br><span class=\"line\">spec:</span><br><span class=\"line\">  containers:</span><br><span class=\"line\">  - name: nginx</span><br><span class=\"line\">    image: nginx</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">    - containerPort: 80</span><br></pre></td></tr></table></figure>\n<h2 id=\"六、kubeadm-其他相关的操作\"><a href=\"#六、kubeadm-其他相关的操作\" class=\"headerlink\" title=\"六、kubeadm 其他相关的操作\"></a>六、kubeadm 其他相关的操作</h2><p>1、删除安装:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ kubeadm reset</span><br></pre></td></tr></table></figure></p>\n<p>2、版本升级<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 查看可升级的版本</span><br><span class=\"line\">$ kubeadm upgrade plan</span><br><span class=\"line\"></span><br><span class=\"line\"># 升级至指定版本</span><br><span class=\"line\">$ kubeadm upgrade apply [version]</span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<ol>\n<li>要执行升级，需要先将 kubeadm 升级到对应的版本；</li>\n<li>kubeadm 并不负责 kubelet 的升级，需要在升级完 master 组件后，手工对 kubelet 进行升级。</li>\n</ol>\n</blockquote>\n<h2 id=\"七、创建过程中的一些-case-记录\"><a href=\"#七、创建过程中的一些-case-记录\" class=\"headerlink\" title=\"七、创建过程中的一些 case 记录\"></a>七、创建过程中的一些 case 记录</h2><h5 id=\"1、flannel-容器启动报错：pod-cidr-not-assgned\"><a href=\"#1、flannel-容器启动报错：pod-cidr-not-assgned\" class=\"headerlink\" title=\"1、flannel 容器启动报错：pod cidr not assgned\"></a>1、flannel 容器启动报错：pod cidr not assgned</h5><p>需要在  /etc/kubernetes/manifests/kube-controller-manager.yaml 文件中添加以下配置：</p>\n<p>–allocate-node-cidrs=true<br>–cluster-cidr=10.244.0.0/16</p>\n<p>参考：<a href=\"https://github.com/coreos/flannel/issues/728\" target=\"_blank\" rel=\"noopener\">https://github.com/coreos/flannel/issues/728</a></p>\n<h5 id=\"2、coredns-容器启动失败报错：-proc-sys-net-ipv6-conf-eth0-accept-dad-no-such-file-or-directory\"><a href=\"#2、coredns-容器启动失败报错：-proc-sys-net-ipv6-conf-eth0-accept-dad-no-such-file-or-directory\" class=\"headerlink\" title=\"2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory\"></a>2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line</span><br><span class=\"line\">$ grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class=\"line\">$ shutdown -r now</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"https://github.com/containernetworking/cni/issues/569\" target=\"_blank\" rel=\"noopener\">https://github.com/containernetworking/cni/issues/569</a></p>\n<h5 id=\"3、kubeadm-证书有效期问题\"><a href=\"#3、kubeadm-证书有效期问题\" class=\"headerlink\" title=\"3、kubeadm 证书有效期问题\"></a>3、kubeadm 证书有效期问题</h5><p>默认情况下，kubeadm 会生成集群运行所需的所有证书，我们也可以通过提供自己的证书来覆盖此行为。要做到这一点，必须把它们放在 –cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录（默认目录为 /etc/kubernetes/pki），如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。所以只要我们自己提供一个有效期很长的证书去覆盖掉默认的证书就可以来避免这个的问题。</p>\n<h5 id=\"4、kubeadm-join-时-token-无法生效\"><a href=\"#4、kubeadm-join-时-token-无法生效\" class=\"headerlink\" title=\"4、kubeadm join 时 token 无法生效\"></a>4、kubeadm join 时 token 无法生效</h5><p>token 的失效为24小时，若忘记或者 token 过期可以使用 <code>kubeadm token create</code> 重新生成 token。</p>\n<h2 id=\"八、总结\"><a href=\"#八、总结\" class=\"headerlink\" title=\"八、总结\"></a>八、总结</h2><p>本篇文章讲述了使用 kubeadm 来搭建一个 kubernetes 集群，kubeadm 暂时还不建议用于生产环境，若部署生产环境请使用二进制文件。kubeadm 搭建出的集群还是有很多不完善的地方，比如，集群 master 组件的参数配置问题，官方默认的并不会满足需求，有许多参数需要根据实际情况进行修改。</p>\n<p>参考：<br><a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\" target=\"_blank\" rel=\"noopener\">Creating a single master cluster with kubeadm</a><br><a href=\"https://github.com/feiskyer/kubernetes-handbook/blob/master/components/kubeadm.md\" target=\"_blank\" rel=\"noopener\">kubeadm 工作原理</a><br><a href=\"http://dockone.io/article/4645\" target=\"_blank\" rel=\"noopener\">DockOne微信分享（一六三）：Kubernetes官方集群部署工具kubeadm原理解析</a><br><a href=\"https://segmentfault.com/a/1190000015787725\" target=\"_blank\" rel=\"noopener\">centos7.2 安装k8s v1.11.0</a></p>\n"},{"title":"kubelet 创建 pod 的流程","date":"2019-01-03T00:15:30.000Z","type":"kubelet","_content":"上篇文章介绍了 [kubelet 的启动流程](http://blog.tianfeiyu.com/2018/12/23/kubelet_init/)，本篇文章主要介绍 kubelet 创建 pod 的流程。\n \n> kubernetes 版本： v1.12 \n\n![kubelet 工作原理](http://cdn.tianfeiyu.com/kubelet-1.png)\n\nkubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。\n\n\n本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。\n\n## kubelet 创建 pod 的流程\n\n![kubelet 创建 pod 的流程](http://cdn.tianfeiyu.com/kubelet-2.png)\n\n\n### 1、kubelet 的控制循环（syncLoop）\n\nsyncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。\n\n\n```\nfunc (kl *Kubelet) syncLoop(updates <-chan kubetypes.PodUpdate, handler SyncHandler) {\n\tglog.Info(\"Starting kubelet main sync loop.\")\n\n\t// syncTicker 每秒检测一次是否有需要同步的 pod workers\n\tsyncTicker := time.NewTicker(time.Second)\n\tdefer syncTicker.Stop()\n\t// 每两秒检测一次是否有需要清理的 pod\n\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\n\tdefer housekeepingTicker.Stop()\n\t// pod 的生命周期变化\n\tplegCh := kl.pleg.Watch()\n\tconst (\n\t\tbase   = 100 * time.Millisecond\n\t\tmax    = 5 * time.Second\n\t\tfactor = 2\n\t)\n\tduration := base\n\tfor {\n\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {\n\t\t\ttime.Sleep(duration)\n\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))\n\t\t\tcontinue\n\t\t}\n        ...\n\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t\t// 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface，\n\t\t// 在该文件开头处定义\n\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {\n\t\t\tbreak\n\t\t}\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t}\n}\n```\n\n \n\n### 2、监听 pod 变化（syncLoopIteration） \n\nsyncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息：\n\n- configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。\n- syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态\n- houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作\n- plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。\n- livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作\n\n\n```\nfunc (kl *Kubelet) syncLoopIteration(configCh <-chan kubetypes.PodUpdate, handler SyncHandler,\n\tsyncCh <-chan time.Time, housekeepingCh <-chan time.Time, plegCh <-chan *pleg.PodLifecycleEvent) bool {\n\tselect {\n\tcase u, open := <-configCh:\n\t\tif !open {\n\t\t\tglog.Errorf(\"Update channel is closed. Exiting the sync loop.\")\n\t\t\treturn false\n\t\t}\n\n\t\tswitch u.Op {\n\t\tcase kubetypes.ADD:\n\t\t\t...\n\t\tcase kubetypes.UPDATE:\n\t\t\t...\n\t\tcase kubetypes.REMOVE:\n\t\t\t...\n\t\tcase kubetypes.RECONCILE:\n\t\t\t...\n\t\tcase kubetypes.DELETE:\n\t\t\t...\n\t\tcase kubetypes.RESTORE:\n\t\t\t...\n\t\tcase kubetypes.SET:\n\t\t\t...\n\t\t}\n\t\t...\n\tcase e := <-plegCh:\n\t\t...\n\tcase <-syncCh:\n\t\t...\n\tcase update := <-kl.livenessManager.Updates():\n\t\t...\n\tcase <-housekeepingCh:\n\t\t...\n\t}\n\treturn true\n}\n```\n\n### 3、处理新增 pod（HandlePodAddtions）\n\n对于事件中的每个 pod，执行以下操作：\n\n- 1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理\n- 2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了\n- 3、如果是 mirror pod 调用其单独的方法\n- 4、验证 pod 是否能在该节点运行，如果不可以直接拒绝\n- 5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理\n- 6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测\n\n```\nfunc (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {\n\tstart := kl.clock.Now()\n\t// 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理\n\tsort.Sort(sliceutils.PodsByCreationTime(pods))\n\tfor _, pod := range pods {\n\t\tif kl.dnsConfigurer != nil && kl.dnsConfigurer.ResolverConfig != \"\" {\n\t\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()\n\t\t}\n\t\texistingPods := kl.podManager.GetPods()\n\t\t// 把 pod 加入到 podManager 中\n\t\tkl.podManager.AddPod(pod)\n\n\t\t// 判断是否是 mirror pod（即 static pod）\n\t\tif kubepod.IsMirrorPod(pod) {\n\t\t\tkl.handleMirrorPod(pod, start)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !kl.podIsTerminated(pod) {\n\t\t\tactivePods := kl.filterOutTerminatedPods(existingPods)\n\t\t\t// 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间)\n\t\t\t// Check if we can admit the pod; if not, reject it.\n\t\t\tif ok, reason, message := kl.canAdmitPod(activePods, pod); !ok {\n\t\t\t\tkl.rejectPod(pod, reason, message)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t\n\t\tmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)\n\t\t// 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法.\n\t\tkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)\n\t\t// 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测\n\t\tkl.probeManager.AddPod(pod)\n\t}\n}\n```\n\n> static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。\n\n\n### 4、下发任务（dispatchWork）\n\ndispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。\n\n```\nfunc (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) {\n\tif kl.podIsTerminated(pod) {\n\t\tif pod.DeletionTimestamp != nil {\n\t\t\tkl.statusManager.TerminatePod(pod)\n\t\t}\n\t\treturn\n\t}\n\t// 落实在 podWorkers 中\n\tkl.podWorkers.UpdatePod(&UpdatePodOptions{\n\t\tPod:        pod,\n\t\tMirrorPod:  mirrorPod,\n\t\tUpdateType: syncType,\n\t\tOnCompleteFunc: func(err error) {\n\t\t\tif err != nil {\n\t\t\t\tmetrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start))\n\t\t\t}\n\t\t},\n\t})\n\tif syncType == kubetypes.SyncPodCreate {\n\t\tmetrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))\n\t}\n}\n```\n\n\n### 5、更新事件的 channel（UpdatePod）\n\npodWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。\n\n\n```\nfunc (p *podWorkers) UpdatePod(options *UpdatePodOptions) {\n\tpod := options.Pod\n\tuid := pod.UID\n\tvar podUpdates chan UpdatePodOptions\n\tvar exists bool\n\n\tp.podLock.Lock()\n\tdefer p.podLock.Unlock()\n\n\t// 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel\n\tif podUpdates, exists = p.podUpdates[uid]; !exists {\n\t\t// 创建 channel\n\t\tpodUpdates = make(chan UpdatePodOptions, 1)\n\t\tp.podUpdates[uid] = podUpdates\n\n\t\t// 启动 goroutine\n\t\tgo func() {\n\t\t\tdefer runtime.HandleCrash()\n\t\t\tp.managePodLoop(podUpdates)\n\t\t}()\n\t}\n\t// 下发更新事件\n\tif !p.isWorking[pod.UID] {\n\t\tp.isWorking[pod.UID] = true\n\t\tpodUpdates <- *options\n\t} else {\n\t\tupdate, found := p.lastUndeliveredWorkUpdate[pod.UID]\n\t\tif !found || update.UpdateType != kubetypes.SyncPodKill {\n\t\t\tp.lastUndeliveredWorkUpdate[pod.UID] = *options\n\t\t}\n\t}\n}\n```\n\n### 6、调用 syncPodFn 方法同步 pod（managePodLoop）\nmanagePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情:\n\n- 将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync\n- 将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。\n\n\n```\nfunc (p *podWorkers) managePodLoop(podUpdates <-chan UpdatePodOptions) {\n\tvar lastSyncTime time.Time\n\tfor update := range podUpdates {\n\t\terr := func() error {\n\t\t\tpodUID := update.Pod.UID\n\t\t\tstatus, err := p.podCache.GetNewerThan(podUID, lastSyncTime)\n\t\t\tif err != nil {\n\t\t\t\t...\n\t\t\t}\n\t\t\terr = p.syncPodFn(syncPodOptions{\n\t\t\t\tmirrorPod:      update.MirrorPod,\n\t\t\t\tpod:            update.Pod,\n\t\t\t\tpodStatus:      status,\n\t\t\t\tkillPodOptions: update.KillPodOptions,\n\t\t\t\tupdateType:     update.UpdateType,\n\t\t\t})\n\t\t\tlastSyncTime = time.Now()\n\t\t\treturn err\n\t\t}()\n\t\tif update.OnCompleteFunc != nil {\n\t\t\tupdate.OnCompleteFunc(err)\n\t\t}\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\tp.wrapUp(update.Pod.UID, err)\n\t}\n}\n```\n\n### 7、完成创建容器前的准备工作（SyncPod）\n\n在这个方法中，主要完成以下几件事情：\n\n- 如果是删除 pod，立即执行并返回\n- 同步 podStatus 到 kubelet.statusManager\n- 检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息\n- 创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup\n- 如果是 static Pod，就创建或者更新对应的 mirrorPod\n- 创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据\n- 然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。\n- 调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑\n\n这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。\n\n```\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error {\n\t// pull out the required options\n\tpod := o.pod\n\tmirrorPod := o.mirrorPod\n\tpodStatus := o.podStatus\n\tupdateType := o.updateType\n\n\t// 是否为 删除 pod\n\tif updateType == kubetypes.SyncPodKill {\n\t\t...\n\t}\n    ...\n\t// 检查 pod 是否能运行在本节点\n\trunnable := kl.canRunPod(pod)\n\tif !runnable.Admit {\n\t\t...\n\t}\n\n\t// 更新 pod 状态\n\tkl.statusManager.SetPodStatus(pod, apiPodStatus)\n\n\t// 如果 pod 非 running 状态则直接 kill 掉\n\tif !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed {\n\t\t...\n\t}\n\n\t// 加载网络插件\n\tif rs := kl.runtimeState.networkErrors(); len(rs) != 0 && !kubecontainer.IsHostNetworkPod(pod) {\n\t\t...\n\t}\n\n\tpcm := kl.containerManager.NewPodContainerManager()\n\tif !kl.podIsTerminated(pod) {\n\t\t...\n\t\t// 创建并更新 pod 的 cgroups\n\t\tif !(podKilled && pod.Spec.RestartPolicy == v1.RestartPolicyNever) {\n\t\t\tif !pcm.Exists(pod) {\n\t\t\t\t...\n\t\t\t}\n\t\t}\n\t}\n\n\t// 为 static pod 创建对应的 mirror pod\n\tif kubepod.IsStaticPod(pod) {\n\t\t...\n\t}\n\n\t// 创建数据目录\n\tif err := kl.makePodDataDirs(pod); err != nil {\n\t\t...\n\t}\n\n\t// 挂载 volume\n\tif !kl.podIsTerminated(pod) {\n\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\t// 获取 secret 信息\n\tpullSecrets := kl.getPullSecretsForPod(pod)\n\n\t// 调用 containerRuntime 的 SyncPod 方法开始创建容器\n\tresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)\n\tkl.reasonCache.Update(pod.UID, result)\n\tif err := result.Error(); err != nil {\n\t\t...\n\t}\n\n\treturn nil\n}\n```\n\n\n### 8、创建容器\n\ncontainerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。\nsyncPod 主要执行以下几个操作：\n- 1、计算 sandbox 和 container 是否发生变化\n- 2、创建 sandbox 容器\n- 3、启动 init 容器\n- 4、启动业务容器\n\ninitContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。\n\n```\nfunc (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {\n\t// 1、计算 sandbox 和 container 是否发生变化\n\tpodContainerChanges := m.computePodActions(pod, podStatus)\n\tif podContainerChanges.CreateSandbox {\n\t\tref, err := ref.GetReference(legacyscheme.Scheme, pod)\n\t\tif err != nil {\n\t\t\tglog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err)\n\t\t}\n\t\t...\n\t}\n\n\t// 2、kill 掉 sandbox 已经改变的 pod\n\tif podContainerChanges.KillPod {\n\t\t...\n\t} else {\n\t\t// 3、kill 掉非 running 状态的 containers\n\t\t...\n\t\tfor containerID, containerInfo := range podContainerChanges.ContainersToKill {\n\t\t\t...\n\t\t\tif err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil {\n\t\t\t\t...\n\t\t\t}\n\t\t}\n\t}\n\n\tm.pruneInitContainersBeforeStart(pod, podStatus)\n\tpodIP := \"\"\n\tif podStatus != nil {\n\t\tpodIP = podStatus.IP\n\t}\n\n\t// 4、创建 sandbox \n\tpodSandboxID := podContainerChanges.SandboxID\n\tif podContainerChanges.CreateSandbox {\n\t\tpodSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\t...\n\t\tpodSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\t// 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置\n\t\tif !kubecontainer.IsHostNetworkPod(pod) {\n\t\t\tpodIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus)\n\t\t\tglog.V(4).Infof(\"Determined the ip %q for pod %q after sandbox changed\", podIP, format.Pod(pod))\n\t\t}\n\t}\n\n\tconfigPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)\n\tresult.AddSyncResult(configPodSandboxResult)\n\t// 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等)\n\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)\n\t...\n\n\t// 5、启动 init container\n\tif container := podContainerChanges.NextInitContainerToStart; container != nil {\n\t\t...\n\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\t// 6、启动业务容器\n\tfor _, idx := range podContainerChanges.ContainersToStart {\n\t\t...\n\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\t\n\treturn\n}\n```\n\n\n### 9、启动容器\n\n最终由 startContainer 完成容器的启动，其主要有以下几个步骤：\n\n- 1、拉取镜像\n- 2、生成业务容器的配置信息\n- 3、调用 docker api 创建容器\n- 4、启动容器\n- 5、执行 post start hook\n\n\n```\nfunc (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) {\n\t// 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。\n\timageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)\n\tif err != nil {\n\t\t...\n\t}\n\n\tref, err := kubecontainer.GenerateContainerRef(pod, container)\n\tif err != nil {\n\t\t...\n\t}\n\n\t// 设置 RestartCount \n\trestartCount := 0\n\tcontainerStatus := podStatus.FindContainerStatusByName(container.Name)\n\tif containerStatus != nil {\n\t\trestartCount = containerStatus.RestartCount + 1\n\t}\n\n\t// 2、生成业务容器的配置信息\n\tcontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType)\n\tif cleanupAction != nil {\n\t\tdefer cleanupAction()\n\t}\n\t...\n\n\t// 3、通过 client.CreateContainer 调用 docker api 创建业务容器\n\tcontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)\n\tif err != nil {\n\t\t...\n\t}\n\terr = m.internalLifecycle.PreStartContainer(pod, container, containerID)\n\tif err != nil {\n\t\t...\n\t}\n\t...\n\n\t// 3、启动业务容器\n\terr = m.runtimeService.StartContainer(containerID)\n\tif err != nil {\n\t\t...\n\t}\n\n\tcontainerMeta := containerConfig.GetMetadata()\n\tsandboxMeta := podSandboxConfig.GetMetadata()\n\tlegacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name,\n\t\tsandboxMeta.Namespace)\n\tcontainerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath)\n\tif _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) {\n\t\tif err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil {\n\t\t\tglog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\",\n\t\t\t\tlegacySymlink, containerID, containerLog, err)\n\t\t}\n\t}\n\n\t// 4、执行 post start hook\n\tif container.Lifecycle != nil && container.Lifecycle.PostStart != nil {\n\t\tkubeContainerID := kubecontainer.ContainerID{\n\t\t\tType: m.runtimeName,\n\t\t\tID:   containerID,\n\t\t}\n\t\t// runner.Run 这个方法的主要作用就是在业务容器起来的时候，\n\t\t// 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。\n\t\t// 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。\n\t\tmsg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)\n\t\tif handlerErr != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\treturn \"\", nil\n}\n```\n\n\n## 总结\n\n本文主要讲述了 kubelet 从监听到容器调度至本节点再到创建容器的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。\n\n\n参考：\n[k8s源码分析-kubelet](https://sycki.com/articles/kubernetes/k8s-code-kubelet)\n[Kubelet源码分析(一):启动流程分析](https://segmentfault.com/a/1190000008267351)\n[kubelet 源码分析：pod 新建流程](http://cizixs.com/2017/06/07/kubelet-source-code-analysis-part-2/)\n[kubelet创建Pod流程解析](https://fatsheep9146.github.io/2018/07/22/kubelet%E5%88%9B%E5%BB%BAPod%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/)\n[Kubelet: Pod Lifecycle Event Generator (PLEG) Design-\tproposals](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md)\n\n","source":"_posts/kubelet_create_pod.md","raw":"---\ntitle: kubelet 创建 pod 的流程\ndate: 2019-01-03 08:15:30\ntags: \"kubelet\"\ntype: \"kubelet\"\n\n---\n上篇文章介绍了 [kubelet 的启动流程](http://blog.tianfeiyu.com/2018/12/23/kubelet_init/)，本篇文章主要介绍 kubelet 创建 pod 的流程。\n \n> kubernetes 版本： v1.12 \n\n![kubelet 工作原理](http://cdn.tianfeiyu.com/kubelet-1.png)\n\nkubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。\n\n\n本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。\n\n## kubelet 创建 pod 的流程\n\n![kubelet 创建 pod 的流程](http://cdn.tianfeiyu.com/kubelet-2.png)\n\n\n### 1、kubelet 的控制循环（syncLoop）\n\nsyncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。\n\n\n```\nfunc (kl *Kubelet) syncLoop(updates <-chan kubetypes.PodUpdate, handler SyncHandler) {\n\tglog.Info(\"Starting kubelet main sync loop.\")\n\n\t// syncTicker 每秒检测一次是否有需要同步的 pod workers\n\tsyncTicker := time.NewTicker(time.Second)\n\tdefer syncTicker.Stop()\n\t// 每两秒检测一次是否有需要清理的 pod\n\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\n\tdefer housekeepingTicker.Stop()\n\t// pod 的生命周期变化\n\tplegCh := kl.pleg.Watch()\n\tconst (\n\t\tbase   = 100 * time.Millisecond\n\t\tmax    = 5 * time.Second\n\t\tfactor = 2\n\t)\n\tduration := base\n\tfor {\n\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {\n\t\t\ttime.Sleep(duration)\n\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))\n\t\t\tcontinue\n\t\t}\n        ...\n\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t\t// 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface，\n\t\t// 在该文件开头处定义\n\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {\n\t\t\tbreak\n\t\t}\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t}\n}\n```\n\n \n\n### 2、监听 pod 变化（syncLoopIteration） \n\nsyncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息：\n\n- configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。\n- syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态\n- houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作\n- plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。\n- livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作\n\n\n```\nfunc (kl *Kubelet) syncLoopIteration(configCh <-chan kubetypes.PodUpdate, handler SyncHandler,\n\tsyncCh <-chan time.Time, housekeepingCh <-chan time.Time, plegCh <-chan *pleg.PodLifecycleEvent) bool {\n\tselect {\n\tcase u, open := <-configCh:\n\t\tif !open {\n\t\t\tglog.Errorf(\"Update channel is closed. Exiting the sync loop.\")\n\t\t\treturn false\n\t\t}\n\n\t\tswitch u.Op {\n\t\tcase kubetypes.ADD:\n\t\t\t...\n\t\tcase kubetypes.UPDATE:\n\t\t\t...\n\t\tcase kubetypes.REMOVE:\n\t\t\t...\n\t\tcase kubetypes.RECONCILE:\n\t\t\t...\n\t\tcase kubetypes.DELETE:\n\t\t\t...\n\t\tcase kubetypes.RESTORE:\n\t\t\t...\n\t\tcase kubetypes.SET:\n\t\t\t...\n\t\t}\n\t\t...\n\tcase e := <-plegCh:\n\t\t...\n\tcase <-syncCh:\n\t\t...\n\tcase update := <-kl.livenessManager.Updates():\n\t\t...\n\tcase <-housekeepingCh:\n\t\t...\n\t}\n\treturn true\n}\n```\n\n### 3、处理新增 pod（HandlePodAddtions）\n\n对于事件中的每个 pod，执行以下操作：\n\n- 1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理\n- 2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了\n- 3、如果是 mirror pod 调用其单独的方法\n- 4、验证 pod 是否能在该节点运行，如果不可以直接拒绝\n- 5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理\n- 6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测\n\n```\nfunc (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) {\n\tstart := kl.clock.Now()\n\t// 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理\n\tsort.Sort(sliceutils.PodsByCreationTime(pods))\n\tfor _, pod := range pods {\n\t\tif kl.dnsConfigurer != nil && kl.dnsConfigurer.ResolverConfig != \"\" {\n\t\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()\n\t\t}\n\t\texistingPods := kl.podManager.GetPods()\n\t\t// 把 pod 加入到 podManager 中\n\t\tkl.podManager.AddPod(pod)\n\n\t\t// 判断是否是 mirror pod（即 static pod）\n\t\tif kubepod.IsMirrorPod(pod) {\n\t\t\tkl.handleMirrorPod(pod, start)\n\t\t\tcontinue\n\t\t}\n\n\t\tif !kl.podIsTerminated(pod) {\n\t\t\tactivePods := kl.filterOutTerminatedPods(existingPods)\n\t\t\t// 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间)\n\t\t\t// Check if we can admit the pod; if not, reject it.\n\t\t\tif ok, reason, message := kl.canAdmitPod(activePods, pod); !ok {\n\t\t\t\tkl.rejectPod(pod, reason, message)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t\n\t\tmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)\n\t\t// 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法.\n\t\tkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)\n\t\t// 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测\n\t\tkl.probeManager.AddPod(pod)\n\t}\n}\n```\n\n> static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。\n\n\n### 4、下发任务（dispatchWork）\n\ndispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。\n\n```\nfunc (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) {\n\tif kl.podIsTerminated(pod) {\n\t\tif pod.DeletionTimestamp != nil {\n\t\t\tkl.statusManager.TerminatePod(pod)\n\t\t}\n\t\treturn\n\t}\n\t// 落实在 podWorkers 中\n\tkl.podWorkers.UpdatePod(&UpdatePodOptions{\n\t\tPod:        pod,\n\t\tMirrorPod:  mirrorPod,\n\t\tUpdateType: syncType,\n\t\tOnCompleteFunc: func(err error) {\n\t\t\tif err != nil {\n\t\t\t\tmetrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start))\n\t\t\t}\n\t\t},\n\t})\n\tif syncType == kubetypes.SyncPodCreate {\n\t\tmetrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))\n\t}\n}\n```\n\n\n### 5、更新事件的 channel（UpdatePod）\n\npodWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。\n\n\n```\nfunc (p *podWorkers) UpdatePod(options *UpdatePodOptions) {\n\tpod := options.Pod\n\tuid := pod.UID\n\tvar podUpdates chan UpdatePodOptions\n\tvar exists bool\n\n\tp.podLock.Lock()\n\tdefer p.podLock.Unlock()\n\n\t// 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel\n\tif podUpdates, exists = p.podUpdates[uid]; !exists {\n\t\t// 创建 channel\n\t\tpodUpdates = make(chan UpdatePodOptions, 1)\n\t\tp.podUpdates[uid] = podUpdates\n\n\t\t// 启动 goroutine\n\t\tgo func() {\n\t\t\tdefer runtime.HandleCrash()\n\t\t\tp.managePodLoop(podUpdates)\n\t\t}()\n\t}\n\t// 下发更新事件\n\tif !p.isWorking[pod.UID] {\n\t\tp.isWorking[pod.UID] = true\n\t\tpodUpdates <- *options\n\t} else {\n\t\tupdate, found := p.lastUndeliveredWorkUpdate[pod.UID]\n\t\tif !found || update.UpdateType != kubetypes.SyncPodKill {\n\t\t\tp.lastUndeliveredWorkUpdate[pod.UID] = *options\n\t\t}\n\t}\n}\n```\n\n### 6、调用 syncPodFn 方法同步 pod（managePodLoop）\nmanagePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情:\n\n- 将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync\n- 将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。\n\n\n```\nfunc (p *podWorkers) managePodLoop(podUpdates <-chan UpdatePodOptions) {\n\tvar lastSyncTime time.Time\n\tfor update := range podUpdates {\n\t\terr := func() error {\n\t\t\tpodUID := update.Pod.UID\n\t\t\tstatus, err := p.podCache.GetNewerThan(podUID, lastSyncTime)\n\t\t\tif err != nil {\n\t\t\t\t...\n\t\t\t}\n\t\t\terr = p.syncPodFn(syncPodOptions{\n\t\t\t\tmirrorPod:      update.MirrorPod,\n\t\t\t\tpod:            update.Pod,\n\t\t\t\tpodStatus:      status,\n\t\t\t\tkillPodOptions: update.KillPodOptions,\n\t\t\t\tupdateType:     update.UpdateType,\n\t\t\t})\n\t\t\tlastSyncTime = time.Now()\n\t\t\treturn err\n\t\t}()\n\t\tif update.OnCompleteFunc != nil {\n\t\t\tupdate.OnCompleteFunc(err)\n\t\t}\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\tp.wrapUp(update.Pod.UID, err)\n\t}\n}\n```\n\n### 7、完成创建容器前的准备工作（SyncPod）\n\n在这个方法中，主要完成以下几件事情：\n\n- 如果是删除 pod，立即执行并返回\n- 同步 podStatus 到 kubelet.statusManager\n- 检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息\n- 创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup\n- 如果是 static Pod，就创建或者更新对应的 mirrorPod\n- 创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据\n- 然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。\n- 调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑\n\n这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。\n\n```\nfunc (kl *Kubelet) syncPod(o syncPodOptions) error {\n\t// pull out the required options\n\tpod := o.pod\n\tmirrorPod := o.mirrorPod\n\tpodStatus := o.podStatus\n\tupdateType := o.updateType\n\n\t// 是否为 删除 pod\n\tif updateType == kubetypes.SyncPodKill {\n\t\t...\n\t}\n    ...\n\t// 检查 pod 是否能运行在本节点\n\trunnable := kl.canRunPod(pod)\n\tif !runnable.Admit {\n\t\t...\n\t}\n\n\t// 更新 pod 状态\n\tkl.statusManager.SetPodStatus(pod, apiPodStatus)\n\n\t// 如果 pod 非 running 状态则直接 kill 掉\n\tif !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed {\n\t\t...\n\t}\n\n\t// 加载网络插件\n\tif rs := kl.runtimeState.networkErrors(); len(rs) != 0 && !kubecontainer.IsHostNetworkPod(pod) {\n\t\t...\n\t}\n\n\tpcm := kl.containerManager.NewPodContainerManager()\n\tif !kl.podIsTerminated(pod) {\n\t\t...\n\t\t// 创建并更新 pod 的 cgroups\n\t\tif !(podKilled && pod.Spec.RestartPolicy == v1.RestartPolicyNever) {\n\t\t\tif !pcm.Exists(pod) {\n\t\t\t\t...\n\t\t\t}\n\t\t}\n\t}\n\n\t// 为 static pod 创建对应的 mirror pod\n\tif kubepod.IsStaticPod(pod) {\n\t\t...\n\t}\n\n\t// 创建数据目录\n\tif err := kl.makePodDataDirs(pod); err != nil {\n\t\t...\n\t}\n\n\t// 挂载 volume\n\tif !kl.podIsTerminated(pod) {\n\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\t// 获取 secret 信息\n\tpullSecrets := kl.getPullSecretsForPod(pod)\n\n\t// 调用 containerRuntime 的 SyncPod 方法开始创建容器\n\tresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)\n\tkl.reasonCache.Update(pod.UID, result)\n\tif err := result.Error(); err != nil {\n\t\t...\n\t}\n\n\treturn nil\n}\n```\n\n\n### 8、创建容器\n\ncontainerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。\nsyncPod 主要执行以下几个操作：\n- 1、计算 sandbox 和 container 是否发生变化\n- 2、创建 sandbox 容器\n- 3、启动 init 容器\n- 4、启动业务容器\n\ninitContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。\n\n```\nfunc (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) {\n\t// 1、计算 sandbox 和 container 是否发生变化\n\tpodContainerChanges := m.computePodActions(pod, podStatus)\n\tif podContainerChanges.CreateSandbox {\n\t\tref, err := ref.GetReference(legacyscheme.Scheme, pod)\n\t\tif err != nil {\n\t\t\tglog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err)\n\t\t}\n\t\t...\n\t}\n\n\t// 2、kill 掉 sandbox 已经改变的 pod\n\tif podContainerChanges.KillPod {\n\t\t...\n\t} else {\n\t\t// 3、kill 掉非 running 状态的 containers\n\t\t...\n\t\tfor containerID, containerInfo := range podContainerChanges.ContainersToKill {\n\t\t\t...\n\t\t\tif err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil {\n\t\t\t\t...\n\t\t\t}\n\t\t}\n\t}\n\n\tm.pruneInitContainersBeforeStart(pod, podStatus)\n\tpodIP := \"\"\n\tif podStatus != nil {\n\t\tpodIP = podStatus.IP\n\t}\n\n\t// 4、创建 sandbox \n\tpodSandboxID := podContainerChanges.SandboxID\n\tif podContainerChanges.CreateSandbox {\n\t\tpodSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\t...\n\t\tpodSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)\n\t\tif err != nil {\n\t\t\t...\n\t\t}\n\t\t// 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置\n\t\tif !kubecontainer.IsHostNetworkPod(pod) {\n\t\t\tpodIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus)\n\t\t\tglog.V(4).Infof(\"Determined the ip %q for pod %q after sandbox changed\", podIP, format.Pod(pod))\n\t\t}\n\t}\n\n\tconfigPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)\n\tresult.AddSyncResult(configPodSandboxResult)\n\t// 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等)\n\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)\n\t...\n\n\t// 5、启动 init container\n\tif container := podContainerChanges.NextInitContainerToStart; container != nil {\n\t\t...\n\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\t// 6、启动业务容器\n\tfor _, idx := range podContainerChanges.ContainersToStart {\n\t\t...\n\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil {\n\t\t\t...\n\t\t}\n\t}\n\t\n\treturn\n}\n```\n\n\n### 9、启动容器\n\n最终由 startContainer 完成容器的启动，其主要有以下几个步骤：\n\n- 1、拉取镜像\n- 2、生成业务容器的配置信息\n- 3、调用 docker api 创建容器\n- 4、启动容器\n- 5、执行 post start hook\n\n\n```\nfunc (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) {\n\t// 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。\n\timageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)\n\tif err != nil {\n\t\t...\n\t}\n\n\tref, err := kubecontainer.GenerateContainerRef(pod, container)\n\tif err != nil {\n\t\t...\n\t}\n\n\t// 设置 RestartCount \n\trestartCount := 0\n\tcontainerStatus := podStatus.FindContainerStatusByName(container.Name)\n\tif containerStatus != nil {\n\t\trestartCount = containerStatus.RestartCount + 1\n\t}\n\n\t// 2、生成业务容器的配置信息\n\tcontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType)\n\tif cleanupAction != nil {\n\t\tdefer cleanupAction()\n\t}\n\t...\n\n\t// 3、通过 client.CreateContainer 调用 docker api 创建业务容器\n\tcontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)\n\tif err != nil {\n\t\t...\n\t}\n\terr = m.internalLifecycle.PreStartContainer(pod, container, containerID)\n\tif err != nil {\n\t\t...\n\t}\n\t...\n\n\t// 3、启动业务容器\n\terr = m.runtimeService.StartContainer(containerID)\n\tif err != nil {\n\t\t...\n\t}\n\n\tcontainerMeta := containerConfig.GetMetadata()\n\tsandboxMeta := podSandboxConfig.GetMetadata()\n\tlegacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name,\n\t\tsandboxMeta.Namespace)\n\tcontainerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath)\n\tif _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) {\n\t\tif err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil {\n\t\t\tglog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\",\n\t\t\t\tlegacySymlink, containerID, containerLog, err)\n\t\t}\n\t}\n\n\t// 4、执行 post start hook\n\tif container.Lifecycle != nil && container.Lifecycle.PostStart != nil {\n\t\tkubeContainerID := kubecontainer.ContainerID{\n\t\t\tType: m.runtimeName,\n\t\t\tID:   containerID,\n\t\t}\n\t\t// runner.Run 这个方法的主要作用就是在业务容器起来的时候，\n\t\t// 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。\n\t\t// 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。\n\t\tmsg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)\n\t\tif handlerErr != nil {\n\t\t\t...\n\t\t}\n\t}\n\n\treturn \"\", nil\n}\n```\n\n\n## 总结\n\n本文主要讲述了 kubelet 从监听到容器调度至本节点再到创建容器的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。\n\n\n参考：\n[k8s源码分析-kubelet](https://sycki.com/articles/kubernetes/k8s-code-kubelet)\n[Kubelet源码分析(一):启动流程分析](https://segmentfault.com/a/1190000008267351)\n[kubelet 源码分析：pod 新建流程](http://cizixs.com/2017/06/07/kubelet-source-code-analysis-part-2/)\n[kubelet创建Pod流程解析](https://fatsheep9146.github.io/2018/07/22/kubelet%E5%88%9B%E5%BB%BAPod%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/)\n[Kubelet: Pod Lifecycle Event Generator (PLEG) Design-\tproposals](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md)\n\n","slug":"kubelet_create_pod","published":1,"updated":"2019-07-21T09:46:50.287Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59s001capwn30fsvydm","content":"<p>上篇文章介绍了 <a href=\"http://blog.tianfeiyu.com/2018/12/23/kubelet_init/\" target=\"_blank\" rel=\"noopener\">kubelet 的启动流程</a>，本篇文章主要介绍 kubelet 创建 pod 的流程。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<p><img src=\"http://cdn.tianfeiyu.com/kubelet-1.png\" alt=\"kubelet 工作原理\"></p>\n<p>kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。</p>\n<p>本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。</p>\n<h2 id=\"kubelet-创建-pod-的流程\"><a href=\"#kubelet-创建-pod-的流程\" class=\"headerlink\" title=\"kubelet 创建 pod 的流程\"></a>kubelet 创建 pod 的流程</h2><p><img src=\"http://cdn.tianfeiyu.com/kubelet-2.png\" alt=\"kubelet 创建 pod 的流程\"></p>\n<h3 id=\"1、kubelet-的控制循环（syncLoop）\"><a href=\"#1、kubelet-的控制循环（syncLoop）\" class=\"headerlink\" title=\"1、kubelet 的控制循环（syncLoop）\"></a>1、kubelet 的控制循环（syncLoop）</h3><p>syncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123;</span><br><span class=\"line\">\tglog.Info(&quot;Starting kubelet main sync loop.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// syncTicker 每秒检测一次是否有需要同步的 pod workers</span><br><span class=\"line\">\tsyncTicker := time.NewTicker(time.Second)</span><br><span class=\"line\">\tdefer syncTicker.Stop()</span><br><span class=\"line\">\t// 每两秒检测一次是否有需要清理的 pod</span><br><span class=\"line\">\thousekeepingTicker := time.NewTicker(housekeepingPeriod)</span><br><span class=\"line\">\tdefer housekeepingTicker.Stop()</span><br><span class=\"line\">\t// pod 的生命周期变化</span><br><span class=\"line\">\tplegCh := kl.pleg.Watch()</span><br><span class=\"line\">\tconst (</span><br><span class=\"line\">\t\tbase   = 100 * time.Millisecond</span><br><span class=\"line\">\t\tmax    = 5 * time.Second</span><br><span class=\"line\">\t\tfactor = 2</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tduration := base</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123;</span><br><span class=\"line\">\t\t\ttime.Sleep(duration)</span><br><span class=\"line\">\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t\t// 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface，</span><br><span class=\"line\">\t\t// 在该文件开头处定义</span><br><span class=\"line\">\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123;</span><br><span class=\"line\">\t\t\tbreak</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2、监听-pod-变化（syncLoopIteration）\"><a href=\"#2、监听-pod-变化（syncLoopIteration）\" class=\"headerlink\" title=\"2、监听 pod 变化（syncLoopIteration）\"></a>2、监听 pod 变化（syncLoopIteration）</h3><p>syncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息：</p>\n<ul>\n<li>configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。</li>\n<li>syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态</li>\n<li>houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作</li>\n<li>plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。</li>\n<li>livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler,</span><br><span class=\"line\">\tsyncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123;</span><br><span class=\"line\">\tselect &#123;</span><br><span class=\"line\">\tcase u, open := &lt;-configCh:</span><br><span class=\"line\">\t\tif !open &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Update channel is closed. Exiting the sync loop.&quot;)</span><br><span class=\"line\">\t\t\treturn false</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tswitch u.Op &#123;</span><br><span class=\"line\">\t\tcase kubetypes.ADD:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.UPDATE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.REMOVE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.RECONCILE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.DELETE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.RESTORE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.SET:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase e := &lt;-plegCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase &lt;-syncCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase update := &lt;-kl.livenessManager.Updates():</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase &lt;-housekeepingCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn true</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3、处理新增-pod（HandlePodAddtions）\"><a href=\"#3、处理新增-pod（HandlePodAddtions）\" class=\"headerlink\" title=\"3、处理新增 pod（HandlePodAddtions）\"></a>3、处理新增 pod（HandlePodAddtions）</h3><p>对于事件中的每个 pod，执行以下操作：</p>\n<ul>\n<li>1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理</li>\n<li>2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了</li>\n<li>3、如果是 mirror pod 调用其单独的方法</li>\n<li>4、验证 pod 是否能在该节点运行，如果不可以直接拒绝</li>\n<li>5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理</li>\n<li>6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) &#123;</span><br><span class=\"line\">\tstart := kl.clock.Now()</span><br><span class=\"line\">\t// 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理</span><br><span class=\"line\">\tsort.Sort(sliceutils.PodsByCreationTime(pods))</span><br><span class=\"line\">\tfor _, pod := range pods &#123;</span><br><span class=\"line\">\t\tif kl.dnsConfigurer != nil &amp;&amp; kl.dnsConfigurer.ResolverConfig != &quot;&quot; &#123;</span><br><span class=\"line\">\t\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\texistingPods := kl.podManager.GetPods()</span><br><span class=\"line\">\t\t// 把 pod 加入到 podManager 中</span><br><span class=\"line\">\t\tkl.podManager.AddPod(pod)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 判断是否是 mirror pod（即 static pod）</span><br><span class=\"line\">\t\tif kubepod.IsMirrorPod(pod) &#123;</span><br><span class=\"line\">\t\t\tkl.handleMirrorPod(pod, start)</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\t\tactivePods := kl.filterOutTerminatedPods(existingPods)</span><br><span class=\"line\">\t\t\t// 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间)</span><br><span class=\"line\">\t\t\t// Check if we can admit the pod; if not, reject it.</span><br><span class=\"line\">\t\t\tif ok, reason, message := kl.canAdmitPod(activePods, pod); !ok &#123;</span><br><span class=\"line\">\t\t\t\tkl.rejectPod(pod, reason, message)</span><br><span class=\"line\">\t\t\t\tcontinue</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)</span><br><span class=\"line\">\t\t// 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法.</span><br><span class=\"line\">\t\tkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)</span><br><span class=\"line\">\t\t// 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测</span><br><span class=\"line\">\t\tkl.probeManager.AddPod(pod)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。</p>\n</blockquote>\n<h3 id=\"4、下发任务（dispatchWork）\"><a href=\"#4、下发任务（dispatchWork）\" class=\"headerlink\" title=\"4、下发任务（dispatchWork）\"></a>4、下发任务（dispatchWork）</h3><p>dispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) &#123;</span><br><span class=\"line\">\tif kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\tif pod.DeletionTimestamp != nil &#123;</span><br><span class=\"line\">\t\t\tkl.statusManager.TerminatePod(pod)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 落实在 podWorkers 中</span><br><span class=\"line\">\tkl.podWorkers.UpdatePod(&amp;UpdatePodOptions&#123;</span><br><span class=\"line\">\t\tPod:        pod,</span><br><span class=\"line\">\t\tMirrorPod:  mirrorPod,</span><br><span class=\"line\">\t\tUpdateType: syncType,</span><br><span class=\"line\">\t\tOnCompleteFunc: func(err error) &#123;</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tmetrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start))</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t&#125;)</span><br><span class=\"line\">\tif syncType == kubetypes.SyncPodCreate &#123;</span><br><span class=\"line\">\t\tmetrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5、更新事件的-channel（UpdatePod）\"><a href=\"#5、更新事件的-channel（UpdatePod）\" class=\"headerlink\" title=\"5、更新事件的 channel（UpdatePod）\"></a>5、更新事件的 channel（UpdatePod）</h3><p>podWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (p *podWorkers) UpdatePod(options *UpdatePodOptions) &#123;</span><br><span class=\"line\">\tpod := options.Pod</span><br><span class=\"line\">\tuid := pod.UID</span><br><span class=\"line\">\tvar podUpdates chan UpdatePodOptions</span><br><span class=\"line\">\tvar exists bool</span><br><span class=\"line\"></span><br><span class=\"line\">\tp.podLock.Lock()</span><br><span class=\"line\">\tdefer p.podLock.Unlock()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel</span><br><span class=\"line\">\tif podUpdates, exists = p.podUpdates[uid]; !exists &#123;</span><br><span class=\"line\">\t\t// 创建 channel</span><br><span class=\"line\">\t\tpodUpdates = make(chan UpdatePodOptions, 1)</span><br><span class=\"line\">\t\tp.podUpdates[uid] = podUpdates</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 启动 goroutine</span><br><span class=\"line\">\t\tgo func() &#123;</span><br><span class=\"line\">\t\t\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t\t\tp.managePodLoop(podUpdates)</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 下发更新事件</span><br><span class=\"line\">\tif !p.isWorking[pod.UID] &#123;</span><br><span class=\"line\">\t\tp.isWorking[pod.UID] = true</span><br><span class=\"line\">\t\tpodUpdates &lt;- *options</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\tupdate, found := p.lastUndeliveredWorkUpdate[pod.UID]</span><br><span class=\"line\">\t\tif !found || update.UpdateType != kubetypes.SyncPodKill &#123;</span><br><span class=\"line\">\t\t\tp.lastUndeliveredWorkUpdate[pod.UID] = *options</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"6、调用-syncPodFn-方法同步-pod（managePodLoop）\"><a href=\"#6、调用-syncPodFn-方法同步-pod（managePodLoop）\" class=\"headerlink\" title=\"6、调用 syncPodFn 方法同步 pod（managePodLoop）\"></a>6、调用 syncPodFn 方法同步 pod（managePodLoop）</h3><p>managePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情:</p>\n<ul>\n<li>将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync</li>\n<li>将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (p *podWorkers) managePodLoop(podUpdates &lt;-chan UpdatePodOptions) &#123;</span><br><span class=\"line\">\tvar lastSyncTime time.Time</span><br><span class=\"line\">\tfor update := range podUpdates &#123;</span><br><span class=\"line\">\t\terr := func() error &#123;</span><br><span class=\"line\">\t\t\tpodUID := update.Pod.UID</span><br><span class=\"line\">\t\t\tstatus, err := p.podCache.GetNewerThan(podUID, lastSyncTime)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\terr = p.syncPodFn(syncPodOptions&#123;</span><br><span class=\"line\">\t\t\t\tmirrorPod:      update.MirrorPod,</span><br><span class=\"line\">\t\t\t\tpod:            update.Pod,</span><br><span class=\"line\">\t\t\t\tpodStatus:      status,</span><br><span class=\"line\">\t\t\t\tkillPodOptions: update.KillPodOptions,</span><br><span class=\"line\">\t\t\t\tupdateType:     update.UpdateType,</span><br><span class=\"line\">\t\t\t&#125;)</span><br><span class=\"line\">\t\t\tlastSyncTime = time.Now()</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\">\t\tif update.OnCompleteFunc != nil &#123;</span><br><span class=\"line\">\t\t\tupdate.OnCompleteFunc(err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tp.wrapUp(update.Pod.UID, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"7、完成创建容器前的准备工作（SyncPod）\"><a href=\"#7、完成创建容器前的准备工作（SyncPod）\" class=\"headerlink\" title=\"7、完成创建容器前的准备工作（SyncPod）\"></a>7、完成创建容器前的准备工作（SyncPod）</h3><p>在这个方法中，主要完成以下几件事情：</p>\n<ul>\n<li>如果是删除 pod，立即执行并返回</li>\n<li>同步 podStatus 到 kubelet.statusManager</li>\n<li>检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息</li>\n<li>创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup</li>\n<li>如果是 static Pod，就创建或者更新对应的 mirrorPod</li>\n<li>创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据</li>\n<li>然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。</li>\n<li>调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑</li>\n</ul>\n<p>这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncPod(o syncPodOptions) error &#123;</span><br><span class=\"line\">\t// pull out the required options</span><br><span class=\"line\">\tpod := o.pod</span><br><span class=\"line\">\tmirrorPod := o.mirrorPod</span><br><span class=\"line\">\tpodStatus := o.podStatus</span><br><span class=\"line\">\tupdateType := o.updateType</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 是否为 删除 pod</span><br><span class=\"line\">\tif updateType == kubetypes.SyncPodKill &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\t// 检查 pod 是否能运行在本节点</span><br><span class=\"line\">\trunnable := kl.canRunPod(pod)</span><br><span class=\"line\">\tif !runnable.Admit &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 更新 pod 状态</span><br><span class=\"line\">\tkl.statusManager.SetPodStatus(pod, apiPodStatus)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 如果 pod 非 running 状态则直接 kill 掉</span><br><span class=\"line\">\tif !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 加载网络插件</span><br><span class=\"line\">\tif rs := kl.runtimeState.networkErrors(); len(rs) != 0 &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpcm := kl.containerManager.NewPodContainerManager()</span><br><span class=\"line\">\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// 创建并更新 pod 的 cgroups</span><br><span class=\"line\">\t\tif !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) &#123;</span><br><span class=\"line\">\t\t\tif !pcm.Exists(pod) &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 为 static pod 创建对应的 mirror pod</span><br><span class=\"line\">\tif kubepod.IsStaticPod(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建数据目录</span><br><span class=\"line\">\tif err := kl.makePodDataDirs(pod); err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 挂载 volume</span><br><span class=\"line\">\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 获取 secret 信息</span><br><span class=\"line\">\tpullSecrets := kl.getPullSecretsForPod(pod)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 调用 containerRuntime 的 SyncPod 方法开始创建容器</span><br><span class=\"line\">\tresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)</span><br><span class=\"line\">\tkl.reasonCache.Update(pod.UID, result)</span><br><span class=\"line\">\tif err := result.Error(); err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"8、创建容器\"><a href=\"#8、创建容器\" class=\"headerlink\" title=\"8、创建容器\"></a>8、创建容器</h3><p>containerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。<br>syncPod 主要执行以下几个操作：</p>\n<ul>\n<li>1、计算 sandbox 和 container 是否发生变化</li>\n<li>2、创建 sandbox 容器</li>\n<li>3、启动 init 容器</li>\n<li>4、启动业务容器</li>\n</ul>\n<p>initContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123;</span><br><span class=\"line\">\t// 1、计算 sandbox 和 container 是否发生变化</span><br><span class=\"line\">\tpodContainerChanges := m.computePodActions(pod, podStatus)</span><br><span class=\"line\">\tif podContainerChanges.CreateSandbox &#123;</span><br><span class=\"line\">\t\tref, err := ref.GetReference(legacyscheme.Scheme, pod)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Couldn&apos;t make a ref to pod %q: &apos;%v&apos;&quot;, format.Pod(pod), err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、kill 掉 sandbox 已经改变的 pod</span><br><span class=\"line\">\tif podContainerChanges.KillPod &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\t// 3、kill 掉非 running 状态的 containers</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tfor containerID, containerInfo := range podContainerChanges.ContainersToKill &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\tif err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tm.pruneInitContainersBeforeStart(pod, podStatus)</span><br><span class=\"line\">\tpodIP := &quot;&quot;</span><br><span class=\"line\">\tif podStatus != nil &#123;</span><br><span class=\"line\">\t\tpodIP = podStatus.IP</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 4、创建 sandbox </span><br><span class=\"line\">\tpodSandboxID := podContainerChanges.SandboxID</span><br><span class=\"line\">\tif podContainerChanges.CreateSandbox &#123;</span><br><span class=\"line\">\t\tpodSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tpodSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置</span><br><span class=\"line\">\t\tif !kubecontainer.IsHostNetworkPod(pod) &#123;</span><br><span class=\"line\">\t\t\tpodIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus)</span><br><span class=\"line\">\t\t\tglog.V(4).Infof(&quot;Determined the ip %q for pod %q after sandbox changed&quot;, podIP, format.Pod(pod))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tconfigPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)</span><br><span class=\"line\">\tresult.AddSyncResult(configPodSandboxResult)</span><br><span class=\"line\">\t// 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等)</span><br><span class=\"line\">\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 5、启动 init container</span><br><span class=\"line\">\tif container := podContainerChanges.NextInitContainerToStart; container != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 6、启动业务容器</span><br><span class=\"line\">\tfor _, idx := range podContainerChanges.ContainersToStart &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\treturn</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"9、启动容器\"><a href=\"#9、启动容器\" class=\"headerlink\" title=\"9、启动容器\"></a>9、启动容器</h3><p>最终由 startContainer 完成容器的启动，其主要有以下几个步骤：</p>\n<ul>\n<li>1、拉取镜像</li>\n<li>2、生成业务容器的配置信息</li>\n<li>3、调用 docker api 创建容器</li>\n<li>4、启动容器</li>\n<li>5、执行 post start hook</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) &#123;</span><br><span class=\"line\">\t// 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。</span><br><span class=\"line\">\timageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tref, err := kubecontainer.GenerateContainerRef(pod, container)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 设置 RestartCount </span><br><span class=\"line\">\trestartCount := 0</span><br><span class=\"line\">\tcontainerStatus := podStatus.FindContainerStatusByName(container.Name)</span><br><span class=\"line\">\tif containerStatus != nil &#123;</span><br><span class=\"line\">\t\trestartCount = containerStatus.RestartCount + 1</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、生成业务容器的配置信息</span><br><span class=\"line\">\tcontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType)</span><br><span class=\"line\">\tif cleanupAction != nil &#123;</span><br><span class=\"line\">\t\tdefer cleanupAction()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、通过 client.CreateContainer 调用 docker api 创建业务容器</span><br><span class=\"line\">\tcontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\terr = m.internalLifecycle.PreStartContainer(pod, container, containerID)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、启动业务容器</span><br><span class=\"line\">\terr = m.runtimeService.StartContainer(containerID)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tcontainerMeta := containerConfig.GetMetadata()</span><br><span class=\"line\">\tsandboxMeta := podSandboxConfig.GetMetadata()</span><br><span class=\"line\">\tlegacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name,</span><br><span class=\"line\">\t\tsandboxMeta.Namespace)</span><br><span class=\"line\">\tcontainerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath)</span><br><span class=\"line\">\tif _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) &#123;</span><br><span class=\"line\">\t\tif err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Failed to create legacy symbolic link %q to container %q log %q: %v&quot;,</span><br><span class=\"line\">\t\t\t\tlegacySymlink, containerID, containerLog, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 4、执行 post start hook</span><br><span class=\"line\">\tif container.Lifecycle != nil &amp;&amp; container.Lifecycle.PostStart != nil &#123;</span><br><span class=\"line\">\t\tkubeContainerID := kubecontainer.ContainerID&#123;</span><br><span class=\"line\">\t\t\tType: m.runtimeName,</span><br><span class=\"line\">\t\t\tID:   containerID,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// runner.Run 这个方法的主要作用就是在业务容器起来的时候，</span><br><span class=\"line\">\t\t// 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。</span><br><span class=\"line\">\t\t// 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。</span><br><span class=\"line\">\t\tmsg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)</span><br><span class=\"line\">\t\tif handlerErr != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn &quot;&quot;, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲述了 kubelet 从监听到容器调度至本节点再到创建容器的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。</p>\n<p>参考：<br><a href=\"https://sycki.com/articles/kubernetes/k8s-code-kubelet\" target=\"_blank\" rel=\"noopener\">k8s源码分析-kubelet</a><br><a href=\"https://segmentfault.com/a/1190000008267351\" target=\"_blank\" rel=\"noopener\">Kubelet源码分析(一):启动流程分析</a><br><a href=\"http://cizixs.com/2017/06/07/kubelet-source-code-analysis-part-2/\" target=\"_blank\" rel=\"noopener\">kubelet 源码分析：pod 新建流程</a><br><a href=\"https://fatsheep9146.github.io/2018/07/22/kubelet%E5%88%9B%E5%BB%BAPod%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">kubelet创建Pod流程解析</a><br><a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md\" target=\"_blank\" rel=\"noopener\">Kubelet: Pod Lifecycle Event Generator (PLEG) Design-    proposals</a></p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/06/09/node_status/\">kubelet 状态上报的方式</a></li><li><a href=\"http://yoursite.com/2019/02/26/k8s_events/\">kubernets 中事件处理机制</a></li><li><a href=\"http://yoursite.com/2018/12/23/kubelet_init/\">kubelet 启动流程分析</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>上篇文章介绍了 <a href=\"http://blog.tianfeiyu.com/2018/12/23/kubelet_init/\" target=\"_blank\" rel=\"noopener\">kubelet 的启动流程</a>，本篇文章主要介绍 kubelet 创建 pod 的流程。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<p><img src=\"http://cdn.tianfeiyu.com/kubelet-1.png\" alt=\"kubelet 工作原理\"></p>\n<p>kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。</p>\n<p>本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。</p>\n<h2 id=\"kubelet-创建-pod-的流程\"><a href=\"#kubelet-创建-pod-的流程\" class=\"headerlink\" title=\"kubelet 创建 pod 的流程\"></a>kubelet 创建 pod 的流程</h2><p><img src=\"http://cdn.tianfeiyu.com/kubelet-2.png\" alt=\"kubelet 创建 pod 的流程\"></p>\n<h3 id=\"1、kubelet-的控制循环（syncLoop）\"><a href=\"#1、kubelet-的控制循环（syncLoop）\" class=\"headerlink\" title=\"1、kubelet 的控制循环（syncLoop）\"></a>1、kubelet 的控制循环（syncLoop）</h3><p>syncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123;</span><br><span class=\"line\">\tglog.Info(&quot;Starting kubelet main sync loop.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// syncTicker 每秒检测一次是否有需要同步的 pod workers</span><br><span class=\"line\">\tsyncTicker := time.NewTicker(time.Second)</span><br><span class=\"line\">\tdefer syncTicker.Stop()</span><br><span class=\"line\">\t// 每两秒检测一次是否有需要清理的 pod</span><br><span class=\"line\">\thousekeepingTicker := time.NewTicker(housekeepingPeriod)</span><br><span class=\"line\">\tdefer housekeepingTicker.Stop()</span><br><span class=\"line\">\t// pod 的生命周期变化</span><br><span class=\"line\">\tplegCh := kl.pleg.Watch()</span><br><span class=\"line\">\tconst (</span><br><span class=\"line\">\t\tbase   = 100 * time.Millisecond</span><br><span class=\"line\">\t\tmax    = 5 * time.Second</span><br><span class=\"line\">\t\tfactor = 2</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tduration := base</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123;</span><br><span class=\"line\">\t\t\ttime.Sleep(duration)</span><br><span class=\"line\">\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">        ...</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t\t// 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface，</span><br><span class=\"line\">\t\t// 在该文件开头处定义</span><br><span class=\"line\">\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123;</span><br><span class=\"line\">\t\t\tbreak</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"2、监听-pod-变化（syncLoopIteration）\"><a href=\"#2、监听-pod-变化（syncLoopIteration）\" class=\"headerlink\" title=\"2、监听 pod 变化（syncLoopIteration）\"></a>2、监听 pod 变化（syncLoopIteration）</h3><p>syncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息：</p>\n<ul>\n<li>configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。</li>\n<li>syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态</li>\n<li>houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作</li>\n<li>plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。</li>\n<li>livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler,</span><br><span class=\"line\">\tsyncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123;</span><br><span class=\"line\">\tselect &#123;</span><br><span class=\"line\">\tcase u, open := &lt;-configCh:</span><br><span class=\"line\">\t\tif !open &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Update channel is closed. Exiting the sync loop.&quot;)</span><br><span class=\"line\">\t\t\treturn false</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tswitch u.Op &#123;</span><br><span class=\"line\">\t\tcase kubetypes.ADD:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.UPDATE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.REMOVE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.RECONCILE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.DELETE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.RESTORE:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\tcase kubetypes.SET:</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase e := &lt;-plegCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase &lt;-syncCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase update := &lt;-kl.livenessManager.Updates():</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\tcase &lt;-housekeepingCh:</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\treturn true</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"3、处理新增-pod（HandlePodAddtions）\"><a href=\"#3、处理新增-pod（HandlePodAddtions）\" class=\"headerlink\" title=\"3、处理新增 pod（HandlePodAddtions）\"></a>3、处理新增 pod（HandlePodAddtions）</h3><p>对于事件中的每个 pod，执行以下操作：</p>\n<ul>\n<li>1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理</li>\n<li>2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了</li>\n<li>3、如果是 mirror pod 调用其单独的方法</li>\n<li>4、验证 pod 是否能在该节点运行，如果不可以直接拒绝</li>\n<li>5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理</li>\n<li>6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) &#123;</span><br><span class=\"line\">\tstart := kl.clock.Now()</span><br><span class=\"line\">\t// 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理</span><br><span class=\"line\">\tsort.Sort(sliceutils.PodsByCreationTime(pods))</span><br><span class=\"line\">\tfor _, pod := range pods &#123;</span><br><span class=\"line\">\t\tif kl.dnsConfigurer != nil &amp;&amp; kl.dnsConfigurer.ResolverConfig != &quot;&quot; &#123;</span><br><span class=\"line\">\t\t\tkl.dnsConfigurer.CheckLimitsForResolvConf()</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\texistingPods := kl.podManager.GetPods()</span><br><span class=\"line\">\t\t// 把 pod 加入到 podManager 中</span><br><span class=\"line\">\t\tkl.podManager.AddPod(pod)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 判断是否是 mirror pod（即 static pod）</span><br><span class=\"line\">\t\tif kubepod.IsMirrorPod(pod) &#123;</span><br><span class=\"line\">\t\t\tkl.handleMirrorPod(pod, start)</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\t\tactivePods := kl.filterOutTerminatedPods(existingPods)</span><br><span class=\"line\">\t\t\t// 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间)</span><br><span class=\"line\">\t\t\t// Check if we can admit the pod; if not, reject it.</span><br><span class=\"line\">\t\t\tif ok, reason, message := kl.canAdmitPod(activePods, pod); !ok &#123;</span><br><span class=\"line\">\t\t\t\tkl.rejectPod(pod, reason, message)</span><br><span class=\"line\">\t\t\t\tcontinue</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t</span><br><span class=\"line\">\t\tmirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)</span><br><span class=\"line\">\t\t// 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法.</span><br><span class=\"line\">\t\tkl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)</span><br><span class=\"line\">\t\t// 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测</span><br><span class=\"line\">\t\tkl.probeManager.AddPod(pod)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。</p>\n</blockquote>\n<h3 id=\"4、下发任务（dispatchWork）\"><a href=\"#4、下发任务（dispatchWork）\" class=\"headerlink\" title=\"4、下发任务（dispatchWork）\"></a>4、下发任务（dispatchWork）</h3><p>dispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) &#123;</span><br><span class=\"line\">\tif kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\tif pod.DeletionTimestamp != nil &#123;</span><br><span class=\"line\">\t\t\tkl.statusManager.TerminatePod(pod)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\treturn</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 落实在 podWorkers 中</span><br><span class=\"line\">\tkl.podWorkers.UpdatePod(&amp;UpdatePodOptions&#123;</span><br><span class=\"line\">\t\tPod:        pod,</span><br><span class=\"line\">\t\tMirrorPod:  mirrorPod,</span><br><span class=\"line\">\t\tUpdateType: syncType,</span><br><span class=\"line\">\t\tOnCompleteFunc: func(err error) &#123;</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tmetrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start))</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t&#125;)</span><br><span class=\"line\">\tif syncType == kubetypes.SyncPodCreate &#123;</span><br><span class=\"line\">\t\tmetrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers)))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"5、更新事件的-channel（UpdatePod）\"><a href=\"#5、更新事件的-channel（UpdatePod）\" class=\"headerlink\" title=\"5、更新事件的 channel（UpdatePod）\"></a>5、更新事件的 channel（UpdatePod）</h3><p>podWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (p *podWorkers) UpdatePod(options *UpdatePodOptions) &#123;</span><br><span class=\"line\">\tpod := options.Pod</span><br><span class=\"line\">\tuid := pod.UID</span><br><span class=\"line\">\tvar podUpdates chan UpdatePodOptions</span><br><span class=\"line\">\tvar exists bool</span><br><span class=\"line\"></span><br><span class=\"line\">\tp.podLock.Lock()</span><br><span class=\"line\">\tdefer p.podLock.Unlock()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel</span><br><span class=\"line\">\tif podUpdates, exists = p.podUpdates[uid]; !exists &#123;</span><br><span class=\"line\">\t\t// 创建 channel</span><br><span class=\"line\">\t\tpodUpdates = make(chan UpdatePodOptions, 1)</span><br><span class=\"line\">\t\tp.podUpdates[uid] = podUpdates</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// 启动 goroutine</span><br><span class=\"line\">\t\tgo func() &#123;</span><br><span class=\"line\">\t\t\tdefer runtime.HandleCrash()</span><br><span class=\"line\">\t\t\tp.managePodLoop(podUpdates)</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t// 下发更新事件</span><br><span class=\"line\">\tif !p.isWorking[pod.UID] &#123;</span><br><span class=\"line\">\t\tp.isWorking[pod.UID] = true</span><br><span class=\"line\">\t\tpodUpdates &lt;- *options</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\tupdate, found := p.lastUndeliveredWorkUpdate[pod.UID]</span><br><span class=\"line\">\t\tif !found || update.UpdateType != kubetypes.SyncPodKill &#123;</span><br><span class=\"line\">\t\t\tp.lastUndeliveredWorkUpdate[pod.UID] = *options</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"6、调用-syncPodFn-方法同步-pod（managePodLoop）\"><a href=\"#6、调用-syncPodFn-方法同步-pod（managePodLoop）\" class=\"headerlink\" title=\"6、调用 syncPodFn 方法同步 pod（managePodLoop）\"></a>6、调用 syncPodFn 方法同步 pod（managePodLoop）</h3><p>managePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情:</p>\n<ul>\n<li>将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync</li>\n<li>将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (p *podWorkers) managePodLoop(podUpdates &lt;-chan UpdatePodOptions) &#123;</span><br><span class=\"line\">\tvar lastSyncTime time.Time</span><br><span class=\"line\">\tfor update := range podUpdates &#123;</span><br><span class=\"line\">\t\terr := func() error &#123;</span><br><span class=\"line\">\t\t\tpodUID := update.Pod.UID</span><br><span class=\"line\">\t\t\tstatus, err := p.podCache.GetNewerThan(podUID, lastSyncTime)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\terr = p.syncPodFn(syncPodOptions&#123;</span><br><span class=\"line\">\t\t\t\tmirrorPod:      update.MirrorPod,</span><br><span class=\"line\">\t\t\t\tpod:            update.Pod,</span><br><span class=\"line\">\t\t\t\tpodStatus:      status,</span><br><span class=\"line\">\t\t\t\tkillPodOptions: update.KillPodOptions,</span><br><span class=\"line\">\t\t\t\tupdateType:     update.UpdateType,</span><br><span class=\"line\">\t\t\t&#125;)</span><br><span class=\"line\">\t\t\tlastSyncTime = time.Now()</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t\t&#125;()</span><br><span class=\"line\">\t\tif update.OnCompleteFunc != nil &#123;</span><br><span class=\"line\">\t\t\tupdate.OnCompleteFunc(err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tp.wrapUp(update.Pod.UID, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"7、完成创建容器前的准备工作（SyncPod）\"><a href=\"#7、完成创建容器前的准备工作（SyncPod）\" class=\"headerlink\" title=\"7、完成创建容器前的准备工作（SyncPod）\"></a>7、完成创建容器前的准备工作（SyncPod）</h3><p>在这个方法中，主要完成以下几件事情：</p>\n<ul>\n<li>如果是删除 pod，立即执行并返回</li>\n<li>同步 podStatus 到 kubelet.statusManager</li>\n<li>检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息</li>\n<li>创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup</li>\n<li>如果是 static Pod，就创建或者更新对应的 mirrorPod</li>\n<li>创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据</li>\n<li>然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。</li>\n<li>调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑</li>\n</ul>\n<p>这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncPod(o syncPodOptions) error &#123;</span><br><span class=\"line\">\t// pull out the required options</span><br><span class=\"line\">\tpod := o.pod</span><br><span class=\"line\">\tmirrorPod := o.mirrorPod</span><br><span class=\"line\">\tpodStatus := o.podStatus</span><br><span class=\"line\">\tupdateType := o.updateType</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 是否为 删除 pod</span><br><span class=\"line\">\tif updateType == kubetypes.SyncPodKill &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\t// 检查 pod 是否能运行在本节点</span><br><span class=\"line\">\trunnable := kl.canRunPod(pod)</span><br><span class=\"line\">\tif !runnable.Admit &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 更新 pod 状态</span><br><span class=\"line\">\tkl.statusManager.SetPodStatus(pod, apiPodStatus)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 如果 pod 非 running 状态则直接 kill 掉</span><br><span class=\"line\">\tif !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 加载网络插件</span><br><span class=\"line\">\tif rs := kl.runtimeState.networkErrors(); len(rs) != 0 &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpcm := kl.containerManager.NewPodContainerManager()</span><br><span class=\"line\">\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// 创建并更新 pod 的 cgroups</span><br><span class=\"line\">\t\tif !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) &#123;</span><br><span class=\"line\">\t\t\tif !pcm.Exists(pod) &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 为 static pod 创建对应的 mirror pod</span><br><span class=\"line\">\tif kubepod.IsStaticPod(pod) &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建数据目录</span><br><span class=\"line\">\tif err := kl.makePodDataDirs(pod); err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 挂载 volume</span><br><span class=\"line\">\tif !kl.podIsTerminated(pod) &#123;</span><br><span class=\"line\">\t\tif err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 获取 secret 信息</span><br><span class=\"line\">\tpullSecrets := kl.getPullSecretsForPod(pod)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 调用 containerRuntime 的 SyncPod 方法开始创建容器</span><br><span class=\"line\">\tresult := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff)</span><br><span class=\"line\">\tkl.reasonCache.Update(pod.UID, result)</span><br><span class=\"line\">\tif err := result.Error(); err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"8、创建容器\"><a href=\"#8、创建容器\" class=\"headerlink\" title=\"8、创建容器\"></a>8、创建容器</h3><p>containerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。<br>syncPod 主要执行以下几个操作：</p>\n<ul>\n<li>1、计算 sandbox 和 container 是否发生变化</li>\n<li>2、创建 sandbox 容器</li>\n<li>3、启动 init 容器</li>\n<li>4、启动业务容器</li>\n</ul>\n<p>initContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123;</span><br><span class=\"line\">\t// 1、计算 sandbox 和 container 是否发生变化</span><br><span class=\"line\">\tpodContainerChanges := m.computePodActions(pod, podStatus)</span><br><span class=\"line\">\tif podContainerChanges.CreateSandbox &#123;</span><br><span class=\"line\">\t\tref, err := ref.GetReference(legacyscheme.Scheme, pod)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Couldn&apos;t make a ref to pod %q: &apos;%v&apos;&quot;, format.Pod(pod), err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、kill 掉 sandbox 已经改变的 pod</span><br><span class=\"line\">\tif podContainerChanges.KillPod &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">\t\t// 3、kill 掉非 running 状态的 containers</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tfor containerID, containerInfo := range podContainerChanges.ContainersToKill &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\tif err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil &#123;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tm.pruneInitContainersBeforeStart(pod, podStatus)</span><br><span class=\"line\">\tpodIP := &quot;&quot;</span><br><span class=\"line\">\tif podStatus != nil &#123;</span><br><span class=\"line\">\t\tpodIP = podStatus.IP</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 4、创建 sandbox </span><br><span class=\"line\">\tpodSandboxID := podContainerChanges.SandboxID</span><br><span class=\"line\">\tif podContainerChanges.CreateSandbox &#123;</span><br><span class=\"line\">\t\tpodSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tpodSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置</span><br><span class=\"line\">\t\tif !kubecontainer.IsHostNetworkPod(pod) &#123;</span><br><span class=\"line\">\t\t\tpodIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus)</span><br><span class=\"line\">\t\t\tglog.V(4).Infof(&quot;Determined the ip %q for pod %q after sandbox changed&quot;, podIP, format.Pod(pod))</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tconfigPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)</span><br><span class=\"line\">\tresult.AddSyncResult(configPodSandboxResult)</span><br><span class=\"line\">\t// 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等)</span><br><span class=\"line\">\tpodSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 5、启动 init container</span><br><span class=\"line\">\tif container := podContainerChanges.NextInitContainerToStart; container != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 6、启动业务容器</span><br><span class=\"line\">\tfor _, idx := range podContainerChanges.ContainersToStart &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tif msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\treturn</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"9、启动容器\"><a href=\"#9、启动容器\" class=\"headerlink\" title=\"9、启动容器\"></a>9、启动容器</h3><p>最终由 startContainer 完成容器的启动，其主要有以下几个步骤：</p>\n<ul>\n<li>1、拉取镜像</li>\n<li>2、生成业务容器的配置信息</li>\n<li>3、调用 docker api 创建容器</li>\n<li>4、启动容器</li>\n<li>5、执行 post start hook</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) &#123;</span><br><span class=\"line\">\t// 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。</span><br><span class=\"line\">\timageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tref, err := kubecontainer.GenerateContainerRef(pod, container)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 设置 RestartCount </span><br><span class=\"line\">\trestartCount := 0</span><br><span class=\"line\">\tcontainerStatus := podStatus.FindContainerStatusByName(container.Name)</span><br><span class=\"line\">\tif containerStatus != nil &#123;</span><br><span class=\"line\">\t\trestartCount = containerStatus.RestartCount + 1</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 2、生成业务容器的配置信息</span><br><span class=\"line\">\tcontainerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType)</span><br><span class=\"line\">\tif cleanupAction != nil &#123;</span><br><span class=\"line\">\t\tdefer cleanupAction()</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、通过 client.CreateContainer 调用 docker api 创建业务容器</span><br><span class=\"line\">\tcontainerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\terr = m.internalLifecycle.PreStartContainer(pod, container, containerID)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 3、启动业务容器</span><br><span class=\"line\">\terr = m.runtimeService.StartContainer(containerID)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tcontainerMeta := containerConfig.GetMetadata()</span><br><span class=\"line\">\tsandboxMeta := podSandboxConfig.GetMetadata()</span><br><span class=\"line\">\tlegacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name,</span><br><span class=\"line\">\t\tsandboxMeta.Namespace)</span><br><span class=\"line\">\tcontainerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath)</span><br><span class=\"line\">\tif _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) &#123;</span><br><span class=\"line\">\t\tif err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil &#123;</span><br><span class=\"line\">\t\t\tglog.Errorf(&quot;Failed to create legacy symbolic link %q to container %q log %q: %v&quot;,</span><br><span class=\"line\">\t\t\t\tlegacySymlink, containerID, containerLog, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 4、执行 post start hook</span><br><span class=\"line\">\tif container.Lifecycle != nil &amp;&amp; container.Lifecycle.PostStart != nil &#123;</span><br><span class=\"line\">\t\tkubeContainerID := kubecontainer.ContainerID&#123;</span><br><span class=\"line\">\t\t\tType: m.runtimeName,</span><br><span class=\"line\">\t\t\tID:   containerID,</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// runner.Run 这个方法的主要作用就是在业务容器起来的时候，</span><br><span class=\"line\">\t\t// 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。</span><br><span class=\"line\">\t\t// 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。</span><br><span class=\"line\">\t\tmsg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)</span><br><span class=\"line\">\t\tif handlerErr != nil &#123;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn &quot;&quot;, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲述了 kubelet 从监听到容器调度至本节点再到创建容器的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。</p>\n<p>参考：<br><a href=\"https://sycki.com/articles/kubernetes/k8s-code-kubelet\" target=\"_blank\" rel=\"noopener\">k8s源码分析-kubelet</a><br><a href=\"https://segmentfault.com/a/1190000008267351\" target=\"_blank\" rel=\"noopener\">Kubelet源码分析(一):启动流程分析</a><br><a href=\"http://cizixs.com/2017/06/07/kubelet-source-code-analysis-part-2/\" target=\"_blank\" rel=\"noopener\">kubelet 源码分析：pod 新建流程</a><br><a href=\"https://fatsheep9146.github.io/2018/07/22/kubelet%E5%88%9B%E5%BB%BAPod%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">kubelet创建Pod流程解析</a><br><a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-lifecycle-event-generator.md\" target=\"_blank\" rel=\"noopener\">Kubelet: Pod Lifecycle Event Generator (PLEG) Design-    proposals</a></p>\n"},{"title":"kubelet 启动流程分析","date":"2018-12-23T13:22:30.000Z","type":"kubelet","_content":"\n上篇文章（[kubelet 架构浅析](https://blog.tianfeiyu.com/2018/12/16/kubelet-modules/) ）已经介绍过 kubelet 在整个集群架构中的功能以及自身各模块的用途，本篇文章主要介绍 kubelet 的启动流程。\n\n > kubernetes 版本： v1.12 \n\n\n## kubelet 启动流程\n\nkubelet 代码结构:\n\n```\n➜  kubernetes git:(release-1.12) ✗ tree cmd/kubelet\ncmd/kubelet\n├── BUILD\n├── OWNERS\n├── app\n│   ├── BUILD\n│   ├── OWNERS\n│   ├── auth.go\n│   ├── init_others.go\n│   ├── init_windows.go\n│   ├── options\n│   │   ├── BUILD\n│   │   ├── container_runtime.go\n│   │   ├── globalflags.go\n│   │   ├── globalflags_linux.go\n│   │   ├── globalflags_other.go\n│   │   ├── options.go\n│   │   ├── options_test.go\n│   │   ├── osflags_others.go\n│   │   └── osflags_windows.go\n│   ├── plugins.go\n│   ├── server.go\n│   ├── server_linux.go\n│   ├── server_test.go\n│   └── server_unsupported.go\n└── kubelet.go\n\n2 directories, 22 files\n```\n\n![kubelet 启动流程时序图](http://cdn.tianfeiyu.com/kubelet-3.png)\n\n#### 1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）\n\n```\nfunc main() {\n\trand.Seed(time.Now().UTC().UnixNano())\n\n\tcommand := app.NewKubeletCommand(server.SetupSignalHandler())\n\tlogs.InitLogs()\n\tdefer logs.FlushLogs()\n\n\tif err := command.Execute(); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"%v\\n\", err)\n\t\tos.Exit(1)\n\t}\n}\n```\n\n#### 2、初始化 kubelet 配置（cmd/kubelet/app/server.go）\n\nNewKubeletCommand() 函数主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。  \n\n```\n// NewKubeletCommand creates a *cobra.Command object with default parameters\nfunc NewKubeletCommand(stopCh <-chan struct{}) *cobra.Command {\n    cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)\n    cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc)\n    // Kubelet配置分两部分:\n    // KubeletFlag: 指那些不允许在 kubelet 运行时进行修改的配置集，或者不能在集群中各个 Nodes 之间共享的配置集。\n    // KubeletConfiguration: 指可以在集群中各个Nodes之间共享的配置集，可以进行动态配置。\n    kubeletFlags := options.NewKubeletFlags()\n\tkubeletConfig, err := options.NewKubeletConfiguration()\n\t...\n\tcmd := &cobra.Command{\n\t\t...\n\t\tRun: func(cmd *cobra.Command, args []string) {\n\t\t\t// 读取 kubelet 配置文件\n\t\t\tif configFile := kubeletFlags.KubeletConfigFile; len(configFile) > 0 {\n\t\t\t\tkubeletConfig, err = loadConfigFile(configFile)\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Fatal(err)\n\t\t\t\t}\n\t\t\t\t...\n\t\t\t}\n\t\t\t// 校验 kubelet 参数\n\t\t\tif err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t\t...\n\t\t\t// 此处初始化了 kubeletDeps\n\t\t\tkubeletDeps, err := UnsecuredDependencies(kubeletServer)\n\t\t\tif err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t\t...\n\t\t\t// 启动程序\n\t\t\tif err := Run(kubeletServer, kubeletDeps, stopCh); err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t},\n\t}\n    ...\n\treturn cmd\n}\n```\nkubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为。主要包括监控功能（cadvisor），cgroup 管理功能（containerManager）等。\n\nNewKubeletCommand() 会调用 Run() 函数，Run() 中主要调用 run() 函数进行一些准备事项。\n\n#### 3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）\n\nrun() 函数的主要功能：\n\n- 1、创建 kubeClient，evnetClient 用来和 apiserver 通信。创建 heartbeatClient 向 apiserver 上报心跳状态。\n- 2、为 kubeDeps 设定一些默认值。\n- 3、启动监听 Healthz 端口的 http server，默认端口是 10248。\n\n```\nfunc run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh <-chan struct{}) (err error) {\n\t...\n\t// 判断 kubelet 的启动模式\n\tif standaloneMode {\n\t...\n\t} else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil {\n\t\t...\n\t\t// 创建对象 kubeClient\n\t\tkubeClient, err = clientset.NewForConfig(clientConfig)\n\n\t\t...\n        // 创建对象 evnetClient\n\t\teventClient, err = v1core.NewForConfig(&eventClientConfig)\n\t\t...\n\t\t// heartbeatClient 上报状态\n\t\theartbeatClient, err = clientset.NewForConfig(&heartbeatClientConfig)\n\t\t...\n\t}\n\n\t// 为 kubeDeps 设定一些默认值\n\tif kubeDeps.Auth == nil {\n\t\t\tauth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tkubeDeps.Auth = auth\n\t\t}\n\n\t\tif kubeDeps.CAdvisorInterface == nil {\n\t\t\timageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)\n\t\t\tkubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// \n\tif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil {\n\t\t\treturn err\n\t}\n\t...\n\t// 启动监听 Healthz 端口的 http server  \n\tif s.HealthzPort > 0 {\n\t\thealthz.DefaultHealthz()\n\t\tgo wait.Until(func() {\n\t\t\terr := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil)\n\t\t\tif err != nil {\n\t\t\t\tglog.Errorf(\"Starting health server failed: %v\", err)\n\t\t\t}\n\t\t}, 5*time.Second, wait.NeverStop)\n\t}\n\t...\n}\n```\nkubelet 对 pod 资源的获取方式有三种：第一种是通过文件获得，文件一般放在 /etc/kubernetes/manifests 目录下面；第二种也是通过文件过得，只不过文件是通过 URL 获取的；第三种是通过 watch kube-apiserver 获取。其中前两种模式下，我们称 kubelet 运行在 standalone 模式下，运行在 standalone 模式下的 kubelet 一般用于调试某些功能。\n\nrun() 中调用 RunKubelet() 函数进行后续操作。\n\n#### 4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）\n\nRunKubelet()  主要功能：\n\n- 1、初始化 kubelet 组件中的各个模块，创建出 kubelet 对象。\n- 2、启动垃圾回收服务。\n\n```\nfunc RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error {\n    ...\n\n \t// 初始化 kubelet 内部模块\n\tk, err := CreateAndInitKubelet(&kubeServer.KubeletConfiguration,\n\t\tkubeDeps,\n\t\t&kubeServer.ContainerRuntimeOptions,\n\t\tkubeServer.ContainerRuntime,\n\t\tkubeServer.RuntimeCgroups,\n\t\tkubeServer.HostnameOverride,\n\t\tkubeServer.NodeIP,\n\t\tkubeServer.ProviderID,\n\t\tkubeServer.CloudProvider,\n\t\tkubeServer.CertDirectory,\n\t\tkubeServer.RootDirectory,\n\t\tkubeServer.RegisterNode,\n\t\tkubeServer.RegisterWithTaints,\n\t\tkubeServer.AllowedUnsafeSysctls,\n\t\tkubeServer.RemoteRuntimeEndpoint,\n\t\tkubeServer.RemoteImageEndpoint,\n\t\tkubeServer.ExperimentalMounterPath,\n\t\tkubeServer.ExperimentalKernelMemcgNotification,\n\t\tkubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount,\n\t\tkubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold,\n\t\tkubeServer.MinimumGCAge,\n\t\tkubeServer.MaxPerPodContainerCount,\n\t\tkubeServer.MaxContainerCount,\n\t\tkubeServer.MasterServiceNamespace,\n\t\tkubeServer.RegisterSchedulable,\n\t\tkubeServer.NonMasqueradeCIDR,\n\t\tkubeServer.KeepTerminatedPodVolumes,\n\t\tkubeServer.NodeLabels,\n\t\tkubeServer.SeccompProfileRoot,\n\t\tkubeServer.BootstrapCheckpointPath,\n\t\tkubeServer.NodeStatusMaxImages)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create kubelet: %v\", err)\n\t}\n\n\t...\n\tif runOnce {\n\t\tif _, err := k.RunOnce(podCfg.Updates()); err != nil {\n\t\t\treturn fmt.Errorf(\"runonce failed: %v\", err)\n\t\t}\n\t\tglog.Infof(\"Started kubelet as runonce\")\n\t} else {\n        // \n\t\tstartKubelet(k, podCfg, &kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer)\n\t\tglog.Infof(\"Started kubelet\")\n\t}\n\n}\n```\n\n\n```\nfunc CreateAndInitKubelet(...){\n\t// NewMainKubelet 实例化一个 kubelet 对象，并对 kubelet 内部各个模块进行初始化\n\tk, err = kubelet.NewMainKubelet(kubeCfg,\n\t\tkubeDeps,\n\t\tcrOptions,\n\t\tcontainerRuntime,\n\t\truntimeCgroups,\n\t\thostnameOverride,\n\t\tnodeIP,\n\t\tproviderID,\n\t\tcloudProvider,\n\t\tcertDirectory,\n\t\trootDirectory,\n\t\tregisterNode,\n\t\tregisterWithTaints,\n\t\tallowedUnsafeSysctls,\n\t\tremoteRuntimeEndpoint,\n\t\tremoteImageEndpoint,\n\t\texperimentalMounterPath,\n\t\texperimentalKernelMemcgNotification,\n\t\texperimentalCheckNodeCapabilitiesBeforeMount,\n\t\texperimentalNodeAllocatableIgnoreEvictionThreshold,\n\t\tminimumGCAge,\n\t\tmaxPerPodContainerCount,\n\t\tmaxContainerCount,\n\t\tmasterServiceNamespace,\n\t\tregisterSchedulable,\n\t\tnonMasqueradeCIDR,\n\t\tkeepTerminatedPodVolumes,\n\t\tnodeLabels,\n\t\tseccompProfileRoot,\n\t\tbootstrapCheckpointPath,\n\t\tnodeStatusMaxImages)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// 通知 apiserver kubelet 启动了\n\tk.BirthCry()\n\t// 启动垃圾回收服务\n\tk.StartGarbageCollection()\n\n\treturn k, nil\n\n}\n```\n\n```\nfunc NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...){\n    ...\n\tif kubeDeps.PodConfig == nil {\n\t\tvar err error\n\t\t// 初始化 makePodSourceConfig，监听 pod 元数据的来源(FILE, URL, api-server)，将不同 source 的 pod configuration 合并到一个结构中\n\t\tkubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n    \n    // kubelet 服务端口，默认 10250\n\tdaemonEndpoints := &v1.NodeDaemonEndpoints{\n\t\tKubeletEndpoint: v1.DaemonEndpoint{Port: kubeCfg.Port},\n\t}\n\n\t// 使用 reflector 把 ListWatch 得到的服务信息实时同步到 serviceStore 对象中\n\tserviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})\n\tif kubeDeps.KubeClient != nil {\n\t\tserviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"services\", metav1.NamespaceAll, fields.Everything())\n\t\tr := cache.NewReflector(serviceLW, &v1.Service{}, serviceIndexer, 0)\n\t\tgo r.Run(wait.NeverStop)\n\t}\n\tserviceLister := corelisters.NewServiceLister(serviceIndexer)\n\n\t// 使用 reflector 把 ListWatch 得到的节点信息实时同步到  nodeStore 对象中\n\tnodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{})\n\tif kubeDeps.KubeClient != nil {\n\t\tfieldSelector := fields.Set{api.ObjectNameField: string(nodeName)}.AsSelector()\n\t\tnodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"nodes\", metav1.NamespaceAll, fieldSelector)\n\t\tr := cache.NewReflector(nodeLW, &v1.Node{}, nodeIndexer, 0)\n\t\tgo r.Run(wait.NeverStop)\n\t}\n\tnodeInfo := &predicates.CachedNodeInfo{NodeLister: corelisters.NewNodeLister(nodeIndexer)}\n\n\t...\n\t// node 资源不足时的驱逐策略的设定\n\tthresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tevictionConfig := eviction.Config{\n\t\tPressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,\n\t\tMaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),\n\t\tThresholds:               thresholds,\n\t\tKernelMemcgNotification:  experimentalKernelMemcgNotification,\n\t\tPodCgroupRoot:            kubeDeps.ContainerManager.GetPodCgroupRoot(),\n\t}\n    ...\n    // 容器引用的管理\n\tcontainerRefManager := kubecontainer.NewRefManager()\n    // oom 监控\n\toomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder)\n\n\t// 根据配置信息和各种对象创建 Kubelet 实例\n\tklet := &Kubelet{\n\t\thostname:                       hostname,\n\t\thostnameOverridden:             len(hostnameOverride) > 0,\n\t\tnodeName:                       nodeName,\n\t\t...\n\t}\n\t\n\t// 从 cAdvisor 获取当前机器的信息\n\tmachineInfo, err := klet.cadvisor.MachineInfo()\n\n\t// 对 pod 的管理（如: 增删改等）\n\tklet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager)\n\n\t// 容器运行时管理\n\truntime, err := kuberuntime.NewKubeGenericRuntimeManager(...)\n\n\t// pleg\n\tklet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock{})\n\n\t// 创建 containerGC 对象，进行周期性的容器清理工作\n\tcontainerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)\n\n\t// 创建 imageManager 管理镜像\n\timageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage)\n\t\n\t// statusManager 实时检测节点上 pod 的状态，并更新到 apiserver 对应的 pod\n\tklet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)\n\n\t// 探针管理\n\tklet.probeManager = prober.NewManager(...)\n\n    // token 管理\n\ttokenManager := token.NewManager(kubeDeps.KubeClient)\n\n\t// 磁盘管理\n\tklet.volumeManager = volumemanager.NewVolumeManager()\n\t\n\t// 将 syncPod() 注入到 podWorkers 中\n\tklet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)\n\n\t// 容器驱逐策略管理\n\tevictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock)\n    ...\n}\n```\nRunKubelet 最后会调用 startKubelet() 进行后续的操作。\n\n#### 5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）\nstartKubelet()  的主要功能：\n\n- 1、以 goroutine 方式启动 kubelet 中的各个模块。\n- 2、启动 kubelet http server。\n\n\n```\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) {\n\tgo wait.Until(func() {\n\t\t// 以 goroutine 方式启动 kubelet 中的各个模块\n\t\tk.Run(podCfg.Updates())\n\t}, 0, wait.NeverStop)\n\n\t// 启动 kubelet http server\t\n\tif enableServer {\n\t\tgo k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling)\n\n\t}\n\tif kubeCfg.ReadOnlyPort > 0 {\n\t\tgo k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort))\n\t}\n}\n```\n\n```\n// Run starts the kubelet reacting to config updates\nfunc (kl *Kubelet) Run(updates <-chan kubetypes.PodUpdate) {\n\tif kl.logServer == nil {\n\t\tkl.logServer = http.StripPrefix(\"/logs/\", http.FileServer(http.Dir(\"/var/log/\")))\n\t}\n\tif kl.kubeClient == nil {\n\t\tglog.Warning(\"No api server defined - no node status update will be sent.\")\n\t}\n\n\t// Start the cloud provider sync manager\n\tif kl.cloudResourceSyncManager != nil {\n\t\tgo kl.cloudResourceSyncManager.Run(wait.NeverStop)\n\t}\n\n\tif err := kl.initializeModules(); err != nil {\n\t\tkl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())\n\t\tglog.Fatal(err)\n\t}\n\n\t// Start volume manager\n\tgo kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)\n\n\tif kl.kubeClient != nil {\n\t\t// Start syncing node status immediately, this may set up things the runtime needs to run.\n\t\tgo wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)\n\t\tgo kl.fastStatusUpdateOnce()\n\n\t\t// start syncing lease\n\t\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) {\n\t\t\tgo kl.nodeLeaseController.Run(wait.NeverStop)\n\t\t}\n\t}\n\tgo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)\n\n\t// Start loop to sync iptables util rules\n\tif kl.makeIPTablesUtilChains {\n\t\tgo wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop)\n\t}\n\n\t// Start a goroutine responsible for killing pods (that are not properly\n\t// handled by pod workers).\n\tgo wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)\n\n\t// Start component sync loops.\n\tkl.statusManager.Start()\n\tkl.probeManager.Start()\n\n\t// Start syncing RuntimeClasses if enabled.\n\tif kl.runtimeClassManager != nil {\n\t\tgo kl.runtimeClassManager.Run(wait.NeverStop)\n\t}\n\n\t// Start the pod lifecycle event generator.\n\tkl.pleg.Start()\n\n\tkl.syncLoop(updates, kl)\n}\n```\nsyncLoop 是 kubelet 的主循环方法，它从不同的管道(FILE,URL, API-SERVER)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 Pod 处于期望的状态。\n\n\n```\nfunc (kl *Kubelet) syncLoop(updates <-chan kubetypes.PodUpdate, handler SyncHandler) {\n\tglog.Info(\"Starting kubelet main sync loop.\")\n\n\t// syncTicker 每秒检测一次是否有需要同步的 pod workers\n\tsyncTicker := time.NewTicker(time.Second)\n\tdefer syncTicker.Stop()\n\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\n\tdefer housekeepingTicker.Stop()\n\tplegCh := kl.pleg.Watch()\n\tconst (\n\t\tbase   = 100 * time.Millisecond\n\t\tmax    = 5 * time.Second\n\t\tfactor = 2\n\t)\n\tduration := base\n\tfor {\n\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {\n\t\t\tglog.Infof(\"skipping pod synchronization - %v\", rs)\n\t\t\t// exponential backoff\n\t\t\ttime.Sleep(duration)\n\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))\n\t\t\tcontinue\n\t\t}\n\t\t// reset backoff if we have a success\n\t\tduration = base\n\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t\t// \n\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {\n\t\t\tbreak\n\t\t}\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t}\n}\n```\n\nsyncLoopIteration()  方法对多个管道进行遍历，如果 pod 发生变化，则会调用相应的 Handler，在 Handler 中通过调用 dispatchWork 分发任务。\n\n\n## 总结\n\n本篇文章主要讲述了 kubelet 组件从加载配置到初始化内部的各个模块再到启动 kubelet 服务的整个流程，上面的时序图能清楚的看到函数之间的调用关系，但是其中每个组件具体的工作方式以及组件之间的交互方式还不得而知，后面会一探究竟。\n\n参考：\n[kubernetes node components – kubelet](http://www.sel.zju.edu.cn/?p=595)\n[Kubelet 源码分析(一):启动流程分析](https://segmentfault.com/a/1190000008267351)\n[kubelet 源码分析：启动流程](https://cizixs.com/2017/06/06/kubelet-source-code-analysis-part-1/)\n[kubernetes 的 kubelet 的工作过程](https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/05/02/Kubernetes-kubelet.html)\n[kubelet 内部实现解析](https://fatsheep9146.github.io/2018/07/08/kubelet%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/)\n","source":"_posts/kubelet_init.md","raw":"---\ntitle: kubelet 启动流程分析\ndate: 2018-12-23 21:22:30\ntags: \"kubelet\"\ntype: \"kubelet\"\n\n---\n\n上篇文章（[kubelet 架构浅析](https://blog.tianfeiyu.com/2018/12/16/kubelet-modules/) ）已经介绍过 kubelet 在整个集群架构中的功能以及自身各模块的用途，本篇文章主要介绍 kubelet 的启动流程。\n\n > kubernetes 版本： v1.12 \n\n\n## kubelet 启动流程\n\nkubelet 代码结构:\n\n```\n➜  kubernetes git:(release-1.12) ✗ tree cmd/kubelet\ncmd/kubelet\n├── BUILD\n├── OWNERS\n├── app\n│   ├── BUILD\n│   ├── OWNERS\n│   ├── auth.go\n│   ├── init_others.go\n│   ├── init_windows.go\n│   ├── options\n│   │   ├── BUILD\n│   │   ├── container_runtime.go\n│   │   ├── globalflags.go\n│   │   ├── globalflags_linux.go\n│   │   ├── globalflags_other.go\n│   │   ├── options.go\n│   │   ├── options_test.go\n│   │   ├── osflags_others.go\n│   │   └── osflags_windows.go\n│   ├── plugins.go\n│   ├── server.go\n│   ├── server_linux.go\n│   ├── server_test.go\n│   └── server_unsupported.go\n└── kubelet.go\n\n2 directories, 22 files\n```\n\n![kubelet 启动流程时序图](http://cdn.tianfeiyu.com/kubelet-3.png)\n\n#### 1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）\n\n```\nfunc main() {\n\trand.Seed(time.Now().UTC().UnixNano())\n\n\tcommand := app.NewKubeletCommand(server.SetupSignalHandler())\n\tlogs.InitLogs()\n\tdefer logs.FlushLogs()\n\n\tif err := command.Execute(); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"%v\\n\", err)\n\t\tos.Exit(1)\n\t}\n}\n```\n\n#### 2、初始化 kubelet 配置（cmd/kubelet/app/server.go）\n\nNewKubeletCommand() 函数主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。  \n\n```\n// NewKubeletCommand creates a *cobra.Command object with default parameters\nfunc NewKubeletCommand(stopCh <-chan struct{}) *cobra.Command {\n    cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)\n    cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc)\n    // Kubelet配置分两部分:\n    // KubeletFlag: 指那些不允许在 kubelet 运行时进行修改的配置集，或者不能在集群中各个 Nodes 之间共享的配置集。\n    // KubeletConfiguration: 指可以在集群中各个Nodes之间共享的配置集，可以进行动态配置。\n    kubeletFlags := options.NewKubeletFlags()\n\tkubeletConfig, err := options.NewKubeletConfiguration()\n\t...\n\tcmd := &cobra.Command{\n\t\t...\n\t\tRun: func(cmd *cobra.Command, args []string) {\n\t\t\t// 读取 kubelet 配置文件\n\t\t\tif configFile := kubeletFlags.KubeletConfigFile; len(configFile) > 0 {\n\t\t\t\tkubeletConfig, err = loadConfigFile(configFile)\n\t\t\t\tif err != nil {\n\t\t\t\t\tglog.Fatal(err)\n\t\t\t\t}\n\t\t\t\t...\n\t\t\t}\n\t\t\t// 校验 kubelet 参数\n\t\t\tif err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t\t...\n\t\t\t// 此处初始化了 kubeletDeps\n\t\t\tkubeletDeps, err := UnsecuredDependencies(kubeletServer)\n\t\t\tif err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t\t...\n\t\t\t// 启动程序\n\t\t\tif err := Run(kubeletServer, kubeletDeps, stopCh); err != nil {\n\t\t\t\tglog.Fatal(err)\n\t\t\t}\n\t\t},\n\t}\n    ...\n\treturn cmd\n}\n```\nkubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为。主要包括监控功能（cadvisor），cgroup 管理功能（containerManager）等。\n\nNewKubeletCommand() 会调用 Run() 函数，Run() 中主要调用 run() 函数进行一些准备事项。\n\n#### 3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）\n\nrun() 函数的主要功能：\n\n- 1、创建 kubeClient，evnetClient 用来和 apiserver 通信。创建 heartbeatClient 向 apiserver 上报心跳状态。\n- 2、为 kubeDeps 设定一些默认值。\n- 3、启动监听 Healthz 端口的 http server，默认端口是 10248。\n\n```\nfunc run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh <-chan struct{}) (err error) {\n\t...\n\t// 判断 kubelet 的启动模式\n\tif standaloneMode {\n\t...\n\t} else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil {\n\t\t...\n\t\t// 创建对象 kubeClient\n\t\tkubeClient, err = clientset.NewForConfig(clientConfig)\n\n\t\t...\n        // 创建对象 evnetClient\n\t\teventClient, err = v1core.NewForConfig(&eventClientConfig)\n\t\t...\n\t\t// heartbeatClient 上报状态\n\t\theartbeatClient, err = clientset.NewForConfig(&heartbeatClientConfig)\n\t\t...\n\t}\n\n\t// 为 kubeDeps 设定一些默认值\n\tif kubeDeps.Auth == nil {\n\t\t\tauth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tkubeDeps.Auth = auth\n\t\t}\n\n\t\tif kubeDeps.CAdvisorInterface == nil {\n\t\t\timageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)\n\t\t\tkubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// \n\tif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil {\n\t\t\treturn err\n\t}\n\t...\n\t// 启动监听 Healthz 端口的 http server  \n\tif s.HealthzPort > 0 {\n\t\thealthz.DefaultHealthz()\n\t\tgo wait.Until(func() {\n\t\t\terr := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil)\n\t\t\tif err != nil {\n\t\t\t\tglog.Errorf(\"Starting health server failed: %v\", err)\n\t\t\t}\n\t\t}, 5*time.Second, wait.NeverStop)\n\t}\n\t...\n}\n```\nkubelet 对 pod 资源的获取方式有三种：第一种是通过文件获得，文件一般放在 /etc/kubernetes/manifests 目录下面；第二种也是通过文件过得，只不过文件是通过 URL 获取的；第三种是通过 watch kube-apiserver 获取。其中前两种模式下，我们称 kubelet 运行在 standalone 模式下，运行在 standalone 模式下的 kubelet 一般用于调试某些功能。\n\nrun() 中调用 RunKubelet() 函数进行后续操作。\n\n#### 4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）\n\nRunKubelet()  主要功能：\n\n- 1、初始化 kubelet 组件中的各个模块，创建出 kubelet 对象。\n- 2、启动垃圾回收服务。\n\n```\nfunc RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error {\n    ...\n\n \t// 初始化 kubelet 内部模块\n\tk, err := CreateAndInitKubelet(&kubeServer.KubeletConfiguration,\n\t\tkubeDeps,\n\t\t&kubeServer.ContainerRuntimeOptions,\n\t\tkubeServer.ContainerRuntime,\n\t\tkubeServer.RuntimeCgroups,\n\t\tkubeServer.HostnameOverride,\n\t\tkubeServer.NodeIP,\n\t\tkubeServer.ProviderID,\n\t\tkubeServer.CloudProvider,\n\t\tkubeServer.CertDirectory,\n\t\tkubeServer.RootDirectory,\n\t\tkubeServer.RegisterNode,\n\t\tkubeServer.RegisterWithTaints,\n\t\tkubeServer.AllowedUnsafeSysctls,\n\t\tkubeServer.RemoteRuntimeEndpoint,\n\t\tkubeServer.RemoteImageEndpoint,\n\t\tkubeServer.ExperimentalMounterPath,\n\t\tkubeServer.ExperimentalKernelMemcgNotification,\n\t\tkubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount,\n\t\tkubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold,\n\t\tkubeServer.MinimumGCAge,\n\t\tkubeServer.MaxPerPodContainerCount,\n\t\tkubeServer.MaxContainerCount,\n\t\tkubeServer.MasterServiceNamespace,\n\t\tkubeServer.RegisterSchedulable,\n\t\tkubeServer.NonMasqueradeCIDR,\n\t\tkubeServer.KeepTerminatedPodVolumes,\n\t\tkubeServer.NodeLabels,\n\t\tkubeServer.SeccompProfileRoot,\n\t\tkubeServer.BootstrapCheckpointPath,\n\t\tkubeServer.NodeStatusMaxImages)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create kubelet: %v\", err)\n\t}\n\n\t...\n\tif runOnce {\n\t\tif _, err := k.RunOnce(podCfg.Updates()); err != nil {\n\t\t\treturn fmt.Errorf(\"runonce failed: %v\", err)\n\t\t}\n\t\tglog.Infof(\"Started kubelet as runonce\")\n\t} else {\n        // \n\t\tstartKubelet(k, podCfg, &kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer)\n\t\tglog.Infof(\"Started kubelet\")\n\t}\n\n}\n```\n\n\n```\nfunc CreateAndInitKubelet(...){\n\t// NewMainKubelet 实例化一个 kubelet 对象，并对 kubelet 内部各个模块进行初始化\n\tk, err = kubelet.NewMainKubelet(kubeCfg,\n\t\tkubeDeps,\n\t\tcrOptions,\n\t\tcontainerRuntime,\n\t\truntimeCgroups,\n\t\thostnameOverride,\n\t\tnodeIP,\n\t\tproviderID,\n\t\tcloudProvider,\n\t\tcertDirectory,\n\t\trootDirectory,\n\t\tregisterNode,\n\t\tregisterWithTaints,\n\t\tallowedUnsafeSysctls,\n\t\tremoteRuntimeEndpoint,\n\t\tremoteImageEndpoint,\n\t\texperimentalMounterPath,\n\t\texperimentalKernelMemcgNotification,\n\t\texperimentalCheckNodeCapabilitiesBeforeMount,\n\t\texperimentalNodeAllocatableIgnoreEvictionThreshold,\n\t\tminimumGCAge,\n\t\tmaxPerPodContainerCount,\n\t\tmaxContainerCount,\n\t\tmasterServiceNamespace,\n\t\tregisterSchedulable,\n\t\tnonMasqueradeCIDR,\n\t\tkeepTerminatedPodVolumes,\n\t\tnodeLabels,\n\t\tseccompProfileRoot,\n\t\tbootstrapCheckpointPath,\n\t\tnodeStatusMaxImages)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// 通知 apiserver kubelet 启动了\n\tk.BirthCry()\n\t// 启动垃圾回收服务\n\tk.StartGarbageCollection()\n\n\treturn k, nil\n\n}\n```\n\n```\nfunc NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...){\n    ...\n\tif kubeDeps.PodConfig == nil {\n\t\tvar err error\n\t\t// 初始化 makePodSourceConfig，监听 pod 元数据的来源(FILE, URL, api-server)，将不同 source 的 pod configuration 合并到一个结构中\n\t\tkubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n    \n    // kubelet 服务端口，默认 10250\n\tdaemonEndpoints := &v1.NodeDaemonEndpoints{\n\t\tKubeletEndpoint: v1.DaemonEndpoint{Port: kubeCfg.Port},\n\t}\n\n\t// 使用 reflector 把 ListWatch 得到的服务信息实时同步到 serviceStore 对象中\n\tserviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})\n\tif kubeDeps.KubeClient != nil {\n\t\tserviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"services\", metav1.NamespaceAll, fields.Everything())\n\t\tr := cache.NewReflector(serviceLW, &v1.Service{}, serviceIndexer, 0)\n\t\tgo r.Run(wait.NeverStop)\n\t}\n\tserviceLister := corelisters.NewServiceLister(serviceIndexer)\n\n\t// 使用 reflector 把 ListWatch 得到的节点信息实时同步到  nodeStore 对象中\n\tnodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{})\n\tif kubeDeps.KubeClient != nil {\n\t\tfieldSelector := fields.Set{api.ObjectNameField: string(nodeName)}.AsSelector()\n\t\tnodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"nodes\", metav1.NamespaceAll, fieldSelector)\n\t\tr := cache.NewReflector(nodeLW, &v1.Node{}, nodeIndexer, 0)\n\t\tgo r.Run(wait.NeverStop)\n\t}\n\tnodeInfo := &predicates.CachedNodeInfo{NodeLister: corelisters.NewNodeLister(nodeIndexer)}\n\n\t...\n\t// node 资源不足时的驱逐策略的设定\n\tthresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tevictionConfig := eviction.Config{\n\t\tPressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,\n\t\tMaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),\n\t\tThresholds:               thresholds,\n\t\tKernelMemcgNotification:  experimentalKernelMemcgNotification,\n\t\tPodCgroupRoot:            kubeDeps.ContainerManager.GetPodCgroupRoot(),\n\t}\n    ...\n    // 容器引用的管理\n\tcontainerRefManager := kubecontainer.NewRefManager()\n    // oom 监控\n\toomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder)\n\n\t// 根据配置信息和各种对象创建 Kubelet 实例\n\tklet := &Kubelet{\n\t\thostname:                       hostname,\n\t\thostnameOverridden:             len(hostnameOverride) > 0,\n\t\tnodeName:                       nodeName,\n\t\t...\n\t}\n\t\n\t// 从 cAdvisor 获取当前机器的信息\n\tmachineInfo, err := klet.cadvisor.MachineInfo()\n\n\t// 对 pod 的管理（如: 增删改等）\n\tklet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager)\n\n\t// 容器运行时管理\n\truntime, err := kuberuntime.NewKubeGenericRuntimeManager(...)\n\n\t// pleg\n\tklet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock{})\n\n\t// 创建 containerGC 对象，进行周期性的容器清理工作\n\tcontainerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)\n\n\t// 创建 imageManager 管理镜像\n\timageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage)\n\t\n\t// statusManager 实时检测节点上 pod 的状态，并更新到 apiserver 对应的 pod\n\tklet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)\n\n\t// 探针管理\n\tklet.probeManager = prober.NewManager(...)\n\n    // token 管理\n\ttokenManager := token.NewManager(kubeDeps.KubeClient)\n\n\t// 磁盘管理\n\tklet.volumeManager = volumemanager.NewVolumeManager()\n\t\n\t// 将 syncPod() 注入到 podWorkers 中\n\tklet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)\n\n\t// 容器驱逐策略管理\n\tevictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock)\n    ...\n}\n```\nRunKubelet 最后会调用 startKubelet() 进行后续的操作。\n\n#### 5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）\nstartKubelet()  的主要功能：\n\n- 1、以 goroutine 方式启动 kubelet 中的各个模块。\n- 2、启动 kubelet http server。\n\n\n```\nfunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) {\n\tgo wait.Until(func() {\n\t\t// 以 goroutine 方式启动 kubelet 中的各个模块\n\t\tk.Run(podCfg.Updates())\n\t}, 0, wait.NeverStop)\n\n\t// 启动 kubelet http server\t\n\tif enableServer {\n\t\tgo k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling)\n\n\t}\n\tif kubeCfg.ReadOnlyPort > 0 {\n\t\tgo k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort))\n\t}\n}\n```\n\n```\n// Run starts the kubelet reacting to config updates\nfunc (kl *Kubelet) Run(updates <-chan kubetypes.PodUpdate) {\n\tif kl.logServer == nil {\n\t\tkl.logServer = http.StripPrefix(\"/logs/\", http.FileServer(http.Dir(\"/var/log/\")))\n\t}\n\tif kl.kubeClient == nil {\n\t\tglog.Warning(\"No api server defined - no node status update will be sent.\")\n\t}\n\n\t// Start the cloud provider sync manager\n\tif kl.cloudResourceSyncManager != nil {\n\t\tgo kl.cloudResourceSyncManager.Run(wait.NeverStop)\n\t}\n\n\tif err := kl.initializeModules(); err != nil {\n\t\tkl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())\n\t\tglog.Fatal(err)\n\t}\n\n\t// Start volume manager\n\tgo kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)\n\n\tif kl.kubeClient != nil {\n\t\t// Start syncing node status immediately, this may set up things the runtime needs to run.\n\t\tgo wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)\n\t\tgo kl.fastStatusUpdateOnce()\n\n\t\t// start syncing lease\n\t\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) {\n\t\t\tgo kl.nodeLeaseController.Run(wait.NeverStop)\n\t\t}\n\t}\n\tgo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)\n\n\t// Start loop to sync iptables util rules\n\tif kl.makeIPTablesUtilChains {\n\t\tgo wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop)\n\t}\n\n\t// Start a goroutine responsible for killing pods (that are not properly\n\t// handled by pod workers).\n\tgo wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)\n\n\t// Start component sync loops.\n\tkl.statusManager.Start()\n\tkl.probeManager.Start()\n\n\t// Start syncing RuntimeClasses if enabled.\n\tif kl.runtimeClassManager != nil {\n\t\tgo kl.runtimeClassManager.Run(wait.NeverStop)\n\t}\n\n\t// Start the pod lifecycle event generator.\n\tkl.pleg.Start()\n\n\tkl.syncLoop(updates, kl)\n}\n```\nsyncLoop 是 kubelet 的主循环方法，它从不同的管道(FILE,URL, API-SERVER)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 Pod 处于期望的状态。\n\n\n```\nfunc (kl *Kubelet) syncLoop(updates <-chan kubetypes.PodUpdate, handler SyncHandler) {\n\tglog.Info(\"Starting kubelet main sync loop.\")\n\n\t// syncTicker 每秒检测一次是否有需要同步的 pod workers\n\tsyncTicker := time.NewTicker(time.Second)\n\tdefer syncTicker.Stop()\n\thousekeepingTicker := time.NewTicker(housekeepingPeriod)\n\tdefer housekeepingTicker.Stop()\n\tplegCh := kl.pleg.Watch()\n\tconst (\n\t\tbase   = 100 * time.Millisecond\n\t\tmax    = 5 * time.Second\n\t\tfactor = 2\n\t)\n\tduration := base\n\tfor {\n\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 {\n\t\t\tglog.Infof(\"skipping pod synchronization - %v\", rs)\n\t\t\t// exponential backoff\n\t\t\ttime.Sleep(duration)\n\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))\n\t\t\tcontinue\n\t\t}\n\t\t// reset backoff if we have a success\n\t\tduration = base\n\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t\t// \n\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) {\n\t\t\tbreak\n\t\t}\n\t\tkl.syncLoopMonitor.Store(kl.clock.Now())\n\t}\n}\n```\n\nsyncLoopIteration()  方法对多个管道进行遍历，如果 pod 发生变化，则会调用相应的 Handler，在 Handler 中通过调用 dispatchWork 分发任务。\n\n\n## 总结\n\n本篇文章主要讲述了 kubelet 组件从加载配置到初始化内部的各个模块再到启动 kubelet 服务的整个流程，上面的时序图能清楚的看到函数之间的调用关系，但是其中每个组件具体的工作方式以及组件之间的交互方式还不得而知，后面会一探究竟。\n\n参考：\n[kubernetes node components – kubelet](http://www.sel.zju.edu.cn/?p=595)\n[Kubelet 源码分析(一):启动流程分析](https://segmentfault.com/a/1190000008267351)\n[kubelet 源码分析：启动流程](https://cizixs.com/2017/06/06/kubelet-source-code-analysis-part-1/)\n[kubernetes 的 kubelet 的工作过程](https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/05/02/Kubernetes-kubelet.html)\n[kubelet 内部实现解析](https://fatsheep9146.github.io/2018/07/08/kubelet%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/)\n","slug":"kubelet_init","published":1,"updated":"2019-07-21T09:47:36.619Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59s001eapwnglnk7khk","content":"<p>上篇文章（<a href=\"https://blog.tianfeiyu.com/2018/12/16/kubelet-modules/\" target=\"_blank\" rel=\"noopener\">kubelet 架构浅析</a> ）已经介绍过 kubelet 在整个集群架构中的功能以及自身各模块的用途，本篇文章主要介绍 kubelet 的启动流程。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<h2 id=\"kubelet-启动流程\"><a href=\"#kubelet-启动流程\" class=\"headerlink\" title=\"kubelet 启动流程\"></a>kubelet 启动流程</h2><p>kubelet 代码结构:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  kubernetes git:(release-1.12) ✗ tree cmd/kubelet</span><br><span class=\"line\">cmd/kubelet</span><br><span class=\"line\">├── BUILD</span><br><span class=\"line\">├── OWNERS</span><br><span class=\"line\">├── app</span><br><span class=\"line\">│   ├── BUILD</span><br><span class=\"line\">│   ├── OWNERS</span><br><span class=\"line\">│   ├── auth.go</span><br><span class=\"line\">│   ├── init_others.go</span><br><span class=\"line\">│   ├── init_windows.go</span><br><span class=\"line\">│   ├── options</span><br><span class=\"line\">│   │   ├── BUILD</span><br><span class=\"line\">│   │   ├── container_runtime.go</span><br><span class=\"line\">│   │   ├── globalflags.go</span><br><span class=\"line\">│   │   ├── globalflags_linux.go</span><br><span class=\"line\">│   │   ├── globalflags_other.go</span><br><span class=\"line\">│   │   ├── options.go</span><br><span class=\"line\">│   │   ├── options_test.go</span><br><span class=\"line\">│   │   ├── osflags_others.go</span><br><span class=\"line\">│   │   └── osflags_windows.go</span><br><span class=\"line\">│   ├── plugins.go</span><br><span class=\"line\">│   ├── server.go</span><br><span class=\"line\">│   ├── server_linux.go</span><br><span class=\"line\">│   ├── server_test.go</span><br><span class=\"line\">│   └── server_unsupported.go</span><br><span class=\"line\">└── kubelet.go</span><br><span class=\"line\"></span><br><span class=\"line\">2 directories, 22 files</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://cdn.tianfeiyu.com/kubelet-3.png\" alt=\"kubelet 启动流程时序图\"></p>\n<h4 id=\"1、kubelet-入口函数-main（cmd-kubelet-kubelet-go）\"><a href=\"#1、kubelet-入口函数-main（cmd-kubelet-kubelet-go）\" class=\"headerlink\" title=\"1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）\"></a>1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\trand.Seed(time.Now().UTC().UnixNano())</span><br><span class=\"line\"></span><br><span class=\"line\">\tcommand := app.NewKubeletCommand(server.SetupSignalHandler())</span><br><span class=\"line\">\tlogs.InitLogs()</span><br><span class=\"line\">\tdefer logs.FlushLogs()</span><br><span class=\"line\"></span><br><span class=\"line\">\tif err := command.Execute(); err != nil &#123;</span><br><span class=\"line\">\t\tfmt.Fprintf(os.Stderr, &quot;%v\\n&quot;, err)</span><br><span class=\"line\">\t\tos.Exit(1)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"2、初始化-kubelet-配置（cmd-kubelet-app-server-go）\"><a href=\"#2、初始化-kubelet-配置（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"2、初始化 kubelet 配置（cmd/kubelet/app/server.go）\"></a>2、初始化 kubelet 配置（cmd/kubelet/app/server.go）</h4><p>NewKubeletCommand() 函数主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// NewKubeletCommand creates a *cobra.Command object with default parameters</span><br><span class=\"line\">func NewKubeletCommand(stopCh &lt;-chan struct&#123;&#125;) *cobra.Command &#123;</span><br><span class=\"line\">    cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)</span><br><span class=\"line\">    cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc)</span><br><span class=\"line\">    // Kubelet配置分两部分:</span><br><span class=\"line\">    // KubeletFlag: 指那些不允许在 kubelet 运行时进行修改的配置集，或者不能在集群中各个 Nodes 之间共享的配置集。</span><br><span class=\"line\">    // KubeletConfiguration: 指可以在集群中各个Nodes之间共享的配置集，可以进行动态配置。</span><br><span class=\"line\">    kubeletFlags := options.NewKubeletFlags()</span><br><span class=\"line\">\tkubeletConfig, err := options.NewKubeletConfiguration()</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\tcmd := &amp;cobra.Command&#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tRun: func(cmd *cobra.Command, args []string) &#123;</span><br><span class=\"line\">\t\t\t// 读取 kubelet 配置文件</span><br><span class=\"line\">\t\t\tif configFile := kubeletFlags.KubeletConfigFile; len(configFile) &gt; 0 &#123;</span><br><span class=\"line\">\t\t\t\tkubeletConfig, err = loadConfigFile(configFile)</span><br><span class=\"line\">\t\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t// 校验 kubelet 参数</span><br><span class=\"line\">\t\t\tif err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\t// 此处初始化了 kubeletDeps</span><br><span class=\"line\">\t\t\tkubeletDeps, err := UnsecuredDependencies(kubeletServer)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\t// 启动程序</span><br><span class=\"line\">\t\t\tif err := Run(kubeletServer, kubeletDeps, stopCh); err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\treturn cmd</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为。主要包括监控功能（cadvisor），cgroup 管理功能（containerManager）等。</p>\n<p>NewKubeletCommand() 会调用 Run() 函数，Run() 中主要调用 run() 函数进行一些准备事项。</p>\n<h4 id=\"3、创建和-apiserver-通信的对象（cmd-kubelet-app-server-go）\"><a href=\"#3、创建和-apiserver-通信的对象（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）\"></a>3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）</h4><p>run() 函数的主要功能：</p>\n<ul>\n<li>1、创建 kubeClient，evnetClient 用来和 apiserver 通信。创建 heartbeatClient 向 apiserver 上报心跳状态。</li>\n<li>2、为 kubeDeps 设定一些默认值。</li>\n<li>3、启动监听 Healthz 端口的 http server，默认端口是 10248。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// 判断 kubelet 的启动模式</span><br><span class=\"line\">\tif standaloneMode &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t&#125; else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// 创建对象 kubeClient</span><br><span class=\"line\">\t\tkubeClient, err = clientset.NewForConfig(clientConfig)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">        // 创建对象 evnetClient</span><br><span class=\"line\">\t\teventClient, err = v1core.NewForConfig(&amp;eventClientConfig)</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// heartbeatClient 上报状态</span><br><span class=\"line\">\t\theartbeatClient, err = clientset.NewForConfig(&amp;heartbeatClientConfig)</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 为 kubeDeps 设定一些默认值</span><br><span class=\"line\">\tif kubeDeps.Auth == nil &#123;</span><br><span class=\"line\">\t\t\tauth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn err</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tkubeDeps.Auth = auth</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif kubeDeps.CAdvisorInterface == nil &#123;</span><br><span class=\"line\">\t\t\timageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)</span><br><span class=\"line\">\t\t\tkubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn err</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// </span><br><span class=\"line\">\tif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// 启动监听 Healthz 端口的 http server  </span><br><span class=\"line\">\tif s.HealthzPort &gt; 0 &#123;</span><br><span class=\"line\">\t\thealthz.DefaultHealthz()</span><br><span class=\"line\">\t\tgo wait.Until(func() &#123;</span><br><span class=\"line\">\t\t\terr := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Errorf(&quot;Starting health server failed: %v&quot;, err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;, 5*time.Second, wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kubelet 对 pod 资源的获取方式有三种：第一种是通过文件获得，文件一般放在 /etc/kubernetes/manifests 目录下面；第二种也是通过文件过得，只不过文件是通过 URL 获取的；第三种是通过 watch kube-apiserver 获取。其中前两种模式下，我们称 kubelet 运行在 standalone 模式下，运行在 standalone 模式下的 kubelet 一般用于调试某些功能。</p>\n<p>run() 中调用 RunKubelet() 函数进行后续操作。</p>\n<h4 id=\"4、初始化-kubelet-组件内部的模块（cmd-kubelet-app-server-go）\"><a href=\"#4、初始化-kubelet-组件内部的模块（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）\"></a>4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）</h4><p>RunKubelet()  主要功能：</p>\n<ul>\n<li>1、初始化 kubelet 组件中的各个模块，创建出 kubelet 对象。</li>\n<li>2、启动垃圾回收服务。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\"> \t// 初始化 kubelet 内部模块</span><br><span class=\"line\">\tk, err := CreateAndInitKubelet(&amp;kubeServer.KubeletConfiguration,</span><br><span class=\"line\">\t\tkubeDeps,</span><br><span class=\"line\">\t\t&amp;kubeServer.ContainerRuntimeOptions,</span><br><span class=\"line\">\t\tkubeServer.ContainerRuntime,</span><br><span class=\"line\">\t\tkubeServer.RuntimeCgroups,</span><br><span class=\"line\">\t\tkubeServer.HostnameOverride,</span><br><span class=\"line\">\t\tkubeServer.NodeIP,</span><br><span class=\"line\">\t\tkubeServer.ProviderID,</span><br><span class=\"line\">\t\tkubeServer.CloudProvider,</span><br><span class=\"line\">\t\tkubeServer.CertDirectory,</span><br><span class=\"line\">\t\tkubeServer.RootDirectory,</span><br><span class=\"line\">\t\tkubeServer.RegisterNode,</span><br><span class=\"line\">\t\tkubeServer.RegisterWithTaints,</span><br><span class=\"line\">\t\tkubeServer.AllowedUnsafeSysctls,</span><br><span class=\"line\">\t\tkubeServer.RemoteRuntimeEndpoint,</span><br><span class=\"line\">\t\tkubeServer.RemoteImageEndpoint,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalMounterPath,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold,</span><br><span class=\"line\">\t\tkubeServer.MinimumGCAge,</span><br><span class=\"line\">\t\tkubeServer.MaxPerPodContainerCount,</span><br><span class=\"line\">\t\tkubeServer.MaxContainerCount,</span><br><span class=\"line\">\t\tkubeServer.MasterServiceNamespace,</span><br><span class=\"line\">\t\tkubeServer.RegisterSchedulable,</span><br><span class=\"line\">\t\tkubeServer.NonMasqueradeCIDR,</span><br><span class=\"line\">\t\tkubeServer.KeepTerminatedPodVolumes,</span><br><span class=\"line\">\t\tkubeServer.NodeLabels,</span><br><span class=\"line\">\t\tkubeServer.SeccompProfileRoot,</span><br><span class=\"line\">\t\tkubeServer.BootstrapCheckpointPath,</span><br><span class=\"line\">\t\tkubeServer.NodeStatusMaxImages)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn fmt.Errorf(&quot;failed to create kubelet: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t...</span><br><span class=\"line\">\tif runOnce &#123;</span><br><span class=\"line\">\t\tif _, err := k.RunOnce(podCfg.Updates()); err != nil &#123;</span><br><span class=\"line\">\t\t\treturn fmt.Errorf(&quot;runonce failed: %v&quot;, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tglog.Infof(&quot;Started kubelet as runonce&quot;)</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">        // </span><br><span class=\"line\">\t\tstartKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer)</span><br><span class=\"line\">\t\tglog.Infof(&quot;Started kubelet&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func CreateAndInitKubelet(...)&#123;</span><br><span class=\"line\">\t// NewMainKubelet 实例化一个 kubelet 对象，并对 kubelet 内部各个模块进行初始化</span><br><span class=\"line\">\tk, err = kubelet.NewMainKubelet(kubeCfg,</span><br><span class=\"line\">\t\tkubeDeps,</span><br><span class=\"line\">\t\tcrOptions,</span><br><span class=\"line\">\t\tcontainerRuntime,</span><br><span class=\"line\">\t\truntimeCgroups,</span><br><span class=\"line\">\t\thostnameOverride,</span><br><span class=\"line\">\t\tnodeIP,</span><br><span class=\"line\">\t\tproviderID,</span><br><span class=\"line\">\t\tcloudProvider,</span><br><span class=\"line\">\t\tcertDirectory,</span><br><span class=\"line\">\t\trootDirectory,</span><br><span class=\"line\">\t\tregisterNode,</span><br><span class=\"line\">\t\tregisterWithTaints,</span><br><span class=\"line\">\t\tallowedUnsafeSysctls,</span><br><span class=\"line\">\t\tremoteRuntimeEndpoint,</span><br><span class=\"line\">\t\tremoteImageEndpoint,</span><br><span class=\"line\">\t\texperimentalMounterPath,</span><br><span class=\"line\">\t\texperimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\texperimentalCheckNodeCapabilitiesBeforeMount,</span><br><span class=\"line\">\t\texperimentalNodeAllocatableIgnoreEvictionThreshold,</span><br><span class=\"line\">\t\tminimumGCAge,</span><br><span class=\"line\">\t\tmaxPerPodContainerCount,</span><br><span class=\"line\">\t\tmaxContainerCount,</span><br><span class=\"line\">\t\tmasterServiceNamespace,</span><br><span class=\"line\">\t\tregisterSchedulable,</span><br><span class=\"line\">\t\tnonMasqueradeCIDR,</span><br><span class=\"line\">\t\tkeepTerminatedPodVolumes,</span><br><span class=\"line\">\t\tnodeLabels,</span><br><span class=\"line\">\t\tseccompProfileRoot,</span><br><span class=\"line\">\t\tbootstrapCheckpointPath,</span><br><span class=\"line\">\t\tnodeStatusMaxImages)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn nil, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 通知 apiserver kubelet 启动了</span><br><span class=\"line\">\tk.BirthCry()</span><br><span class=\"line\">\t// 启动垃圾回收服务</span><br><span class=\"line\">\tk.StartGarbageCollection()</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn k, nil</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...)&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\tif kubeDeps.PodConfig == nil &#123;</span><br><span class=\"line\">\t\tvar err error</span><br><span class=\"line\">\t\t// 初始化 makePodSourceConfig，监听 pod 元数据的来源(FILE, URL, api-server)，将不同 source 的 pod configuration 合并到一个结构中</span><br><span class=\"line\">\t\tkubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn nil, err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    // kubelet 服务端口，默认 10250</span><br><span class=\"line\">\tdaemonEndpoints := &amp;v1.NodeDaemonEndpoints&#123;</span><br><span class=\"line\">\t\tKubeletEndpoint: v1.DaemonEndpoint&#123;Port: kubeCfg.Port&#125;,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用 reflector 把 ListWatch 得到的服务信息实时同步到 serviceStore 对象中</span><br><span class=\"line\">\tserviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;)</span><br><span class=\"line\">\tif kubeDeps.KubeClient != nil &#123;</span><br><span class=\"line\">\t\tserviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;services&quot;, metav1.NamespaceAll, fields.Everything())</span><br><span class=\"line\">\t\tr := cache.NewReflector(serviceLW, &amp;v1.Service&#123;&#125;, serviceIndexer, 0)</span><br><span class=\"line\">\t\tgo r.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tserviceLister := corelisters.NewServiceLister(serviceIndexer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用 reflector 把 ListWatch 得到的节点信息实时同步到  nodeStore 对象中</span><br><span class=\"line\">\tnodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;&#125;)</span><br><span class=\"line\">\tif kubeDeps.KubeClient != nil &#123;</span><br><span class=\"line\">\t\tfieldSelector := fields.Set&#123;api.ObjectNameField: string(nodeName)&#125;.AsSelector()</span><br><span class=\"line\">\t\tnodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;nodes&quot;, metav1.NamespaceAll, fieldSelector)</span><br><span class=\"line\">\t\tr := cache.NewReflector(nodeLW, &amp;v1.Node&#123;&#125;, nodeIndexer, 0)</span><br><span class=\"line\">\t\tgo r.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tnodeInfo := &amp;predicates.CachedNodeInfo&#123;NodeLister: corelisters.NewNodeLister(nodeIndexer)&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// node 资源不足时的驱逐策略的设定</span><br><span class=\"line\">\tthresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn nil, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tevictionConfig := eviction.Config&#123;</span><br><span class=\"line\">\t\tPressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,</span><br><span class=\"line\">\t\tMaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),</span><br><span class=\"line\">\t\tThresholds:               thresholds,</span><br><span class=\"line\">\t\tKernelMemcgNotification:  experimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\tPodCgroupRoot:            kubeDeps.ContainerManager.GetPodCgroupRoot(),</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    // 容器引用的管理</span><br><span class=\"line\">\tcontainerRefManager := kubecontainer.NewRefManager()</span><br><span class=\"line\">    // oom 监控</span><br><span class=\"line\">\toomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 根据配置信息和各种对象创建 Kubelet 实例</span><br><span class=\"line\">\tklet := &amp;Kubelet&#123;</span><br><span class=\"line\">\t\thostname:                       hostname,</span><br><span class=\"line\">\t\thostnameOverridden:             len(hostnameOverride) &gt; 0,</span><br><span class=\"line\">\t\tnodeName:                       nodeName,</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 从 cAdvisor 获取当前机器的信息</span><br><span class=\"line\">\tmachineInfo, err := klet.cadvisor.MachineInfo()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 对 pod 的管理（如: 增删改等）</span><br><span class=\"line\">\tklet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 容器运行时管理</span><br><span class=\"line\">\truntime, err := kuberuntime.NewKubeGenericRuntimeManager(...)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// pleg</span><br><span class=\"line\">\tklet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 containerGC 对象，进行周期性的容器清理工作</span><br><span class=\"line\">\tcontainerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 imageManager 管理镜像</span><br><span class=\"line\">\timageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// statusManager 实时检测节点上 pod 的状态，并更新到 apiserver 对应的 pod</span><br><span class=\"line\">\tklet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 探针管理</span><br><span class=\"line\">\tklet.probeManager = prober.NewManager(...)</span><br><span class=\"line\"></span><br><span class=\"line\">    // token 管理</span><br><span class=\"line\">\ttokenManager := token.NewManager(kubeDeps.KubeClient)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 磁盘管理</span><br><span class=\"line\">\tklet.volumeManager = volumemanager.NewVolumeManager()</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 将 syncPod() 注入到 podWorkers 中</span><br><span class=\"line\">\tklet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 容器驱逐策略管理</span><br><span class=\"line\">\tevictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RunKubelet 最后会调用 startKubelet() 进行后续的操作。</p>\n<h4 id=\"5、启动-kubelet-内部的模块及服务（cmd-kubelet-app-server-go）\"><a href=\"#5、启动-kubelet-内部的模块及服务（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）\"></a>5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）</h4><p>startKubelet()  的主要功能：</p>\n<ul>\n<li>1、以 goroutine 方式启动 kubelet 中的各个模块。</li>\n<li>2、启动 kubelet http server。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) &#123;</span><br><span class=\"line\">\tgo wait.Until(func() &#123;</span><br><span class=\"line\">\t\t// 以 goroutine 方式启动 kubelet 中的各个模块</span><br><span class=\"line\">\t\tk.Run(podCfg.Updates())</span><br><span class=\"line\">\t&#125;, 0, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 启动 kubelet http server\t</span><br><span class=\"line\">\tif enableServer &#123;</span><br><span class=\"line\">\t\tgo k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif kubeCfg.ReadOnlyPort &gt; 0 &#123;</span><br><span class=\"line\">\t\tgo k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Run starts the kubelet reacting to config updates</span><br><span class=\"line\">func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123;</span><br><span class=\"line\">\tif kl.logServer == nil &#123;</span><br><span class=\"line\">\t\tkl.logServer = http.StripPrefix(&quot;/logs/&quot;, http.FileServer(http.Dir(&quot;/var/log/&quot;)))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif kl.kubeClient == nil &#123;</span><br><span class=\"line\">\t\tglog.Warning(&quot;No api server defined - no node status update will be sent.&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start the cloud provider sync manager</span><br><span class=\"line\">\tif kl.cloudResourceSyncManager != nil &#123;</span><br><span class=\"line\">\t\tgo kl.cloudResourceSyncManager.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tif err := kl.initializeModules(); err != nil &#123;</span><br><span class=\"line\">\t\tkl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())</span><br><span class=\"line\">\t\tglog.Fatal(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start volume manager</span><br><span class=\"line\">\tgo kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\tif kl.kubeClient != nil &#123;</span><br><span class=\"line\">\t\t// Start syncing node status immediately, this may set up things the runtime needs to run.</span><br><span class=\"line\">\t\tgo wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)</span><br><span class=\"line\">\t\tgo kl.fastStatusUpdateOnce()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// start syncing lease</span><br><span class=\"line\">\t\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123;</span><br><span class=\"line\">\t\t\tgo kl.nodeLeaseController.Run(wait.NeverStop)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tgo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start loop to sync iptables util rules</span><br><span class=\"line\">\tif kl.makeIPTablesUtilChains &#123;</span><br><span class=\"line\">\t\tgo wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start a goroutine responsible for killing pods (that are not properly</span><br><span class=\"line\">\t// handled by pod workers).</span><br><span class=\"line\">\tgo wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start component sync loops.</span><br><span class=\"line\">\tkl.statusManager.Start()</span><br><span class=\"line\">\tkl.probeManager.Start()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start syncing RuntimeClasses if enabled.</span><br><span class=\"line\">\tif kl.runtimeClassManager != nil &#123;</span><br><span class=\"line\">\t\tgo kl.runtimeClassManager.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start the pod lifecycle event generator.</span><br><span class=\"line\">\tkl.pleg.Start()</span><br><span class=\"line\"></span><br><span class=\"line\">\tkl.syncLoop(updates, kl)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncLoop 是 kubelet 的主循环方法，它从不同的管道(FILE,URL, API-SERVER)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 Pod 处于期望的状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123;</span><br><span class=\"line\">\tglog.Info(&quot;Starting kubelet main sync loop.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// syncTicker 每秒检测一次是否有需要同步的 pod workers</span><br><span class=\"line\">\tsyncTicker := time.NewTicker(time.Second)</span><br><span class=\"line\">\tdefer syncTicker.Stop()</span><br><span class=\"line\">\thousekeepingTicker := time.NewTicker(housekeepingPeriod)</span><br><span class=\"line\">\tdefer housekeepingTicker.Stop()</span><br><span class=\"line\">\tplegCh := kl.pleg.Watch()</span><br><span class=\"line\">\tconst (</span><br><span class=\"line\">\t\tbase   = 100 * time.Millisecond</span><br><span class=\"line\">\t\tmax    = 5 * time.Second</span><br><span class=\"line\">\t\tfactor = 2</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tduration := base</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123;</span><br><span class=\"line\">\t\t\tglog.Infof(&quot;skipping pod synchronization - %v&quot;, rs)</span><br><span class=\"line\">\t\t\t// exponential backoff</span><br><span class=\"line\">\t\t\ttime.Sleep(duration)</span><br><span class=\"line\">\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// reset backoff if we have a success</span><br><span class=\"line\">\t\tduration = base</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t\t// </span><br><span class=\"line\">\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123;</span><br><span class=\"line\">\t\t\tbreak</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncLoopIteration()  方法对多个管道进行遍历，如果 pod 发生变化，则会调用相应的 Handler，在 Handler 中通过调用 dispatchWork 分发任务。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本篇文章主要讲述了 kubelet 组件从加载配置到初始化内部的各个模块再到启动 kubelet 服务的整个流程，上面的时序图能清楚的看到函数之间的调用关系，但是其中每个组件具体的工作方式以及组件之间的交互方式还不得而知，后面会一探究竟。</p>\n<p>参考：<br><a href=\"http://www.sel.zju.edu.cn/?p=595\" target=\"_blank\" rel=\"noopener\">kubernetes node components – kubelet</a><br><a href=\"https://segmentfault.com/a/1190000008267351\" target=\"_blank\" rel=\"noopener\">Kubelet 源码分析(一):启动流程分析</a><br><a href=\"https://cizixs.com/2017/06/06/kubelet-source-code-analysis-part-1/\" target=\"_blank\" rel=\"noopener\">kubelet 源码分析：启动流程</a><br><a href=\"https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/05/02/Kubernetes-kubelet.html\" target=\"_blank\" rel=\"noopener\">kubernetes 的 kubelet 的工作过程</a><br><a href=\"https://fatsheep9146.github.io/2018/07/08/kubelet%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">kubelet 内部实现解析</a></p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/06/09/node_status/\">kubelet 状态上报的方式</a></li><li><a href=\"http://yoursite.com/2019/02/26/k8s_events/\">kubernets 中事件处理机制</a></li><li><a href=\"http://yoursite.com/2019/01/03/kubelet_create_pod/\">kubelet 创建 pod 的流程</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>上篇文章（<a href=\"https://blog.tianfeiyu.com/2018/12/16/kubelet-modules/\" target=\"_blank\" rel=\"noopener\">kubelet 架构浅析</a> ）已经介绍过 kubelet 在整个集群架构中的功能以及自身各模块的用途，本篇文章主要介绍 kubelet 的启动流程。</p>\n<blockquote>\n<p>kubernetes 版本： v1.12 </p>\n</blockquote>\n<h2 id=\"kubelet-启动流程\"><a href=\"#kubelet-启动流程\" class=\"headerlink\" title=\"kubelet 启动流程\"></a>kubelet 启动流程</h2><p>kubelet 代码结构:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">➜  kubernetes git:(release-1.12) ✗ tree cmd/kubelet</span><br><span class=\"line\">cmd/kubelet</span><br><span class=\"line\">├── BUILD</span><br><span class=\"line\">├── OWNERS</span><br><span class=\"line\">├── app</span><br><span class=\"line\">│   ├── BUILD</span><br><span class=\"line\">│   ├── OWNERS</span><br><span class=\"line\">│   ├── auth.go</span><br><span class=\"line\">│   ├── init_others.go</span><br><span class=\"line\">│   ├── init_windows.go</span><br><span class=\"line\">│   ├── options</span><br><span class=\"line\">│   │   ├── BUILD</span><br><span class=\"line\">│   │   ├── container_runtime.go</span><br><span class=\"line\">│   │   ├── globalflags.go</span><br><span class=\"line\">│   │   ├── globalflags_linux.go</span><br><span class=\"line\">│   │   ├── globalflags_other.go</span><br><span class=\"line\">│   │   ├── options.go</span><br><span class=\"line\">│   │   ├── options_test.go</span><br><span class=\"line\">│   │   ├── osflags_others.go</span><br><span class=\"line\">│   │   └── osflags_windows.go</span><br><span class=\"line\">│   ├── plugins.go</span><br><span class=\"line\">│   ├── server.go</span><br><span class=\"line\">│   ├── server_linux.go</span><br><span class=\"line\">│   ├── server_test.go</span><br><span class=\"line\">│   └── server_unsupported.go</span><br><span class=\"line\">└── kubelet.go</span><br><span class=\"line\"></span><br><span class=\"line\">2 directories, 22 files</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://cdn.tianfeiyu.com/kubelet-3.png\" alt=\"kubelet 启动流程时序图\"></p>\n<h4 id=\"1、kubelet-入口函数-main（cmd-kubelet-kubelet-go）\"><a href=\"#1、kubelet-入口函数-main（cmd-kubelet-kubelet-go）\" class=\"headerlink\" title=\"1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）\"></a>1、kubelet 入口函数 main（cmd/kubelet/kubelet.go）</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func main() &#123;</span><br><span class=\"line\">\trand.Seed(time.Now().UTC().UnixNano())</span><br><span class=\"line\"></span><br><span class=\"line\">\tcommand := app.NewKubeletCommand(server.SetupSignalHandler())</span><br><span class=\"line\">\tlogs.InitLogs()</span><br><span class=\"line\">\tdefer logs.FlushLogs()</span><br><span class=\"line\"></span><br><span class=\"line\">\tif err := command.Execute(); err != nil &#123;</span><br><span class=\"line\">\t\tfmt.Fprintf(os.Stderr, &quot;%v\\n&quot;, err)</span><br><span class=\"line\">\t\tos.Exit(1)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"2、初始化-kubelet-配置（cmd-kubelet-app-server-go）\"><a href=\"#2、初始化-kubelet-配置（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"2、初始化 kubelet 配置（cmd/kubelet/app/server.go）\"></a>2、初始化 kubelet 配置（cmd/kubelet/app/server.go）</h4><p>NewKubeletCommand() 函数主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// NewKubeletCommand creates a *cobra.Command object with default parameters</span><br><span class=\"line\">func NewKubeletCommand(stopCh &lt;-chan struct&#123;&#125;) *cobra.Command &#123;</span><br><span class=\"line\">    cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError)</span><br><span class=\"line\">    cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc)</span><br><span class=\"line\">    // Kubelet配置分两部分:</span><br><span class=\"line\">    // KubeletFlag: 指那些不允许在 kubelet 运行时进行修改的配置集，或者不能在集群中各个 Nodes 之间共享的配置集。</span><br><span class=\"line\">    // KubeletConfiguration: 指可以在集群中各个Nodes之间共享的配置集，可以进行动态配置。</span><br><span class=\"line\">    kubeletFlags := options.NewKubeletFlags()</span><br><span class=\"line\">\tkubeletConfig, err := options.NewKubeletConfiguration()</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\tcmd := &amp;cobra.Command&#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\tRun: func(cmd *cobra.Command, args []string) &#123;</span><br><span class=\"line\">\t\t\t// 读取 kubelet 配置文件</span><br><span class=\"line\">\t\t\tif configFile := kubeletFlags.KubeletConfigFile; len(configFile) &gt; 0 &#123;</span><br><span class=\"line\">\t\t\t\tkubeletConfig, err = loadConfigFile(configFile)</span><br><span class=\"line\">\t\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t\t&#125;</span><br><span class=\"line\">\t\t\t\t...</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t// 校验 kubelet 参数</span><br><span class=\"line\">\t\t\tif err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\t// 此处初始化了 kubeletDeps</span><br><span class=\"line\">\t\t\tkubeletDeps, err := UnsecuredDependencies(kubeletServer)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\t...</span><br><span class=\"line\">\t\t\t// 启动程序</span><br><span class=\"line\">\t\t\tif err := Run(kubeletServer, kubeletDeps, stopCh); err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Fatal(err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\treturn cmd</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为。主要包括监控功能（cadvisor），cgroup 管理功能（containerManager）等。</p>\n<p>NewKubeletCommand() 会调用 Run() 函数，Run() 中主要调用 run() 函数进行一些准备事项。</p>\n<h4 id=\"3、创建和-apiserver-通信的对象（cmd-kubelet-app-server-go）\"><a href=\"#3、创建和-apiserver-通信的对象（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）\"></a>3、创建和 apiserver 通信的对象（cmd/kubelet/app/server.go）</h4><p>run() 函数的主要功能：</p>\n<ul>\n<li>1、创建 kubeClient，evnetClient 用来和 apiserver 通信。创建 heartbeatClient 向 apiserver 上报心跳状态。</li>\n<li>2、为 kubeDeps 设定一些默认值。</li>\n<li>3、启动监听 Healthz 端口的 http server，默认端口是 10248。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// 判断 kubelet 的启动模式</span><br><span class=\"line\">\tif standaloneMode &#123;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t&#125; else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil &#123;</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// 创建对象 kubeClient</span><br><span class=\"line\">\t\tkubeClient, err = clientset.NewForConfig(clientConfig)</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">        // 创建对象 evnetClient</span><br><span class=\"line\">\t\teventClient, err = v1core.NewForConfig(&amp;eventClientConfig)</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t\t// heartbeatClient 上报状态</span><br><span class=\"line\">\t\theartbeatClient, err = clientset.NewForConfig(&amp;heartbeatClientConfig)</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 为 kubeDeps 设定一些默认值</span><br><span class=\"line\">\tif kubeDeps.Auth == nil &#123;</span><br><span class=\"line\">\t\t\tauth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn err</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t\tkubeDeps.Auth = auth</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tif kubeDeps.CAdvisorInterface == nil &#123;</span><br><span class=\"line\">\t\t\timageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint)</span><br><span class=\"line\">\t\t\tkubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint))</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\treturn err</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// </span><br><span class=\"line\">\tif err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil &#123;</span><br><span class=\"line\">\t\t\treturn err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// 启动监听 Healthz 端口的 http server  </span><br><span class=\"line\">\tif s.HealthzPort &gt; 0 &#123;</span><br><span class=\"line\">\t\thealthz.DefaultHealthz()</span><br><span class=\"line\">\t\tgo wait.Until(func() &#123;</span><br><span class=\"line\">\t\t\terr := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil)</span><br><span class=\"line\">\t\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\t\tglog.Errorf(&quot;Starting health server failed: %v&quot;, err)</span><br><span class=\"line\">\t\t\t&#125;</span><br><span class=\"line\">\t\t&#125;, 5*time.Second, wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kubelet 对 pod 资源的获取方式有三种：第一种是通过文件获得，文件一般放在 /etc/kubernetes/manifests 目录下面；第二种也是通过文件过得，只不过文件是通过 URL 获取的；第三种是通过 watch kube-apiserver 获取。其中前两种模式下，我们称 kubelet 运行在 standalone 模式下，运行在 standalone 模式下的 kubelet 一般用于调试某些功能。</p>\n<p>run() 中调用 RunKubelet() 函数进行后续操作。</p>\n<h4 id=\"4、初始化-kubelet-组件内部的模块（cmd-kubelet-app-server-go）\"><a href=\"#4、初始化-kubelet-组件内部的模块（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）\"></a>4、初始化 kubelet 组件内部的模块（cmd/kubelet/app/server.go）</h4><p>RunKubelet()  主要功能：</p>\n<ul>\n<li>1、初始化 kubelet 组件中的各个模块，创建出 kubelet 对象。</li>\n<li>2、启动垃圾回收服务。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\"></span><br><span class=\"line\"> \t// 初始化 kubelet 内部模块</span><br><span class=\"line\">\tk, err := CreateAndInitKubelet(&amp;kubeServer.KubeletConfiguration,</span><br><span class=\"line\">\t\tkubeDeps,</span><br><span class=\"line\">\t\t&amp;kubeServer.ContainerRuntimeOptions,</span><br><span class=\"line\">\t\tkubeServer.ContainerRuntime,</span><br><span class=\"line\">\t\tkubeServer.RuntimeCgroups,</span><br><span class=\"line\">\t\tkubeServer.HostnameOverride,</span><br><span class=\"line\">\t\tkubeServer.NodeIP,</span><br><span class=\"line\">\t\tkubeServer.ProviderID,</span><br><span class=\"line\">\t\tkubeServer.CloudProvider,</span><br><span class=\"line\">\t\tkubeServer.CertDirectory,</span><br><span class=\"line\">\t\tkubeServer.RootDirectory,</span><br><span class=\"line\">\t\tkubeServer.RegisterNode,</span><br><span class=\"line\">\t\tkubeServer.RegisterWithTaints,</span><br><span class=\"line\">\t\tkubeServer.AllowedUnsafeSysctls,</span><br><span class=\"line\">\t\tkubeServer.RemoteRuntimeEndpoint,</span><br><span class=\"line\">\t\tkubeServer.RemoteImageEndpoint,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalMounterPath,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount,</span><br><span class=\"line\">\t\tkubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold,</span><br><span class=\"line\">\t\tkubeServer.MinimumGCAge,</span><br><span class=\"line\">\t\tkubeServer.MaxPerPodContainerCount,</span><br><span class=\"line\">\t\tkubeServer.MaxContainerCount,</span><br><span class=\"line\">\t\tkubeServer.MasterServiceNamespace,</span><br><span class=\"line\">\t\tkubeServer.RegisterSchedulable,</span><br><span class=\"line\">\t\tkubeServer.NonMasqueradeCIDR,</span><br><span class=\"line\">\t\tkubeServer.KeepTerminatedPodVolumes,</span><br><span class=\"line\">\t\tkubeServer.NodeLabels,</span><br><span class=\"line\">\t\tkubeServer.SeccompProfileRoot,</span><br><span class=\"line\">\t\tkubeServer.BootstrapCheckpointPath,</span><br><span class=\"line\">\t\tkubeServer.NodeStatusMaxImages)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn fmt.Errorf(&quot;failed to create kubelet: %v&quot;, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t...</span><br><span class=\"line\">\tif runOnce &#123;</span><br><span class=\"line\">\t\tif _, err := k.RunOnce(podCfg.Updates()); err != nil &#123;</span><br><span class=\"line\">\t\t\treturn fmt.Errorf(&quot;runonce failed: %v&quot;, err)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tglog.Infof(&quot;Started kubelet as runonce&quot;)</span><br><span class=\"line\">\t&#125; else &#123;</span><br><span class=\"line\">        // </span><br><span class=\"line\">\t\tstartKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer)</span><br><span class=\"line\">\t\tglog.Infof(&quot;Started kubelet&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func CreateAndInitKubelet(...)&#123;</span><br><span class=\"line\">\t// NewMainKubelet 实例化一个 kubelet 对象，并对 kubelet 内部各个模块进行初始化</span><br><span class=\"line\">\tk, err = kubelet.NewMainKubelet(kubeCfg,</span><br><span class=\"line\">\t\tkubeDeps,</span><br><span class=\"line\">\t\tcrOptions,</span><br><span class=\"line\">\t\tcontainerRuntime,</span><br><span class=\"line\">\t\truntimeCgroups,</span><br><span class=\"line\">\t\thostnameOverride,</span><br><span class=\"line\">\t\tnodeIP,</span><br><span class=\"line\">\t\tproviderID,</span><br><span class=\"line\">\t\tcloudProvider,</span><br><span class=\"line\">\t\tcertDirectory,</span><br><span class=\"line\">\t\trootDirectory,</span><br><span class=\"line\">\t\tregisterNode,</span><br><span class=\"line\">\t\tregisterWithTaints,</span><br><span class=\"line\">\t\tallowedUnsafeSysctls,</span><br><span class=\"line\">\t\tremoteRuntimeEndpoint,</span><br><span class=\"line\">\t\tremoteImageEndpoint,</span><br><span class=\"line\">\t\texperimentalMounterPath,</span><br><span class=\"line\">\t\texperimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\texperimentalCheckNodeCapabilitiesBeforeMount,</span><br><span class=\"line\">\t\texperimentalNodeAllocatableIgnoreEvictionThreshold,</span><br><span class=\"line\">\t\tminimumGCAge,</span><br><span class=\"line\">\t\tmaxPerPodContainerCount,</span><br><span class=\"line\">\t\tmaxContainerCount,</span><br><span class=\"line\">\t\tmasterServiceNamespace,</span><br><span class=\"line\">\t\tregisterSchedulable,</span><br><span class=\"line\">\t\tnonMasqueradeCIDR,</span><br><span class=\"line\">\t\tkeepTerminatedPodVolumes,</span><br><span class=\"line\">\t\tnodeLabels,</span><br><span class=\"line\">\t\tseccompProfileRoot,</span><br><span class=\"line\">\t\tbootstrapCheckpointPath,</span><br><span class=\"line\">\t\tnodeStatusMaxImages)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn nil, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 通知 apiserver kubelet 启动了</span><br><span class=\"line\">\tk.BirthCry()</span><br><span class=\"line\">\t// 启动垃圾回收服务</span><br><span class=\"line\">\tk.StartGarbageCollection()</span><br><span class=\"line\"></span><br><span class=\"line\">\treturn k, nil</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...)&#123;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">\tif kubeDeps.PodConfig == nil &#123;</span><br><span class=\"line\">\t\tvar err error</span><br><span class=\"line\">\t\t// 初始化 makePodSourceConfig，监听 pod 元数据的来源(FILE, URL, api-server)，将不同 source 的 pod configuration 合并到一个结构中</span><br><span class=\"line\">\t\tkubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath)</span><br><span class=\"line\">\t\tif err != nil &#123;</span><br><span class=\"line\">\t\t\treturn nil, err</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    // kubelet 服务端口，默认 10250</span><br><span class=\"line\">\tdaemonEndpoints := &amp;v1.NodeDaemonEndpoints&#123;</span><br><span class=\"line\">\t\tKubeletEndpoint: v1.DaemonEndpoint&#123;Port: kubeCfg.Port&#125;,</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用 reflector 把 ListWatch 得到的服务信息实时同步到 serviceStore 对象中</span><br><span class=\"line\">\tserviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;)</span><br><span class=\"line\">\tif kubeDeps.KubeClient != nil &#123;</span><br><span class=\"line\">\t\tserviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;services&quot;, metav1.NamespaceAll, fields.Everything())</span><br><span class=\"line\">\t\tr := cache.NewReflector(serviceLW, &amp;v1.Service&#123;&#125;, serviceIndexer, 0)</span><br><span class=\"line\">\t\tgo r.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tserviceLister := corelisters.NewServiceLister(serviceIndexer)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 使用 reflector 把 ListWatch 得到的节点信息实时同步到  nodeStore 对象中</span><br><span class=\"line\">\tnodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;&#125;)</span><br><span class=\"line\">\tif kubeDeps.KubeClient != nil &#123;</span><br><span class=\"line\">\t\tfieldSelector := fields.Set&#123;api.ObjectNameField: string(nodeName)&#125;.AsSelector()</span><br><span class=\"line\">\t\tnodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;nodes&quot;, metav1.NamespaceAll, fieldSelector)</span><br><span class=\"line\">\t\tr := cache.NewReflector(nodeLW, &amp;v1.Node&#123;&#125;, nodeIndexer, 0)</span><br><span class=\"line\">\t\tgo r.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tnodeInfo := &amp;predicates.CachedNodeInfo&#123;NodeLister: corelisters.NewNodeLister(nodeIndexer)&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t...</span><br><span class=\"line\">\t// node 资源不足时的驱逐策略的设定</span><br><span class=\"line\">\tthresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn nil, err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tevictionConfig := eviction.Config&#123;</span><br><span class=\"line\">\t\tPressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,</span><br><span class=\"line\">\t\tMaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),</span><br><span class=\"line\">\t\tThresholds:               thresholds,</span><br><span class=\"line\">\t\tKernelMemcgNotification:  experimentalKernelMemcgNotification,</span><br><span class=\"line\">\t\tPodCgroupRoot:            kubeDeps.ContainerManager.GetPodCgroupRoot(),</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    // 容器引用的管理</span><br><span class=\"line\">\tcontainerRefManager := kubecontainer.NewRefManager()</span><br><span class=\"line\">    // oom 监控</span><br><span class=\"line\">\toomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 根据配置信息和各种对象创建 Kubelet 实例</span><br><span class=\"line\">\tklet := &amp;Kubelet&#123;</span><br><span class=\"line\">\t\thostname:                       hostname,</span><br><span class=\"line\">\t\thostnameOverridden:             len(hostnameOverride) &gt; 0,</span><br><span class=\"line\">\t\tnodeName:                       nodeName,</span><br><span class=\"line\">\t\t...</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 从 cAdvisor 获取当前机器的信息</span><br><span class=\"line\">\tmachineInfo, err := klet.cadvisor.MachineInfo()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 对 pod 的管理（如: 增删改等）</span><br><span class=\"line\">\tklet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 容器运行时管理</span><br><span class=\"line\">\truntime, err := kuberuntime.NewKubeGenericRuntimeManager(...)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// pleg</span><br><span class=\"line\">\tklet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock&#123;&#125;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 containerGC 对象，进行周期性的容器清理工作</span><br><span class=\"line\">\tcontainerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 创建 imageManager 管理镜像</span><br><span class=\"line\">\timageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// statusManager 实时检测节点上 pod 的状态，并更新到 apiserver 对应的 pod</span><br><span class=\"line\">\tklet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 探针管理</span><br><span class=\"line\">\tklet.probeManager = prober.NewManager(...)</span><br><span class=\"line\"></span><br><span class=\"line\">    // token 管理</span><br><span class=\"line\">\ttokenManager := token.NewManager(kubeDeps.KubeClient)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 磁盘管理</span><br><span class=\"line\">\tklet.volumeManager = volumemanager.NewVolumeManager()</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 将 syncPod() 注入到 podWorkers 中</span><br><span class=\"line\">\tklet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 容器驱逐策略管理</span><br><span class=\"line\">\tevictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock)</span><br><span class=\"line\">    ...</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>RunKubelet 最后会调用 startKubelet() 进行后续的操作。</p>\n<h4 id=\"5、启动-kubelet-内部的模块及服务（cmd-kubelet-app-server-go）\"><a href=\"#5、启动-kubelet-内部的模块及服务（cmd-kubelet-app-server-go）\" class=\"headerlink\" title=\"5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）\"></a>5、启动 kubelet 内部的模块及服务（cmd/kubelet/app/server.go）</h4><p>startKubelet()  的主要功能：</p>\n<ul>\n<li>1、以 goroutine 方式启动 kubelet 中的各个模块。</li>\n<li>2、启动 kubelet http server。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) &#123;</span><br><span class=\"line\">\tgo wait.Until(func() &#123;</span><br><span class=\"line\">\t\t// 以 goroutine 方式启动 kubelet 中的各个模块</span><br><span class=\"line\">\t\tk.Run(podCfg.Updates())</span><br><span class=\"line\">\t&#125;, 0, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 启动 kubelet http server\t</span><br><span class=\"line\">\tif enableServer &#123;</span><br><span class=\"line\">\t\tgo k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling)</span><br><span class=\"line\"></span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif kubeCfg.ReadOnlyPort &gt; 0 &#123;</span><br><span class=\"line\">\t\tgo k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// Run starts the kubelet reacting to config updates</span><br><span class=\"line\">func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123;</span><br><span class=\"line\">\tif kl.logServer == nil &#123;</span><br><span class=\"line\">\t\tkl.logServer = http.StripPrefix(&quot;/logs/&quot;, http.FileServer(http.Dir(&quot;/var/log/&quot;)))</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tif kl.kubeClient == nil &#123;</span><br><span class=\"line\">\t\tglog.Warning(&quot;No api server defined - no node status update will be sent.&quot;)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start the cloud provider sync manager</span><br><span class=\"line\">\tif kl.cloudResourceSyncManager != nil &#123;</span><br><span class=\"line\">\t\tgo kl.cloudResourceSyncManager.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tif err := kl.initializeModules(); err != nil &#123;</span><br><span class=\"line\">\t\tkl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())</span><br><span class=\"line\">\t\tglog.Fatal(err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start volume manager</span><br><span class=\"line\">\tgo kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\tif kl.kubeClient != nil &#123;</span><br><span class=\"line\">\t\t// Start syncing node status immediately, this may set up things the runtime needs to run.</span><br><span class=\"line\">\t\tgo wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)</span><br><span class=\"line\">\t\tgo kl.fastStatusUpdateOnce()</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t// start syncing lease</span><br><span class=\"line\">\t\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123;</span><br><span class=\"line\">\t\t\tgo kl.nodeLeaseController.Run(wait.NeverStop)</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tgo wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start loop to sync iptables util rules</span><br><span class=\"line\">\tif kl.makeIPTablesUtilChains &#123;</span><br><span class=\"line\">\t\tgo wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start a goroutine responsible for killing pods (that are not properly</span><br><span class=\"line\">\t// handled by pod workers).</span><br><span class=\"line\">\tgo wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start component sync loops.</span><br><span class=\"line\">\tkl.statusManager.Start()</span><br><span class=\"line\">\tkl.probeManager.Start()</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start syncing RuntimeClasses if enabled.</span><br><span class=\"line\">\tif kl.runtimeClassManager != nil &#123;</span><br><span class=\"line\">\t\tgo kl.runtimeClassManager.Run(wait.NeverStop)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// Start the pod lifecycle event generator.</span><br><span class=\"line\">\tkl.pleg.Start()</span><br><span class=\"line\"></span><br><span class=\"line\">\tkl.syncLoop(updates, kl)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncLoop 是 kubelet 的主循环方法，它从不同的管道(FILE,URL, API-SERVER)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 Pod 处于期望的状态。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123;</span><br><span class=\"line\">\tglog.Info(&quot;Starting kubelet main sync loop.&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">\t// syncTicker 每秒检测一次是否有需要同步的 pod workers</span><br><span class=\"line\">\tsyncTicker := time.NewTicker(time.Second)</span><br><span class=\"line\">\tdefer syncTicker.Stop()</span><br><span class=\"line\">\thousekeepingTicker := time.NewTicker(housekeepingPeriod)</span><br><span class=\"line\">\tdefer housekeepingTicker.Stop()</span><br><span class=\"line\">\tplegCh := kl.pleg.Watch()</span><br><span class=\"line\">\tconst (</span><br><span class=\"line\">\t\tbase   = 100 * time.Millisecond</span><br><span class=\"line\">\t\tmax    = 5 * time.Second</span><br><span class=\"line\">\t\tfactor = 2</span><br><span class=\"line\">\t)</span><br><span class=\"line\">\tduration := base</span><br><span class=\"line\">\tfor &#123;</span><br><span class=\"line\">\t\tif rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123;</span><br><span class=\"line\">\t\t\tglog.Infof(&quot;skipping pod synchronization - %v&quot;, rs)</span><br><span class=\"line\">\t\t\t// exponential backoff</span><br><span class=\"line\">\t\t\ttime.Sleep(duration)</span><br><span class=\"line\">\t\t\tduration = time.Duration(math.Min(float64(max), factor*float64(duration)))</span><br><span class=\"line\">\t\t\tcontinue</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t// reset backoff if we have a success</span><br><span class=\"line\">\t\tduration = base</span><br><span class=\"line\"></span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t\t// </span><br><span class=\"line\">\t\tif !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123;</span><br><span class=\"line\">\t\t\tbreak</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tkl.syncLoopMonitor.Store(kl.clock.Now())</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncLoopIteration()  方法对多个管道进行遍历，如果 pod 发生变化，则会调用相应的 Handler，在 Handler 中通过调用 dispatchWork 分发任务。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本篇文章主要讲述了 kubelet 组件从加载配置到初始化内部的各个模块再到启动 kubelet 服务的整个流程，上面的时序图能清楚的看到函数之间的调用关系，但是其中每个组件具体的工作方式以及组件之间的交互方式还不得而知，后面会一探究竟。</p>\n<p>参考：<br><a href=\"http://www.sel.zju.edu.cn/?p=595\" target=\"_blank\" rel=\"noopener\">kubernetes node components – kubelet</a><br><a href=\"https://segmentfault.com/a/1190000008267351\" target=\"_blank\" rel=\"noopener\">Kubelet 源码分析(一):启动流程分析</a><br><a href=\"https://cizixs.com/2017/06/06/kubelet-source-code-analysis-part-1/\" target=\"_blank\" rel=\"noopener\">kubelet 源码分析：启动流程</a><br><a href=\"https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2017/05/02/Kubernetes-kubelet.html\" target=\"_blank\" rel=\"noopener\">kubernetes 的 kubelet 的工作过程</a><br><a href=\"https://fatsheep9146.github.io/2018/07/08/kubelet%E5%86%85%E9%83%A8%E5%AE%9E%E7%8E%B0%E8%A7%A3%E6%9E%90/\" target=\"_blank\" rel=\"noopener\">kubelet 内部实现解析</a></p>\n"},{"title":"kubelet 架构浅析","date":"2018-12-16T09:35:30.000Z","type":"kubelet","_content":"\n## 一、概要\nkubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。\n\nkubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。\n\n## 二、kubelet 的主要功能\n\n1、kubelet  默认监听四个端口，分别为 10250 、10255、10248、4194。\n\n```\nLISTEN     0      128          *:10250                    *:*                   users:((\"kubelet\",pid=48500,fd=28))\nLISTEN     0      128          *:10255                    *:*                   users:((\"kubelet\",pid=48500,fd=26))\nLISTEN     0      128          *:4194                     *:*                   users:((\"kubelet\",pid=48500,fd=13))\nLISTEN     0      128    127.0.0.1:10248                    *:*                   users:((\"kubelet\",pid=48500,fd=23))\n```\n\n- 10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。\n\n- 10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 `--healthz-port` 和 `--healthz-bind-address` 来指定监听的地址和端口。\n```\n  $ curl http://127.0.0.1:10248/healthz\n  ok\n```\n- 4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 http://localhost:4194 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 `--cadvisor-port` 可以指定启动的端口。\n\n```\n  $ curl  http://127.0.0.1:4194/metrics\n```\n\n- 10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。\n``` \n  //  获取 pod 的接口，与 apiserver 的 \n  // http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName=  接口类似\n  $ curl  http://127.0.0.1:10255/pods\n\n  // 节点信息接口,提供磁盘、网络、CPU、内存等信息\n  $ curl http://127.0.0.1:10255/spec/\n```\n\n2、kubelet 主要功能：\n\n- pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。\n\n- 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。\n\n- 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。\n\n\n## 三、kubelet 组件中的模块\n\n ![kubelet 组件中的模块](http://cdn.tianfeiyu.com/kubelet-4.png)\n\n上图展示了 kubelet 组件中的模块以及模块间的划分。\n\n- 1、PLEG(Pod Lifecycle Event Generator）\nPLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。\n\n- 2、cAdvisor \ncAdvisor（https://github.com/google/cadvisor）是 google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。\n\n- 3、OOMWatcher \n系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。\n\n- 4、probeManager \nprobeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。\nlivenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。\nreadinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 \n\n- 5、statusManager \nstatusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。\n\n- 6、containerRefManager\n容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。\n\n- 7、evictionManager \n当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 `--eviction-hard=` 来决定 evict 的策略值。\n\n- 8、imageGC \nimageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 `--image-gc-high-threshold` 和 `--image-gc-low-threshold` 来设置。\n\n- 9、containerGC \ncontainerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。\n\n- 10、imageManager \n调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。\n\n- 11、volumeManager \n负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。\n\n- 12、containerManager \n负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 `--cgroups-per-qos` 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 `true`，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。\n\n- 13、runtimeManager \ncontainerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 `--container-runtime` 来定义是使用docker 还是 rkt，默认是 `docker`。\n\n- 14、podManager \npodManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 \n\n\n在 v1.12 中，kubelet 组件有18个 manager：\n\n```\ncertificateManager\ncgroupManager\ncontainerManager\ncpuManager\nnodeContainerManager\nconfigmapManager\ncontainerReferenceManager\nevictionManager\nnvidiaGpuManager\nimageGCManager\nkuberuntimeManager\nhostportManager\npodManager\nproberManager\nsecretManager\nstatusManager\nvolumeManager\t\ntokenManager\n```\n\n其中比较重要的模块后面会进行一一分析。\n\n\n参考：\n[微软资深工程师详解 K8S 容器运行时](https://juejin.im/entry/5bc71b3d6fb9a05cf23029b8)\n[kubernetes 简介： kubelet 和 pod](https://cizixs.com/2016/10/25/kubernetes-intro-kubelet/)\n[Kubelet 组件解析](https://blog.csdn.net/jettery/article/details/78891733)\n\n\n\n","source":"_posts/kubelet-modules.md","raw":"---\ntitle: kubelet 架构浅析\ndate: 2018-12-16 17:35:30\ntags: \"kubelet\"\ntype: \"kubelet\"\n\n---\n\n## 一、概要\nkubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。\n\nkubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。\n\n## 二、kubelet 的主要功能\n\n1、kubelet  默认监听四个端口，分别为 10250 、10255、10248、4194。\n\n```\nLISTEN     0      128          *:10250                    *:*                   users:((\"kubelet\",pid=48500,fd=28))\nLISTEN     0      128          *:10255                    *:*                   users:((\"kubelet\",pid=48500,fd=26))\nLISTEN     0      128          *:4194                     *:*                   users:((\"kubelet\",pid=48500,fd=13))\nLISTEN     0      128    127.0.0.1:10248                    *:*                   users:((\"kubelet\",pid=48500,fd=23))\n```\n\n- 10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。\n\n- 10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 `--healthz-port` 和 `--healthz-bind-address` 来指定监听的地址和端口。\n```\n  $ curl http://127.0.0.1:10248/healthz\n  ok\n```\n- 4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 http://localhost:4194 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 `--cadvisor-port` 可以指定启动的端口。\n\n```\n  $ curl  http://127.0.0.1:4194/metrics\n```\n\n- 10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。\n``` \n  //  获取 pod 的接口，与 apiserver 的 \n  // http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName=  接口类似\n  $ curl  http://127.0.0.1:10255/pods\n\n  // 节点信息接口,提供磁盘、网络、CPU、内存等信息\n  $ curl http://127.0.0.1:10255/spec/\n```\n\n2、kubelet 主要功能：\n\n- pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。\n\n- 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。\n\n- 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。\n\n\n## 三、kubelet 组件中的模块\n\n ![kubelet 组件中的模块](http://cdn.tianfeiyu.com/kubelet-4.png)\n\n上图展示了 kubelet 组件中的模块以及模块间的划分。\n\n- 1、PLEG(Pod Lifecycle Event Generator）\nPLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。\n\n- 2、cAdvisor \ncAdvisor（https://github.com/google/cadvisor）是 google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。\n\n- 3、OOMWatcher \n系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。\n\n- 4、probeManager \nprobeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。\nlivenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。\nreadinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 \n\n- 5、statusManager \nstatusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。\n\n- 6、containerRefManager\n容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。\n\n- 7、evictionManager \n当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 `--eviction-hard=` 来决定 evict 的策略值。\n\n- 8、imageGC \nimageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 `--image-gc-high-threshold` 和 `--image-gc-low-threshold` 来设置。\n\n- 9、containerGC \ncontainerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。\n\n- 10、imageManager \n调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。\n\n- 11、volumeManager \n负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。\n\n- 12、containerManager \n负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 `--cgroups-per-qos` 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 `true`，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。\n\n- 13、runtimeManager \ncontainerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 `--container-runtime` 来定义是使用docker 还是 rkt，默认是 `docker`。\n\n- 14、podManager \npodManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 \n\n\n在 v1.12 中，kubelet 组件有18个 manager：\n\n```\ncertificateManager\ncgroupManager\ncontainerManager\ncpuManager\nnodeContainerManager\nconfigmapManager\ncontainerReferenceManager\nevictionManager\nnvidiaGpuManager\nimageGCManager\nkuberuntimeManager\nhostportManager\npodManager\nproberManager\nsecretManager\nstatusManager\nvolumeManager\t\ntokenManager\n```\n\n其中比较重要的模块后面会进行一一分析。\n\n\n参考：\n[微软资深工程师详解 K8S 容器运行时](https://juejin.im/entry/5bc71b3d6fb9a05cf23029b8)\n[kubernetes 简介： kubelet 和 pod](https://cizixs.com/2016/10/25/kubernetes-intro-kubelet/)\n[Kubelet 组件解析](https://blog.csdn.net/jettery/article/details/78891733)\n\n\n\n","slug":"kubelet-modules","published":1,"updated":"2019-07-21T09:49:06.807Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59t001gapwn64qwq40a","content":"<h2 id=\"一、概要\"><a href=\"#一、概要\" class=\"headerlink\" title=\"一、概要\"></a>一、概要</h2><p>kubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。</p>\n<p>kubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。</p>\n<h2 id=\"二、kubelet-的主要功能\"><a href=\"#二、kubelet-的主要功能\" class=\"headerlink\" title=\"二、kubelet 的主要功能\"></a>二、kubelet 的主要功能</h2><p>1、kubelet  默认监听四个端口，分别为 10250 、10255、10248、4194。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LISTEN     0      128          *:10250                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=28))</span><br><span class=\"line\">LISTEN     0      128          *:10255                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=26))</span><br><span class=\"line\">LISTEN     0      128          *:4194                     *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=13))</span><br><span class=\"line\">LISTEN     0      128    127.0.0.1:10248                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=23))</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。</p>\n</li>\n<li><p>10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 <code>--healthz-port</code> 和 <code>--healthz-bind-address</code> 来指定监听的地址和端口。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://127.0.0.1:10248/healthz</span><br><span class=\"line\">ok</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 <a href=\"http://localhost:4194\" target=\"_blank\" rel=\"noopener\">http://localhost:4194</a> 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 <code>--cadvisor-port</code> 可以指定启动的端口。</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl  http://127.0.0.1:4194/metrics</span><br></pre></td></tr></table></figure>\n<ul>\n<li>10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//  获取 pod 的接口，与 apiserver 的 </span><br><span class=\"line\">// http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName=  接口类似</span><br><span class=\"line\">$ curl  http://127.0.0.1:10255/pods</span><br><span class=\"line\"></span><br><span class=\"line\">// 节点信息接口,提供磁盘、网络、CPU、内存等信息</span><br><span class=\"line\">$ curl http://127.0.0.1:10255/spec/</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>2、kubelet 主要功能：</p>\n<ul>\n<li><p>pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。</p>\n</li>\n<li><p>容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。</p>\n</li>\n<li><p>容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。</p>\n</li>\n</ul>\n<h2 id=\"三、kubelet-组件中的模块\"><a href=\"#三、kubelet-组件中的模块\" class=\"headerlink\" title=\"三、kubelet 组件中的模块\"></a>三、kubelet 组件中的模块</h2><p> <img src=\"http://cdn.tianfeiyu.com/kubelet-4.png\" alt=\"kubelet 组件中的模块\"></p>\n<p>上图展示了 kubelet 组件中的模块以及模块间的划分。</p>\n<ul>\n<li><p>1、PLEG(Pod Lifecycle Event Generator）<br>PLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。</p>\n</li>\n<li><p>2、cAdvisor<br>cAdvisor（<a href=\"https://github.com/google/cadvisor）是\" target=\"_blank\" rel=\"noopener\">https://github.com/google/cadvisor）是</a> google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。</p>\n</li>\n<li><p>3、OOMWatcher<br>系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。</p>\n</li>\n<li><p>4、probeManager<br>probeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。<br>livenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。<br>readinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 </p>\n</li>\n<li><p>5、statusManager<br>statusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。</p>\n</li>\n<li><p>6、containerRefManager<br>容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。</p>\n</li>\n<li><p>7、evictionManager<br>当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 <code>--eviction-hard=</code> 来决定 evict 的策略值。</p>\n</li>\n<li><p>8、imageGC<br>imageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 <code>--image-gc-high-threshold</code> 和 <code>--image-gc-low-threshold</code> 来设置。</p>\n</li>\n<li><p>9、containerGC<br>containerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。</p>\n</li>\n<li><p>10、imageManager<br>调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。</p>\n</li>\n<li><p>11、volumeManager<br>负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。</p>\n</li>\n<li><p>12、containerManager<br>负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 <code>--cgroups-per-qos</code> 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 <code>true</code>，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。</p>\n</li>\n<li><p>13、runtimeManager<br>containerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 <code>--container-runtime</code> 来定义是使用docker 还是 rkt，默认是 <code>docker</code>。</p>\n</li>\n<li><p>14、podManager<br>podManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 </p>\n</li>\n</ul>\n<p>在 v1.12 中，kubelet 组件有18个 manager：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">certificateManager</span><br><span class=\"line\">cgroupManager</span><br><span class=\"line\">containerManager</span><br><span class=\"line\">cpuManager</span><br><span class=\"line\">nodeContainerManager</span><br><span class=\"line\">configmapManager</span><br><span class=\"line\">containerReferenceManager</span><br><span class=\"line\">evictionManager</span><br><span class=\"line\">nvidiaGpuManager</span><br><span class=\"line\">imageGCManager</span><br><span class=\"line\">kuberuntimeManager</span><br><span class=\"line\">hostportManager</span><br><span class=\"line\">podManager</span><br><span class=\"line\">proberManager</span><br><span class=\"line\">secretManager</span><br><span class=\"line\">statusManager</span><br><span class=\"line\">volumeManager\t</span><br><span class=\"line\">tokenManager</span><br></pre></td></tr></table></figure>\n<p>其中比较重要的模块后面会进行一一分析。</p>\n<p>参考：<br><a href=\"https://juejin.im/entry/5bc71b3d6fb9a05cf23029b8\" target=\"_blank\" rel=\"noopener\">微软资深工程师详解 K8S 容器运行时</a><br><a href=\"https://cizixs.com/2016/10/25/kubernetes-intro-kubelet/\" target=\"_blank\" rel=\"noopener\">kubernetes 简介： kubelet 和 pod</a><br><a href=\"https://blog.csdn.net/jettery/article/details/78891733\" target=\"_blank\" rel=\"noopener\">Kubelet 组件解析</a></p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/06/09/node_status/\">kubelet 状态上报的方式</a></li><li><a href=\"http://yoursite.com/2019/02/26/k8s_events/\">kubernets 中事件处理机制</a></li><li><a href=\"http://yoursite.com/2019/01/03/kubelet_create_pod/\">kubelet 创建 pod 的流程</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<h2 id=\"一、概要\"><a href=\"#一、概要\" class=\"headerlink\" title=\"一、概要\"></a>一、概要</h2><p>kubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。</p>\n<p>kubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。</p>\n<h2 id=\"二、kubelet-的主要功能\"><a href=\"#二、kubelet-的主要功能\" class=\"headerlink\" title=\"二、kubelet 的主要功能\"></a>二、kubelet 的主要功能</h2><p>1、kubelet  默认监听四个端口，分别为 10250 、10255、10248、4194。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">LISTEN     0      128          *:10250                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=28))</span><br><span class=\"line\">LISTEN     0      128          *:10255                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=26))</span><br><span class=\"line\">LISTEN     0      128          *:4194                     *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=13))</span><br><span class=\"line\">LISTEN     0      128    127.0.0.1:10248                    *:*                   users:((&quot;kubelet&quot;,pid=48500,fd=23))</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。</p>\n</li>\n<li><p>10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 <code>--healthz-port</code> 和 <code>--healthz-bind-address</code> 来指定监听的地址和端口。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl http://127.0.0.1:10248/healthz</span><br><span class=\"line\">ok</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 <a href=\"http://localhost:4194\" target=\"_blank\" rel=\"noopener\">http://localhost:4194</a> 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 <code>--cadvisor-port</code> 可以指定启动的端口。</p>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ curl  http://127.0.0.1:4194/metrics</span><br></pre></td></tr></table></figure>\n<ul>\n<li>10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//  获取 pod 的接口，与 apiserver 的 </span><br><span class=\"line\">// http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName=  接口类似</span><br><span class=\"line\">$ curl  http://127.0.0.1:10255/pods</span><br><span class=\"line\"></span><br><span class=\"line\">// 节点信息接口,提供磁盘、网络、CPU、内存等信息</span><br><span class=\"line\">$ curl http://127.0.0.1:10255/spec/</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>2、kubelet 主要功能：</p>\n<ul>\n<li><p>pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。</p>\n</li>\n<li><p>容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。</p>\n</li>\n<li><p>容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。</p>\n</li>\n</ul>\n<h2 id=\"三、kubelet-组件中的模块\"><a href=\"#三、kubelet-组件中的模块\" class=\"headerlink\" title=\"三、kubelet 组件中的模块\"></a>三、kubelet 组件中的模块</h2><p> <img src=\"http://cdn.tianfeiyu.com/kubelet-4.png\" alt=\"kubelet 组件中的模块\"></p>\n<p>上图展示了 kubelet 组件中的模块以及模块间的划分。</p>\n<ul>\n<li><p>1、PLEG(Pod Lifecycle Event Generator）<br>PLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。</p>\n</li>\n<li><p>2、cAdvisor<br>cAdvisor（<a href=\"https://github.com/google/cadvisor）是\" target=\"_blank\" rel=\"noopener\">https://github.com/google/cadvisor）是</a> google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。</p>\n</li>\n<li><p>3、OOMWatcher<br>系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。</p>\n</li>\n<li><p>4、probeManager<br>probeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。<br>livenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。<br>readinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 </p>\n</li>\n<li><p>5、statusManager<br>statusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。</p>\n</li>\n<li><p>6、containerRefManager<br>容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。</p>\n</li>\n<li><p>7、evictionManager<br>当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 <code>--eviction-hard=</code> 来决定 evict 的策略值。</p>\n</li>\n<li><p>8、imageGC<br>imageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 <code>--image-gc-high-threshold</code> 和 <code>--image-gc-low-threshold</code> 来设置。</p>\n</li>\n<li><p>9、containerGC<br>containerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。</p>\n</li>\n<li><p>10、imageManager<br>调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。</p>\n</li>\n<li><p>11、volumeManager<br>负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。</p>\n</li>\n<li><p>12、containerManager<br>负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 <code>--cgroups-per-qos</code> 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 <code>true</code>，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。</p>\n</li>\n<li><p>13、runtimeManager<br>containerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 <code>--container-runtime</code> 来定义是使用docker 还是 rkt，默认是 <code>docker</code>。</p>\n</li>\n<li><p>14、podManager<br>podManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 </p>\n</li>\n</ul>\n<p>在 v1.12 中，kubelet 组件有18个 manager：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">certificateManager</span><br><span class=\"line\">cgroupManager</span><br><span class=\"line\">containerManager</span><br><span class=\"line\">cpuManager</span><br><span class=\"line\">nodeContainerManager</span><br><span class=\"line\">configmapManager</span><br><span class=\"line\">containerReferenceManager</span><br><span class=\"line\">evictionManager</span><br><span class=\"line\">nvidiaGpuManager</span><br><span class=\"line\">imageGCManager</span><br><span class=\"line\">kuberuntimeManager</span><br><span class=\"line\">hostportManager</span><br><span class=\"line\">podManager</span><br><span class=\"line\">proberManager</span><br><span class=\"line\">secretManager</span><br><span class=\"line\">statusManager</span><br><span class=\"line\">volumeManager\t</span><br><span class=\"line\">tokenManager</span><br></pre></td></tr></table></figure>\n<p>其中比较重要的模块后面会进行一一分析。</p>\n<p>参考：<br><a href=\"https://juejin.im/entry/5bc71b3d6fb9a05cf23029b8\" target=\"_blank\" rel=\"noopener\">微软资深工程师详解 K8S 容器运行时</a><br><a href=\"https://cizixs.com/2016/10/25/kubernetes-intro-kubelet/\" target=\"_blank\" rel=\"noopener\">kubernetes 简介： kubelet 和 pod</a><br><a href=\"https://blog.csdn.net/jettery/article/details/78891733\" target=\"_blank\" rel=\"noopener\">Kubelet 组件解析</a></p>\n"},{"title":"kubernetes 学习笔记","date":"2017-02-12T14:58:00.000Z","type":"kubernetes","_content":"\n1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 **kubernetes + docker** 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 `API` 进行某些操作。\n\n\n## 1，kubernetes 搭建\n\n[kubernetes](https://github.com/kubernetes/kubernetes) 是 google 的一个开源软件，其社区活跃量远超 **Mesos，Coreos** 的，若想深入学习建议参考**《kubernetes 权威指南》**，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。\n\n\n![kubernetes 架构图](http://cdn.tianfeiyu.com/bank.png)\n\n\nkubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 **gitbook** 上面某大神的一本书 一 [etcd3学习笔记](https://skyao.gitbooks.io/leaning-etcd3/content/documentation/leaning/)。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。\n\n> 注意：内核版本 > 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 \n\netcd 节点：\n\t\n\t# yum install -y etcd \n\t# systemctl start etcd  \n\nmaster 节点：\n\n\t# yum install -y kubernetes-master\n\t# systemctl start kube-apiserver \n\t# systemctl start kube-controller-manager \n\t# systemctl start kube-scheduler \n\nminion 节点：\n\n\t# yum install -y kubernetes  docker\n\t# systemctl start kubelet \n\t# systemctl start kube-proxy \n\t# systemctl start docker \n\t\n\n## 2，kubernetes 版本升级\n\n以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。\n\n下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 [kubernetes-server-linux-amd64.tar.gz](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#downloads-for-v160-alpha1)  这个包，下载你所要更新到的版本。\n\n**升级步骤**：\n\n- 1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上\n- 2，备份二进制程序（kubectl,kube-proxy）\n- 3，将解压后的二进制程序覆盖以前的版本\n- 4，最后重新启动服务\n\n\t\n\t# systemctl stop docker\n\t# which kubectl kube-proxy \n\t/usr/bin/kubectl\n\t/usr/bin/kube-proxy\n\n\t# cp /usr/bin/{kubectl,kube-proxy} /tmp/\n\t# yes | cp bin/{kubectl,kube-proxy} /usr/bin/\n\t\n\t# systemctl status {kubectl,kube-proxy}\n\n\t# systemctl start docker \n\n\n## 3，kubeconfig 使用\n\n若你使用的 kubelet 版本为 1.4，使用 `systemctl status kubelet`  会看到这样一句话：\n\n\t--api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml\n\n使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 [官方文档](http://kubernetes.io/docs/user-guide/kubeconfig-file/) 有格式说明， 本人已将其翻译，翻译文档见下文。\n\n**kubeconfig** 文件示例：\n\n\t\tapiVersion: v1\n\t\tclusters:\n\t\t- cluster:\n\t\t    server: http://localhost:8080\n\t\t  name: local-server\n\t\tcontexts:\n\t\t- context:\n\t\t    cluster: local-server\n\t\t    namespace: the-right-prefix\n\t\t    user: myself\n\t\t  name: default-context\n\t\tcurrent-context: default-context\n\t\tkind: Config\n\t\tpreferences: {}\n\t\tusers:\n\t\t- name: myself\n\t\t  user:\n\t\t    password: secret\n\t\t    username: admin\n\n---\n\n\t   # kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true\n\n\nkubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。\n\nrequire-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。\n\n关于 kubeconfig 的一个 **issue**，[Kubelet won't read apiserver from kubeconfig](https://github.com/kubernetes/kubernetes/issues/36745)。\n\n**升级步骤**，当然前提是你的 kubelet 版本已经到了 1.5：\n\n* 1，关闭 kubelet、kube-proxy 服务；\n* 2，注释掉 `/etc/kubernetes/kubelet` 文件中下面这一行:\n\n    `KUBELET_API_SERVER=\"--api-servers=http://127.0.0.1:8080\"`\n\n然后在 **KUBELET_ARGS** 中添加： \n\n\t--kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true\n\n这里的路径是你 yaml 文件放置的路径。 \n\n- 3，重新启动刚关掉的两个服务\n\n---\n## 4，以下为 [kubeconfig 配置官方文档](https://kubernetes.io/docs/user-guide/kubeconfig-file/)的翻译\n\n### kubernetes 中的验证对于不同的群体可以使用不同的方法.\n\n* 运行 kubelet 可能有的一种认证方式（即证书）。\n* 用户可能有不同的认证方式（即 token）。\n* 管理员可以为每个用户提供一个证书列表。\n* 可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 \n\n因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。\n\n此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。\n\n如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。\n\n### 相关讨论\n\n\thttp://issue.k8s.io/1755\n\n### kubeconfig 文件的组件 \n\nkubeconfig 文件示例：\n\n\tcurrent-context: federal-context\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    api-version: v1\n\t    server: http://cow.org:8080\n\t  name: cow-cluster\n\t- cluster:\n\t    certificate-authority: path/to/my/cafile\n\t    server: https://horse.org:4443\n\t  name: horse-cluster\n\t- cluster:\n\t    insecure-skip-tls-verify: true\n\t    server: https://pig.org:443\n\t  name: pig-cluster\n\tcontexts:\n\t- context:\n\t    cluster: horse-cluster\n\t    namespace: chisel-ns\n\t    user: green-user\n\t  name: federal-context\n\t- context:\n\t    cluster: pig-cluster\n\t    namespace: saw-ns\n\t    user: black-user\n\t  name: queen-anne-context\n\tkind: Config\n\tpreferences:\n\t  colors: true\n\tusers:\n\t- name: blue-user\n\t  user:\n\t    token: blue-token\n\t- name: green-user\n\t  user:\n\t    client-certificate: path/to/my/client/cert\n\t    client-key: path/to/my/client/key\n\n### 组件的解释\n\n#### cluster\n\n\tclusters:\n\t- cluster:\n\t    certificate-authority: path/to/my/cafile\n\t    server: https://horse.org:4443\n\t  name: horse-cluster\n\t- cluster:\n\t    insecure-skip-tls-verify: true\n\t    server: https://pig.org:443\n\t  name: pig-cluster\n\n\ncluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。\n\n#### user\n\n\tusers:\n\t- name: blue-user\n\t  user:\n\t    token: blue-token\n\t- name: green-user\n\t  user:\n\t    client-certificate: path/to/my/client/cert\n\t    client-key: path/to/my/client/key\n\n用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。\n\n### context\n\n\tcontexts:\n\t- context:\n\t    cluster: horse-cluster\n\t    namespace: chisel-ns\n\t    user: green-user\n\t  name: federal-context\n\ncontext 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。\n三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。\n\n#### current-context\n\n\tcurrent-context: federal-context\n\ncurrent-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 --context=CONTEXT, --cluster=CLUSTER, --user=USER, and/or --namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。\n\n#### 杂项\n\n\tapiVersion: v1\n\tkind: Config\n\tpreferences:\n\t  colors: true\n\napiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。\npreferences 指定选项(和当前未使用的) kubectl preferences.\n\n### 查看 kubeconfig 文件\n\nkubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 --minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。\n\n### 创建你的 kubeconfig 文件\n\n注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。\n\n在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。\n\n因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改...\n\n以上文件可能对应于使用--token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样：\n\t\n\tblue-user,blue-user,1\n\tmister-red,mister-red,2\n\n此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。\n\n* 由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。\n* 类似地，我们也可以选择改变 current-context 的值为 “blue-user”。\n* \n在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过\n\n\n### 加载和合并规则\n\n加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建：\n\n1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：\n如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。\n\n否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。\n\n对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。\n\n2，此链中第一个被匹配的 context 将被使用：\n\n* 1，命令行参数 - 命令行选项中 context 的值\n* 2，合并文件中的 current-context\n* 3，此段允许为空\n\n3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）：\n\n* 1，命令行参数 - user 是用户名，cluster 是集群名\n* 2，如果存在 context 则使用\n* 3，允许为空\n\n4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）：\n\n* 1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify\n* 2，如果存在集群信息并且该属性的值存在，则使用它。\n* 3，如果没有 server 位置则出错。\n\n5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法：\n\n* 1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段\n* 2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。\n* 3，如果两者有冲突则失败\n\n6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。\n\n7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。\n\nkubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。\n\n\n### 通过 kubectl config <subcommand> 操作 kubeconfig\n\n为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。\n\n例如：\n\n\t$ kubectl config set-credentials myself --username=admin --password=secret\n\t$ kubectl config set-cluster local-server --server=http://localhost:8080\n\t$ kubectl config set-context default-context --cluster=local-server --user=myself\n\t$ kubectl config use-context default-context\n\t$ kubectl config set contexts.default-context.namespace the-right-prefix\n\t$ kubectl config view\n\n输出：\n\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    server: http://localhost:8080\n\t  name: local-server\n\tcontexts:\n\t- context:\n\t    cluster: local-server\n\t    namespace: the-right-prefix\n\t    user: myself\n\t  name: default-context\n\tcurrent-context: default-context\n\tkind: Config\n\tpreferences: {}\n\tusers:\n\t- name: myself\n\t  user:\n\t    password: secret\n\t    username: admin\n\n一个 kubeconfig 文件类似这样：\n\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    server: http://localhost:8080\n\t  name: local-server\n\tcontexts:\n\t- context:\n\t    cluster: local-server\n\t    namespace: the-right-prefix\n\t    user: myself\n\t  name: default-context\n\tcurrent-context: default-context\n\tkind: Config\n\tpreferences: {}\n\tusers:\n\t- name: myself\n\t  user:\n\t    password: secret\n\t    username: admin\n\n示例文件的命令操作：\n\n\t$ kubectl config set preferences.colors true\n\t$ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1\n\t$ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile\n\t$ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true\n\t$ kubectl config set-credentials blue-user --token=blue-token\n\t$ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key\n\t$ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns\n\t$ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns\n\t$ kubectl config use-context federal-context\n\n最后的总结：\n\n所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了：\n\n* 仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证\n* 将上面的代码段替换为你集群的 api-server endpoint 的信息。\n* 确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。\n","source":"_posts/kubernetes-learn.md","raw":"---\ntitle: kubernetes 学习笔记\ndate: 2017-02-12 22:58:00\ntype: \"kubernetes\"\n\n---\n\n1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 **kubernetes + docker** 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 `API` 进行某些操作。\n\n\n## 1，kubernetes 搭建\n\n[kubernetes](https://github.com/kubernetes/kubernetes) 是 google 的一个开源软件，其社区活跃量远超 **Mesos，Coreos** 的，若想深入学习建议参考**《kubernetes 权威指南》**，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。\n\n\n![kubernetes 架构图](http://cdn.tianfeiyu.com/bank.png)\n\n\nkubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 **gitbook** 上面某大神的一本书 一 [etcd3学习笔记](https://skyao.gitbooks.io/leaning-etcd3/content/documentation/leaning/)。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。\n\n> 注意：内核版本 > 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 \n\netcd 节点：\n\t\n\t# yum install -y etcd \n\t# systemctl start etcd  \n\nmaster 节点：\n\n\t# yum install -y kubernetes-master\n\t# systemctl start kube-apiserver \n\t# systemctl start kube-controller-manager \n\t# systemctl start kube-scheduler \n\nminion 节点：\n\n\t# yum install -y kubernetes  docker\n\t# systemctl start kubelet \n\t# systemctl start kube-proxy \n\t# systemctl start docker \n\t\n\n## 2，kubernetes 版本升级\n\n以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。\n\n下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 [kubernetes-server-linux-amd64.tar.gz](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#downloads-for-v160-alpha1)  这个包，下载你所要更新到的版本。\n\n**升级步骤**：\n\n- 1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上\n- 2，备份二进制程序（kubectl,kube-proxy）\n- 3，将解压后的二进制程序覆盖以前的版本\n- 4，最后重新启动服务\n\n\t\n\t# systemctl stop docker\n\t# which kubectl kube-proxy \n\t/usr/bin/kubectl\n\t/usr/bin/kube-proxy\n\n\t# cp /usr/bin/{kubectl,kube-proxy} /tmp/\n\t# yes | cp bin/{kubectl,kube-proxy} /usr/bin/\n\t\n\t# systemctl status {kubectl,kube-proxy}\n\n\t# systemctl start docker \n\n\n## 3，kubeconfig 使用\n\n若你使用的 kubelet 版本为 1.4，使用 `systemctl status kubelet`  会看到这样一句话：\n\n\t--api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml\n\n使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 [官方文档](http://kubernetes.io/docs/user-guide/kubeconfig-file/) 有格式说明， 本人已将其翻译，翻译文档见下文。\n\n**kubeconfig** 文件示例：\n\n\t\tapiVersion: v1\n\t\tclusters:\n\t\t- cluster:\n\t\t    server: http://localhost:8080\n\t\t  name: local-server\n\t\tcontexts:\n\t\t- context:\n\t\t    cluster: local-server\n\t\t    namespace: the-right-prefix\n\t\t    user: myself\n\t\t  name: default-context\n\t\tcurrent-context: default-context\n\t\tkind: Config\n\t\tpreferences: {}\n\t\tusers:\n\t\t- name: myself\n\t\t  user:\n\t\t    password: secret\n\t\t    username: admin\n\n---\n\n\t   # kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true\n\n\nkubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。\n\nrequire-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。\n\n关于 kubeconfig 的一个 **issue**，[Kubelet won't read apiserver from kubeconfig](https://github.com/kubernetes/kubernetes/issues/36745)。\n\n**升级步骤**，当然前提是你的 kubelet 版本已经到了 1.5：\n\n* 1，关闭 kubelet、kube-proxy 服务；\n* 2，注释掉 `/etc/kubernetes/kubelet` 文件中下面这一行:\n\n    `KUBELET_API_SERVER=\"--api-servers=http://127.0.0.1:8080\"`\n\n然后在 **KUBELET_ARGS** 中添加： \n\n\t--kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true\n\n这里的路径是你 yaml 文件放置的路径。 \n\n- 3，重新启动刚关掉的两个服务\n\n---\n## 4，以下为 [kubeconfig 配置官方文档](https://kubernetes.io/docs/user-guide/kubeconfig-file/)的翻译\n\n### kubernetes 中的验证对于不同的群体可以使用不同的方法.\n\n* 运行 kubelet 可能有的一种认证方式（即证书）。\n* 用户可能有不同的认证方式（即 token）。\n* 管理员可以为每个用户提供一个证书列表。\n* 可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 \n\n因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。\n\n此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。\n\n如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。\n\n### 相关讨论\n\n\thttp://issue.k8s.io/1755\n\n### kubeconfig 文件的组件 \n\nkubeconfig 文件示例：\n\n\tcurrent-context: federal-context\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    api-version: v1\n\t    server: http://cow.org:8080\n\t  name: cow-cluster\n\t- cluster:\n\t    certificate-authority: path/to/my/cafile\n\t    server: https://horse.org:4443\n\t  name: horse-cluster\n\t- cluster:\n\t    insecure-skip-tls-verify: true\n\t    server: https://pig.org:443\n\t  name: pig-cluster\n\tcontexts:\n\t- context:\n\t    cluster: horse-cluster\n\t    namespace: chisel-ns\n\t    user: green-user\n\t  name: federal-context\n\t- context:\n\t    cluster: pig-cluster\n\t    namespace: saw-ns\n\t    user: black-user\n\t  name: queen-anne-context\n\tkind: Config\n\tpreferences:\n\t  colors: true\n\tusers:\n\t- name: blue-user\n\t  user:\n\t    token: blue-token\n\t- name: green-user\n\t  user:\n\t    client-certificate: path/to/my/client/cert\n\t    client-key: path/to/my/client/key\n\n### 组件的解释\n\n#### cluster\n\n\tclusters:\n\t- cluster:\n\t    certificate-authority: path/to/my/cafile\n\t    server: https://horse.org:4443\n\t  name: horse-cluster\n\t- cluster:\n\t    insecure-skip-tls-verify: true\n\t    server: https://pig.org:443\n\t  name: pig-cluster\n\n\ncluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。\n\n#### user\n\n\tusers:\n\t- name: blue-user\n\t  user:\n\t    token: blue-token\n\t- name: green-user\n\t  user:\n\t    client-certificate: path/to/my/client/cert\n\t    client-key: path/to/my/client/key\n\n用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。\n\n### context\n\n\tcontexts:\n\t- context:\n\t    cluster: horse-cluster\n\t    namespace: chisel-ns\n\t    user: green-user\n\t  name: federal-context\n\ncontext 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。\n三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。\n\n#### current-context\n\n\tcurrent-context: federal-context\n\ncurrent-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 --context=CONTEXT, --cluster=CLUSTER, --user=USER, and/or --namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。\n\n#### 杂项\n\n\tapiVersion: v1\n\tkind: Config\n\tpreferences:\n\t  colors: true\n\napiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。\npreferences 指定选项(和当前未使用的) kubectl preferences.\n\n### 查看 kubeconfig 文件\n\nkubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 --minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。\n\n### 创建你的 kubeconfig 文件\n\n注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。\n\n在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。\n\n因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改...\n\n以上文件可能对应于使用--token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样：\n\t\n\tblue-user,blue-user,1\n\tmister-red,mister-red,2\n\n此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。\n\n* 由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。\n* 类似地，我们也可以选择改变 current-context 的值为 “blue-user”。\n* \n在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过\n\n\n### 加载和合并规则\n\n加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建：\n\n1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：\n如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。\n\n否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。\n\n对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。\n\n2，此链中第一个被匹配的 context 将被使用：\n\n* 1，命令行参数 - 命令行选项中 context 的值\n* 2，合并文件中的 current-context\n* 3，此段允许为空\n\n3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）：\n\n* 1，命令行参数 - user 是用户名，cluster 是集群名\n* 2，如果存在 context 则使用\n* 3，允许为空\n\n4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）：\n\n* 1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify\n* 2，如果存在集群信息并且该属性的值存在，则使用它。\n* 3，如果没有 server 位置则出错。\n\n5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法：\n\n* 1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段\n* 2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。\n* 3，如果两者有冲突则失败\n\n6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。\n\n7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。\n\nkubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。\n\n\n### 通过 kubectl config <subcommand> 操作 kubeconfig\n\n为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。\n\n例如：\n\n\t$ kubectl config set-credentials myself --username=admin --password=secret\n\t$ kubectl config set-cluster local-server --server=http://localhost:8080\n\t$ kubectl config set-context default-context --cluster=local-server --user=myself\n\t$ kubectl config use-context default-context\n\t$ kubectl config set contexts.default-context.namespace the-right-prefix\n\t$ kubectl config view\n\n输出：\n\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    server: http://localhost:8080\n\t  name: local-server\n\tcontexts:\n\t- context:\n\t    cluster: local-server\n\t    namespace: the-right-prefix\n\t    user: myself\n\t  name: default-context\n\tcurrent-context: default-context\n\tkind: Config\n\tpreferences: {}\n\tusers:\n\t- name: myself\n\t  user:\n\t    password: secret\n\t    username: admin\n\n一个 kubeconfig 文件类似这样：\n\n\tapiVersion: v1\n\tclusters:\n\t- cluster:\n\t    server: http://localhost:8080\n\t  name: local-server\n\tcontexts:\n\t- context:\n\t    cluster: local-server\n\t    namespace: the-right-prefix\n\t    user: myself\n\t  name: default-context\n\tcurrent-context: default-context\n\tkind: Config\n\tpreferences: {}\n\tusers:\n\t- name: myself\n\t  user:\n\t    password: secret\n\t    username: admin\n\n示例文件的命令操作：\n\n\t$ kubectl config set preferences.colors true\n\t$ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1\n\t$ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile\n\t$ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true\n\t$ kubectl config set-credentials blue-user --token=blue-token\n\t$ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key\n\t$ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns\n\t$ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns\n\t$ kubectl config use-context federal-context\n\n最后的总结：\n\n所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了：\n\n* 仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证\n* 将上面的代码段替换为你集群的 api-server endpoint 的信息。\n* 确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。\n","slug":"kubernetes-learn","published":1,"updated":"2019-07-21T09:41:08.891Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59u001iapwnzwztcfbm","content":"<p>1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 <strong>kubernetes + docker</strong> 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 <code>API</code> 进行某些操作。</p>\n<h2 id=\"1，kubernetes-搭建\"><a href=\"#1，kubernetes-搭建\" class=\"headerlink\" title=\"1，kubernetes 搭建\"></a>1，kubernetes 搭建</h2><p><a href=\"https://github.com/kubernetes/kubernetes\" target=\"_blank\" rel=\"noopener\">kubernetes</a> 是 google 的一个开源软件，其社区活跃量远超 <strong>Mesos，Coreos</strong> 的，若想深入学习建议参考<strong>《kubernetes 权威指南》</strong>，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/bank.png\" alt=\"kubernetes 架构图\"></p>\n<p>kubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 <strong>gitbook</strong> 上面某大神的一本书 一 <a href=\"https://skyao.gitbooks.io/leaning-etcd3/content/documentation/leaning/\" target=\"_blank\" rel=\"noopener\">etcd3学习笔记</a>。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。</p>\n<blockquote>\n<p>注意：内核版本 &gt; 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 </p>\n</blockquote>\n<p>etcd 节点：</p>\n<pre><code># yum install -y etcd \n# systemctl start etcd  \n</code></pre><p>master 节点：</p>\n<pre><code># yum install -y kubernetes-master\n# systemctl start kube-apiserver \n# systemctl start kube-controller-manager \n# systemctl start kube-scheduler \n</code></pre><p>minion 节点：</p>\n<pre><code># yum install -y kubernetes  docker\n# systemctl start kubelet \n# systemctl start kube-proxy \n# systemctl start docker \n</code></pre><h2 id=\"2，kubernetes-版本升级\"><a href=\"#2，kubernetes-版本升级\" class=\"headerlink\" title=\"2，kubernetes 版本升级\"></a>2，kubernetes 版本升级</h2><p>以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。</p>\n<p>下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#downloads-for-v160-alpha1\" target=\"_blank\" rel=\"noopener\">kubernetes-server-linux-amd64.tar.gz</a>  这个包，下载你所要更新到的版本。</p>\n<p><strong>升级步骤</strong>：</p>\n<ul>\n<li>1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上</li>\n<li>2，备份二进制程序（kubectl,kube-proxy）</li>\n<li>3，将解压后的二进制程序覆盖以前的版本</li>\n<li>4，最后重新启动服务</li>\n</ul>\n<pre><code># systemctl stop docker\n# which kubectl kube-proxy \n/usr/bin/kubectl\n/usr/bin/kube-proxy\n\n# cp /usr/bin/{kubectl,kube-proxy} /tmp/\n# yes | cp bin/{kubectl,kube-proxy} /usr/bin/\n\n# systemctl status {kubectl,kube-proxy}\n\n# systemctl start docker \n</code></pre><h2 id=\"3，kubeconfig-使用\"><a href=\"#3，kubeconfig-使用\" class=\"headerlink\" title=\"3，kubeconfig 使用\"></a>3，kubeconfig 使用</h2><p>若你使用的 kubelet 版本为 1.4，使用 <code>systemctl status kubelet</code>  会看到这样一句话：</p>\n<pre><code>--api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml\n</code></pre><p>使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 <a href=\"http://kubernetes.io/docs/user-guide/kubeconfig-file/\" target=\"_blank\" rel=\"noopener\">官方文档</a> 有格式说明， 本人已将其翻译，翻译文档见下文。</p>\n<p><strong>kubeconfig</strong> 文件示例：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><hr>\n<pre><code># kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true\n</code></pre><p>kubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。</p>\n<p>require-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。</p>\n<p>关于 kubeconfig 的一个 <strong>issue</strong>，<a href=\"https://github.com/kubernetes/kubernetes/issues/36745\" target=\"_blank\" rel=\"noopener\">Kubelet won’t read apiserver from kubeconfig</a>。</p>\n<p><strong>升级步骤</strong>，当然前提是你的 kubelet 版本已经到了 1.5：</p>\n<ul>\n<li>1，关闭 kubelet、kube-proxy 服务；</li>\n<li><p>2，注释掉 <code>/etc/kubernetes/kubelet</code> 文件中下面这一行:</p>\n<p>  <code>KUBELET_API_SERVER=&quot;--api-servers=http://127.0.0.1:8080&quot;</code></p>\n</li>\n</ul>\n<p>然后在 <strong>KUBELET_ARGS</strong> 中添加： </p>\n<pre><code>--kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true\n</code></pre><p>这里的路径是你 yaml 文件放置的路径。 </p>\n<ul>\n<li>3，重新启动刚关掉的两个服务</li>\n</ul>\n<hr>\n<h2 id=\"4，以下为-kubeconfig-配置官方文档的翻译\"><a href=\"#4，以下为-kubeconfig-配置官方文档的翻译\" class=\"headerlink\" title=\"4，以下为 kubeconfig 配置官方文档的翻译\"></a>4，以下为 <a href=\"https://kubernetes.io/docs/user-guide/kubeconfig-file/\" target=\"_blank\" rel=\"noopener\">kubeconfig 配置官方文档</a>的翻译</h2><h3 id=\"kubernetes-中的验证对于不同的群体可以使用不同的方法\"><a href=\"#kubernetes-中的验证对于不同的群体可以使用不同的方法\" class=\"headerlink\" title=\"kubernetes 中的验证对于不同的群体可以使用不同的方法.\"></a>kubernetes 中的验证对于不同的群体可以使用不同的方法.</h3><ul>\n<li>运行 kubelet 可能有的一种认证方式（即证书）。</li>\n<li>用户可能有不同的认证方式（即 token）。</li>\n<li>管理员可以为每个用户提供一个证书列表。</li>\n<li>可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 </li>\n</ul>\n<p>因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。</p>\n<p>此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。</p>\n<p>如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。</p>\n<h3 id=\"相关讨论\"><a href=\"#相关讨论\" class=\"headerlink\" title=\"相关讨论\"></a>相关讨论</h3><pre><code>http://issue.k8s.io/1755\n</code></pre><h3 id=\"kubeconfig-文件的组件\"><a href=\"#kubeconfig-文件的组件\" class=\"headerlink\" title=\"kubeconfig 文件的组件\"></a>kubeconfig 文件的组件</h3><p>kubeconfig 文件示例：</p>\n<pre><code>current-context: federal-context\napiVersion: v1\nclusters:\n- cluster:\n    api-version: v1\n    server: http://cow.org:8080\n  name: cow-cluster\n- cluster:\n    certificate-authority: path/to/my/cafile\n    server: https://horse.org:4443\n  name: horse-cluster\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://pig.org:443\n  name: pig-cluster\ncontexts:\n- context:\n    cluster: horse-cluster\n    namespace: chisel-ns\n    user: green-user\n  name: federal-context\n- context:\n    cluster: pig-cluster\n    namespace: saw-ns\n    user: black-user\n  name: queen-anne-context\nkind: Config\npreferences:\n  colors: true\nusers:\n- name: blue-user\n  user:\n    token: blue-token\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert\n    client-key: path/to/my/client/key\n</code></pre><h3 id=\"组件的解释\"><a href=\"#组件的解释\" class=\"headerlink\" title=\"组件的解释\"></a>组件的解释</h3><h4 id=\"cluster\"><a href=\"#cluster\" class=\"headerlink\" title=\"cluster\"></a>cluster</h4><pre><code>clusters:\n- cluster:\n    certificate-authority: path/to/my/cafile\n    server: https://horse.org:4443\n  name: horse-cluster\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://pig.org:443\n  name: pig-cluster\n</code></pre><p>cluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。</p>\n<h4 id=\"user\"><a href=\"#user\" class=\"headerlink\" title=\"user\"></a>user</h4><pre><code>users:\n- name: blue-user\n  user:\n    token: blue-token\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert\n    client-key: path/to/my/client/key\n</code></pre><p>用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。</p>\n<h3 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h3><pre><code>contexts:\n- context:\n    cluster: horse-cluster\n    namespace: chisel-ns\n    user: green-user\n  name: federal-context\n</code></pre><p>context 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。<br>三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。</p>\n<h4 id=\"current-context\"><a href=\"#current-context\" class=\"headerlink\" title=\"current-context\"></a>current-context</h4><pre><code>current-context: federal-context\n</code></pre><p>current-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 –context=CONTEXT, –cluster=CLUSTER, –user=USER, and/or –namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。</p>\n<h4 id=\"杂项\"><a href=\"#杂项\" class=\"headerlink\" title=\"杂项\"></a>杂项</h4><pre><code>apiVersion: v1\nkind: Config\npreferences:\n  colors: true\n</code></pre><p>apiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。<br>preferences 指定选项(和当前未使用的) kubectl preferences.</p>\n<h3 id=\"查看-kubeconfig-文件\"><a href=\"#查看-kubeconfig-文件\" class=\"headerlink\" title=\"查看 kubeconfig 文件\"></a>查看 kubeconfig 文件</h3><p>kubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 –minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。</p>\n<h3 id=\"创建你的-kubeconfig-文件\"><a href=\"#创建你的-kubeconfig-文件\" class=\"headerlink\" title=\"创建你的 kubeconfig 文件\"></a>创建你的 kubeconfig 文件</h3><p>注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。</p>\n<p>在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。</p>\n<p>因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改…</p>\n<p>以上文件可能对应于使用–token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样：</p>\n<pre><code>blue-user,blue-user,1\nmister-red,mister-red,2\n</code></pre><p>此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。</p>\n<ul>\n<li>由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。</li>\n<li>类似地，我们也可以选择改变 current-context 的值为 “blue-user”。</li>\n<li>在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过</li>\n</ul>\n<h3 id=\"加载和合并规则\"><a href=\"#加载和合并规则\" class=\"headerlink\" title=\"加载和合并规则\"></a>加载和合并规则</h3><p>加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建：</p>\n<p>1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：<br>如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。</p>\n<p>否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。</p>\n<p>对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。</p>\n<p>2，此链中第一个被匹配的 context 将被使用：</p>\n<ul>\n<li>1，命令行参数 - 命令行选项中 context 的值</li>\n<li>2，合并文件中的 current-context</li>\n<li>3，此段允许为空</li>\n</ul>\n<p>3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）：</p>\n<ul>\n<li>1，命令行参数 - user 是用户名，cluster 是集群名</li>\n<li>2，如果存在 context 则使用</li>\n<li>3，允许为空</li>\n</ul>\n<p>4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）：</p>\n<ul>\n<li>1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify</li>\n<li>2，如果存在集群信息并且该属性的值存在，则使用它。</li>\n<li>3，如果没有 server 位置则出错。</li>\n</ul>\n<p>5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法：</p>\n<ul>\n<li>1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段</li>\n<li>2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。</li>\n<li>3，如果两者有冲突则失败</li>\n</ul>\n<p>6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。</p>\n<p>7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。</p>\n<p>kubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。</p>\n<h3 id=\"通过-kubectl-config-操作-kubeconfig\"><a href=\"#通过-kubectl-config-操作-kubeconfig\" class=\"headerlink\" title=\"通过 kubectl config  操作 kubeconfig\"></a>通过 kubectl config <subcommand> 操作 kubeconfig</subcommand></h3><p>为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。</p>\n<p>例如：</p>\n<pre><code>$ kubectl config set-credentials myself --username=admin --password=secret\n$ kubectl config set-cluster local-server --server=http://localhost:8080\n$ kubectl config set-context default-context --cluster=local-server --user=myself\n$ kubectl config use-context default-context\n$ kubectl config set contexts.default-context.namespace the-right-prefix\n$ kubectl config view\n</code></pre><p>输出：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><p>一个 kubeconfig 文件类似这样：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><p>示例文件的命令操作：</p>\n<pre><code>$ kubectl config set preferences.colors true\n$ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1\n$ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile\n$ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true\n$ kubectl config set-credentials blue-user --token=blue-token\n$ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key\n$ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns\n$ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns\n$ kubectl config use-context federal-context\n</code></pre><p>最后的总结：</p>\n<p>所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了：</p>\n<ul>\n<li>仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证</li>\n<li>将上面的代码段替换为你集群的 api-server endpoint 的信息。</li>\n<li>确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。</li>\n</ul>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 <strong>kubernetes + docker</strong> 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 <code>API</code> 进行某些操作。</p>\n<h2 id=\"1，kubernetes-搭建\"><a href=\"#1，kubernetes-搭建\" class=\"headerlink\" title=\"1，kubernetes 搭建\"></a>1，kubernetes 搭建</h2><p><a href=\"https://github.com/kubernetes/kubernetes\" target=\"_blank\" rel=\"noopener\">kubernetes</a> 是 google 的一个开源软件，其社区活跃量远超 <strong>Mesos，Coreos</strong> 的，若想深入学习建议参考<strong>《kubernetes 权威指南》</strong>，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/bank.png\" alt=\"kubernetes 架构图\"></p>\n<p>kubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 <strong>gitbook</strong> 上面某大神的一本书 一 <a href=\"https://skyao.gitbooks.io/leaning-etcd3/content/documentation/leaning/\" target=\"_blank\" rel=\"noopener\">etcd3学习笔记</a>。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。</p>\n<blockquote>\n<p>注意：内核版本 &gt; 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 </p>\n</blockquote>\n<p>etcd 节点：</p>\n<pre><code># yum install -y etcd \n# systemctl start etcd  \n</code></pre><p>master 节点：</p>\n<pre><code># yum install -y kubernetes-master\n# systemctl start kube-apiserver \n# systemctl start kube-controller-manager \n# systemctl start kube-scheduler \n</code></pre><p>minion 节点：</p>\n<pre><code># yum install -y kubernetes  docker\n# systemctl start kubelet \n# systemctl start kube-proxy \n# systemctl start docker \n</code></pre><h2 id=\"2，kubernetes-版本升级\"><a href=\"#2，kubernetes-版本升级\" class=\"headerlink\" title=\"2，kubernetes 版本升级\"></a>2，kubernetes 版本升级</h2><p>以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。</p>\n<p>下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG.md#downloads-for-v160-alpha1\" target=\"_blank\" rel=\"noopener\">kubernetes-server-linux-amd64.tar.gz</a>  这个包，下载你所要更新到的版本。</p>\n<p><strong>升级步骤</strong>：</p>\n<ul>\n<li>1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上</li>\n<li>2，备份二进制程序（kubectl,kube-proxy）</li>\n<li>3，将解压后的二进制程序覆盖以前的版本</li>\n<li>4，最后重新启动服务</li>\n</ul>\n<pre><code># systemctl stop docker\n# which kubectl kube-proxy \n/usr/bin/kubectl\n/usr/bin/kube-proxy\n\n# cp /usr/bin/{kubectl,kube-proxy} /tmp/\n# yes | cp bin/{kubectl,kube-proxy} /usr/bin/\n\n# systemctl status {kubectl,kube-proxy}\n\n# systemctl start docker \n</code></pre><h2 id=\"3，kubeconfig-使用\"><a href=\"#3，kubeconfig-使用\" class=\"headerlink\" title=\"3，kubeconfig 使用\"></a>3，kubeconfig 使用</h2><p>若你使用的 kubelet 版本为 1.4，使用 <code>systemctl status kubelet</code>  会看到这样一句话：</p>\n<pre><code>--api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml\n</code></pre><p>使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 <a href=\"http://kubernetes.io/docs/user-guide/kubeconfig-file/\" target=\"_blank\" rel=\"noopener\">官方文档</a> 有格式说明， 本人已将其翻译，翻译文档见下文。</p>\n<p><strong>kubeconfig</strong> 文件示例：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><hr>\n<pre><code># kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true\n</code></pre><p>kubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。</p>\n<p>require-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。</p>\n<p>关于 kubeconfig 的一个 <strong>issue</strong>，<a href=\"https://github.com/kubernetes/kubernetes/issues/36745\" target=\"_blank\" rel=\"noopener\">Kubelet won’t read apiserver from kubeconfig</a>。</p>\n<p><strong>升级步骤</strong>，当然前提是你的 kubelet 版本已经到了 1.5：</p>\n<ul>\n<li>1，关闭 kubelet、kube-proxy 服务；</li>\n<li><p>2，注释掉 <code>/etc/kubernetes/kubelet</code> 文件中下面这一行:</p>\n<p>  <code>KUBELET_API_SERVER=&quot;--api-servers=http://127.0.0.1:8080&quot;</code></p>\n</li>\n</ul>\n<p>然后在 <strong>KUBELET_ARGS</strong> 中添加： </p>\n<pre><code>--kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true\n</code></pre><p>这里的路径是你 yaml 文件放置的路径。 </p>\n<ul>\n<li>3，重新启动刚关掉的两个服务</li>\n</ul>\n<hr>\n<h2 id=\"4，以下为-kubeconfig-配置官方文档的翻译\"><a href=\"#4，以下为-kubeconfig-配置官方文档的翻译\" class=\"headerlink\" title=\"4，以下为 kubeconfig 配置官方文档的翻译\"></a>4，以下为 <a href=\"https://kubernetes.io/docs/user-guide/kubeconfig-file/\" target=\"_blank\" rel=\"noopener\">kubeconfig 配置官方文档</a>的翻译</h2><h3 id=\"kubernetes-中的验证对于不同的群体可以使用不同的方法\"><a href=\"#kubernetes-中的验证对于不同的群体可以使用不同的方法\" class=\"headerlink\" title=\"kubernetes 中的验证对于不同的群体可以使用不同的方法.\"></a>kubernetes 中的验证对于不同的群体可以使用不同的方法.</h3><ul>\n<li>运行 kubelet 可能有的一种认证方式（即证书）。</li>\n<li>用户可能有不同的认证方式（即 token）。</li>\n<li>管理员可以为每个用户提供一个证书列表。</li>\n<li>可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 </li>\n</ul>\n<p>因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。</p>\n<p>此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。</p>\n<p>如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。</p>\n<h3 id=\"相关讨论\"><a href=\"#相关讨论\" class=\"headerlink\" title=\"相关讨论\"></a>相关讨论</h3><pre><code>http://issue.k8s.io/1755\n</code></pre><h3 id=\"kubeconfig-文件的组件\"><a href=\"#kubeconfig-文件的组件\" class=\"headerlink\" title=\"kubeconfig 文件的组件\"></a>kubeconfig 文件的组件</h3><p>kubeconfig 文件示例：</p>\n<pre><code>current-context: federal-context\napiVersion: v1\nclusters:\n- cluster:\n    api-version: v1\n    server: http://cow.org:8080\n  name: cow-cluster\n- cluster:\n    certificate-authority: path/to/my/cafile\n    server: https://horse.org:4443\n  name: horse-cluster\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://pig.org:443\n  name: pig-cluster\ncontexts:\n- context:\n    cluster: horse-cluster\n    namespace: chisel-ns\n    user: green-user\n  name: federal-context\n- context:\n    cluster: pig-cluster\n    namespace: saw-ns\n    user: black-user\n  name: queen-anne-context\nkind: Config\npreferences:\n  colors: true\nusers:\n- name: blue-user\n  user:\n    token: blue-token\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert\n    client-key: path/to/my/client/key\n</code></pre><h3 id=\"组件的解释\"><a href=\"#组件的解释\" class=\"headerlink\" title=\"组件的解释\"></a>组件的解释</h3><h4 id=\"cluster\"><a href=\"#cluster\" class=\"headerlink\" title=\"cluster\"></a>cluster</h4><pre><code>clusters:\n- cluster:\n    certificate-authority: path/to/my/cafile\n    server: https://horse.org:4443\n  name: horse-cluster\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://pig.org:443\n  name: pig-cluster\n</code></pre><p>cluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。</p>\n<h4 id=\"user\"><a href=\"#user\" class=\"headerlink\" title=\"user\"></a>user</h4><pre><code>users:\n- name: blue-user\n  user:\n    token: blue-token\n- name: green-user\n  user:\n    client-certificate: path/to/my/client/cert\n    client-key: path/to/my/client/key\n</code></pre><p>用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。</p>\n<h3 id=\"context\"><a href=\"#context\" class=\"headerlink\" title=\"context\"></a>context</h3><pre><code>contexts:\n- context:\n    cluster: horse-cluster\n    namespace: chisel-ns\n    user: green-user\n  name: federal-context\n</code></pre><p>context 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。<br>三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。</p>\n<h4 id=\"current-context\"><a href=\"#current-context\" class=\"headerlink\" title=\"current-context\"></a>current-context</h4><pre><code>current-context: federal-context\n</code></pre><p>current-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 –context=CONTEXT, –cluster=CLUSTER, –user=USER, and/or –namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。</p>\n<h4 id=\"杂项\"><a href=\"#杂项\" class=\"headerlink\" title=\"杂项\"></a>杂项</h4><pre><code>apiVersion: v1\nkind: Config\npreferences:\n  colors: true\n</code></pre><p>apiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。<br>preferences 指定选项(和当前未使用的) kubectl preferences.</p>\n<h3 id=\"查看-kubeconfig-文件\"><a href=\"#查看-kubeconfig-文件\" class=\"headerlink\" title=\"查看 kubeconfig 文件\"></a>查看 kubeconfig 文件</h3><p>kubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 –minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。</p>\n<h3 id=\"创建你的-kubeconfig-文件\"><a href=\"#创建你的-kubeconfig-文件\" class=\"headerlink\" title=\"创建你的 kubeconfig 文件\"></a>创建你的 kubeconfig 文件</h3><p>注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。</p>\n<p>在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。</p>\n<p>因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改…</p>\n<p>以上文件可能对应于使用–token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样：</p>\n<pre><code>blue-user,blue-user,1\nmister-red,mister-red,2\n</code></pre><p>此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。</p>\n<ul>\n<li>由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。</li>\n<li>类似地，我们也可以选择改变 current-context 的值为 “blue-user”。</li>\n<li>在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过</li>\n</ul>\n<h3 id=\"加载和合并规则\"><a href=\"#加载和合并规则\" class=\"headerlink\" title=\"加载和合并规则\"></a>加载和合并规则</h3><p>加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建：</p>\n<p>1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：<br>如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。</p>\n<p>否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。</p>\n<p>对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。</p>\n<p>2，此链中第一个被匹配的 context 将被使用：</p>\n<ul>\n<li>1，命令行参数 - 命令行选项中 context 的值</li>\n<li>2，合并文件中的 current-context</li>\n<li>3，此段允许为空</li>\n</ul>\n<p>3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）：</p>\n<ul>\n<li>1，命令行参数 - user 是用户名，cluster 是集群名</li>\n<li>2，如果存在 context 则使用</li>\n<li>3，允许为空</li>\n</ul>\n<p>4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）：</p>\n<ul>\n<li>1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify</li>\n<li>2，如果存在集群信息并且该属性的值存在，则使用它。</li>\n<li>3，如果没有 server 位置则出错。</li>\n</ul>\n<p>5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法：</p>\n<ul>\n<li>1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段</li>\n<li>2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。</li>\n<li>3，如果两者有冲突则失败</li>\n</ul>\n<p>6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。</p>\n<p>7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。</p>\n<p>kubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。</p>\n<h3 id=\"通过-kubectl-config-操作-kubeconfig\"><a href=\"#通过-kubectl-config-操作-kubeconfig\" class=\"headerlink\" title=\"通过 kubectl config  操作 kubeconfig\"></a>通过 kubectl config <subcommand> 操作 kubeconfig</subcommand></h3><p>为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。</p>\n<p>例如：</p>\n<pre><code>$ kubectl config set-credentials myself --username=admin --password=secret\n$ kubectl config set-cluster local-server --server=http://localhost:8080\n$ kubectl config set-context default-context --cluster=local-server --user=myself\n$ kubectl config use-context default-context\n$ kubectl config set contexts.default-context.namespace the-right-prefix\n$ kubectl config view\n</code></pre><p>输出：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><p>一个 kubeconfig 文件类似这样：</p>\n<pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: http://localhost:8080\n  name: local-server\ncontexts:\n- context:\n    cluster: local-server\n    namespace: the-right-prefix\n    user: myself\n  name: default-context\ncurrent-context: default-context\nkind: Config\npreferences: {}\nusers:\n- name: myself\n  user:\n    password: secret\n    username: admin\n</code></pre><p>示例文件的命令操作：</p>\n<pre><code>$ kubectl config set preferences.colors true\n$ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1\n$ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile\n$ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true\n$ kubectl config set-credentials blue-user --token=blue-token\n$ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key\n$ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns\n$ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns\n$ kubectl config use-context federal-context\n</code></pre><p>最后的总结：</p>\n<p>所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了：</p>\n<ul>\n<li>仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证</li>\n<li>将上面的代码段替换为你集群的 api-server endpoint 的信息。</li>\n<li>确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。</li>\n</ul>\n"},{"title":"kubernetes 常用 API","date":"2018-09-02T05:13:00.000Z","type":"kubernetes","_content":"\nkubectl  的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数  -v=9  可以看调用的相关 API，例：\n `$ kubectl get node -v=9` \n\n以下为 kubernetes 开发中常用的 API：\n![deployment 常用 API](http://cdn.tianfeiyu.com/deploy-1.png)\n\n![statefulset 常用 API](http://cdn.tianfeiyu.com/sts-1.png)\n\n![pod 常用 API](http://cdn.tianfeiyu.com/pod-1.png)\n\n\n![service 常用 API](http://cdn.tianfeiyu.com/service-1.png)\n\n![endpoints 常用 API](http://cdn.tianfeiyu.com/endpoints-1.png)\n\n![namespace 常用 API](http://cdn.tianfeiyu.com/namespace-1.png)\n\n![node 常用 API](http://cdn.tianfeiyu.com/nodes-1.png)\n\n![pv 常用 API](http://cdn.tianfeiyu.com/pv-1.png)\n\n Markdown 表格显示过大，此仅以图片格式展示。\n\n","source":"_posts/kubernetes-api.md","raw":"---\ntitle: kubernetes 常用 API\ndate: 2018-09-02 13:13:00\ntype: \"kubernetes\"\n\n---\n\nkubectl  的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数  -v=9  可以看调用的相关 API，例：\n `$ kubectl get node -v=9` \n\n以下为 kubernetes 开发中常用的 API：\n![deployment 常用 API](http://cdn.tianfeiyu.com/deploy-1.png)\n\n![statefulset 常用 API](http://cdn.tianfeiyu.com/sts-1.png)\n\n![pod 常用 API](http://cdn.tianfeiyu.com/pod-1.png)\n\n\n![service 常用 API](http://cdn.tianfeiyu.com/service-1.png)\n\n![endpoints 常用 API](http://cdn.tianfeiyu.com/endpoints-1.png)\n\n![namespace 常用 API](http://cdn.tianfeiyu.com/namespace-1.png)\n\n![node 常用 API](http://cdn.tianfeiyu.com/nodes-1.png)\n\n![pv 常用 API](http://cdn.tianfeiyu.com/pv-1.png)\n\n Markdown 表格显示过大，此仅以图片格式展示。\n\n","slug":"kubernetes-api","published":1,"updated":"2019-07-21T09:55:13.515Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59u001kapwnzwokduit","content":"<p>kubectl  的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数  -v=9  可以看调用的相关 API，例：<br> <code>$ kubectl get node -v=9</code> </p>\n<p>以下为 kubernetes 开发中常用的 API：<br><img src=\"http://cdn.tianfeiyu.com/deploy-1.png\" alt=\"deployment 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/sts-1.png\" alt=\"statefulset 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/pod-1.png\" alt=\"pod 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/service-1.png\" alt=\"service 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/endpoints-1.png\" alt=\"endpoints 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/namespace-1.png\" alt=\"namespace 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/nodes-1.png\" alt=\"node 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/pv-1.png\" alt=\"pv 常用 API\"></p>\n<p> Markdown 表格显示过大，此仅以图片格式展示。</p>\n","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p>kubectl  的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数  -v=9  可以看调用的相关 API，例：<br> <code>$ kubectl get node -v=9</code> </p>\n<p>以下为 kubernetes 开发中常用的 API：<br><img src=\"http://cdn.tianfeiyu.com/deploy-1.png\" alt=\"deployment 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/sts-1.png\" alt=\"statefulset 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/pod-1.png\" alt=\"pod 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/service-1.png\" alt=\"service 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/endpoints-1.png\" alt=\"endpoints 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/namespace-1.png\" alt=\"namespace 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/nodes-1.png\" alt=\"node 常用 API\"></p>\n<p><img src=\"http://cdn.tianfeiyu.com/pv-1.png\" alt=\"pv 常用 API\"></p>\n<p> Markdown 表格显示过大，此仅以图片格式展示。</p>\n"},{"title":"kubelet 状态上报的方式","date":"2019-06-09T12:37:30.000Z","type":"node status","_content":"\n\n 分布式系统中服务端会通过心跳机制确认客户端是否存活，在 k8s 中，kubelet 也会定时上报心跳到 apiserver，以此判断该 node 是否存活，若 node 超过一定时间没有上报心跳，其状态会被置为 NotReady，宿主上容器的状态也会被置为 Nodelost 或者 Unknown 状态。kubelet 自身会定期更新状态到 apiserver，通过参数 `--node-status-update-frequency` 指定上报频率，默认是 10s 上报一次，kubelet 不止上报心跳信息还会上报自身的一些数据信息。\n\n\n\n### 一、kubelet 上报哪些状态\n\n 在 k8s 中，一个 node 的状态包含以下几个信息：\n\n- [Addresses](https://kubernetes.io/docs/concepts/architecture/nodes/#addresses)\n- [Condition](https://kubernetes.io/docs/concepts/architecture/nodes/#condition)\n- [Capacity](https://kubernetes.io/docs/concepts/architecture/nodes/#capacity)\n- [Info](https://kubernetes.io/docs/concepts/architecture/nodes/#info)\n\n##### 1、Addresses\n\n主要包含以下几个字段：\n\n- HostName：Hostname 。可以通过 kubelet 的 `--hostname-override` 参数进行覆盖。\n- ExternalIP：通常是可以外部路由的 node IP 地址（从集群外可访问）。\n- InternalIP：通常是仅可在集群内部路由的 node IP 地址。\n\n##### 2、Condition\n\n`conditions` 字段描述了所有 `Running` nodes 的状态。\n\n![condition](http://cdn.tianfeiyu.com/1262158-05b37df015e527b7.png)\n\n##### 3、Capacity\n\n描述 node 上的可用资源：CPU、内存和可以调度到该 node 上的最大 pod 数量。\n\n##### 4、Info\n\n描述 node 的一些通用信息，例如内核版本、Kubernetes 版本（kubelet 和 kube-proxy 版本）、Docker 版本 （如果使用了）和系统版本，这些信息由 kubelet 从 node 上获取到。\n\n使用 `kubectl get node xxx -o yaml` 可以看到 node 所有的状态的信息，其中 status 中的信息都是 kubelet 需要上报的，所以 kubelet 不止上报心跳信息还上报节点信息、节点 OOD 信息、内存磁盘压力状态、节点监控状态、是否调度等。\n\n\n\n![node 状态信息](http://cdn.tianfeiyu.com/status.png)\n\n\n\n\n\n###  二、kubelet 状态异常时的影响\n\n如果一个 node 处于非 Ready 状态超过 `pod-eviction-timeout`的值(默认为 5 分钟，在 kube-controller-manager 中定义)，在 v1.5 之前的版本中 kube-controller-manager 会 `force delete pod` 然后调度该宿主上的 pods 到其他宿主，在 v1.5 之后的版本中，kube-controller-manager 不会 `force delete pod`，pod 会一直处于`Terminating` 或`Unknown` 状态直到 node 被从 master 中删除或 kubelet 状态变为 Ready。在 node NotReady 期间，Daemonset 的 Pod 状态变为 Nodelost，Deployment、Statefulset 和 Static Pod 的状态先变为 NodeLost，然后马上变为 Unknown。Deployment 的 pod 会 recreate，Static Pod 和 Statefulset 的 Pod 会一直处于 Unknown 状态。\n\n当 kubelet 变为 Ready 状态时，Daemonset的pod不会recreate，旧pod状态直接变为Running，Deployment的则是将kubelet进程停止的Node删除，Statefulset的Pod会重新recreate，Staic Pod 会被删除。\n\n\n\n### 三、kubelet 状态上报的实现\n\nkubelet 有两种上报状态的方式，第一种定期向 apiserver 发送心跳消息，简单理解就是启动一个 goroutine 然后定期向 APIServer 发送消息。\n\n第二中被称为 NodeLease，在 v1.13 之前的版本中，节点的心跳只有 NodeStatus，从 v1.13 开始，NodeLease feature 作为 alpha 特性引入。当启用 NodeLease feature 时，每个节点在“kube-node-lease”名称空间中都有一个关联的“Lease”对象，该对象由节点定期更新，NodeStatus 和 NodeLease 都被视为来自节点的心跳。NodeLease 会频繁更新，而只有在 NodeStatus 发生改变或者超过了一定时间(默认值为1分钟，node-monitor-grace-period 的默认值为 40s)，才会将 NodeStatus 上报给 master。由于 NodeLease 比 NodeStatus 更轻量级，该特性在集群规模扩展性和性能上有明显提升。本文主要分析第一种上报方式的实现。\n\n\n\n> kubernetes 版本 ：v1.13\n\n\n\nkubelet 上报状态的代码大部分在 `kubernetes/pkg/kubelet/kubelet_node_status.go` 中实现。状态上报的功能是在 `kubernetes/pkg/kubelet/kubelet.go#Run` 方法以 goroutine 形式中启动的，kubelet 中多个重要的功能都是在该方法中启动的。\n\n `kubernetes/pkg/kubelet/kubelet.go#Run`\n\n```\nfunc (kl *Kubelet) Run(updates <-chan kubetypes.PodUpdate) {  \n  \t// ...\n  \tif kl.kubeClient != nil {\n        // Start syncing node status immediately, this may set up things the runtime needs to run.\n        go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)\n        go kl.fastStatusUpdateOnce()\n\t\t\t\t\n\t    // 一种新的状态上报方式\n\t    // start syncing lease\n        if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) {\n            go kl.nodeLeaseController.Run(wait.NeverStop)\n        }\n    }\n\t\t// ...    \n}\n```\n\nkl.syncNodeStatus 便是上报状态的，此处 kl.nodeStatusUpdateFrequency 使用的是默认设置的 10s，也就是说节点间同步状态的函数 kl.syncNodeStatus 每 10s 执行一次。\n\n\n\nsyncNodeStatus 是状态上报的入口函数，其后所调用的多个函数也都是在同一个文件中实现的。\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#syncNodeStatus`\n\n```\nfunc (kl *Kubelet) syncNodeStatus() {\n    kl.syncNodeStatusMux.Lock()\n    defer kl.syncNodeStatusMux.Unlock()\n\n    if kl.kubeClient == nil || kl.heartbeatClient == nil {\n        return\n    }\n    \n    // 是否为注册节点\n    if kl.registerNode {\n        // This will exit immediately if it doesn't need to do anything.\n        kl.registerWithAPIServer()\n    }\n    if err := kl.updateNodeStatus(); err != nil {\n        klog.Errorf(\"Unable to update node status: %v\", err)\n    }\n}\n```\n\nsyncNodeStatus 调用 updateNodeStatus， 然后又调用 tryUpdateNodeStatus 来进行上报操作，而最终调用的是 setNodeStatus。这里还进行了同步状态判断，如果是注册节点，则执行 registerWithAPIServer，否则，执行 updateNodeStatus。\n\n\n\nupdateNodeStatus 主要是调用 tryUpdateNodeStatus 进行后续的操作，该函数中定义了状态上报重试的次数，nodeStatusUpdateRetry 默认定义为 5 次。\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#updateNodeStatus`\n\n```\nfunc (kl *Kubelet) updateNodeStatus() error {\n    klog.V(5).Infof(\"Updating node status\")\n    for i := 0; i < nodeStatusUpdateRetry; i++ {\n        if err := kl.tryUpdateNodeStatus(i); err != nil {\n            if i > 0 && kl.onRepeatedHeartbeatFailure != nil {\n                kl.onRepeatedHeartbeatFailure()\n            }\n            klog.Errorf(\"Error updating node status, will retry: %v\", err)\n        } else {\n            return nil\n        }\n    }\n    return fmt.Errorf(\"update node status exceeds retry count\")\n}\n```\n\n\n\ntryUpdateNodeStatus 是主要的上报逻辑，先给 node 设置状态，然后上报 node 的状态到 master。\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#tryUpdateNodeStatus`\n\n```\nfunc (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error {\n\topts := metav1.GetOptions{}\n\tif tryNumber == 0 {\n\t\tutil.FromApiserverCache(&opts)\n\t}\n\t\n\t// 获取 node 信息\n\tnode, err := kl.heartbeatClient.CoreV1().Nodes().Get(string(kl.nodeName), opts)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error getting node %q: %v\", kl.nodeName, err)\n\t}\n\n\toriginalNode := node.DeepCopy()\n\tif originalNode == nil {\n\t\treturn fmt.Errorf(\"nil %q node object\", kl.nodeName)\n\t}\n\n\tpodCIDRChanged := false\n\tif node.Spec.PodCIDR != \"\" {\n\t\tif podCIDRChanged, err = kl.updatePodCIDR(node.Spec.PodCIDR); err != nil {\n\t\t\tklog.Errorf(err.Error())\n\t\t}\n\t}\n\n\t// 设置 node 状态\n\tkl.setNodeStatus(node)\n\n\tnow := kl.clock.Now()\n\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) && now.Before(kl.lastStatusReportTime.Add(kl.nodeStatusReportFrequency)) {\n\t\tif !podCIDRChanged && !nodeStatusHasChanged(&originalNode.Status, &node.Status) {\n\t\t\tkl.volumeManager.MarkVolumesAsReportedInUse(node.Status.VolumesInUse)\n\t\t\treturn nil\n\t\t}\n\t}\n\n    // 更新 node 信息到 master\n\t// Patch the current status on the API server\n\tupdatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node)\n\tif err != nil {\n\t\treturn err\n\t}\n\tkl.lastStatusReportTime = now\n\tkl.setLastObservedNodeAddresses(updatedNode.Status.Addresses)\n\t// If update finishes successfully, mark the volumeInUse as reportedInUse to indicate\n\t// those volumes are already updated in the node's status\n\tkl.volumeManager.MarkVolumesAsReportedInUse(updatedNode.Status.VolumesInUse)\n\treturn nil\n}\n```\n\ntryUpdateNodeStatus 中调用 setNodeStatus 设置 node 的状态。setNodeStatus 会获取一次 node 的所有状态，然后会将 kubelet 中保存的所有状态改为最新的值，也就是会重置 node status 中的所有字段。\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#setNodeStatus`\n\n```\nfunc (kl *Kubelet) setNodeStatus(node *v1.Node) {\n    for i, f := range kl.setNodeStatusFuncs {\n        klog.V(5).Infof(\"Setting node status at position %v\", i)\n        if err := f(node); err != nil {\n            klog.Warningf(\"Failed to set some node status fields: %s\", err)\n        }\n    }\n}\n```\n\nsetNodeStatus 通过 setNodeStatusFuncs 方法覆盖 node 结构体中所有的字段，setNodeStatusFuncs 是在\n\nNewMainKubelet(pkg/kubelet/kubelet.go) 中初始化的。\n\n`kubernetes/pkg/kubelet/kubelet.go#NewMainKubelet`\n\n```\n func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,\n \t\t// ...\n \t\t// Generating the status funcs should be the last thing we do,\n    klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()\n\n    return klet, nil\n}\n```\n\n\n\ndefaultNodeStatusFuncs 是生成状态的函数，通过获取 node 的所有状态指标后使用工厂函数生成状态\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#defaultNodeStatusFuncs`\n\n```\nfunc (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error {\n    // if cloud is not nil, we expect the cloud resource sync manager to exist\n    var nodeAddressesFunc func() ([]v1.NodeAddress, error)\n    if kl.cloud != nil {\n        nodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses\n    }\n    var validateHostFunc func() error\n    if kl.appArmorValidator != nil {\n        validateHostFunc = kl.appArmorValidator.ValidateHost\n    }\n    var setters []func(n *v1.Node) error\n    setters = append(setters,\n        nodestatus.NodeAddress(kl.nodeIP, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc),\n        nodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity,\n            kl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent),\n        nodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version),\n        nodestatus.DaemonEndpoints(kl.daemonEndpoints),\n        nodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList),\n        nodestatus.GoRuntime(),\n    )\n    if utilfeature.DefaultFeatureGate.Enabled(features.AttachVolumeLimit) {\n        setters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits))\n    }\n    setters = append(setters,\n        nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),\n        nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),\n        nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),\n        nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors, validateHostFunc, kl.containerManager.  Status, kl.recordNodeStatusEvent),\n        nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),\n        nodestatus.RemoveOutOfDiskCondition(),\n        // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event\n        // and record state back to the Kubelet runtime object. In the future, I'd like to isolate\n        // these side-effects by decoupling the decisions to send events and partial status recording\n        // from the Node setters.\n        kl.recordNodeSchedulableEvent,\n    )\n    return setters\n}\n```\n\ndefaultNodeStatusFuncs 可以看到 node 上报的所有信息，主要有 MemoryPressureCondition、DiskPressureCondition、PIDPressureCondition、ReadyCondition 等。每一种 nodestatus 都返回一个 setters，所有 setters 的定义在 pkg/kubelet/nodestatus/setters.go 文件中。\n\n对于二次开发而言，如果我们需要 APIServer 掌握更多的 Node 信息，可以在此处添加自定义函数，例如，上报磁盘信息等。\n\n\n\ntryUpdateNodeStatus 中最后调用 PatchNodeStatus 上报 node 的状态到 master。\n\n`kubernetes/pkg/util/node/node.go#PatchNodeStatus`\n\n```\n// PatchNodeStatus patches node status.\nfunc PatchNodeStatus(c v1core.CoreV1Interface, nodeName types.NodeName, oldNode *v1.Node, newNode *v1.Node) (*v1.Node, []byte, error) {\n\t\t// 计算 patch \n    patchBytes, err := preparePatchBytesforNodeStatus(nodeName, oldNode, newNode)\n    if err != nil {\n        return nil, nil, err\n    }\n\n    updatedNode, err := c.Nodes().Patch(string(nodeName), types.StrategicMergePatchType, patchBytes,       \"status\")\n    if err != nil {\n        return nil, nil, fmt.Errorf(\"failed to patch status %q for node %q: %v\", patchBytes, nodeName, err)\n    }\n    return updatedNode, patchBytes, nil\n}\n```\n\n在 PatchNodeStatus 会调用已注册的那些方法将状态把状态发给 APIServer。\n\n\n\n### 四、总结\n\n本文主要讲述了 kubelet 上报状态的方式及其实现，node 状态上报的方式目前有两种，本文仅分析了第一种状态上报的方式。在大规模集群中由于节点数量比较多，所有 node 都频繁报状态对 etcd 会有一定的压力，当 node 与 master 通信时由于网络导致心跳上报失败也会影响 node 的状态，为了避免类似问题的出现才有 NodeLease 方式，对于该功能的实现后文会继续进行分析。\n\n\n参考：\nhttps://www.qikqiak.com/post/kubelet-sync-node-status/\nhttps://www.jianshu.com/p/054450557818\nhttps://blog.csdn.net/shida_csdn/article/details/84286058\nhttps://kubernetes.io/docs/concepts/architecture/nodes/\n","source":"_posts/node_status.md","raw":"---\ntitle: kubelet 状态上报的方式\ndate: 2019-06-09 20:37:30\ntags: [\"node status\",\"kubelet\"]\ntype: \"node status\"\n\n---\n\n\n 分布式系统中服务端会通过心跳机制确认客户端是否存活，在 k8s 中，kubelet 也会定时上报心跳到 apiserver，以此判断该 node 是否存活，若 node 超过一定时间没有上报心跳，其状态会被置为 NotReady，宿主上容器的状态也会被置为 Nodelost 或者 Unknown 状态。kubelet 自身会定期更新状态到 apiserver，通过参数 `--node-status-update-frequency` 指定上报频率，默认是 10s 上报一次，kubelet 不止上报心跳信息还会上报自身的一些数据信息。\n\n\n\n### 一、kubelet 上报哪些状态\n\n 在 k8s 中，一个 node 的状态包含以下几个信息：\n\n- [Addresses](https://kubernetes.io/docs/concepts/architecture/nodes/#addresses)\n- [Condition](https://kubernetes.io/docs/concepts/architecture/nodes/#condition)\n- [Capacity](https://kubernetes.io/docs/concepts/architecture/nodes/#capacity)\n- [Info](https://kubernetes.io/docs/concepts/architecture/nodes/#info)\n\n##### 1、Addresses\n\n主要包含以下几个字段：\n\n- HostName：Hostname 。可以通过 kubelet 的 `--hostname-override` 参数进行覆盖。\n- ExternalIP：通常是可以外部路由的 node IP 地址（从集群外可访问）。\n- InternalIP：通常是仅可在集群内部路由的 node IP 地址。\n\n##### 2、Condition\n\n`conditions` 字段描述了所有 `Running` nodes 的状态。\n\n![condition](http://cdn.tianfeiyu.com/1262158-05b37df015e527b7.png)\n\n##### 3、Capacity\n\n描述 node 上的可用资源：CPU、内存和可以调度到该 node 上的最大 pod 数量。\n\n##### 4、Info\n\n描述 node 的一些通用信息，例如内核版本、Kubernetes 版本（kubelet 和 kube-proxy 版本）、Docker 版本 （如果使用了）和系统版本，这些信息由 kubelet 从 node 上获取到。\n\n使用 `kubectl get node xxx -o yaml` 可以看到 node 所有的状态的信息，其中 status 中的信息都是 kubelet 需要上报的，所以 kubelet 不止上报心跳信息还上报节点信息、节点 OOD 信息、内存磁盘压力状态、节点监控状态、是否调度等。\n\n\n\n![node 状态信息](http://cdn.tianfeiyu.com/status.png)\n\n\n\n\n\n###  二、kubelet 状态异常时的影响\n\n如果一个 node 处于非 Ready 状态超过 `pod-eviction-timeout`的值(默认为 5 分钟，在 kube-controller-manager 中定义)，在 v1.5 之前的版本中 kube-controller-manager 会 `force delete pod` 然后调度该宿主上的 pods 到其他宿主，在 v1.5 之后的版本中，kube-controller-manager 不会 `force delete pod`，pod 会一直处于`Terminating` 或`Unknown` 状态直到 node 被从 master 中删除或 kubelet 状态变为 Ready。在 node NotReady 期间，Daemonset 的 Pod 状态变为 Nodelost，Deployment、Statefulset 和 Static Pod 的状态先变为 NodeLost，然后马上变为 Unknown。Deployment 的 pod 会 recreate，Static Pod 和 Statefulset 的 Pod 会一直处于 Unknown 状态。\n\n当 kubelet 变为 Ready 状态时，Daemonset的pod不会recreate，旧pod状态直接变为Running，Deployment的则是将kubelet进程停止的Node删除，Statefulset的Pod会重新recreate，Staic Pod 会被删除。\n\n\n\n### 三、kubelet 状态上报的实现\n\nkubelet 有两种上报状态的方式，第一种定期向 apiserver 发送心跳消息，简单理解就是启动一个 goroutine 然后定期向 APIServer 发送消息。\n\n第二中被称为 NodeLease，在 v1.13 之前的版本中，节点的心跳只有 NodeStatus，从 v1.13 开始，NodeLease feature 作为 alpha 特性引入。当启用 NodeLease feature 时，每个节点在“kube-node-lease”名称空间中都有一个关联的“Lease”对象，该对象由节点定期更新，NodeStatus 和 NodeLease 都被视为来自节点的心跳。NodeLease 会频繁更新，而只有在 NodeStatus 发生改变或者超过了一定时间(默认值为1分钟，node-monitor-grace-period 的默认值为 40s)，才会将 NodeStatus 上报给 master。由于 NodeLease 比 NodeStatus 更轻量级，该特性在集群规模扩展性和性能上有明显提升。本文主要分析第一种上报方式的实现。\n\n\n\n> kubernetes 版本 ：v1.13\n\n\n\nkubelet 上报状态的代码大部分在 `kubernetes/pkg/kubelet/kubelet_node_status.go` 中实现。状态上报的功能是在 `kubernetes/pkg/kubelet/kubelet.go#Run` 方法以 goroutine 形式中启动的，kubelet 中多个重要的功能都是在该方法中启动的。\n\n `kubernetes/pkg/kubelet/kubelet.go#Run`\n\n```\nfunc (kl *Kubelet) Run(updates <-chan kubetypes.PodUpdate) {  \n  \t// ...\n  \tif kl.kubeClient != nil {\n        // Start syncing node status immediately, this may set up things the runtime needs to run.\n        go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)\n        go kl.fastStatusUpdateOnce()\n\t\t\t\t\n\t    // 一种新的状态上报方式\n\t    // start syncing lease\n        if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) {\n            go kl.nodeLeaseController.Run(wait.NeverStop)\n        }\n    }\n\t\t// ...    \n}\n```\n\nkl.syncNodeStatus 便是上报状态的，此处 kl.nodeStatusUpdateFrequency 使用的是默认设置的 10s，也就是说节点间同步状态的函数 kl.syncNodeStatus 每 10s 执行一次。\n\n\n\nsyncNodeStatus 是状态上报的入口函数，其后所调用的多个函数也都是在同一个文件中实现的。\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#syncNodeStatus`\n\n```\nfunc (kl *Kubelet) syncNodeStatus() {\n    kl.syncNodeStatusMux.Lock()\n    defer kl.syncNodeStatusMux.Unlock()\n\n    if kl.kubeClient == nil || kl.heartbeatClient == nil {\n        return\n    }\n    \n    // 是否为注册节点\n    if kl.registerNode {\n        // This will exit immediately if it doesn't need to do anything.\n        kl.registerWithAPIServer()\n    }\n    if err := kl.updateNodeStatus(); err != nil {\n        klog.Errorf(\"Unable to update node status: %v\", err)\n    }\n}\n```\n\nsyncNodeStatus 调用 updateNodeStatus， 然后又调用 tryUpdateNodeStatus 来进行上报操作，而最终调用的是 setNodeStatus。这里还进行了同步状态判断，如果是注册节点，则执行 registerWithAPIServer，否则，执行 updateNodeStatus。\n\n\n\nupdateNodeStatus 主要是调用 tryUpdateNodeStatus 进行后续的操作，该函数中定义了状态上报重试的次数，nodeStatusUpdateRetry 默认定义为 5 次。\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#updateNodeStatus`\n\n```\nfunc (kl *Kubelet) updateNodeStatus() error {\n    klog.V(5).Infof(\"Updating node status\")\n    for i := 0; i < nodeStatusUpdateRetry; i++ {\n        if err := kl.tryUpdateNodeStatus(i); err != nil {\n            if i > 0 && kl.onRepeatedHeartbeatFailure != nil {\n                kl.onRepeatedHeartbeatFailure()\n            }\n            klog.Errorf(\"Error updating node status, will retry: %v\", err)\n        } else {\n            return nil\n        }\n    }\n    return fmt.Errorf(\"update node status exceeds retry count\")\n}\n```\n\n\n\ntryUpdateNodeStatus 是主要的上报逻辑，先给 node 设置状态，然后上报 node 的状态到 master。\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#tryUpdateNodeStatus`\n\n```\nfunc (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error {\n\topts := metav1.GetOptions{}\n\tif tryNumber == 0 {\n\t\tutil.FromApiserverCache(&opts)\n\t}\n\t\n\t// 获取 node 信息\n\tnode, err := kl.heartbeatClient.CoreV1().Nodes().Get(string(kl.nodeName), opts)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error getting node %q: %v\", kl.nodeName, err)\n\t}\n\n\toriginalNode := node.DeepCopy()\n\tif originalNode == nil {\n\t\treturn fmt.Errorf(\"nil %q node object\", kl.nodeName)\n\t}\n\n\tpodCIDRChanged := false\n\tif node.Spec.PodCIDR != \"\" {\n\t\tif podCIDRChanged, err = kl.updatePodCIDR(node.Spec.PodCIDR); err != nil {\n\t\t\tklog.Errorf(err.Error())\n\t\t}\n\t}\n\n\t// 设置 node 状态\n\tkl.setNodeStatus(node)\n\n\tnow := kl.clock.Now()\n\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) && now.Before(kl.lastStatusReportTime.Add(kl.nodeStatusReportFrequency)) {\n\t\tif !podCIDRChanged && !nodeStatusHasChanged(&originalNode.Status, &node.Status) {\n\t\t\tkl.volumeManager.MarkVolumesAsReportedInUse(node.Status.VolumesInUse)\n\t\t\treturn nil\n\t\t}\n\t}\n\n    // 更新 node 信息到 master\n\t// Patch the current status on the API server\n\tupdatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node)\n\tif err != nil {\n\t\treturn err\n\t}\n\tkl.lastStatusReportTime = now\n\tkl.setLastObservedNodeAddresses(updatedNode.Status.Addresses)\n\t// If update finishes successfully, mark the volumeInUse as reportedInUse to indicate\n\t// those volumes are already updated in the node's status\n\tkl.volumeManager.MarkVolumesAsReportedInUse(updatedNode.Status.VolumesInUse)\n\treturn nil\n}\n```\n\ntryUpdateNodeStatus 中调用 setNodeStatus 设置 node 的状态。setNodeStatus 会获取一次 node 的所有状态，然后会将 kubelet 中保存的所有状态改为最新的值，也就是会重置 node status 中的所有字段。\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#setNodeStatus`\n\n```\nfunc (kl *Kubelet) setNodeStatus(node *v1.Node) {\n    for i, f := range kl.setNodeStatusFuncs {\n        klog.V(5).Infof(\"Setting node status at position %v\", i)\n        if err := f(node); err != nil {\n            klog.Warningf(\"Failed to set some node status fields: %s\", err)\n        }\n    }\n}\n```\n\nsetNodeStatus 通过 setNodeStatusFuncs 方法覆盖 node 结构体中所有的字段，setNodeStatusFuncs 是在\n\nNewMainKubelet(pkg/kubelet/kubelet.go) 中初始化的。\n\n`kubernetes/pkg/kubelet/kubelet.go#NewMainKubelet`\n\n```\n func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,\n \t\t// ...\n \t\t// Generating the status funcs should be the last thing we do,\n    klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()\n\n    return klet, nil\n}\n```\n\n\n\ndefaultNodeStatusFuncs 是生成状态的函数，通过获取 node 的所有状态指标后使用工厂函数生成状态\n\n`kubernetes/pkg/kubelet/kubelet_node_status.go#defaultNodeStatusFuncs`\n\n```\nfunc (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error {\n    // if cloud is not nil, we expect the cloud resource sync manager to exist\n    var nodeAddressesFunc func() ([]v1.NodeAddress, error)\n    if kl.cloud != nil {\n        nodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses\n    }\n    var validateHostFunc func() error\n    if kl.appArmorValidator != nil {\n        validateHostFunc = kl.appArmorValidator.ValidateHost\n    }\n    var setters []func(n *v1.Node) error\n    setters = append(setters,\n        nodestatus.NodeAddress(kl.nodeIP, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc),\n        nodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity,\n            kl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent),\n        nodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version),\n        nodestatus.DaemonEndpoints(kl.daemonEndpoints),\n        nodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList),\n        nodestatus.GoRuntime(),\n    )\n    if utilfeature.DefaultFeatureGate.Enabled(features.AttachVolumeLimit) {\n        setters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits))\n    }\n    setters = append(setters,\n        nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),\n        nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),\n        nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),\n        nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors, validateHostFunc, kl.containerManager.  Status, kl.recordNodeStatusEvent),\n        nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),\n        nodestatus.RemoveOutOfDiskCondition(),\n        // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event\n        // and record state back to the Kubelet runtime object. In the future, I'd like to isolate\n        // these side-effects by decoupling the decisions to send events and partial status recording\n        // from the Node setters.\n        kl.recordNodeSchedulableEvent,\n    )\n    return setters\n}\n```\n\ndefaultNodeStatusFuncs 可以看到 node 上报的所有信息，主要有 MemoryPressureCondition、DiskPressureCondition、PIDPressureCondition、ReadyCondition 等。每一种 nodestatus 都返回一个 setters，所有 setters 的定义在 pkg/kubelet/nodestatus/setters.go 文件中。\n\n对于二次开发而言，如果我们需要 APIServer 掌握更多的 Node 信息，可以在此处添加自定义函数，例如，上报磁盘信息等。\n\n\n\ntryUpdateNodeStatus 中最后调用 PatchNodeStatus 上报 node 的状态到 master。\n\n`kubernetes/pkg/util/node/node.go#PatchNodeStatus`\n\n```\n// PatchNodeStatus patches node status.\nfunc PatchNodeStatus(c v1core.CoreV1Interface, nodeName types.NodeName, oldNode *v1.Node, newNode *v1.Node) (*v1.Node, []byte, error) {\n\t\t// 计算 patch \n    patchBytes, err := preparePatchBytesforNodeStatus(nodeName, oldNode, newNode)\n    if err != nil {\n        return nil, nil, err\n    }\n\n    updatedNode, err := c.Nodes().Patch(string(nodeName), types.StrategicMergePatchType, patchBytes,       \"status\")\n    if err != nil {\n        return nil, nil, fmt.Errorf(\"failed to patch status %q for node %q: %v\", patchBytes, nodeName, err)\n    }\n    return updatedNode, patchBytes, nil\n}\n```\n\n在 PatchNodeStatus 会调用已注册的那些方法将状态把状态发给 APIServer。\n\n\n\n### 四、总结\n\n本文主要讲述了 kubelet 上报状态的方式及其实现，node 状态上报的方式目前有两种，本文仅分析了第一种状态上报的方式。在大规模集群中由于节点数量比较多，所有 node 都频繁报状态对 etcd 会有一定的压力，当 node 与 master 通信时由于网络导致心跳上报失败也会影响 node 的状态，为了避免类似问题的出现才有 NodeLease 方式，对于该功能的实现后文会继续进行分析。\n\n\n参考：\nhttps://www.qikqiak.com/post/kubelet-sync-node-status/\nhttps://www.jianshu.com/p/054450557818\nhttps://blog.csdn.net/shida_csdn/article/details/84286058\nhttps://kubernetes.io/docs/concepts/architecture/nodes/\n","slug":"node_status","published":1,"updated":"2019-06-09T12:49:18.948Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck21ro59v001mapwnc05c8k3n","content":"<p> 分布式系统中服务端会通过心跳机制确认客户端是否存活，在 k8s 中，kubelet 也会定时上报心跳到 apiserver，以此判断该 node 是否存活，若 node 超过一定时间没有上报心跳，其状态会被置为 NotReady，宿主上容器的状态也会被置为 Nodelost 或者 Unknown 状态。kubelet 自身会定期更新状态到 apiserver，通过参数 <code>--node-status-update-frequency</code> 指定上报频率，默认是 10s 上报一次，kubelet 不止上报心跳信息还会上报自身的一些数据信息。</p>\n<h3 id=\"一、kubelet-上报哪些状态\"><a href=\"#一、kubelet-上报哪些状态\" class=\"headerlink\" title=\"一、kubelet 上报哪些状态\"></a>一、kubelet 上报哪些状态</h3><p> 在 k8s 中，一个 node 的状态包含以下几个信息：</p>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#addresses\" target=\"_blank\" rel=\"noopener\">Addresses</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#condition\" target=\"_blank\" rel=\"noopener\">Condition</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#capacity\" target=\"_blank\" rel=\"noopener\">Capacity</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#info\" target=\"_blank\" rel=\"noopener\">Info</a></li>\n</ul>\n<h5 id=\"1、Addresses\"><a href=\"#1、Addresses\" class=\"headerlink\" title=\"1、Addresses\"></a>1、Addresses</h5><p>主要包含以下几个字段：</p>\n<ul>\n<li>HostName：Hostname 。可以通过 kubelet 的 <code>--hostname-override</code> 参数进行覆盖。</li>\n<li>ExternalIP：通常是可以外部路由的 node IP 地址（从集群外可访问）。</li>\n<li>InternalIP：通常是仅可在集群内部路由的 node IP 地址。</li>\n</ul>\n<h5 id=\"2、Condition\"><a href=\"#2、Condition\" class=\"headerlink\" title=\"2、Condition\"></a>2、Condition</h5><p><code>conditions</code> 字段描述了所有 <code>Running</code> nodes 的状态。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/1262158-05b37df015e527b7.png\" alt=\"condition\"></p>\n<h5 id=\"3、Capacity\"><a href=\"#3、Capacity\" class=\"headerlink\" title=\"3、Capacity\"></a>3、Capacity</h5><p>描述 node 上的可用资源：CPU、内存和可以调度到该 node 上的最大 pod 数量。</p>\n<h5 id=\"4、Info\"><a href=\"#4、Info\" class=\"headerlink\" title=\"4、Info\"></a>4、Info</h5><p>描述 node 的一些通用信息，例如内核版本、Kubernetes 版本（kubelet 和 kube-proxy 版本）、Docker 版本 （如果使用了）和系统版本，这些信息由 kubelet 从 node 上获取到。</p>\n<p>使用 <code>kubectl get node xxx -o yaml</code> 可以看到 node 所有的状态的信息，其中 status 中的信息都是 kubelet 需要上报的，所以 kubelet 不止上报心跳信息还上报节点信息、节点 OOD 信息、内存磁盘压力状态、节点监控状态、是否调度等。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/status.png\" alt=\"node 状态信息\"></p>\n<h3 id=\"二、kubelet-状态异常时的影响\"><a href=\"#二、kubelet-状态异常时的影响\" class=\"headerlink\" title=\"二、kubelet 状态异常时的影响\"></a>二、kubelet 状态异常时的影响</h3><p>如果一个 node 处于非 Ready 状态超过 <code>pod-eviction-timeout</code>的值(默认为 5 分钟，在 kube-controller-manager 中定义)，在 v1.5 之前的版本中 kube-controller-manager 会 <code>force delete pod</code> 然后调度该宿主上的 pods 到其他宿主，在 v1.5 之后的版本中，kube-controller-manager 不会 <code>force delete pod</code>，pod 会一直处于<code>Terminating</code> 或<code>Unknown</code> 状态直到 node 被从 master 中删除或 kubelet 状态变为 Ready。在 node NotReady 期间，Daemonset 的 Pod 状态变为 Nodelost，Deployment、Statefulset 和 Static Pod 的状态先变为 NodeLost，然后马上变为 Unknown。Deployment 的 pod 会 recreate，Static Pod 和 Statefulset 的 Pod 会一直处于 Unknown 状态。</p>\n<p>当 kubelet 变为 Ready 状态时，Daemonset的pod不会recreate，旧pod状态直接变为Running，Deployment的则是将kubelet进程停止的Node删除，Statefulset的Pod会重新recreate，Staic Pod 会被删除。</p>\n<h3 id=\"三、kubelet-状态上报的实现\"><a href=\"#三、kubelet-状态上报的实现\" class=\"headerlink\" title=\"三、kubelet 状态上报的实现\"></a>三、kubelet 状态上报的实现</h3><p>kubelet 有两种上报状态的方式，第一种定期向 apiserver 发送心跳消息，简单理解就是启动一个 goroutine 然后定期向 APIServer 发送消息。</p>\n<p>第二中被称为 NodeLease，在 v1.13 之前的版本中，节点的心跳只有 NodeStatus，从 v1.13 开始，NodeLease feature 作为 alpha 特性引入。当启用 NodeLease feature 时，每个节点在“kube-node-lease”名称空间中都有一个关联的“Lease”对象，该对象由节点定期更新，NodeStatus 和 NodeLease 都被视为来自节点的心跳。NodeLease 会频繁更新，而只有在 NodeStatus 发生改变或者超过了一定时间(默认值为1分钟，node-monitor-grace-period 的默认值为 40s)，才会将 NodeStatus 上报给 master。由于 NodeLease 比 NodeStatus 更轻量级，该特性在集群规模扩展性和性能上有明显提升。本文主要分析第一种上报方式的实现。</p>\n<blockquote>\n<p>kubernetes 版本 ：v1.13</p>\n</blockquote>\n<p>kubelet 上报状态的代码大部分在 <code>kubernetes/pkg/kubelet/kubelet_node_status.go</code> 中实现。状态上报的功能是在 <code>kubernetes/pkg/kubelet/kubelet.go#Run</code> 方法以 goroutine 形式中启动的，kubelet 中多个重要的功能都是在该方法中启动的。</p>\n<p> <code>kubernetes/pkg/kubelet/kubelet.go#Run</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123;  </span><br><span class=\"line\">  \t// ...</span><br><span class=\"line\">  \tif kl.kubeClient != nil &#123;</span><br><span class=\"line\">        // Start syncing node status immediately, this may set up things the runtime needs to run.</span><br><span class=\"line\">        go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)</span><br><span class=\"line\">        go kl.fastStatusUpdateOnce()</span><br><span class=\"line\">\t\t\t\t</span><br><span class=\"line\">\t    // 一种新的状态上报方式</span><br><span class=\"line\">\t    // start syncing lease</span><br><span class=\"line\">        if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123;</span><br><span class=\"line\">            go kl.nodeLeaseController.Run(wait.NeverStop)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t\t// ...    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kl.syncNodeStatus 便是上报状态的，此处 kl.nodeStatusUpdateFrequency 使用的是默认设置的 10s，也就是说节点间同步状态的函数 kl.syncNodeStatus 每 10s 执行一次。</p>\n<p>syncNodeStatus 是状态上报的入口函数，其后所调用的多个函数也都是在同一个文件中实现的。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#syncNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncNodeStatus() &#123;</span><br><span class=\"line\">    kl.syncNodeStatusMux.Lock()</span><br><span class=\"line\">    defer kl.syncNodeStatusMux.Unlock()</span><br><span class=\"line\"></span><br><span class=\"line\">    if kl.kubeClient == nil || kl.heartbeatClient == nil &#123;</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 是否为注册节点</span><br><span class=\"line\">    if kl.registerNode &#123;</span><br><span class=\"line\">        // This will exit immediately if it doesn&apos;t need to do anything.</span><br><span class=\"line\">        kl.registerWithAPIServer()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if err := kl.updateNodeStatus(); err != nil &#123;</span><br><span class=\"line\">        klog.Errorf(&quot;Unable to update node status: %v&quot;, err)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncNodeStatus 调用 updateNodeStatus， 然后又调用 tryUpdateNodeStatus 来进行上报操作，而最终调用的是 setNodeStatus。这里还进行了同步状态判断，如果是注册节点，则执行 registerWithAPIServer，否则，执行 updateNodeStatus。</p>\n<p>updateNodeStatus 主要是调用 tryUpdateNodeStatus 进行后续的操作，该函数中定义了状态上报重试的次数，nodeStatusUpdateRetry 默认定义为 5 次。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#updateNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) updateNodeStatus() error &#123;</span><br><span class=\"line\">    klog.V(5).Infof(&quot;Updating node status&quot;)</span><br><span class=\"line\">    for i := 0; i &lt; nodeStatusUpdateRetry; i++ &#123;</span><br><span class=\"line\">        if err := kl.tryUpdateNodeStatus(i); err != nil &#123;</span><br><span class=\"line\">            if i &gt; 0 &amp;&amp; kl.onRepeatedHeartbeatFailure != nil &#123;</span><br><span class=\"line\">                kl.onRepeatedHeartbeatFailure()</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            klog.Errorf(&quot;Error updating node status, will retry: %v&quot;, err)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            return nil</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return fmt.Errorf(&quot;update node status exceeds retry count&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>tryUpdateNodeStatus 是主要的上报逻辑，先给 node 设置状态，然后上报 node 的状态到 master。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#tryUpdateNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error &#123;</span><br><span class=\"line\">\topts := metav1.GetOptions&#123;&#125;</span><br><span class=\"line\">\tif tryNumber == 0 &#123;</span><br><span class=\"line\">\t\tutil.FromApiserverCache(&amp;opts)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 获取 node 信息</span><br><span class=\"line\">\tnode, err := kl.heartbeatClient.CoreV1().Nodes().Get(string(kl.nodeName), opts)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn fmt.Errorf(&quot;error getting node %q: %v&quot;, kl.nodeName, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\toriginalNode := node.DeepCopy()</span><br><span class=\"line\">\tif originalNode == nil &#123;</span><br><span class=\"line\">\t\treturn fmt.Errorf(&quot;nil %q node object&quot;, kl.nodeName)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpodCIDRChanged := false</span><br><span class=\"line\">\tif node.Spec.PodCIDR != &quot;&quot; &#123;</span><br><span class=\"line\">\t\tif podCIDRChanged, err = kl.updatePodCIDR(node.Spec.PodCIDR); err != nil &#123;</span><br><span class=\"line\">\t\t\tklog.Errorf(err.Error())</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 设置 node 状态</span><br><span class=\"line\">\tkl.setNodeStatus(node)</span><br><span class=\"line\"></span><br><span class=\"line\">\tnow := kl.clock.Now()</span><br><span class=\"line\">\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &amp;&amp; now.Before(kl.lastStatusReportTime.Add(kl.nodeStatusReportFrequency)) &#123;</span><br><span class=\"line\">\t\tif !podCIDRChanged &amp;&amp; !nodeStatusHasChanged(&amp;originalNode.Status, &amp;node.Status) &#123;</span><br><span class=\"line\">\t\t\tkl.volumeManager.MarkVolumesAsReportedInUse(node.Status.VolumesInUse)</span><br><span class=\"line\">\t\t\treturn nil</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 更新 node 信息到 master</span><br><span class=\"line\">\t// Patch the current status on the API server</span><br><span class=\"line\">\tupdatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tkl.lastStatusReportTime = now</span><br><span class=\"line\">\tkl.setLastObservedNodeAddresses(updatedNode.Status.Addresses)</span><br><span class=\"line\">\t// If update finishes successfully, mark the volumeInUse as reportedInUse to indicate</span><br><span class=\"line\">\t// those volumes are already updated in the node&apos;s status</span><br><span class=\"line\">\tkl.volumeManager.MarkVolumesAsReportedInUse(updatedNode.Status.VolumesInUse)</span><br><span class=\"line\">\treturn nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>tryUpdateNodeStatus 中调用 setNodeStatus 设置 node 的状态。setNodeStatus 会获取一次 node 的所有状态，然后会将 kubelet 中保存的所有状态改为最新的值，也就是会重置 node status 中的所有字段。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#setNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) setNodeStatus(node *v1.Node) &#123;</span><br><span class=\"line\">    for i, f := range kl.setNodeStatusFuncs &#123;</span><br><span class=\"line\">        klog.V(5).Infof(&quot;Setting node status at position %v&quot;, i)</span><br><span class=\"line\">        if err := f(node); err != nil &#123;</span><br><span class=\"line\">            klog.Warningf(&quot;Failed to set some node status fields: %s&quot;, err)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>setNodeStatus 通过 setNodeStatusFuncs 方法覆盖 node 结构体中所有的字段，setNodeStatusFuncs 是在</p>\n<p>NewMainKubelet(pkg/kubelet/kubelet.go) 中初始化的。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet.go#NewMainKubelet</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,</span><br><span class=\"line\"> \t\t// ...</span><br><span class=\"line\"> \t\t// Generating the status funcs should be the last thing we do,</span><br><span class=\"line\">    klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()</span><br><span class=\"line\"></span><br><span class=\"line\">    return klet, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>defaultNodeStatusFuncs 是生成状态的函数，通过获取 node 的所有状态指标后使用工厂函数生成状态</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#defaultNodeStatusFuncs</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error &#123;</span><br><span class=\"line\">    // if cloud is not nil, we expect the cloud resource sync manager to exist</span><br><span class=\"line\">    var nodeAddressesFunc func() ([]v1.NodeAddress, error)</span><br><span class=\"line\">    if kl.cloud != nil &#123;</span><br><span class=\"line\">        nodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    var validateHostFunc func() error</span><br><span class=\"line\">    if kl.appArmorValidator != nil &#123;</span><br><span class=\"line\">        validateHostFunc = kl.appArmorValidator.ValidateHost</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    var setters []func(n *v1.Node) error</span><br><span class=\"line\">    setters = append(setters,</span><br><span class=\"line\">        nodestatus.NodeAddress(kl.nodeIP, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc),</span><br><span class=\"line\">        nodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity,</span><br><span class=\"line\">            kl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent),</span><br><span class=\"line\">        nodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version),</span><br><span class=\"line\">        nodestatus.DaemonEndpoints(kl.daemonEndpoints),</span><br><span class=\"line\">        nodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList),</span><br><span class=\"line\">        nodestatus.GoRuntime(),</span><br><span class=\"line\">    )</span><br><span class=\"line\">    if utilfeature.DefaultFeatureGate.Enabled(features.AttachVolumeLimit) &#123;</span><br><span class=\"line\">        setters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    setters = append(setters,</span><br><span class=\"line\">        nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),</span><br><span class=\"line\">        nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),</span><br><span class=\"line\">        nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),</span><br><span class=\"line\">        nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors, validateHostFunc, kl.containerManager.  Status, kl.recordNodeStatusEvent),</span><br><span class=\"line\">        nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),</span><br><span class=\"line\">        nodestatus.RemoveOutOfDiskCondition(),</span><br><span class=\"line\">        // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event</span><br><span class=\"line\">        // and record state back to the Kubelet runtime object. In the future, I&apos;d like to isolate</span><br><span class=\"line\">        // these side-effects by decoupling the decisions to send events and partial status recording</span><br><span class=\"line\">        // from the Node setters.</span><br><span class=\"line\">        kl.recordNodeSchedulableEvent,</span><br><span class=\"line\">    )</span><br><span class=\"line\">    return setters</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>defaultNodeStatusFuncs 可以看到 node 上报的所有信息，主要有 MemoryPressureCondition、DiskPressureCondition、PIDPressureCondition、ReadyCondition 等。每一种 nodestatus 都返回一个 setters，所有 setters 的定义在 pkg/kubelet/nodestatus/setters.go 文件中。</p>\n<p>对于二次开发而言，如果我们需要 APIServer 掌握更多的 Node 信息，可以在此处添加自定义函数，例如，上报磁盘信息等。</p>\n<p>tryUpdateNodeStatus 中最后调用 PatchNodeStatus 上报 node 的状态到 master。</p>\n<p><code>kubernetes/pkg/util/node/node.go#PatchNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// PatchNodeStatus patches node status.</span><br><span class=\"line\">func PatchNodeStatus(c v1core.CoreV1Interface, nodeName types.NodeName, oldNode *v1.Node, newNode *v1.Node) (*v1.Node, []byte, error) &#123;</span><br><span class=\"line\">\t\t// 计算 patch </span><br><span class=\"line\">    patchBytes, err := preparePatchBytesforNodeStatus(nodeName, oldNode, newNode)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return nil, nil, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    updatedNode, err := c.Nodes().Patch(string(nodeName), types.StrategicMergePatchType, patchBytes,       &quot;status&quot;)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return nil, nil, fmt.Errorf(&quot;failed to patch status %q for node %q: %v&quot;, patchBytes, nodeName, err)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return updatedNode, patchBytes, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 PatchNodeStatus 会调用已注册的那些方法将状态把状态发给 APIServer。</p>\n<h3 id=\"四、总结\"><a href=\"#四、总结\" class=\"headerlink\" title=\"四、总结\"></a>四、总结</h3><p>本文主要讲述了 kubelet 上报状态的方式及其实现，node 状态上报的方式目前有两种，本文仅分析了第一种状态上报的方式。在大规模集群中由于节点数量比较多，所有 node 都频繁报状态对 etcd 会有一定的压力，当 node 与 master 通信时由于网络导致心跳上报失败也会影响 node 的状态，为了避免类似问题的出现才有 NodeLease 方式，对于该功能的实现后文会继续进行分析。</p>\n<p>参考：<br><a href=\"https://www.qikqiak.com/post/kubelet-sync-node-status/\" target=\"_blank\" rel=\"noopener\">https://www.qikqiak.com/post/kubelet-sync-node-status/</a><br><a href=\"https://www.jianshu.com/p/054450557818\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/054450557818</a><br><a href=\"https://blog.csdn.net/shida_csdn/article/details/84286058\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/shida_csdn/article/details/84286058</a><br><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/architecture/nodes/</a></p>\n<div><br/><h1>相关推荐<span style=\"font-size:0.45em; color:gray\"></span></h1><ul><li><a href=\"http://yoursite.com/2019/02/26/k8s_events/\">kubernets 中事件处理机制</a></li><li><a href=\"http://yoursite.com/2019/01/03/kubelet_create_pod/\">kubelet 创建 pod 的流程</a></li><li><a href=\"http://yoursite.com/2018/12/23/kubelet_init/\">kubelet 启动流程分析</a></li></ul></div>","site":{"data":{"recommended_posts":{"http://yoursite.com/2019/01/03/kubelet_create_pod/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/04/14/k8s_metrics_server/":[],"http://yoursite.com/2019/07/12/k8s_components_ha/":[{"title":"kubernetes从入门到放弃--k8s基本概念","permalink":"https://hanyajun.com/kubernetes/kubenetes_concept/"}],"http://yoursite.com/2019/02/16/k8s-crontab/":[{"title":"Linux 定时任务与 crontab 简介","permalink":"https://abelsu7.top/2019/05/08/crontab-intro/"}],"http://yoursite.com/2018/12/05/docker-introduces/":[],"http://yoursite.com/2019/03/05/k8s_v1.12/":[],"http://yoursite.com/2019/04/22/k8s_dashboard_prometheus/":[],"http://yoursite.com/2019/08/18/k8s_auth_rbac/":[],"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"}],"http://yoursite.com/2018/09/02/kubernetes-api/":[],"http://yoursite.com/2019/07/02/k8s_crd_verify/":[{"title":"使用 code-generator 为 CustomResources 生成代码","permalink":"http://yoursite.com/2019/08/06/code_generator/"}],"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/":[{"title":"kube-on-kube-operator 开发(三)","permalink":"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2017/03/02/etcd-backup/":[],"http://yoursite.com/2018/12/16/kubelet-modules/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2019/02/26/k8s_events/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2017/03/15/etcd-enable-https/":[],"http://yoursite.com/2019/01/09/kubeconfig/":[],"http://yoursite.com/2019/09/01/kube_on_kube_operator_3/":[{"title":"kube-on-kube-operator 开发(二)","permalink":"http://yoursite.com/2019/08/07/kube_on_kube_operator_2/"},{"title":"kube-on-kube-operator 开发(一)","permalink":"http://yoursite.com/2019/08/05/kube_on_kube_operator_1/"}],"http://yoursite.com/2019/01/30/k8s-audit-webhook/":[],"http://yoursite.com/2019/05/16/kubectl_plugin/":[],"http://yoursite.com/2019/01/17/kubeadm/":[],"http://yoursite.com/2019/06/09/node_status/":[{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"},{"title":"kubelet 启动流程分析","permalink":"http://yoursite.com/2018/12/23/kubelet_init/"}],"http://yoursite.com/2019/06/22/golang_modules/":[],"http://yoursite.com/2019/03/13/k8s_leader_election/":[],"http://yoursite.com/2019/05/17/client-go_informer/":[],"http://yoursite.com/2019/08/06/code_generator/":[{"title":"kubernetes 自定义资源（CRD）的校验","permalink":"http://yoursite.com/2019/07/02/k8s_crd_verify/"}],"http://yoursite.com/2018/12/23/kubelet_init/":[{"title":"kubelet 状态上报的方式","permalink":"http://yoursite.com/2019/06/09/node_status/"},{"title":"kubernets 中事件处理机制","permalink":"http://yoursite.com/2019/02/26/k8s_events/"},{"title":"kubelet 创建 pod 的流程","permalink":"http://yoursite.com/2019/01/03/kubelet_create_pod/"}],"http://yoursite.com/2017/02/12/kubernetes-learn/":[]}}},"excerpt":"","more":"<p> 分布式系统中服务端会通过心跳机制确认客户端是否存活，在 k8s 中，kubelet 也会定时上报心跳到 apiserver，以此判断该 node 是否存活，若 node 超过一定时间没有上报心跳，其状态会被置为 NotReady，宿主上容器的状态也会被置为 Nodelost 或者 Unknown 状态。kubelet 自身会定期更新状态到 apiserver，通过参数 <code>--node-status-update-frequency</code> 指定上报频率，默认是 10s 上报一次，kubelet 不止上报心跳信息还会上报自身的一些数据信息。</p>\n<h3 id=\"一、kubelet-上报哪些状态\"><a href=\"#一、kubelet-上报哪些状态\" class=\"headerlink\" title=\"一、kubelet 上报哪些状态\"></a>一、kubelet 上报哪些状态</h3><p> 在 k8s 中，一个 node 的状态包含以下几个信息：</p>\n<ul>\n<li><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#addresses\" target=\"_blank\" rel=\"noopener\">Addresses</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#condition\" target=\"_blank\" rel=\"noopener\">Condition</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#capacity\" target=\"_blank\" rel=\"noopener\">Capacity</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/#info\" target=\"_blank\" rel=\"noopener\">Info</a></li>\n</ul>\n<h5 id=\"1、Addresses\"><a href=\"#1、Addresses\" class=\"headerlink\" title=\"1、Addresses\"></a>1、Addresses</h5><p>主要包含以下几个字段：</p>\n<ul>\n<li>HostName：Hostname 。可以通过 kubelet 的 <code>--hostname-override</code> 参数进行覆盖。</li>\n<li>ExternalIP：通常是可以外部路由的 node IP 地址（从集群外可访问）。</li>\n<li>InternalIP：通常是仅可在集群内部路由的 node IP 地址。</li>\n</ul>\n<h5 id=\"2、Condition\"><a href=\"#2、Condition\" class=\"headerlink\" title=\"2、Condition\"></a>2、Condition</h5><p><code>conditions</code> 字段描述了所有 <code>Running</code> nodes 的状态。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/1262158-05b37df015e527b7.png\" alt=\"condition\"></p>\n<h5 id=\"3、Capacity\"><a href=\"#3、Capacity\" class=\"headerlink\" title=\"3、Capacity\"></a>3、Capacity</h5><p>描述 node 上的可用资源：CPU、内存和可以调度到该 node 上的最大 pod 数量。</p>\n<h5 id=\"4、Info\"><a href=\"#4、Info\" class=\"headerlink\" title=\"4、Info\"></a>4、Info</h5><p>描述 node 的一些通用信息，例如内核版本、Kubernetes 版本（kubelet 和 kube-proxy 版本）、Docker 版本 （如果使用了）和系统版本，这些信息由 kubelet 从 node 上获取到。</p>\n<p>使用 <code>kubectl get node xxx -o yaml</code> 可以看到 node 所有的状态的信息，其中 status 中的信息都是 kubelet 需要上报的，所以 kubelet 不止上报心跳信息还上报节点信息、节点 OOD 信息、内存磁盘压力状态、节点监控状态、是否调度等。</p>\n<p><img src=\"http://cdn.tianfeiyu.com/status.png\" alt=\"node 状态信息\"></p>\n<h3 id=\"二、kubelet-状态异常时的影响\"><a href=\"#二、kubelet-状态异常时的影响\" class=\"headerlink\" title=\"二、kubelet 状态异常时的影响\"></a>二、kubelet 状态异常时的影响</h3><p>如果一个 node 处于非 Ready 状态超过 <code>pod-eviction-timeout</code>的值(默认为 5 分钟，在 kube-controller-manager 中定义)，在 v1.5 之前的版本中 kube-controller-manager 会 <code>force delete pod</code> 然后调度该宿主上的 pods 到其他宿主，在 v1.5 之后的版本中，kube-controller-manager 不会 <code>force delete pod</code>，pod 会一直处于<code>Terminating</code> 或<code>Unknown</code> 状态直到 node 被从 master 中删除或 kubelet 状态变为 Ready。在 node NotReady 期间，Daemonset 的 Pod 状态变为 Nodelost，Deployment、Statefulset 和 Static Pod 的状态先变为 NodeLost，然后马上变为 Unknown。Deployment 的 pod 会 recreate，Static Pod 和 Statefulset 的 Pod 会一直处于 Unknown 状态。</p>\n<p>当 kubelet 变为 Ready 状态时，Daemonset的pod不会recreate，旧pod状态直接变为Running，Deployment的则是将kubelet进程停止的Node删除，Statefulset的Pod会重新recreate，Staic Pod 会被删除。</p>\n<h3 id=\"三、kubelet-状态上报的实现\"><a href=\"#三、kubelet-状态上报的实现\" class=\"headerlink\" title=\"三、kubelet 状态上报的实现\"></a>三、kubelet 状态上报的实现</h3><p>kubelet 有两种上报状态的方式，第一种定期向 apiserver 发送心跳消息，简单理解就是启动一个 goroutine 然后定期向 APIServer 发送消息。</p>\n<p>第二中被称为 NodeLease，在 v1.13 之前的版本中，节点的心跳只有 NodeStatus，从 v1.13 开始，NodeLease feature 作为 alpha 特性引入。当启用 NodeLease feature 时，每个节点在“kube-node-lease”名称空间中都有一个关联的“Lease”对象，该对象由节点定期更新，NodeStatus 和 NodeLease 都被视为来自节点的心跳。NodeLease 会频繁更新，而只有在 NodeStatus 发生改变或者超过了一定时间(默认值为1分钟，node-monitor-grace-period 的默认值为 40s)，才会将 NodeStatus 上报给 master。由于 NodeLease 比 NodeStatus 更轻量级，该特性在集群规模扩展性和性能上有明显提升。本文主要分析第一种上报方式的实现。</p>\n<blockquote>\n<p>kubernetes 版本 ：v1.13</p>\n</blockquote>\n<p>kubelet 上报状态的代码大部分在 <code>kubernetes/pkg/kubelet/kubelet_node_status.go</code> 中实现。状态上报的功能是在 <code>kubernetes/pkg/kubelet/kubelet.go#Run</code> 方法以 goroutine 形式中启动的，kubelet 中多个重要的功能都是在该方法中启动的。</p>\n<p> <code>kubernetes/pkg/kubelet/kubelet.go#Run</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123;  </span><br><span class=\"line\">  \t// ...</span><br><span class=\"line\">  \tif kl.kubeClient != nil &#123;</span><br><span class=\"line\">        // Start syncing node status immediately, this may set up things the runtime needs to run.</span><br><span class=\"line\">        go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop)</span><br><span class=\"line\">        go kl.fastStatusUpdateOnce()</span><br><span class=\"line\">\t\t\t\t</span><br><span class=\"line\">\t    // 一种新的状态上报方式</span><br><span class=\"line\">\t    // start syncing lease</span><br><span class=\"line\">        if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123;</span><br><span class=\"line\">            go kl.nodeLeaseController.Run(wait.NeverStop)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">\t\t// ...    </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>kl.syncNodeStatus 便是上报状态的，此处 kl.nodeStatusUpdateFrequency 使用的是默认设置的 10s，也就是说节点间同步状态的函数 kl.syncNodeStatus 每 10s 执行一次。</p>\n<p>syncNodeStatus 是状态上报的入口函数，其后所调用的多个函数也都是在同一个文件中实现的。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#syncNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) syncNodeStatus() &#123;</span><br><span class=\"line\">    kl.syncNodeStatusMux.Lock()</span><br><span class=\"line\">    defer kl.syncNodeStatusMux.Unlock()</span><br><span class=\"line\"></span><br><span class=\"line\">    if kl.kubeClient == nil || kl.heartbeatClient == nil &#123;</span><br><span class=\"line\">        return</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    // 是否为注册节点</span><br><span class=\"line\">    if kl.registerNode &#123;</span><br><span class=\"line\">        // This will exit immediately if it doesn&apos;t need to do anything.</span><br><span class=\"line\">        kl.registerWithAPIServer()</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    if err := kl.updateNodeStatus(); err != nil &#123;</span><br><span class=\"line\">        klog.Errorf(&quot;Unable to update node status: %v&quot;, err)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>syncNodeStatus 调用 updateNodeStatus， 然后又调用 tryUpdateNodeStatus 来进行上报操作，而最终调用的是 setNodeStatus。这里还进行了同步状态判断，如果是注册节点，则执行 registerWithAPIServer，否则，执行 updateNodeStatus。</p>\n<p>updateNodeStatus 主要是调用 tryUpdateNodeStatus 进行后续的操作，该函数中定义了状态上报重试的次数，nodeStatusUpdateRetry 默认定义为 5 次。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#updateNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) updateNodeStatus() error &#123;</span><br><span class=\"line\">    klog.V(5).Infof(&quot;Updating node status&quot;)</span><br><span class=\"line\">    for i := 0; i &lt; nodeStatusUpdateRetry; i++ &#123;</span><br><span class=\"line\">        if err := kl.tryUpdateNodeStatus(i); err != nil &#123;</span><br><span class=\"line\">            if i &gt; 0 &amp;&amp; kl.onRepeatedHeartbeatFailure != nil &#123;</span><br><span class=\"line\">                kl.onRepeatedHeartbeatFailure()</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            klog.Errorf(&quot;Error updating node status, will retry: %v&quot;, err)</span><br><span class=\"line\">        &#125; else &#123;</span><br><span class=\"line\">            return nil</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return fmt.Errorf(&quot;update node status exceeds retry count&quot;)</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>tryUpdateNodeStatus 是主要的上报逻辑，先给 node 设置状态，然后上报 node 的状态到 master。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#tryUpdateNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error &#123;</span><br><span class=\"line\">\topts := metav1.GetOptions&#123;&#125;</span><br><span class=\"line\">\tif tryNumber == 0 &#123;</span><br><span class=\"line\">\t\tutil.FromApiserverCache(&amp;opts)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t// 获取 node 信息</span><br><span class=\"line\">\tnode, err := kl.heartbeatClient.CoreV1().Nodes().Get(string(kl.nodeName), opts)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn fmt.Errorf(&quot;error getting node %q: %v&quot;, kl.nodeName, err)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\toriginalNode := node.DeepCopy()</span><br><span class=\"line\">\tif originalNode == nil &#123;</span><br><span class=\"line\">\t\treturn fmt.Errorf(&quot;nil %q node object&quot;, kl.nodeName)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\tpodCIDRChanged := false</span><br><span class=\"line\">\tif node.Spec.PodCIDR != &quot;&quot; &#123;</span><br><span class=\"line\">\t\tif podCIDRChanged, err = kl.updatePodCIDR(node.Spec.PodCIDR); err != nil &#123;</span><br><span class=\"line\">\t\t\tklog.Errorf(err.Error())</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t// 设置 node 状态</span><br><span class=\"line\">\tkl.setNodeStatus(node)</span><br><span class=\"line\"></span><br><span class=\"line\">\tnow := kl.clock.Now()</span><br><span class=\"line\">\tif utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &amp;&amp; now.Before(kl.lastStatusReportTime.Add(kl.nodeStatusReportFrequency)) &#123;</span><br><span class=\"line\">\t\tif !podCIDRChanged &amp;&amp; !nodeStatusHasChanged(&amp;originalNode.Status, &amp;node.Status) &#123;</span><br><span class=\"line\">\t\t\tkl.volumeManager.MarkVolumesAsReportedInUse(node.Status.VolumesInUse)</span><br><span class=\"line\">\t\t\treturn nil</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    // 更新 node 信息到 master</span><br><span class=\"line\">\t// Patch the current status on the API server</span><br><span class=\"line\">\tupdatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node)</span><br><span class=\"line\">\tif err != nil &#123;</span><br><span class=\"line\">\t\treturn err</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tkl.lastStatusReportTime = now</span><br><span class=\"line\">\tkl.setLastObservedNodeAddresses(updatedNode.Status.Addresses)</span><br><span class=\"line\">\t// If update finishes successfully, mark the volumeInUse as reportedInUse to indicate</span><br><span class=\"line\">\t// those volumes are already updated in the node&apos;s status</span><br><span class=\"line\">\tkl.volumeManager.MarkVolumesAsReportedInUse(updatedNode.Status.VolumesInUse)</span><br><span class=\"line\">\treturn nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>tryUpdateNodeStatus 中调用 setNodeStatus 设置 node 的状态。setNodeStatus 会获取一次 node 的所有状态，然后会将 kubelet 中保存的所有状态改为最新的值，也就是会重置 node status 中的所有字段。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#setNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) setNodeStatus(node *v1.Node) &#123;</span><br><span class=\"line\">    for i, f := range kl.setNodeStatusFuncs &#123;</span><br><span class=\"line\">        klog.V(5).Infof(&quot;Setting node status at position %v&quot;, i)</span><br><span class=\"line\">        if err := f(node); err != nil &#123;</span><br><span class=\"line\">            klog.Warningf(&quot;Failed to set some node status fields: %s&quot;, err)</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>setNodeStatus 通过 setNodeStatusFuncs 方法覆盖 node 结构体中所有的字段，setNodeStatusFuncs 是在</p>\n<p>NewMainKubelet(pkg/kubelet/kubelet.go) 中初始化的。</p>\n<p><code>kubernetes/pkg/kubelet/kubelet.go#NewMainKubelet</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,</span><br><span class=\"line\"> \t\t// ...</span><br><span class=\"line\"> \t\t// Generating the status funcs should be the last thing we do,</span><br><span class=\"line\">    klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()</span><br><span class=\"line\"></span><br><span class=\"line\">    return klet, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>defaultNodeStatusFuncs 是生成状态的函数，通过获取 node 的所有状态指标后使用工厂函数生成状态</p>\n<p><code>kubernetes/pkg/kubelet/kubelet_node_status.go#defaultNodeStatusFuncs</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">func (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error &#123;</span><br><span class=\"line\">    // if cloud is not nil, we expect the cloud resource sync manager to exist</span><br><span class=\"line\">    var nodeAddressesFunc func() ([]v1.NodeAddress, error)</span><br><span class=\"line\">    if kl.cloud != nil &#123;</span><br><span class=\"line\">        nodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    var validateHostFunc func() error</span><br><span class=\"line\">    if kl.appArmorValidator != nil &#123;</span><br><span class=\"line\">        validateHostFunc = kl.appArmorValidator.ValidateHost</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    var setters []func(n *v1.Node) error</span><br><span class=\"line\">    setters = append(setters,</span><br><span class=\"line\">        nodestatus.NodeAddress(kl.nodeIP, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc),</span><br><span class=\"line\">        nodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity,</span><br><span class=\"line\">            kl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent),</span><br><span class=\"line\">        nodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version),</span><br><span class=\"line\">        nodestatus.DaemonEndpoints(kl.daemonEndpoints),</span><br><span class=\"line\">        nodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList),</span><br><span class=\"line\">        nodestatus.GoRuntime(),</span><br><span class=\"line\">    )</span><br><span class=\"line\">    if utilfeature.DefaultFeatureGate.Enabled(features.AttachVolumeLimit) &#123;</span><br><span class=\"line\">        setters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits))</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    setters = append(setters,</span><br><span class=\"line\">        nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),</span><br><span class=\"line\">        nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),</span><br><span class=\"line\">        nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),</span><br><span class=\"line\">        nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors, validateHostFunc, kl.containerManager.  Status, kl.recordNodeStatusEvent),</span><br><span class=\"line\">        nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),</span><br><span class=\"line\">        nodestatus.RemoveOutOfDiskCondition(),</span><br><span class=\"line\">        // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event</span><br><span class=\"line\">        // and record state back to the Kubelet runtime object. In the future, I&apos;d like to isolate</span><br><span class=\"line\">        // these side-effects by decoupling the decisions to send events and partial status recording</span><br><span class=\"line\">        // from the Node setters.</span><br><span class=\"line\">        kl.recordNodeSchedulableEvent,</span><br><span class=\"line\">    )</span><br><span class=\"line\">    return setters</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>defaultNodeStatusFuncs 可以看到 node 上报的所有信息，主要有 MemoryPressureCondition、DiskPressureCondition、PIDPressureCondition、ReadyCondition 等。每一种 nodestatus 都返回一个 setters，所有 setters 的定义在 pkg/kubelet/nodestatus/setters.go 文件中。</p>\n<p>对于二次开发而言，如果我们需要 APIServer 掌握更多的 Node 信息，可以在此处添加自定义函数，例如，上报磁盘信息等。</p>\n<p>tryUpdateNodeStatus 中最后调用 PatchNodeStatus 上报 node 的状态到 master。</p>\n<p><code>kubernetes/pkg/util/node/node.go#PatchNodeStatus</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// PatchNodeStatus patches node status.</span><br><span class=\"line\">func PatchNodeStatus(c v1core.CoreV1Interface, nodeName types.NodeName, oldNode *v1.Node, newNode *v1.Node) (*v1.Node, []byte, error) &#123;</span><br><span class=\"line\">\t\t// 计算 patch </span><br><span class=\"line\">    patchBytes, err := preparePatchBytesforNodeStatus(nodeName, oldNode, newNode)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return nil, nil, err</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    updatedNode, err := c.Nodes().Patch(string(nodeName), types.StrategicMergePatchType, patchBytes,       &quot;status&quot;)</span><br><span class=\"line\">    if err != nil &#123;</span><br><span class=\"line\">        return nil, nil, fmt.Errorf(&quot;failed to patch status %q for node %q: %v&quot;, patchBytes, nodeName, err)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    return updatedNode, patchBytes, nil</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>在 PatchNodeStatus 会调用已注册的那些方法将状态把状态发给 APIServer。</p>\n<h3 id=\"四、总结\"><a href=\"#四、总结\" class=\"headerlink\" title=\"四、总结\"></a>四、总结</h3><p>本文主要讲述了 kubelet 上报状态的方式及其实现，node 状态上报的方式目前有两种，本文仅分析了第一种状态上报的方式。在大规模集群中由于节点数量比较多，所有 node 都频繁报状态对 etcd 会有一定的压力，当 node 与 master 通信时由于网络导致心跳上报失败也会影响 node 的状态，为了避免类似问题的出现才有 NodeLease 方式，对于该功能的实现后文会继续进行分析。</p>\n<p>参考：<br><a href=\"https://www.qikqiak.com/post/kubelet-sync-node-status/\" target=\"_blank\" rel=\"noopener\">https://www.qikqiak.com/post/kubelet-sync-node-status/</a><br><a href=\"https://www.jianshu.com/p/054450557818\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/054450557818</a><br><a href=\"https://blog.csdn.net/shida_csdn/article/details/84286058\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/shida_csdn/article/details/84286058</a><br><a href=\"https://kubernetes.io/docs/concepts/architecture/nodes/\" target=\"_blank\" rel=\"noopener\">https://kubernetes.io/docs/concepts/architecture/nodes/</a></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ck21ro58y0000apwncnpi87h9","tag_id":"ck21ro5940002apwnovc4co4z","_id":"ck21ro59a000bapwn7zed52qf"},{"post_id":"ck21ro58y0000apwncnpi87h9","tag_id":"ck21ro5970006apwnuzdrvpp6","_id":"ck21ro59b000dapwn4r5eoo8a"},{"post_id":"ck21ro5950003apwndnp522i3","tag_id":"ck21ro5990009apwnfspzdc26","_id":"ck21ro59e000japwn52lznrs5"},{"post_id":"ck21ro5950003apwndnp522i3","tag_id":"ck21ro59b000eapwnkqn9kox2","_id":"ck21ro59f000lapwnx07oo1h7"},{"post_id":"ck21ro5960005apwnte4gfbr0","tag_id":"ck21ro59d000hapwn1hr3dahw","_id":"ck21ro59i000sapwnm035hly7"},{"post_id":"ck21ro5960005apwnte4gfbr0","tag_id":"ck21ro59g000napwnt8qm3xyh","_id":"ck21ro59j000uapwnvejqr6mv"},{"post_id":"ck21ro5980008apwnvcp843r8","tag_id":"ck21ro59h000qapwnw8199n2e","_id":"ck21ro59k000xapwn5az0rqfc"},{"post_id":"ck21ro599000aapwni8fjlsrl","tag_id":"ck21ro59j000vapwnm0tajg4t","_id":"ck21ro59r0018apwnq4lje3vv"},{"post_id":"ck21ro599000aapwni8fjlsrl","tag_id":"ck21ro59l000zapwn26w5suju","_id":"ck21ro59r001aapwn0n4j353a"},{"post_id":"ck21ro599000aapwni8fjlsrl","tag_id":"ck21ro59o0013apwnhk3h4fyu","_id":"ck21ro59s001dapwnmh9045ad"},{"post_id":"ck21ro59a000capwn0txujkah","tag_id":"ck21ro59q0016apwn15xh3iux","_id":"ck21ro59u001hapwnenlxy13n"},{"post_id":"ck21ro59a000capwn0txujkah","tag_id":"ck21ro59s001bapwndbkbalfg","_id":"ck21ro59u001japwn4938yo75"},{"post_id":"ck21ro59c000fapwn06p4tnjb","tag_id":"ck21ro59t001fapwn4wmrp9ip","_id":"ck21ro59w001papwnl2wu016s"},{"post_id":"ck21ro59c000fapwn06p4tnjb","tag_id":"ck21ro59v001lapwnwa2cirhi","_id":"ck21ro59w001qapwn8nyvlqk5"},{"post_id":"ck21ro59c000fapwn06p4tnjb","tag_id":"ck21ro59w001napwnpx2b464x","_id":"ck21ro59x001sapwnhaldcks6"},{"post_id":"ck21ro59c000gapwnna87fzyr","tag_id":"ck21ro5970006apwnuzdrvpp6","_id":"ck21ro59x001tapwnqsuxqixz"},{"post_id":"ck21ro59c000gapwnna87fzyr","tag_id":"ck21ro59w001oapwn833wu8ev","_id":"ck21ro59x001vapwne34dlyob"},{"post_id":"ck21ro59d000iapwn4owb2d6g","tag_id":"ck21ro59w001rapwn8edg46hs","_id":"ck21ro59y001xapwn3ta547xm"},{"post_id":"ck21ro59d000iapwn4owb2d6g","tag_id":"ck21ro59x001uapwnbbz22xz0","_id":"ck21ro59y001yapwn6crs57qj"},{"post_id":"ck21ro59e000kapwnp91sazgv","tag_id":"ck21ro59x001wapwn903tgk7g","_id":"ck21ro5a00021apwnf2i1uquw"},{"post_id":"ck21ro59e000kapwnp91sazgv","tag_id":"ck21ro59g000napwnt8qm3xyh","_id":"ck21ro5a00022apwnfqbx32g0"},{"post_id":"ck21ro59f000mapwnd36ph6g8","tag_id":"ck21ro5a00020apwn7tqa2a64","_id":"ck21ro5a10025apwnizbkx5zc"},{"post_id":"ck21ro59f000mapwnd36ph6g8","tag_id":"ck21ro5a00023apwn1a22aih4","_id":"ck21ro5a10026apwny7agnrkx"},{"post_id":"ck21ro59g000oapwn9m1mzxxy","tag_id":"ck21ro5a10024apwnr9gppkpm","_id":"ck21ro5a10029apwnzn7esdni"},{"post_id":"ck21ro59g000oapwn9m1mzxxy","tag_id":"ck21ro5a10027apwnk760s8q9","_id":"ck21ro5a2002aapwn8dmryfdo"},{"post_id":"ck21ro59h000papwnwstrw469","tag_id":"ck21ro5a10028apwndwtzchlg","_id":"ck21ro5a2002capwnr8wb7u0f"},{"post_id":"ck21ro59i000rapwn32b2jula","tag_id":"ck21ro5a2002bapwnim0bgylz","_id":"ck21ro5a3002eapwnxtfg2bg9"},{"post_id":"ck21ro59i000tapwnroqcz5fp","tag_id":"ck21ro59x001wapwn903tgk7g","_id":"ck21ro5a4002hapwnnoqrrgcy"},{"post_id":"ck21ro59i000tapwnroqcz5fp","tag_id":"ck21ro5a3002fapwnt8iuq11y","_id":"ck21ro5a4002iapwnys6pfz0i"},{"post_id":"ck21ro59j000wapwnb44cupgf","tag_id":"ck21ro5a3002gapwnw4uf2gma","_id":"ck21ro5a4002lapwngj46kf9j"},{"post_id":"ck21ro59j000wapwnb44cupgf","tag_id":"ck21ro5a4002japwndodjyv15","_id":"ck21ro5a4002mapwn1lab4wz9"},{"post_id":"ck21ro59k000yapwn2sicnf0w","tag_id":"ck21ro5a4002kapwnkynxp99f","_id":"ck21ro5a5002papwnnkolvcd5"},{"post_id":"ck21ro59k000yapwn2sicnf0w","tag_id":"ck21ro5a4002napwn3mkmdjok","_id":"ck21ro5a5002qapwnpf92z4vd"},{"post_id":"ck21ro59l0010apwn0xhnvj2v","tag_id":"ck21ro5a3002gapwnw4uf2gma","_id":"ck21ro5a6002tapwnzqyhtn7z"},{"post_id":"ck21ro59l0010apwn0xhnvj2v","tag_id":"ck21ro5a4002japwndodjyv15","_id":"ck21ro5a6002uapwntsoxkjbq"},{"post_id":"ck21ro59n0011apwnopz4pp34","tag_id":"ck21ro5a6002sapwno6p1ox36","_id":"ck21ro5a7002wapwn6l1hxb3z"},{"post_id":"ck21ro59o0012apwn3j4u0rp8","tag_id":"ck21ro5a3002gapwnw4uf2gma","_id":"ck21ro5a8002zapwnc37xqnp7"},{"post_id":"ck21ro59o0012apwn3j4u0rp8","tag_id":"ck21ro5a4002japwndodjyv15","_id":"ck21ro5a80030apwnwy1ii4dw"},{"post_id":"ck21ro59o0014apwng6dgzqzx","tag_id":"ck21ro5a7002yapwnsk2z47ve","_id":"ck21ro5a80032apwnl6d4yp4u"},{"post_id":"ck21ro59p0015apwng52xrzxh","tag_id":"ck21ro5a80031apwnacygnkig","_id":"ck21ro5a80034apwn02ieooml"},{"post_id":"ck21ro59q0017apwnxjy2qojx","tag_id":"ck21ro5a80033apwnmh66d62p","_id":"ck21ro5a90036apwn5sovohnn"},{"post_id":"ck21ro59r0019apwn9y2gd0sn","tag_id":"ck21ro5a80035apwnro9kq34y","_id":"ck21ro5a90038apwneje42xf5"},{"post_id":"ck21ro59s001capwn30fsvydm","tag_id":"ck21ro5a00023apwn1a22aih4","_id":"ck21ro5aa003aapwn9lf7ssae"},{"post_id":"ck21ro59s001eapwnglnk7khk","tag_id":"ck21ro5a00023apwn1a22aih4","_id":"ck21ro5aa003capwn1k9kwyr7"},{"post_id":"ck21ro59t001gapwn64qwq40a","tag_id":"ck21ro5a00023apwn1a22aih4","_id":"ck21ro5ab003eapwnjoc7abpi"},{"post_id":"ck21ro59v001mapwnc05c8k3n","tag_id":"ck21ro5ab003dapwn581c3e1v","_id":"ck21ro5ac003gapwnud07ov71"},{"post_id":"ck21ro59v001mapwnc05c8k3n","tag_id":"ck21ro5a00023apwn1a22aih4","_id":"ck21ro5ac003hapwn0n3bxemr"}],"Tag":[{"name":"code-generator","_id":"ck21ro5940002apwnovc4co4z"},{"name":"crd","_id":"ck21ro5970006apwnuzdrvpp6"},{"name":"client-go","_id":"ck21ro5990009apwnfspzdc26"},{"name":"informer","_id":"ck21ro59b000eapwnkqn9kox2"},{"name":"etcd","_id":"ck21ro59d000hapwn1hr3dahw"},{"name":"improvements","_id":"ck21ro59g000napwnt8qm3xyh"},{"name":"go module","_id":"ck21ro59h000qapwnw8199n2e"},{"name":"crontab","_id":"ck21ro59j000vapwnm0tajg4t"},{"name":"wait","_id":"ck21ro59l000zapwn26w5suju"},{"name":"k8s","_id":"ck21ro59o0013apwnhk3h4fyu"},{"name":"audit","_id":"ck21ro59q0016apwn15xh3iux"},{"name":"log","_id":"ck21ro59s001bapwndbkbalfg"},{"name":"Authentication","_id":"ck21ro59t001fapwn4wmrp9ip"},{"name":"Authorization","_id":"ck21ro59v001lapwnwa2cirhi"},{"name":"RBAC","_id":"ck21ro59w001napwnpx2b464x"},{"name":"admission control","_id":"ck21ro59w001oapwn833wu8ev"},{"name":"kube-dashboard","_id":"ck21ro59w001rapwn8edg46hs"},{"name":"prometheus","_id":"ck21ro59x001uapwnbbz22xz0"},{"name":"kubernetes","_id":"ck21ro59x001wapwn903tgk7g"},{"name":"events","_id":"ck21ro5a00020apwn7tqa2a64"},{"name":"kubelet","_id":"ck21ro5a00023apwn1a22aih4"},{"name":"leader-election","_id":"ck21ro5a10024apwnr9gppkpm"},{"name":"component","_id":"ck21ro5a10027apwnk760s8q9"},{"name":"metrics-server","_id":"ck21ro5a10028apwndwtzchlg"},{"name":"kubernetes v1.12","_id":"ck21ro5a2002bapwnim0bgylz"},{"name":"HA","_id":"ck21ro5a3002fapwnt8iuq11y"},{"name":"operator","_id":"ck21ro5a3002gapwnw4uf2gma"},{"name":"kube-on-kube","_id":"ck21ro5a4002japwndodjyv15"},{"name":"kind","_id":"ck21ro5a4002kapwnkynxp99f"},{"name":"deploy","_id":"ck21ro5a4002napwn3mkmdjok"},{"name":"release version","_id":"ck21ro5a6002sapwno6p1ox36"},{"name":"kube-scheduler","_id":"ck21ro5a7002yapwnsk2z47ve"},{"name":"kubeconfig","_id":"ck21ro5a80031apwnacygnkig"},{"name":"kubectl plugin","_id":"ck21ro5a80033apwnmh66d62p"},{"name":"kubeadm","_id":"ck21ro5a80035apwnro9kq34y"},{"name":"node status","_id":"ck21ro5ab003dapwn581c3e1v"}]}}